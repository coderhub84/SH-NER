{"id": 151316, "text": "With the introduction of annotation tools such as brat [ Stenetorp et al. , , 2012 ] , WebAnno [ Yimam et al. , 2013 ] , or INCEpTION [ Klie et al. , , 2018 ] , the use of label suggestions became more feasible ; leading to an increased investigation of label suggestions in the context of NLP .", "Comments": [], "label": [[50, 54, "Software-Entity"], [87, 94, "Software-Entity"], [124, 133, "Software-Entity"]]}
{"id": 151318, "text": "We conduct our annotation experiments using INCEpTION [ Klie et al. , , 2018 ] which allows us to integrate label suggestions using recommendation models .", "Comments": [], "label": [[44, 53, "Software-Entity"]]}
{"id": 151319, "text": "To obtain label suggestions , we use a German version of BERT ( Ger-BERT ) that is available through the HuggingFace library [ Wolf et al. , 2020 ] . [ 5 ] We perform a random hyperparameter search ( cf .", "Comments": [], "label": [[105, 116, "Software-Entity"]]}
{"id": 151320, "text": "All experiments were conducted on a desktop machine with a 6-core 3.8 GHz CPU and a GeForce RTX 2060 GPU ( 8GB ) .", "Comments": [], "label": [[59, 65, "Device-Count"], [66, 77, "Hardware-device"], [84, 104, "Hardware-device"], [107, 110, "Device-Memory"]]}
{"id": 151325, "text": "The label NoMeasure corresponds to Unrelated , label NoOpinion corresponds to Comment , ProOpinion corresponds to Support and ConOpinion corresponds to Refute Figure 8 : A screenshot of the annotation interface using INCEpTION [ Klie et al. , , 2018 ] and the user navigates through all texts using the navigation bar above the text window .", "Comments": [], "label": [[217, 226, "Software-Entity"]]}
{"id": 151339, "text": "Human Evaluation In addition to automatic evaluation we elicited judgements from crowdworkers on Amazon Mechanical Turk .", "Comments": [], "label": [[97, 119, "Cloud-Platform"]]}
{"id": 151340, "text": "D Reproducibility Notes All experiments were run on a single Nvidia RTX 2080 Ti GPU .", "Comments": [], "label": [[54, 60, "Device-Count"], [61, 83, "Hardware-device"]]}
{"id": 151341, "text": "F Ordering of the Encoding Space Figure [ 5 ] shows that the semantic encodings zsem are tightly clustered by paraphrase , but the set of ( a ) Semantic encodings ( b ) Syntactic encodings Figure 5 : Visualisations of zsem and zsyn using t-SNE [ van der Maaten and Hinton , 2008 ] , coloured by paraphrase cluster .", "Comments": [], "label": [[238, 243, "Software-Entity"]]}
{"id": 151349, "text": "We have humans evaluate fluency , consistency , and novelty on Amazon Mechanical Turk .", "Comments": [], "label": [[63, 85, "Cloud-Platform"]]}
{"id": 151351, "text": "We also compare against a machine-translation approach ( see Sec [ 6 ] , pivoting through German using Transformer [ Vaswani et al. , , 2017 ] models trained on WMT19 data [ Barrault et al. , , 2019 ] .", "Comments": [], "label": [[103, 114, "Software-Entity"]]}
{"id": 151352, "text": "Metrics : For human evaluation , over 200 examples we ask 3 raters on Amazon Mechanical Turk about coherence between h and o1 , o2 , o +o2 , and overall quality on 4-value likert scales .", "Comments": [], "label": [[70, 92, "Cloud-Platform"]]}
{"id": 151353, "text": "Baselines : Parameters for REFLECTIVE DECOD-ING are given in [ §B.4 . ] We include baselines from the original work : different supervised variants of GPT-2 large with access to the observations , and optionally commonsense embeddings or generations from COMET [ Bosselut et al. , , 2019 ] .", "Comments": [], "label": [[255, 260, "Software-Entity"]]}
{"id": 151358, "text": "Ethical Considerations In order to complete our human evaluation we used Amazon Mechanical Turk .", "Comments": [], "label": [[73, 95, "Cloud-Platform"]]}
{"id": 151362, "text": "We use transformer language models ( Mega size ) trained on TPU pods ( TensorFlow ) of size 512 .", "Comments": [], "label": [[60, 68, "Hardware-device"], [71, 81, "Software-Entity"]]}
{"id": 151363, "text": "For generation we used 2 NVIDIA Titan Xp GPUs .", "Comments": [], "label": [[23, 24, "Device-Count"], [25, 45, "Hardware-device"]]}
{"id": 151392, "text": "5.5 Analysis in Efficiency and Model Size We use the same FLAT method to evaluate the parallel and non-parallel inference speed of MECT on an NVIDIA GeForce RTX 2080Ti card , using batch size = 16 and batch size = 1 .", "Comments": [], "label": [[142, 167, "Hardware-device"]]}
{"id": 151406, "text": "Each training and testing process is run on a single RTX 2080 Ti GPU .", "Comments": [], "label": [[46, 52, "Device-Count"], [53, 68, "Hardware-device"]]}
{"id": 151410, "text": "For rule detection system , we used the grammarbot API , [ 2 ] , and Grammarly [ 3 ] to help us create a set of rules .", "Comments": [], "label": [[40, 50, "Software-Entity"], [69, 78, "Software-Entity"]]}
{"id": 151412, "text": "4.2 Fine-tuning models We employed the transformers library [ Wolf et al. , 2019 ] to independently fine-tune the BERTbased encoder and decoder model for each method in 20,000 steps ( intrinsic evaluation ) , and fine-tune the BERT-based and RoBERTa-based classification models for each tweet sentiment analysis task in 10,000 steps ( extrinsic evaluation ) .", "Comments": [], "label": [[39, 51, "Software-Entity"]]}
{"id": 151415, "text": "The whole experiment is carried out on 1 TITANX GPU .", "Comments": [], "label": [[39, 40, "Device-Count"], [41, 51, "Hardware-device"]]}
{"id": 151422, "text": "The computing infrastructure used is Linux 4.4.0- 138-generic x86_64 with the NVIDIA GPU GTX-1080 .", "Comments": [], "label": [[78, 97, "Hardware-device"]]}
{"id": 151424, "text": "We include symbols for the set of choices yielding best performance : removing punctuation ♠ , removing stopwords using NLTK [ Bird , 2006 ] ⊕ , and stemming via NLTK 's SnowballStemmer † .", "Comments": [], "label": [[120, 124, "Software-Entity"], [162, 166, "Software-Entity"]]}
{"id": 151429, "text": "Training took about 13hrs on a single TitanX GPU . 5.2 Results & Analysis Table [ 3 ] shows that the PJSD model ( ρ = .540 ) significantly outperforms % -IN-T .", "Comments": [], "label": [[31, 37, "Device-Count"], [38, 48, "Hardware-device"]]}
{"id": 151437, "text": "We trained BERT-base [ Devlin et al. , , 2019 ] and RoBERTa-base [ Liu et al. , , 2019 ] on this data for 10 epochs with early stopping , and a batch size of 8 × 2 gradient accumulation steps — all other parameters are defaults set by Huggingface [ 14 ] .", "Comments": [], "label": [[235, 246, "Software-Entity"]]}
{"id": 151443, "text": "The training of the proposed model was done on an Nvidia Telsa V100 32G GPU .", "Comments": [], "label": [[50, 67, "Hardware-device"], [68, 71, "Device-Memory"]]}
{"id": 151471, "text": "A.6 Infrastructure and Reproducibility We run all experiments on a single 32GB NVIDIA Tesla V100 GPU .", "Comments": [], "label": [[67, 73, "Device-Count"], [74, 78, "Device-Memory"], [79, 100, "Hardware-device"]]}
{"id": 151501, "text": "We run all model training , adversarial attacks and evaluation on a shared HPC cluster with Nvidia RTX 2080ti , Tesla K40 and V100 GPUs .", "Comments": [], "label": [[92, 109, "Hardware-device"], [112, 121, "Hardware-device"], [126, 135, "Hardware-device"]]}
{"id": 151504, "text": "For the composition attacks with soft-truth score , we use the KMeans clustering implementation from scikit − learn .", "Comments": [], "label": [[101, 115, "Software-Entity"]]}
{"id": 151522, "text": "We show the class distribution from the final logits ( on the target language ) using t-SNE plots .", "Comments": [], "label": [[86, 91, "Software-Entity"]]}
{"id": 151523, "text": "Figures [ 4 ] a ) and [ 4 ] b ) present the loss distribution in a scatter plot by sorting the sentences based Figure 3 : ( a-b ) Effect of training with confidence penalty in the warm-up step on target ( * Spanish * ) language XNER classification using t-SNE plots .", "Comments": [], "label": [[254, 259, "Software-Entity"]]}
{"id": 151527, "text": "We used V100 GPUs to do the experiments .", "Comments": [], "label": [[8, 17, "Hardware-device"]]}
{"id": 151528, "text": "We perform each of the experiments in a single GPU setup with * float32 * precision .", "Comments": [], "label": [[40, 50, "Device-Count"]]}
{"id": 151546, "text": "All models are trained on 4 Tesla V100 GPUs with 32GB memory for 10 to 12 hours .", "Comments": [], "label": [[26, 27, "Device-Count"], [28, 43, "Hardware-device"], [49, 60, "Device-Memory"]]}
{"id": 151564, "text": "To this end , we monitored median HIT completion times for each published batch , adjusting the monetary reward such that at least 80 % of workers always received > $ 15/hour , which is roughly double the minimum wage in the United States ( the country of residence for most Amazon Mechanical Turk workers ) .", "Comments": [], "label": [[275, 297, "Cloud-Platform"]]}
{"id": 151565, "text": "Language Variety : The dataset is available in English , with mainstream US Englishes being the dominant variety , as per the demographic of Amazon Mechanical Turk workers .", "Comments": [], "label": [[141, 163, "Cloud-Platform"]]}
{"id": 151566, "text": "We use two Titan X GPUs to train and evaluate all models , except Dynamic Relational Attention [ Tan et al. , , 2019 ] , which was trained on a single Titan Xp GPU .", "Comments": [], "label": [[7, 10, "Device-Count"], [11, 23, "Hardware-device"], [144, 150, "Device-Count"], [151, 163, "Hardware-device"]]}
{"id": 151576, "text": "We randomly select 150 original claims for an Amazon Mechanical Turk task .", "Comments": [], "label": [[46, 68, "Cloud-Platform"]]}
{"id": 151578, "text": "The fine-tuned RoBERTa is available on Huggingface [ 3 ] .", "Comments": [], "label": [[39, 50, "Software-Entity"]]}
{"id": 151580, "text": "We presented 150 randomly selected claims to Amazon Mechanical Turk workers .", "Comments": [], "label": [[45, 67, "Cloud-Platform"]]}
{"id": 151583, "text": "Amazon Mechanical Turk workers were given a claim and the 5 automatically selected candidate evidence sentences .", "Comments": [], "label": [[0, 22, "Cloud-Platform"]]}
{"id": 151586, "text": "We asses the quality of the majority vote annotations by comparing the gold evidence label annotations with an independent re-annotation by three Amazon Mechanical Turk workers .", "Comments": [], "label": [[146, 168, "Cloud-Platform"]]}
{"id": 151589, "text": "A Model implementation details We used fairseq library [ Ott et al. , , 2019a ] for RoBERTa model training .", "Comments": [], "label": [[39, 46, "Software-Entity"]]}
{"id": 151593, "text": "POS tags obtained using the flair python library tagger .", "Comments": [], "label": [[28, 33, "Software-Entity"]]}
{"id": 151618, "text": "All experiments are run on 16 NVIDIA V100 GPUs .", "Comments": [], "label": [[27, 29, "Device-Count"], [30, 46, "Hardware-device"]]}
{"id": 151619, "text": "Training Time Measuring Protocol We strictly measure the training time saving of EarlyBERT on the QQP task in GLUE using CUDA benchmark mode .", "Comments": [], "label": [[121, 125, "Software-Entity"]]}
{"id": 151635, "text": "A.2 Experimental Details Implementation We use language model implementations from * HuggingFace Transfromers * library [ Wolf et al. , , 2019 ] .", "Comments": [], "label": [[85, 96, "Software-Entity"]]}
{"id": 151636, "text": "Each experiment was performed on a single v100 GPU .", "Comments": [], "label": [[35, 41, "Device-Count"], [42, 50, "Hardware-device"]]}
{"id": 151651, "text": "All models are implemented with Pytorch deep learning toolkit and run on a single Nvidia GTX 1080Ti graphic card .", "Comments": [], "label": [[32, 39, "Software-Entity"], [75, 81, "Device-Count"], [82, 99, "Hardware-device"]]}
{"id": 151655, "text": "Imagination ability : Figure [ 3 ] visualizes the distribution of the ground-truth multimodal embeddings ( hˆ in Figure [ 2 ] and MMIN imagined multimodal embeddings ( h 0 in Figure [ 2 ] for a male speaker and female speaker using t-SNE [ Maaten and Hin ] [ ton , 2008 ] .", "Comments": [], "label": [[232, 237, "Software-Entity"]]}
{"id": 151670, "text": "Table 3 : Zero-Shot : We report de-tokenized BLEU using sacreBLEU in OPUS-100 .", "Comments": [], "label": [[56, 65, "Software-Entity"]]}
{"id": 151678, "text": "We use 8 × 4 NVIDIA V100 with update frequency 50 to train the models and each batch contains about 3 million tokens . b ) We enlarge the number of layers from 6 to 12 and observe significant improvements for multilingual NMT .", "Comments": [], "label": [[7, 12, "Device-Count"], [13, 24, "Hardware-device"]]}
{"id": 151692, "text": "To facilitate visualization , we apply T-SNE dimension reduction to reduce the 1024-dim representations to 2-dim .", "Comments": [], "label": [[39, 44, "Software-Entity"]]}
{"id": 151694, "text": "Some work also attempted to explicitly learn shared semantic representation of different languages to im- Figure 4 : Bivariate kernel density estimation plots of representations after using T-SNE dimensionality reduction to 2 dimension .", "Comments": [], "label": [[190, 195, "Software-Entity"]]}
{"id": 151700, "text": "We carried out all the annotation tasks on Amazon Mechanical Turk ( AMT ) .", "Comments": [], "label": [[43, 65, "Cloud-Platform"]]}
{"id": 151708, "text": "A sample questionnaire for the final annotation task is shown in Figure [ 6 . ] The hourly compensation rate for annotators on Amazon Mechanical Turk was US $ 7.50/hr .", "Comments": [], "label": [[127, 149, "Cloud-Platform"]]}
{"id": 151710, "text": "Training Times We trained all our models on the Tesla T4 GPU .", "Comments": [], "label": [[48, 60, "Hardware-device"]]}
{"id": 151711, "text": "The number of GPU ( s ) used is 1 .", "Comments": [], "label": [[32, 33, "Device-Count"]]}
{"id": 151726, "text": "We modify the padding size on large datasets , so that our the experiments can be conducted on a single GPU .", "Comments": [], "label": [[97, 107, "Device-Count"]]}
{"id": 151727, "text": "Chinese datasets are embedded with Tencent Embedding [ Song et al. , , 2018 ] and English datasets use fastText embeddings [ Bo ] [ janowski et al. , , 2017 ] .", "Comments": [], "label": [[103, 111, "Software-Entity"]]}
{"id": 151729, "text": "KATID3 is implemented using scikit-learn 0.24 .", "Comments": [], "label": [[28, 45, "Software-Entity"]]}
{"id": 151730, "text": "HIF is implemented with PyTorch 1.4.0 in Python 3.7.6 .", "Comments": [], "label": [[24, 31, "Software-Entity"]]}
{"id": 151731, "text": "We also use Numpy 1.19.2 for matrix calculation .", "Comments": [], "label": [[12, 17, "Software-Entity"]]}
{"id": 151732, "text": "All the experiments are evaluated on a single NVIDIA 3090 GPU with 24GiB GRAM .", "Comments": [], "label": [[39, 45, "Device-Count"], [46, 61, "Hardware-device"], [67, 77, "Device-Memory"]]}
{"id": 151741, "text": "5.1 Text-Only Model Open-Source NLP Toolkit : Many open-source NLP toolkits , such as spaCy [ Honnibal et al. , , 2020 ] and Stanza [ Qi et al. , , 2020 ] , support Chinese NER .", "Comments": [], "label": [[86, 91, "Software-Entity"], [125, 131, "Software-Entity"]]}
{"id": 151742, "text": "In spaCy , a multitask CNN is employed .", "Comments": [], "label": [[3, 8, "Software-Entity"]]}
{"id": 151743, "text": "In Stanza , a contextualized string representation based tagger from [ Akbik et al. , 2018 ] is adopted .", "Comments": [], "label": [[3, 9, "Software-Entity"]]}
{"id": 151744, "text": "In both spaCy and Stanza , the tagger is trained on OntoNote [ Weischedel et al. , , 2011 ] .", "Comments": [], "label": [[8, 13, "Software-Entity"], [18, 24, "Software-Entity"]]}
{"id": 151749, "text": "Computing Infrastructure : All experiments are conducted on an NVIDIA GeForce RTX 2080 Ti ( 11 GB of memory ) . 7.2 Main Results Table [ 3 ] shows the results of baselines and our proposed model on CNERTA .", "Comments": [], "label": [[63, 89, "Hardware-device"], [92, 107, "Device-Memory"]]}
{"id": 151753, "text": "We adopted the open-source toolkit Fairseq [ Ott et al. , , 2019 ] to implement the model .", "Comments": [], "label": [[35, 42, "Software-Entity"]]}
{"id": 151754, "text": "We used the TRANSFORMER-BASE model for preliminary experiments ( [ §2.3 ] and the constrained scenario ( [ §3.2 ] for efficiency .", "Comments": [], "label": [[12, 23, "Software-Entity"]]}
{"id": 151758, "text": "We used 16 Nvidia V100 GPUs to conduct the experiments and selected the final model by the best perplexity on the validation set .", "Comments": [], "label": [[8, 10, "Device-Count"], [11, 27, "Hardware-device"]]}
{"id": 151759, "text": "For the En⇒Zh task , we added the option -- tok zh to SacreBLEU .", "Comments": [], "label": [[54, 63, "Software-Entity"]]}
{"id": 151767, "text": "The results are reported with de-tokenized case-sensitive SacreBLEU .", "Comments": [], "label": [[58, 67, "Software-Entity"]]}
{"id": 151796, "text": "We build our PyTorch implementation on top of HuggingFace 's Transformers library [ Wolf et al. , 2020 ] .", "Comments": [], "label": [[13, 20, "Software-Entity"], [46, 60, "Software-Entity"]]}
{"id": 151797, "text": "We tokenize sentences by SentencePiece [ 9 ] [ Kudo , 2018 ] and build a shared vocabulary with the size of 50k for each language pair .", "Comments": [], "label": [[25, 38, "Software-Entity"]]}
{"id": 151799, "text": "We train 12 epochs for each language pair ( 30 epochs for English-Italian because of nearly half number of parallel sentences ) with the Adam [ sentencepiece ] ( https : //github.com/google/sentencepiece ) optimizer , learning rate of 0.001 with warm-up strategy for 3 epochs ( 6 epochs for English-Italian ) and dropout-probability of 0.1 on a single TITAN X Pascal GPU with the batch size of 128 paired sentences .", "Comments": [], "label": [[345, 351, "Device-Count"], [352, 370, "Hardware-device"]]}
{"id": 151800, "text": "Training loss for each language pair can converge within 10 GPU ( 12GB ) ×days , which is far more efficient than most cross-lingual sentence representation learning methods .", "Comments": [], "label": [[57, 63, "Device-Count"]]}
{"id": 151802, "text": "We conduct our evaluations in a zeroshot scenario : we train and validate a new linear < https : //www.statmt.org/europarl/ > < http : //opus.nlpl.eu/ParaCrawl-v5.php > [ https : //github.com/google/ ] ( https : //github.com/google/sentencepiece ) Note that it is impractical to compare the efficiency with LASER , which is trained by 80 V100 GPU×days due to different training data settings .", "Comments": [], "label": [[335, 337, "Device-Count"], [338, 346, "Hardware-device"]]}
{"id": 151804, "text": "We implement the evaluation by facebook 's MLDoc library . [ 11 ] As shown in Table [ 3 , ] our lightweight transformer model obtains the best results for most language pairs compared with previous fixed-dimensional word and sentence representation learning methods .", "Comments": [], "label": [[43, 56, "Software-Entity"]]}
{"id": 151806, "text": "The results are reported by using a single V100 GPU card with the batch size of 128 sentences . 2-layer is the default setting for our lightweight model .", "Comments": [], "label": [[36, 42, "Device-Count"], [43, 51, "Hardware-device"]]}
{"id": 151829, "text": "Following [ Lau et al. , 2017 ] , we tokenize words and sentences using Stanford CoreNLP [ Klein and Manning , 2003 ] , lowercase all word tokens , and filter out word tokens that occur less than 10 times .", "Comments": [], "label": [[81, 88, "Software-Entity"]]}
{"id": 151842, "text": "We splits sentences using the Stanford CoreNLP toolkit [ 7 ] and pre-process the dataset following [ Liu and Lapata , 2019 ] . .", "Comments": [], "label": [[30, 54, "Software-Entity"]]}
{"id": 151852, "text": "We fine-tune all models in four Nvidia GeForce RTX2080 TI GPUs .", "Comments": [], "label": [[27, 31, "Device-Count"], [32, 62, "Hardware-device"]]}
{"id": 151859, "text": "We ensure fair comparison by setting the hyper-parameters related to the PLM backbones the same with HuggingFace Transformers [ Wolf et al. , , 2020 ] . 4.3 Baseline methods We compare with the previous BERT early exiting methods and compare other methods that speed up BERT inference .", "Comments": [], "label": [[101, 112, "Software-Entity"]]}
{"id": 151860, "text": "Best performances are bolded , `` * '' indicates the performance gains are statistically significant . include the results of FastBERT when it adopts the PABEE 's exiting strategy . 4.4 Experimental settings We implement LeeBERT on the base of Hugging-Face 's Transformers .", "Comments": [], "label": [[244, 259, "Software-Entity"]]}
{"id": 151861, "text": "We conduct our experiments on a single Nvidia V100 16GB GPU .", "Comments": [], "label": [[32, 38, "Device-Count"], [39, 59, "Hardware-device"]]}
{"id": 151864, "text": "A sentence segmentation component called Sentencizer ( by spaCy ) is utilized to split this document into individual sentences [ Gupta and Nishu , 2020 ] .", "Comments": [], "label": [[58, 63, "Software-Entity"]]}
{"id": 151866, "text": "We also utilized the pre-trained BERTLARGE model afforded by the Transformers library of HuggingFace [ 3 ] .", "Comments": [], "label": [[89, 100, "Software-Entity"]]}
{"id": 151869, "text": "We ran our experiments on NVIDIA GeForce RTX 2080 Ti .", "Comments": [], "label": [[26, 52, "Hardware-device"]]}
{"id": 151874, "text": "Average accuracy and F1 score are employed to measure the performance . < https : //query.wikidata.org > ConvQuestions : This is a large-scale conversational KBQA dataset [ 4 ] created via Amazon Mechanical Turk [ Christmann et al. , , 2019 ] .", "Comments": [], "label": [[189, 211, "Cloud-Platform"]]}
{"id": 151875, "text": "Specifically , we first recognize the named entities in the questions via the AllenNLP NER tool [ 6 ] and retrieve the corresponding entities via SPARQL .", "Comments": [], "label": [[78, 86, "Software-Entity"]]}
{"id": 151877, "text": "Implementation Details : We implement our method by PyTorch on Nvidia V440.64.00-32GB GPU cards .", "Comments": [], "label": [[52, 59, "Software-Entity"], [63, 80, "Hardware-device"], [81, 89, "Device-Memory"]]}
{"id": 151885, "text": "First , we use spaCy [ 7 ] to perform * Named Entity Recognition * , and then link these entity mentions as well as Wikipedia 's mentions with hyper-links to Wikidata items , thus we obtain the Wikidata ID for each entity .", "Comments": [], "label": [[15, 20, "Software-Entity"]]}
{"id": 151886, "text": "4.2 Pre-training Details We initialize ERICABERT and ERICARoBERTa with * bert-base-uncased * and * roberta-base * checkpoints released by Google [ 8 ] and Huggingface [ 9 ] .", "Comments": [], "label": [[155, 166, "Software-Entity"]]}
{"id": 151888, "text": "We train both models with 8 NVIDIA Tesla P40 GPUs for 2 , 500 steps . 4.3 Relation Extraction Relation extraction aims to extract the relation between two recognized entities from a pre-defined relation set .", "Comments": [], "label": [[26, 27, "Device-Count"], [28, 49, "Hardware-device"]]}
{"id": 151892, "text": "Entity Distribution Shifting The entities in supervised datasets of DocRED are recognized by human annotators while our pre-training data is processed by spaCy .", "Comments": [], "label": [[154, 159, "Software-Entity"]]}
{"id": 151893, "text": "5.5 Embedding Visualization In Figure [ 6 , ] we show the learned entity and relation embeddings of BERT and ERICABERT on DocRED 's dev set by t-distributed stochastic neighbor embedding ( t-SNE ) [ Hinton and Roweis , 2002 ] .", "Comments": [], "label": [[143, 186, "Software-Entity"], [189, 194, "Software-Entity"]]}
{"id": 151894, "text": "BERT : relation ERICA-BERT : relation Figure 6 : t-SNE plots of learned entity and relation embeddings on DocRED comparing BERT and ERICABERT . 6 Conclusions In this paper , we present ERICA , a general framework for PLMs to improve entity and relation understanding via contrastive learning .", "Comments": [], "label": [[49, 54, "Software-Entity"]]}
{"id": 151895, "text": "We implement all models based on Huggingface transformers [ 14 ] .", "Comments": [], "label": [[33, 44, "Software-Entity"]]}
{"id": 151918, "text": "For each split , we run 50 iterations to get the best model on the validation set , which takes an average time of around 23 minutes per split , when conducted on a NVIDIA GTX 1080Ti .", "Comments": [], "label": [[165, 182, "Hardware-device"]]}
{"id": 151920, "text": "Training took two days on a single v100 GPU .", "Comments": [], "label": [[28, 34, "Device-Count"], [35, 43, "Hardware-device"]]}
{"id": 151922, "text": "Each run took 3.5-4.5 hours using 10 v100 GPUs . 7 Improving Key Point Quality Previous work did not attempt to explicitly define the desired properties KPs should have , or to develop a model that identifies good KP candidates .", "Comments": [], "label": [[34, 36, "Device-Count"], [37, 46, "Hardware-device"]]}
{"id": 151923, "text": "We sampled from the restaurant and hotel reviews in the train set 2,000 sentences comprising 3-8 tokens and minimal argument quality of tquality . each sentence was annotated for each of the above criteria [ 7 ] by 10 crowd annotators , using the Appen platform [ 8 ] .", "Comments": [], "label": [[247, 252, "Software-Entity"]]}
{"id": 151928, "text": "Similar to the KP quality dataset , the eight samples of 400 pairs ( two domains × four configurations ) were annotated in the Appen crowdsourcing platform .", "Comments": [], "label": [[127, 132, "Software-Entity"]]}
{"id": 151940, "text": "The pre-training of XLM-ALIGN takes about six days with two Nvidia DGX-2 stations .", "Comments": [], "label": [[56, 59, "Device-Count"], [60, 72, "Hardware-device"]]}
{"id": 151961, "text": "We tokenize and truecase the sentences with MOSES [ Koehn et al. , , 2007 ] tools , applying BPE [ Sennrich et al. , , 2016 ] with 30000 merging operations .", "Comments": [], "label": [[44, 49, "Software-Entity"], [93, 96, "Software-Entity"]]}
{"id": 152000, "text": "We further build much stronger Transformer baselines by fine-tuning on mBART25 [ Liu et al. , , 2020 ] .", "Comments": [], "label": [[31, 42, "Software-Entity"]]}
{"id": 152043, "text": "B.2 Training Settings We build our experiments based on Transformer implemented by Fairseq [ Ott et al. , , 2019 ] .", "Comments": [], "label": [[83, 90, "Software-Entity"]]}
{"id": 152046, "text": "We train base and big models on 4 GPUs of Navidia 2080ti , and large model on 4 GPUs of v100 .", "Comments": [], "label": [[32, 33, "Device-Count"], [34, 56, "Hardware-device"], [78, 79, "Device-Count"], [80, 92, "Hardware-device"]]}
{"id": 152065, "text": "For Zh→En , case-sensitive BLEU scores are calculated by Moses * mteval-v13a.pl * script [ 6 ] .", "Comments": [], "label": [[57, 62, "Software-Entity"]]}
{"id": 152067, "text": "We conduct experiments based on the Transformer [ Vaswani et al. , , 2017 ] and implement our approaches with the opensource tooklit * Opennmt-py * [ Klein et al. , , 2017 ] .", "Comments": [], "label": [[36, 47, "Software-Entity"], [135, 145, "Software-Entity"]]}
{"id": 152069, "text": "All three tasks are trained with 8 NVIDIA V100 GPUs , and the batch size for each GPU is 4096 tokens .", "Comments": [], "label": [[33, 34, "Device-Count"], [35, 51, "Hardware-device"]]}
{"id": 152076, "text": "( MRT * in [ Shen et al. , , 2016 ] is RNN-based , and the result reported here is implemented on Transformer by [ Weng et al. , 2020b ] .", "Comments": [], "label": [[98, 109, "Software-Entity"]]}
{"id": 152077, "text": "* * : we re-implement Simple Fusion on upon of Transformer . )", "Comments": [], "label": [[47, 58, "Software-Entity"]]}
{"id": 152078, "text": "implement the Transformer model [ Vaswani et al. , , 2017 ] as our baseline .", "Comments": [], "label": [[14, 25, "Software-Entity"]]}
{"id": 152093, "text": "The training stage of our model lasts about 28 minutes on single Tesla T4 GPU ( 16 GB of memory ) . 5.2 Main Results Table [ 5 ] and [ 6 ] show the experiment results with seven different models on two benchmark slot filling datasets Snips-NSD and ATIS-NSD constructed by Remove strategy .", "Comments": [], "label": [[58, 64, "Device-Count"], [65, 77, "Hardware-device"], [80, 95, "Device-Memory"]]}
{"id": 152154, "text": "We implement GIT under PyTorch [ Paszke et al. , 2017 ] and DGL [ Wang et al. , , 2019 ] based on codes provided by [ Zheng et al. , 2019 ] .", "Comments": [], "label": [[23, 30, "Software-Entity"], [60, 63, "Software-Entity"]]}
{"id": 152155, "text": "All the experiments ( including the baselines ) are run with the same 8 Tesla-V100 GPUs and the same version of python dependencies to ensure the fairness .", "Comments": [], "label": [[70, 71, "Device-Count"], [72, 87, "Hardware-device"]]}
{"id": 152169, "text": "BERT [ Devlin et al. , , 2019 ] and Flair [ Akbik et al. , , 2018 ] are the most commonly used contextual word representations in previous work , and have also been proved that they can substantially improve the model performance .", "Comments": [], "label": [[36, 41, "Software-Entity"]]}
{"id": 152170, "text": "Labels [ B ] and [ F ] stand for BERT and Flair contextual word representations respectively .", "Comments": [], "label": [[42, 47, "Software-Entity"]]}
{"id": 152171, "text": "Our model [ 3 ] is implemented with PyTorch [ Paszke et al. , , 2019 ] and we run experiments on GeForce GTX 1080Ti with 11 GB memory . 3.4 Experimental Results Table [ 2 ] shows the performance of previous work and our model on the ACE2004 , ACE2005 , and GENIA datasets .", "Comments": [], "label": [[36, 43, "Software-Entity"], [97, 115, "Hardware-device"], [121, 133, "Device-Memory"]]}
{"id": 152172, "text": "In the case of utilizing BERT and further employing Flair , our method consistently outperforms [ Shibuya and Hovy , 2020 ] by 1.09 and 0.60 by F scores , respectively .", "Comments": [], "label": [[52, 57, "Software-Entity"]]}
{"id": 152181, "text": "We can use an efficient similarity search library Faiss ( Johnson et al. , 2017 ) for fast NN retrieval in large embedding space .", "Comments": [], "label": [[50, 55, "Software-Entity"]]}
{"id": 152183, "text": "We conduct the experiment using a workstation with an Intel Xeon E5- 1620 3.50GHz CPU and a NVIDIA GeForce RTX 2080 Ti GPU .", "Comments": [], "label": [[54, 85, "Hardware-device"], [92, 122, "Hardware-device"]]}
{"id": 152194, "text": "Experiments are performed on a GeForce GTX 1080 Ti GPU ( 11GB ) .", "Comments": [], "label": [[31, 54, "Hardware-device"], [57, 61, "Device-Memory"]]}
{"id": 152204, "text": "Each pretrained model was fetched from the Huggingface transformers library [ Wolf et al. , , 2019 ] , from which we use bert-large-cased , roberta-large , and gpt2-xl respectively .", "Comments": [], "label": [[43, 54, "Software-Entity"]]}
{"id": 152205, "text": "As baseline methods , we also consider three pre-trained word embedding models , which have been shown to provide competitive results in analogy tasks , as explained in Section [ 2.2 : ] Word2vec [ Mikolov et al. , , 2013a ] , GloVe [ Pennington et al. , , 2014 ] , and FastText [ Bojanowski et al. , , 2017 ] .", "Comments": [], "label": [[270, 278, "Software-Entity"]]}
{"id": 152207, "text": "We also observe that word embeddings perform surprisingly well , with FastText and GloVe outperforming BERT on most datasets , as well as GPT-2 and RoBERTa with default hyperparameters .", "Comments": [], "label": [[70, 78, "Software-Entity"]]}
{"id": 152208, "text": "FastText achieves the best overall accuracy on the Google dataset , confirming that this dataset is particularly well-suited to word embeddings ( see Section [ 2.2 ] .", "Comments": [], "label": [[0, 8, "Software-Entity"]]}
{"id": 152219, "text": "In preliminary experiments we also used * word2vec * and GloVe models , obtaining slightly lower results than * fastText * .", "Comments": [], "label": [[112, 120, "Software-Entity"]]}
{"id": 152220, "text": "For reasons of clarity , we include only * fastText * embeddings and the best contextualized model ( BERT ) .", "Comments": [], "label": [[43, 51, "Software-Entity"]]}
{"id": 152221, "text": "In Exp1 and Exp2 , where the context plays a crucial role , * fastText * models correctly labeled between 50 % /60 % of the examples ( depending on the language and vector type , with better results for * Sent * and * Syn * ) .", "Comments": [], "label": [[62, 70, "Software-Entity"]]}
{"id": 152223, "text": "Finally , * fastText * obtains better results than BERT on Exp4 ( where the three instances have the same context ) , reaching 0.81 in Spanish with an average across languages of 0.64 ( always with * WVs * ) .", "Comments": [], "label": [[12, 20, "Software-Entity"]]}
{"id": 152231, "text": "Acknowledgments We would like to thank the anonymous reviewers for their valuable comments , and NVIDIA Corporation for the donation of a Titan Xp GPU .", "Comments": [], "label": [[138, 150, "Hardware-device"]]}
{"id": 152233, "text": "Figure 3 : Results across layers and models for English ( top ) , Portuguese ( middle ) , and Spanish ( bottom ) . * Sent * and * WV * ( dashed ) are macro-average values . * MacroAvg|Syn * is the macro-average per layer ( Transformers ) and the macro-average of the * Syn * strategy ( * fastText * ) .", "Comments": [], "label": [[288, 296, "Software-Entity"]]}
{"id": 152234, "text": "DmBERT is the multilingual version of DistilBERT , and * fastT * the * fastText * embeddings . * Ma * and * Mi * refer to the macro-average and micro-average results across the four experiments , respectively . * F * are the micro-average values on the whole dataset .", "Comments": [], "label": [[71, 79, "Software-Entity"]]}
{"id": 152235, "text": "The corpus was divided into 90 % /10 % splits for train and development . * fastText * model : We trained a * fastText * skip-gram model for 15 iterations with 300 dimensions , window size of 5 , negative sampling of 25 , and a minimum word frequency of 5 .", "Comments": [], "label": [[110, 118, "Software-Entity"]]}
{"id": 152238, "text": "Both models were trained with the * Transformers * library [ Wolf et al. , , 2020 ] on a single NVIDIA Titan XP GPU ( 12GB ) , a block size of 128 , a learning rate of 0.0001 , a masked language modeling ( MLM ) probability of 0.15 , and a weight decay of 0.01 .", "Comments": [], "label": [[36, 48, "Software-Entity"], [89, 95, "Device-Count"], [96, 115, "Hardware-device"], [118, 122, "Device-Memory"]]}
{"id": 152259, "text": "We use NLTK Sentence Tokenizer for text-based system , and a segmentation model [ Chen et al. , , 2018 ] for ASR ( Automatic Speech Recognition ) -based system as the segmentation tool .", "Comments": [], "label": [[7, 11, "Software-Entity"]]}
{"id": 152280, "text": "We run experiments on a server of eight GTX 1080 GPUs .", "Comments": [], "label": [[34, 39, "Device-Count"], [40, 53, "Hardware-device"]]}
{"id": 152297, "text": "CNN/DM We used the standard split [ Her ] [ mann et al. , , 2015 ] for training , validation , and test ( 90,266/1,220/1,093 for CNN and 196,96/12.148/10,397 for Daily Mail ) , with splitting sentences by Stanford CoreNLP [ Manning et al. , 2014 ] toolkit and pre-processing the dataset following [ See et al. , , 2017 ] and [ Zhong et al. , , 2020 ] .", "Comments": [], "label": [[205, 221, "Software-Entity"]]}
{"id": 152300, "text": "Sentences were split with the Stanford CoreNLP toolkit [ Manning et al. , , 2014 ] .", "Comments": [], "label": [[30, 46, "Software-Entity"]]}
{"id": 152301, "text": "Input documents were truncated to 800 BPE tokens too . 4.2 Parameters Our code is based on Pytorch [ Paszke et al. , , 2019 ] and the pre-trained model employed in DifferSum is 'albert-xxlarge-v2 ' , which is based on the huggingface/transformers [ 2 ] .", "Comments": [], "label": [[91, 98, "Software-Entity"], [222, 246, "Software-Entity"]]}
{"id": 152302, "text": "We train DifferSum two days for 100,000 steps on 2GPUs ( Nvidia Tesla V100 , 32GB ) with gradient accumulation every two steps .", "Comments": [], "label": [[49, 54, "Device-Count"], [57, 74, "Hardware-device"], [77, 81, "Device-Memory"]]}
{"id": 152333, "text": "The data were generated by Amazon Mechanical Turk workers who summarized 8 input review texts .", "Comments": [], "label": [[27, 49, "Cloud-Platform"]]}
{"id": 152334, "text": "Simple data statistics are shown in Table [ 1 , ] and other details can be found in Appendix [ A.1 . ] 6.2 Experimental Details All the models [ 2 ] were implemented with Py-Torch [ Paszke et al. , , 2019 ] , and we used the Transformers library from Hugging Face [ Wolf et al. , , 2020 ] as the backbone skeleton .", "Comments": [], "label": [[171, 179, "Software-Entity"], [251, 263, "Software-Entity"]]}
{"id": 152335, "text": "We trained the entire models using the Adam optimizer [ Kingma and Ba , 2014 ] with a linear learning rate decay on NVIDIA V100s .", "Comments": [], "label": [[116, 128, "Hardware-device"]]}
{"id": 152353, "text": "Run time for text modality pretraining was 16h on 4 GPUs , and it took 41h and 43h on 2 GPUs for image and table modality training , respectively .", "Comments": [], "label": [[50, 56, "Device-Count"], [86, 92, "Device-Count"]]}
{"id": 152354, "text": "For final multimodal training , it took 14h on 8 GPUs .", "Comments": [], "label": [[47, 53, "Device-Count"]]}
{"id": 152369, "text": "E Details on Transformer Models Model Architecture We implemented uncased BERT-base models [ Devlin et al. , , 2019 ] using the transformers Python library [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[128, 140, "Software-Entity"]]}
{"id": 152373, "text": "Computation We ran all computations on a Microsoft Azure `` Standard NC24 '' server equipped with two NVIDIA Tesla K80 GPU cards .", "Comments": [], "label": [[41, 56, "Cloud-Platform"], [98, 101, "Device-Count"], [102, 122, "Hardware-device"]]}
{"id": 152390, "text": "Experimental details : We use the HuggingFace implementation [ Wolf et al. , , 2020a ] of the T5 model [ Raffel et al. , , 2020 ] .", "Comments": [], "label": [[34, 45, "Software-Entity"]]}
{"id": 152403, "text": "A Experimental Details Computing infrastructure : We run the experiments in Table [ 1 ] on 4 GPUs , and the rest of the experiments on 1 GPU on a heterogeneous cluster with Tesla V100 , Tesla A100 , Tesla P4 , and GTX1080ti GPUs .", "Comments": [], "label": [[91, 97, "Device-Count"], [135, 140, "Device-Count"], [173, 183, "Hardware-device"], [186, 196, "Hardware-device"], [199, 207, "Hardware-device"], [214, 228, "Hardware-device"]]}
{"id": 152406, "text": "Data pre-processing : We download all datasets from the HuggingFace Datasets library [ Wolf et al. , , 2020b ] .", "Comments": [], "label": [[56, 67, "Software-Entity"]]}
{"id": 152411, "text": "In this work , the Stanza toolkit ( Qi et al. , 2020 ) is used to extract the universal dependency relations as the first step .", "Comments": [], "label": [[19, 25, "Software-Entity"]]}
{"id": 152412, "text": "We deploy the same Stanza toolkit ( Qi et al. , 2020 ) to assign ( factual ) POS tags P for all tokens .", "Comments": [], "label": [[19, 25, "Software-Entity"]]}
{"id": 152414, "text": "We remove Thai ( th ) and Swahili ( sw ) from our experiments since these two languages are not supported by Stanza .", "Comments": [], "label": [[109, 115, "Software-Entity"]]}
{"id": 152415, "text": "Our implementation of pre-trained language models ( mBERT and XLM-R ) is based on HuggingFaces 's Transformers ( Wolf et al. , 2020 ) .", "Comments": [], "label": [[82, 97, "Software-Entity"]]}
{"id": 152416, "text": "All experiments are conducted on a workstation with dual NVIDIA V100 32GB GPUs . 4.3 Results We compare our method with naive fine-tuning and the state-of-the-art methods .", "Comments": [], "label": [[52, 56, "Device-Count"], [57, 78, "Hardware-device"]]}
{"id": 152424, "text": "We first preprocess collected fact-checks by extracting the main article content and verdicts from HTML webpages using a customized parser , and tokenizing the content with NLTK [ Bird , 2006 ] .", "Comments": [], "label": [[173, 177, "Software-Entity"]]}
{"id": 152425, "text": "For initialization , we train word embeddings using Gensim [ Re ] [ hurek and Sojka , 2011 ] on the entire corpus .", "Comments": [], "label": [[52, 58, "Software-Entity"]]}
{"id": 152428, "text": "We used the Microsoft Language Understanding Intelligent Service ( LUIS ) to relabel the dataset , and merged some similar label types , such as insist and vagueprice into counter-noprice , and intro and great into greet .", "Comments": [], "label": [[12, 73, "Cloud-Platform"]]}
{"id": 152430, "text": "The dialog managers we compare are described in Section [ 5 . ] For the utterance parser , we use Microsoft Language Understanding Intelligent Service ( LUIS ) [ Williams et al. , , 2015 ] with 10 annotated training examples for each dialog act .", "Comments": [], "label": [[98, 159, "Cloud-Platform"]]}
{"id": 152432, "text": "We include more detailed analysis and t-SNE visualization in appendix [ B . ] 8 Conclusion In this work , we proposed a novel framework to integrate the concept of Theory of Mind ( ToM ) into generating task-oriented dialogs .", "Comments": [], "label": [[38, 43, "Software-Entity"]]}
{"id": 152433, "text": "The initial dataset was collected using crowd workers on Amazon Mechanical Turk ( AMT ) playing the role of buyers and sellers .", "Comments": [], "label": [[57, 87, "Cloud-Platform"]]}
{"id": 152435, "text": "System Design Parser : We use Microsoft Language Understanding Intelligent Service ( LUIS ) [ Williams et al. , , 2015 ] with 10 starting training examples for each dialog act in our experiment .", "Comments": [], "label": [[30, 91, "Cloud-Platform"]]}
{"id": 152439, "text": "Each run on training each manager was performed using a single NVIDIA GTX 2080 Ti GPU with 16GB RAM in approximate 2 hours .", "Comments": [], "label": [[56, 62, "Device-Count"], [63, 85, "Hardware-device"], [91, 99, "Device-Memory"]]}
{"id": 152440, "text": "B.2 Visualization of Latent Variables Figure 5 : t-SNE and PCA visualization of latent variables in the explicit ToM model ( left ) and the implicit ToM model ( right ) .", "Comments": [], "label": [[49, 54, "Software-Entity"]]}
{"id": 152446, "text": "5.1.3 Training Details We build our proposed CitationIE methods on top of the SciREX repository [ 7 ] [ Jain et al. , , 2020 ] in the AllenNLP framework [ Gardner et al. , , 2018 ] .", "Comments": [], "label": [[134, 142, "Software-Entity"]]}
{"id": 152447, "text": "We train each model on a single GPU with batch size 4 for up to 20 epochs .", "Comments": [], "label": [[25, 35, "Device-Count"]]}
{"id": 152450, "text": "A Appendices A.1 Training Configurations We train each model on a single 11GB NVIDIA GeForce RTX 2080 Ti GPU with a batch size of 4 .", "Comments": [], "label": [[66, 72, "Device-Count"], [73, 77, "Device-Memory"], [78, 108, "Hardware-device"]]}
{"id": 152451, "text": "We train for up to 20 epochs , and set the patience parameter in AllenNLP to 10 ; if the validation metric does not improve for 10 consecutive epochs , we stop training early .", "Comments": [], "label": [[65, 73, "Software-Entity"]]}
{"id": 152456, "text": "Best model is in bold for each metric . 133/1337/13370 is the default seed setting in AllenNLP .", "Comments": [], "label": [[86, 94, "Software-Entity"]]}
{"id": 152460, "text": "Vocal and Accompaniment Separation For each rap song , we utilize * Spleeter * [ Hennequin et al. , , 2020 ] [ 1 ] , a public music separation tool , to separate the vocal ( containing rap singing ) and accompaniment ( containing rhythmic beats ) from the crawled rap audio .", "Comments": [], "label": [[68, 76, "Software-Entity"]]}
{"id": 152461, "text": "Therefore , we use a beat track detection tool , * Librosa * [ McFee et al. , , 2020 ] [ 4 ] , to track the timestamp of each beat from the separated accompaniment that obtained from the second step .", "Comments": [], "label": [[51, 58, "Software-Entity"]]}
{"id": 152464, "text": "We use Transformer [ Vaswani et al. , , 2017 ] to build an autoregressive language model [ Radford et al. , ] [ 2018 , 2019 ] LINE_ [ START_TIME ] LYRICS Figure 2 : An example of a rap song with aligned beats in our mined `` D-RAP '' dataset . ' * ' means a beat is aligned with the word right after ' * ' .", "Comments": [], "label": [[7, 18, "Software-Entity"]]}
{"id": 152467, "text": "We use the vowel in the Pinyin [ 5 ] of Chinese characters to represent their rhymes .", "Comments": [], "label": [[24, 30, "Software-Entity"]]}
{"id": 152471, "text": "Our model is trained with a batch size of 8 songs on 4 NVIDIA TITAN V GPUs .", "Comments": [], "label": [[53, 54, "Device-Count"], [55, 74, "Hardware-device"]]}
{"id": 152482, "text": "Implementation Details We implement AdvPicker using PyTorch 1.6.0 .", "Comments": [], "label": [[52, 59, "Software-Entity"]]}
{"id": 152483, "text": "E in Eq . [ 1 ] ) and student model ( i.e . h T stu in Eq . [ 7 ] ) , we employ the pre-trained cased multilingual BERT in Hugging-Face 's Transformers [ Wolf et al. , , 2020 ] [ 4 ] as backbone model , which has 12 transformer blocks , 12 attention heads , and 768 hidden units .", "Comments": [], "label": [[123, 138, "Software-Entity"]]}
{"id": 152486, "text": "Our models are trained on a Tesla P100 GPU ( 16GB ) .", "Comments": [], "label": [[28, 42, "Hardware-device"], [45, 49, "Device-Memory"]]}
{"id": 152495, "text": "Following existing approaches ( e.g . [ Sakor et al. , , 2019 ] [ Wu et al. , 2020 ] , we use off-the-shelf lookup tools such as DBpedia lookup [ 2 ] to retrieve top-100 candidates for each mention .", "Comments": [], "label": [[129, 136, "Software-Entity"]]}
{"id": 152498, "text": "We also employ a suite of pretrained or custom trained neural language models to compute the similarity of m and eij . * Pre-trained Embedding Models . * These include SpaCy 's semantic similarity [ 6 ] function that uses Glove [ Pennington et al. , , 2014 ] trained on Common Crawl .", "Comments": [], "label": [[168, 173, "Software-Entity"]]}
{"id": 152499, "text": "In addition to SpaCy , we also use scores from an entity linking system such as BLINK [ Wu et al. , , 2020 ] ( a state-of-the-art entity linking model ) as a feature function in our system .", "Comments": [], "label": [[15, 20, "Software-Entity"]]}
{"id": 152502, "text": "Furthermore , we use the following variants of our approach : ( 4 ) * RuleEL * : a baseline rule-based EL approach with manually defined weights and thresholds , ( 5 ) * LogicEL * : a baseline approach built on RuleEL where only the thresholds are learnable , based on product t-norm ( see Section [ 3.2 ] , ( 6 ) * LNN-EL * : our core LNN-based method using non-embedding features plus SpaCy , and ( 7 ) * LNN-EL * ens : an ensemble combining core LNN-EL with additional features from existing EL approaches , namely BLINK and Box ( we consider Box , as it outperforms BERT and BERTWiki on all datasets ) .", "Comments": [], "label": [[387, 392, "Software-Entity"]]}
{"id": 152505, "text": "We used two Nvidia V100 GPUs with 16GB memory each .", "Comments": [], "label": [[8, 11, "Device-Count"], [12, 28, "Hardware-device"], [34, 45, "Device-Memory"]]}
{"id": 152507, "text": "A.3 LNN-EL Rules In our experiments , we explore the following modules , implemented in PyTorch .", "Comments": [], "label": [[88, 95, "Software-Entity"]]}
{"id": 152513, "text": "All these models were trained on cluster with 2 CPUs each with 20 cores , 384 GB DDR4 RAM and Inter Xeon Gold 6148 processor .", "Comments": [], "label": [[46, 52, "Device-Count"], [74, 89, "Device-Memory"], [94, 114, "Hardware-device"]]}
{"id": 152514, "text": "The set up also utilized 4 NVIDIA Tesla V100 GPUs each with 32 GB memory .", "Comments": [], "label": [[25, 26, "Device-Count"], [27, 49, "Hardware-device"], [60, 72, "Device-Memory"]]}
{"id": 152515, "text": "Experiments with IDG were performed on a system with Intel Core i7-8550U 1.80GHz CPU with 16 GB RAM .", "Comments": [], "label": [[53, 84, "Hardware-device"], [90, 99, "Device-Memory"]]}
{"id": 152526, "text": "Implementation We implemented our model in PyTorch [ Paszke et al. , , 2017 ] using AllenNLP [ Gardner et al. , , 2018 ] .", "Comments": [], "label": [[43, 50, "Software-Entity"], [84, 92, "Software-Entity"]]}
{"id": 152527, "text": "We used the NT-Xent loss function implemented by the PyTorch Metric Learning library [ Musgrave et al. , , 2019 ] and the pretrained transformer architecture and weights from the Transformers library [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[53, 60, "Software-Entity"], [179, 199, "Software-Entity"]]}
{"id": 152528, "text": "All models were trained on up to four NVIDIA Tesla V100 16 or 32GB GPUs .", "Comments": [], "label": [[33, 37, "Device-Count"], [38, 55, "Hardware-device"], [56, 71, "Device-Memory"]]}
{"id": 152534, "text": "We include the performance of averaged GloVe [ 7 ] and fastText [ 8 ] word vectors as weak baselines .", "Comments": [], "label": [[55, 63, "Software-Entity"]]}
{"id": 152550, "text": "Therefore , we use the Transformer for all the experiments in this paper .", "Comments": [], "label": [[23, 34, "Software-Entity"]]}
{"id": 152554, "text": "We used the implementation of the Transformer model in the Fairseq toolkit [ Ott et al. , , 2019 ] [ 10 ] with characterlevel transduction [ Wu et al. , , 2020 ] for morphology learning in low-resource settings .", "Comments": [], "label": [[59, 66, "Software-Entity"]]}
{"id": 152557, "text": "All models have been trained on an NVIDIA [ explore/collections/MDR ] ( https : //digital.library.unt.edu/explore/collections/MDR ) [ https : //www.sil.org/resources/search/ ] ( https : //www.sil.org/resources/search/language/ntu ) [ language/ntu ] ( https : //www.sil.org/resources/search/language/ntu ) GP102 [ TITAN Xp ] GPU for 10k maximum updates with a batch size of 400 .", "Comments": [], "label": [[305, 310, "Hardware-device"], [313, 321, "Hardware-device"]]}
{"id": 152566, "text": "We train wav2vec 2.0 [ Baevski et al. , , 2020 ] models with original hyper-parameter settings using fairseq [ Ott et al. , , 2019 ] , except for Table [ 7 ] where we use wav2letter [ Pratap et al. , , 2018 ] and follow [ Talnikar et al. , 2020 ] to do finetuning using both supervised CTC [ Graves et al. , , 2006 ] loss and unsupervised wav2vec 2.0 loss .", "Comments": [], "label": [[101, 108, "Software-Entity"]]}
{"id": 152567, "text": "The largest model ( `` VP-100K '' ) takes 10 days on 128 V100 GPUs for 1M updates .", "Comments": [], "label": [[53, 56, "Device-Count"], [57, 66, "Hardware-device"]]}
{"id": 152568, "text": "For non-wav2vec models , we train Transformer [ Vaswani et al. , , 2017 ] with cross-entropy criterion using fairseq S2T [ Wang et al. , 2020a ] .", "Comments": [], "label": [[109, 116, "Software-Entity"]]}
{"id": 152572, "text": "We build n-gram language models for decoding ( when specified ) using KenLM [ Heafield , 2011 ] .", "Comments": [], "label": [[70, 75, "Software-Entity"]]}
{"id": 152574, "text": "Our model outperforms the baseline ( even without LM ) while using less supervised train data . †Deepspeech Polyglot . 4.2 Speech Recognition ( ASR ) Baselines We provide monolingual Transformer baselines for the 14 languages that have more than 10 hours of transcribed data ( see Table [ 1 ] .", "Comments": [], "label": [[97, 116, "Software-Entity"]]}
{"id": 152577, "text": "We also evaluate our multilingual model ( VP-50K ) under the normal setup ( CV v5.1 ) and report test WER in Table [ 7 . ] They are compared with supervised baselines from DeepSpeech-Polyglot [ 1 ] , which leverage extended CV train sets and several other corpora for training as well as LM for decoding .", "Comments": [], "label": [[172, 191, "Software-Entity"]]}
{"id": 152589, "text": "The implementation is based on PyTorch [ Paszke et al. , 2019 ] and the Huggingface Transformers Library [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[31, 38, "Software-Entity"], [72, 83, "Software-Entity"]]}
{"id": 152590, "text": "Training/fine-tuning and inference are done on a single NVIDIA Tesla V100 GPU .", "Comments": [], "label": [[49, 55, "Device-Count"], [56, 77, "Hardware-device"]]}
{"id": 152591, "text": "Since we are evaluating the selective prediction performance of different models instead of pursuing state-of-the-art results , we do not extensively tune hyperparameters ; instead , most experiment settings such as hidden sizes , learning rates , and batch sizes are kept unchanged from the Huggingface Library .", "Comments": [], "label": [[292, 303, "Software-Entity"]]}
{"id": 152594, "text": "We use the [ torchprofile ] ( https : //github.com/zhijian-liu/torchprofile ) toolkit to measure multiply– accumulate operations ( MACs ) , and then double the number to obtain floating point operations ( FLOPs ) . base .", "Comments": [], "label": [[13, 25, "Software-Entity"]]}
{"id": 152595, "text": "For models that require pre-trained , we use the following ones provided by the Huggingface Transformer Library [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[80, 91, "Software-Entity"]]}
{"id": 152601, "text": "We use the pretrained model from Huggingface [ Wolf et al. , , 2019 ] .", "Comments": [], "label": [[33, 44, "Software-Entity"]]}
{"id": 152606, "text": "The learning rate , number of epochs , and other hyper-parameters are presented on table [ 8 ] of Appendix [ A . ] Hardware Details We trained all models using a single NVIDIA V100 GPU .", "Comments": [], "label": [[162, 168, "Device-Count"], [169, 184, "Hardware-device"]]}
{"id": 152608, "text": "All experiments were run using the PyTorch [ 1 ] framework . 4.2 Results Table [ 1 ] presents the results of MATE-KD on the GLUE dev set .", "Comments": [], "label": [[35, 42, "Software-Entity"]]}
{"id": 152621, "text": "Formally , the probability of individual i answering item j correctly is modeled as : 2.1 IRT with Variational Inference We use variational inference to infer IRT parameters from model response patterns using Pyro [ Ran ] [ ganath et al. , , 2014 ] [ Bingham et al. , , 2019 ] .", "Comments": [], "label": [[209, 213, "Software-Entity"]]}
{"id": 152631, "text": "We use the jiant [ Pruk ] [ sachatkun et al. , , 2020b ] library which is based on PyTorch [ Paszke et al. , , 2019 ] and HuggingFace Transformers [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[11, 16, "Software-Entity"], [83, 90, "Software-Entity"], [122, 133, "Software-Entity"]]}
{"id": 152633, "text": "We use NVIDIA V100 Tensor Core GPUs for our experiments .", "Comments": [], "label": [[7, 18, "Hardware-device"]]}
{"id": 152647, "text": "The backbone model and our model implementation is based on the HuggingFace 's Transformers library [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[64, 78, "Software-Entity"]]}
{"id": 152648, "text": "Trainings and evaluations were conducted on a dual 2080Ti 11GB GPU machine with multiple runs .", "Comments": [], "label": [[46, 50, "Device-Count"], [51, 66, "Hardware-device"]]}
{"id": 152659, "text": "To calculate these variables we run the NLTK POS tagger [ Loper and ] [ Bird , 2002 ] on the tweets , and collect the distribution of POS tags for each word .", "Comments": [], "label": [[40, 44, "Software-Entity"]]}
{"id": 152664, "text": "Training was done using an NVIDIA GeForce GTX 1080 8GB GPU and took around 1 to 1.5 days per model .", "Comments": [], "label": [[27, 50, "Hardware-device"], [51, 58, "Device-Memory"]]}
{"id": 152678, "text": "We use `` g2p en '' Python package [ Lee and Kim , 2018 ] to convert the training text into the corresponding phoneme representation , which is based on the CMU English dictionary .", "Comments": [], "label": [[10, 13, "Software-Entity"]]}
{"id": 152679, "text": "We further extend the phoneme set by distinguishing the first phoneme in the word with an additional `` `` mark appended , which is similar to the notation in the SentencePiece process .", "Comments": [], "label": [[163, 176, "Software-Entity"]]}
{"id": 152682, "text": "Instead of using phoneme representation , the target labels are encoded with SentencePiece [ Kudo and Richardson , 2018 ] .", "Comments": [], "label": [[77, 90, "Software-Entity"]]}
{"id": 152683, "text": "The maximum tokens per mini-batch is 2048 with 8 V100 GPU cards .", "Comments": [], "label": [[47, 48, "Cloud-Platform"], [49, 57, "Hardware-device"]]}
{"id": 152684, "text": "The model is trained using 16 A100 GPU cards with update frequency 12 .", "Comments": [], "label": [[27, 29, "Device-Count"], [30, 38, "Hardware-device"]]}
{"id": 152685, "text": "Fine-tuning The model is fine-tuned on the downstream task with learning rate 0.0003 and 8 V100 GPU cards .", "Comments": [], "label": [[89, 90, "Device-Count"], [91, 99, "Hardware-device"]]}
{"id": 152703, "text": "Table 1 : Latency ( in Milliseconds ) on a V100 GPU and number of parameters ( million ) of our models . 4.2 Implementation details Teacher/Student model settings We use BART Large [ Lewis et al. , , 2020 ] as our teacher model , which has 12 layers in the encoder and decoder .", "Comments": [], "label": [[43, 51, "Hardware-device"]]}
{"id": 152710, "text": "All our models are trained on 8 NVIDIA V100 GPUs .", "Comments": [], "label": [[30, 31, "Device-Count"], [32, 48, "Hardware-device"]]}
{"id": 152742, "text": "Model Details All of our parsers are based on the Transformer architecture [ Vaswani et al. , , 2017 ] , adapted to the graph action sequence ( see Appendix [ A ] .", "Comments": [], "label": [[50, 61, "Software-Entity"]]}
{"id": 152752, "text": "Training takes about 3 hours on a single Nvidia Titan RTX GPU with 24 GB memory with floating-point 16 mixed precision training for the FULLTOGRAPH parser , and the time increases proportionally for PREFIXTOGRAPH with the prefix data size when combining different prefix lengths .", "Comments": [], "label": [[34, 40, "Device-Count"], [41, 61, "Hardware-device"], [66, 79, "Device-Memory"]]}
{"id": 152753, "text": "For the LMCOMPLETE model , we fine-tune BARTlarge for 12 epochs and take the best checkpoint , following the standard recipe from FAIRSEQ .", "Comments": [], "label": [[130, 137, "Software-Entity"]]}
{"id": 152755, "text": "All the models are implemented and trained with the FAIRSEQ toolkit [ Ott et al. , , 2019 ] .", "Comments": [], "label": [[52, 59, "Software-Entity"]]}
{"id": 152780, "text": "All trainings are done with a fixed learning rate of 2e−5 and 40 epochs using 8 V100 GPUs .", "Comments": [], "label": [[78, 79, "Device-Count"], [80, 89, "Hardware-device"]]}
{"id": 152782, "text": "We use 16 and 8 V100 GPUs for NQ and WebQ respectively .", "Comments": [], "label": [[7, 9, "Device-Count"], [14, 15, "Device-Count"], [16, 25, "Hardware-device"]]}
{"id": 152788, "text": "For instance , BioBERT was pre-trained for 23 days on 8 NVIDIA V100 GPUs [ Lee et al. , , 2020 ] .", "Comments": [], "label": [[54, 55, "Device-Count"], [56, 72, "Hardware-device"]]}
{"id": 152799, "text": "The experiments were run on a single RTX 3090 24 GB GPU , and the training codes were implemented in PyTorch .", "Comments": [], "label": [[30, 36, "Device-Count"], [37, 45, "Hardware-device"], [46, 55, "Device-Memory"], [101, 108, "Software-Entity"]]}
{"id": 152803, "text": "As revealed in the results , even though TAPT showed improved results in the original study with Google Cloud TPU , it was unstable with the small batch size and sequence length ; the performances were even degraded in the general GPU environment .", "Comments": [], "label": [[97, 109, "Cloud-Platform"], [110, 113, "Hardware-device"]]}
{"id": 152834, "text": "Unless specified otherwise , we use the T5X library [ 6 ] and pre-trained checkpoints from [ Raffel et al. , 2020 ] [ 7 ] .", "Comments": [], "label": [[40, 43, "Software-Entity"]]}
{"id": 152838, "text": "When finetuning , we use a learning rate of 10− and a batch size of 1 on a single Nvidia V100 GPU .", "Comments": [], "label": [[75, 81, "Device-Count"], [82, 97, "Hardware-device"]]}
{"id": 152841, "text": "Computational Budget and Environmental Impact We fine-tune all T5 models on Cloud TPU v3 hardware ; each takes approximately 4 hours on 16 TPUs in pod configuration .", "Comments": [], "label": [[82, 88, "Hardware-device"], [136, 143, "Device-Count"]]}
{"id": 152845, "text": "This series of experiments was run using an older version of T5X , so are not exactly comparable to Table [ 3 . ] exists in RGF as well as baselines .", "Comments": [], "label": [[61, 64, "Software-Entity"]]}
{"id": 152846, "text": "These experiments were run on an older version of T5X , so RGF ( 1X ) values are reported differently from Table [ 3 . ] We observe that adding more RGF data ( 3X or more ) for augmentation can hurt performance .", "Comments": [], "label": [[50, 53, "Software-Entity"]]}
{"id": 152868, "text": "Each experiment is run on one NVIDIA Quadro RTX 8000 GPU .", "Comments": [], "label": [[26, 29, "Device-Count"], [30, 56, "Hardware-device"]]}
{"id": 152877, "text": "In all experiments , all texts are tokenized with the same sentencepiece [ Kudo , 2018 ] tokenizer as XLM-R. 2.2 Model Architecture SixT+ is a Transformer-based NMT model with ∼0.7B model parameters .", "Comments": [], "label": [[59, 72, "Software-Entity"]]}
{"id": 152880, "text": "We report detokenized BLEU for all directions using sacrebleu [ 3 ] .", "Comments": [], "label": [[52, 61, "Software-Entity"]]}
{"id": 152886, "text": "For all other XTREME tasks , we follow the training and evaluation protocol in [ Hu et al. , 2020 ] and implement with the jiant toolkit [ Phang et al. , , 2020 ] .", "Comments": [], "label": [[123, 128, "Software-Entity"]]}
{"id": 152891, "text": "The Europarl De-En dataset is only used in the experiment of Table [ 4 . ] All texts are tokenized by the same XLM-R sentencepiece [ Kudo , 2018 ] model .", "Comments": [], "label": [[117, 130, "Software-Entity"]]}
{"id": 152892, "text": "The average context length after performing sentencepiece is 669 tokens .", "Comments": [], "label": [[44, 57, "Software-Entity"]]}
{"id": 152893, "text": "The languages used in this paper are shown in Table [ 11 . ] C Model and Training Details Since the SixT+ embeddings are initialized with XLM-R , all texts are tokenized with the same sentencepiece [ Kudo , ] [ 2018 , ] SPM ) tokenizer as XLM-R .", "Comments": [], "label": [[184, 197, "Software-Entity"]]}
{"id": 152894, "text": "The tokenizer is learned on the full Common Crawl data that includes 250k sentencepiece tokens .", "Comments": [], "label": [[74, 87, "Software-Entity"]]}
{"id": 152895, "text": "SixT+ is trained on 128 Nvidia V100 GPUs ( 32GB ) with 100k and 10k steps for the first and second training stage .", "Comments": [], "label": [[20, 23, "Device-Count"], [24, 40, "Hardware-device"], [43, 47, "Device-Memory"]]}
{"id": 152898, "text": "All experiments are done with the fairseq toolkit [ Ott et al. , , 2019 ] .", "Comments": [], "label": [[34, 41, "Software-Entity"]]}
{"id": 152901, "text": "G XTREME benchmark results All models are evaluated on the XTREME benchmark [ Hu et al. , , 2020 ] with jiant toolkit [ 20 ] .", "Comments": [], "label": [[104, 109, "Software-Entity"]]}
{"id": 152907, "text": "For entity recognition and linking , we use SLING [ Ringgaard et al. , , 2017 ] , [ 3 ] which builds an entity linker for arbitrary languages available in Wikipedia .", "Comments": [], "label": [[44, 49, "Software-Entity"]]}
{"id": 152910, "text": "We tokenize the text using the same sentencepiece model as [ Liu et al. , 2020 ] , and train on sequences of 512 subwords .", "Comments": [], "label": [[36, 49, "Software-Entity"]]}
{"id": 152916, "text": "We use the same sentencepiece model and the vocabulary from [ Liu et al. , 2020 ] .", "Comments": [], "label": [[16, 29, "Software-Entity"]]}
{"id": 152919, "text": "All models are pre-trained on one TPUv3 ( 128GB ) for about 12 hours for 50K steps . [ 4 ] We apply the noise function on the monolingual data on the fly for each epoch , and this takes only a few minutes by multiprocessing in Fairseq [ Ott et al. , , 2019 ] .", "Comments": [], "label": [[30, 33, "Device-Count"], [34, 39, "Hardware-device"], [42, 47, "Device-Memory"], [227, 234, "Software-Entity"]]}
{"id": 152920, "text": "Single-task ( multi-task ) finetuning takes about 16 ( 32 ) hours on 2 RTX 3090 GPUs .", "Comments": [], "label": [[69, 70, "Device-Count"], [71, 84, "Hardware-device"]]}
{"id": 152924, "text": "[ 2 ] ( 2 ) In light of recent studies exploring how model design and overall perfomance interplay with gender bias [ Roberts et al. , , 2020 ] [ Gaido et al. , , 2021 ] , we rely on our manually curated resource to compare three ST models , which are trained on varying amounts of data , and built with different segmentation techniques : character and byte-pairencoding ( BPE ) [ Sennrich et al. , , 2016 ] .", "Comments": [], "label": [[354, 379, "Software-Entity"]]}
{"id": 152951, "text": "The target text was tokenized with Moses . [ 28 ] We normalized audio per-speaker and extracted 40 features with 25ms windows sliding by 10ms with XNMT [ 29 ] [ Neubig et al. , , 2018 ] .", "Comments": [], "label": [[35, 40, "Software-Entity"]]}
{"id": 152956, "text": "Each mini-batch consists of 8 samples , we set the update frequency to 8 , and we train on 4 GPUs , so a batch contains 256 samples .", "Comments": [], "label": [[91, 97, "Device-Count"]]}
{"id": 152959, "text": "Our code is built on the Fairseq library [ Ott et al. , , 2019 ] and trainings are performed on 4 K80 GPUs , lasted 4 days for the MuST-C-only trainings and 12 days for the rich-data models .", "Comments": [], "label": [[25, 32, "Software-Entity"], [96, 97, "Device-Count"], [98, 106, "Hardware-device"]]}
{"id": 152974, "text": "Table 7 : QA performance and inference efficiency of our systems with different configurations on the dev set of AMBIGQA . * Sec/Q * denotes seconds per question when using a single V100 GPU .", "Comments": [], "label": [[175, 181, "Device-Count"], [182, 190, "Hardware-device"]]}
{"id": 152977, "text": "By also increasing αneg and choosing a smaller k , as shown by the last row of Table [ 7 , ] the overall time needed to answer a question on the dev set of AMBIGQA can be reduced to 1.88 sec on a single V100 GPU while also obtaining state-of-the-art F1 scores ( 50.7/38.2 ) .", "Comments": [], "label": [[196, 202, "Device-Count"], [203, 211, "Hardware-device"]]}
{"id": 152980, "text": "All experiments were conducted on a single machine with eight V100 GPUs .", "Comments": [], "label": [[56, 61, "Device-Count"], [62, 71, "Hardware-device"]]}
{"id": 152992, "text": "We train our model on 32 Nvidia Tesla V100 32GB GPUs with mixed-precision training .", "Comments": [], "label": [[22, 24, "Device-Count"], [25, 52, "Hardware-device"]]}
{"id": 152993, "text": "For inference , we use 1 Nvidia Tesla V100 32GB GPU and Intel ( R ) Xeon ( R ) Platinum 8269CY CPU @ 2.50GHz to estimate the GPU and CPU throughput ( with a batch size of 128 for GPU and 1 for CPU ) .", "Comments": [], "label": [[23, 24, "Device-Count"], [25, 42, "Hardware-device"], [43, 51, "Device-Memory"], [56, 108, "Hardware-device"]]}
{"id": 153014, "text": "For all datasets mentioned above , we use Moses toolkit to tokenize and clean data .", "Comments": [], "label": [[42, 47, "Software-Entity"]]}
{"id": 153020, "text": "We train all models on 16 NVIDIA V100 Tensor Core GPUs .", "Comments": [], "label": [[23, 25, "Device-Count"], [26, 37, "Hardware-device"]]}
{"id": 153039, "text": "All the experiments in this paper can be run on a single Tesla V100 GPU .", "Comments": [], "label": [[50, 56, "Device-Count"], [57, 71, "Hardware-device"]]}
{"id": 153040, "text": "Practically , we make use HuggingFace [ Wolf et al. , , 2020 ] implementation of zero-shot text classification [ Yin et al. , , 2019 ] to classify missing TPE into 48 pre-defined categories with the input of concatenated table caption , columns , and cell values .", "Comments": [], "label": [[26, 37, "Software-Entity"]]}
{"id": 153043, "text": "The training process lasts 12 hours on a single 16GB Tesla V100 GPU .", "Comments": [], "label": [[41, 47, "Device-Count"], [48, 52, "Device-Memory"], [53, 67, "Hardware-device"]]}
{"id": 153045, "text": "The training epoch is 20 with a batch size of 30 ; The dropout rate is 0.2 ; The training process lasts 9 hours on a single 16GB Tesla V100 GPU .", "Comments": [], "label": [[117, 123, "Device-Count"], [124, 128, "Device-Memory"], [129, 143, "Hardware-device"]]}
{"id": 153048, "text": "The training process lasts 24 hours on a single 32GB Tesla V100 GPU .", "Comments": [], "label": [[41, 47, "Device-Count"], [48, 52, "Device-Memory"], [53, 67, "Hardware-device"]]}
{"id": 153064, "text": "The training time is measured with 8 NVIDIA RTX 2080Ti . h is the abbreviation of hour .", "Comments": [], "label": [[35, 36, "Device-Count"], [37, 54, "Hardware-device"]]}
{"id": 153079, "text": "Our experiments are done using the GPUs provided by Google Colab free and pro . 4.2 Probing Results Here , BERT [ Devlin et al. , , 2019 ] , RoBERTa [ Liu et al. , 2019 ] , and ELECTRA [ Clark et al. , , 2020 ] represent our PLMs .", "Comments": [], "label": [[52, 64, "Hardware-device"]]}
{"id": 153080, "text": "Due to our resource limitations , we conduct all experiments on the * base * version of the models ( 12 layers , 768 hidden size , 110M parameters ) implemented in HuggingFace 's Transfomers [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[164, 178, "Software-Entity"]]}
{"id": 153098, "text": "We implemented the code using PyTorch for tensor computations and Hugging Face [ 3 ] for language model checkpoints .", "Comments": [], "label": [[30, 37, "Software-Entity"], [66, 78, "Software-Entity"]]}
{"id": 153099, "text": "We performed the experiments on a workstation with a GPU Nvidia RTX 3090 of 24GB memory , 64GB of RAM , and a processor Intel ( R ) Core ( TM ) i9-10900X CPU @ 3.70GHz .", "Comments": [], "label": [[57, 72, "Hardware-device"], [76, 87, "Device-Memory"], [90, 101, "Device-Memory"], [120, 167, "Hardware-device"]]}
{"id": 153123, "text": "We use Py-Torch Lightning to implement the data processing and training pipelines .", "Comments": [], "label": [[7, 25, "Software-Entity"]]}
{"id": 153126, "text": "We train all our models on a NVIDIA Tesla V100 GPU .", "Comments": [], "label": [[29, 50, "Hardware-device"]]}
{"id": 153134, "text": "The pretraining was done for 200k iterations on a NVIDIA V100 GPU , taking around 26 hours ( on Microsoft platform 's Azure NC6s_v3 machine ) .", "Comments": [], "label": [[50, 65, "Hardware-device"], [96, 117, "Cloud-Platform"], [118, 131, "Cloud-Platform"]]}
{"id": 153135, "text": "We would also like to extend our immense gratitude to Microsoft 's * AI4Accessibility * program ( via Microsoft Philanthropies India ) for granting us the compute required to carry out all the experiments in this work , through Microsoft Azure cloud platform .", "Comments": [], "label": [[228, 243, "Cloud-Platform"]]}
{"id": 153136, "text": "The latency of these variants on Intel Xeon E5- 2690 v4 CPU with a frame-size of 640x480 were 142.59ms , 55.28ms , and 35.37ms respectively per frame .", "Comments": [], "label": [[33, 59, "Hardware-device"]]}
{"id": 153170, "text": "We implement all the models with PyTorch , and run experiments on a RTX3090 GPU .", "Comments": [], "label": [[33, 40, "Software-Entity"], [68, 79, "Hardware-device"]]}
{"id": 153178, "text": "We used the lemma of the words for unigrams and bigrams using the nltk lemmatizer ( Loper , 2002 ) and selected unigrams and bigrams that occurred in the training dataset at least fifty times .", "Comments": [], "label": [[66, 70, "Software-Entity"]]}
{"id": 153179, "text": "We used the spaCy POS-tagger and considered POS unigrams , bigrams and trigrams that occur at least 10 times in the training dataset .", "Comments": [], "label": [[12, 17, "Software-Entity"]]}
{"id": 153180, "text": "We relied on LightGBM , an ensemble of decision trees trained with gradient boosting [ Ke et al. , , 2017 ] .", "Comments": [], "label": [[13, 21, "Software-Entity"]]}
{"id": 153183, "text": "SHAP [ Lundberg and Lee , 2017 ] assigns to each feature an importance value ( called Shapley values ) for a particular prediction depending on the extent of its contribution ( a detailed introduction to Shapley values and SHAP can be found in [ Molnar , 2020 ] ) .", "Comments": [], "label": [[0, 4, "Software-Entity"], [223, 227, "Software-Entity"]]}
{"id": 153184, "text": "SHAP is a modelagnostic framework , therefore the values associ- ated with a set of features can be compared across models .", "Comments": [], "label": [[0, 4, "Software-Entity"]]}
{"id": 153185, "text": "It should be noted that SHAP produces explanations on a case-by-case basis , therefore it can both provide local and global explanations .", "Comments": [], "label": [[24, 28, "Software-Entity"]]}
{"id": 153186, "text": "For the Gradient Boosting model , we use an adapted version of SHAP [ Lundberg et al. , , 2018 ] , called TreeSHAP .", "Comments": [], "label": [[63, 67, "Software-Entity"], [106, 114, "Software-Entity"]]}
{"id": 153187, "text": "5 Experiments and results 5.1 Experimental setting To detect the best set of features , we used Light-GBM and proceeded incrementally , by adding the group of features we thought to be most likely associated with hedges .", "Comments": [], "label": [[96, 105, "Software-Entity"]]}
{"id": 153192, "text": "5.3 In-depth analysis of the informative features We trained the SHAP explanation models on Light-GBM with all features .", "Comments": [], "label": [[65, 69, "Software-Entity"], [92, 101, "Software-Entity"]]}
{"id": 153193, "text": "In addition , SHAP highlights the importance of novel features whose function was not identified in the hedges literature : * ( i ) * what LIWC classifies as informal words but that are mostly interjections like * ah * and * oh * are strongly associated with Apologizer , as are disfluencies ( n=12 ) ; * ( ii ) * the use of POS tags seems to be very relevant for characterizing the different classes ( 2-gram of POS tag features [ 3 ] occur in the top-ranked features of all the Note that there is strong redundancy between some features of LIWC and the spaCy POS tagger that both produce a `` Pronoun '' category , using a lexicon in the first case , and a neural inference in the second .", "Comments": [], "label": [[14, 18, "Software-Entity"], [555, 560, "Software-Entity"]]}
{"id": 153195, "text": "Table 6 : Most important clause-level features for LightGBM according to the SHAP analysis . classes ( see Figures in the Appendix ) .", "Comments": [], "label": [[51, 59, "Software-Entity"], [77, 81, "Software-Entity"]]}
{"id": 153196, "text": "It might be more useful to cluster clauses based on the importance that SHAP gives to that feature in its classification , as this could help discover sub-classes of hedges that are differentiated from the rest by their interaction with a specific feature ( in the way that some Apologizers are characterized by an `` oh '' ) .", "Comments": [], "label": [[72, 76, "Software-Entity"]]}
{"id": 153197, "text": "Regarding the SHAP analysis , most of the features that are considered as important are coherent with the definition of the classes ( * I * for subjectivizers , * sorry * for apologizers , * just * for propositional hedges ) .", "Comments": [], "label": [[14, 18, "Software-Entity"]]}
{"id": 153198, "text": "A limitation of SHAP is that it makes a feature independence assumption , which prompts the explanatory model to underestimate the importance of redundant features ( like pronouns in our work ) .", "Comments": [], "label": [[16, 20, "Software-Entity"]]}
{"id": 153199, "text": "A Additional information on the experimental settings We used PyTorch [ Paszke et al. , , 2019 ] to implement the neural models .", "Comments": [], "label": [[62, 69, "Software-Entity"]]}
{"id": 153200, "text": "For each set of features , hyperparameters were selected using Optuna ( Akiba , 2019 ) , a parameter search framework .", "Comments": [], "label": [[63, 69, "Software-Entity"]]}
{"id": 153202, "text": "To compute the SHAP values mentioned in the paper , we kept one split to perform the 5-split of the dataset , and leave 1 split to validate and early stop the model , in order to avoid overfitting .", "Comments": [], "label": [[15, 19, "Software-Entity"]]}
{"id": 153203, "text": "The BERT model was fine-tuned on a Nvidia Quadro RTX 8000 GPU .", "Comments": [], "label": [[35, 61, "Hardware-device"]]}
{"id": 153204, "text": "Figure 2 : Absolute averaged feature contribution , as indicated by SHAP .", "Comments": [], "label": [[68, 72, "Software-Entity"]]}
{"id": 153205, "text": "Figure 3 : Averaged contribution of features to the detection of the `` Not indirect '' class , as indicated by SHAP .", "Comments": [], "label": [[112, 116, "Software-Entity"]]}
{"id": 153206, "text": "Figure 4 : Averaged contribution of features to the detection of `` Apologizers '' , as indicated by SHAP .", "Comments": [], "label": [[101, 105, "Software-Entity"]]}
{"id": 153207, "text": "Figure 5 : Averaged contribution of features to the detection of `` Propositional hedges '' , as indicated by SHAP .", "Comments": [], "label": [[110, 114, "Software-Entity"]]}
{"id": 153208, "text": "Figure 6 : Averaged contribution of features to the detection of `` Subjectivizers '' , as indicated by SHAP .", "Comments": [], "label": [[104, 108, "Software-Entity"]]}
{"id": 153221, "text": "5.3 Parsing speed Inspired by [ Zhang et al. , 2020b ] and [ Rush , 2020 ] who independently propose to batchify the Eisner algorithm using Pytorch , we batchify our proposed method so that O ( n 2 ) out of O ( n 3 ) can be computed in parallel , which greatly accelerates parsing .", "Comments": [], "label": [[140, 147, "Software-Entity"]]}
{"id": 153222, "text": "We achieve a similar parsing speed of our method to the fast implementation of the Eisner algorithm by [ Zhang et al. , 2020b ] : it parses 273 sentences per second , using BERT as the encoder under a single TITAN RTX GPU . 6 Related work Dependency parsing with more complex subtree information .", "Comments": [], "label": [[201, 207, "Device-Count"], [208, 221, "Hardware-device"]]}
{"id": 153240, "text": "B.2 Details for Adversarial Attack We use textattack [ Morris et al. , , 2020 ] to implement the adversarial attack methods .", "Comments": [], "label": [[42, 52, "Software-Entity"]]}
{"id": 153243, "text": "After we obtain all the relevant articles , we use the NLTK package [ Bird et al. , , 2009 ] to split the corpus into 69,666 sentences from these articles for further annotation .", "Comments": [], "label": [[55, 59, "Software-Entity"]]}
{"id": 153248, "text": "All models are run with V100 GPU .", "Comments": [], "label": [[24, 32, "Hardware-device"]]}
{"id": 153263, "text": "A.4 Computing Infrastructure Experiments are conducted on NVIDIA GeForce RTX 3090 GPU .", "Comments": [], "label": [[58, 85, "Hardware-device"]]}
{"id": 153272, "text": "We implement baselines and our approach under Transformerbase and Transformerbig settings based on the open-source toolkit fairseq [ Ott et al. , , 2019 ] with mixed precision [ Ott et al. , , 2018 ] .", "Comments": [], "label": [[123, 130, "Software-Entity"]]}
{"id": 153274, "text": "All the experiments are conducted on 8 NVIDIA Tesla V100 GPUs , and each batch on each GPU contains approximately 4096 tokens .", "Comments": [], "label": [[37, 38, "Device-Count"], [39, 61, "Hardware-device"]]}
{"id": 153276, "text": "We use * multibleu.perl * to calculate case-sensitive BLEU for WMT14 En-De and * SacreBLEU * [ 6 ] to calculate case-sensitive BLEU for WMT19 Zh-En .", "Comments": [], "label": [[81, 90, "Software-Entity"]]}
{"id": 153279, "text": "Thus we set scale to 0.1 in our following experiments . 4.4 Baseline Systems We implement our approach based on the Transformer [ Vaswani et al. , , 2017 ] and compare it with some mainstream adaptive training methods ( detailed hyperparameter settings are provided in Appendix [ C ] .", "Comments": [], "label": [[116, 127, "Software-Entity"]]}
{"id": 153296, "text": "[ 6 ] We use sentencepiece [ Kudo and Richard ] [ son , 2018 ] to tokenize text into byte-pair encodings Other metrics such as regular ( asymmetric ) KL divergence or JS divergence can also be used in [ 4 ] , but we find that symmetrized KL divergence yields the best results .", "Comments": [], "label": [[13, 26, "Software-Entity"]]}
{"id": 153299, "text": "Following previous work [ Vaswani et al. , , 2017 ] [ Nguyen et al. , , 2019 ] [ Xu et al. , 2021 ] , we compute tokenized BLEU with multi_bleu.perl [ 8 ] for IWSLT14 and TED datasets , additionally apply compound-splitting for WMT14 En-De [ 9 ] and SacreBLEU [ 10 ] [ Post , 2018 ] for IWSLT17 datasets .", "Comments": [], "label": [[250, 259, "Software-Entity"]]}
{"id": 153320, "text": "For training the on WMT14 En→De dataset [ 21 ] , we use Transformer Base config , dubbed transformer_wmt_en_de in fairseq toolkit , with 6 layers of encoder and decoder with 8 attention heads , embedding size of 512 , feed-forward size of 2048 , dropout 0.1 .", "Comments": [], "label": [[114, 121, "Software-Entity"]]}
{"id": 153322, "text": "The transformer_iwslt_de_en models ( for IWSLT14 , IWSLT17 and TED datasets ) were run on 2 Titan RTX GPUs while the transformer_wmt_en_de model for WMT14 En-De was run on 8 A6000 GPUs .", "Comments": [], "label": [[90, 91, "Device-Count"], [92, 106, "Hardware-device"], [172, 173, "Device-Count"], [174, 184, "Hardware-device"]]}
{"id": 153346, "text": "On PTB , it takes only 4.5 hours to train the model using BERT as the encoder with a single Titan V GPU .", "Comments": [], "label": [[85, 91, "Device-Count"], [92, 103, "Hardware-device"]]}
{"id": 153347, "text": "Table [ 5 ] shows the speed comparison on parsing the PTB test set ( we report values based on a single Titan V GPU and not using BERT as encoder following [ Nguyen et al. , 2021a ] ) .", "Comments": [], "label": [[97, 103, "Device-Count"], [104, 115, "Hardware-device"]]}
{"id": 153353, "text": "All experiments are conducted with an NVIDIA GeForce RTX 2080 Ti .", "Comments": [], "label": [[38, 64, "Hardware-device"]]}
{"id": 153375, "text": "* * Evaluation Datasets and Metrics * * : For evaluating translation systems we use the test sets available in the respective corpora and use SacreBLEU [ Post , 2018 ] as the metric .", "Comments": [], "label": [[142, 151, "Software-Entity"]]}
{"id": 153379, "text": "* * F Reproducibility * * * * Compute Infrastructure * * : We use V100 ( 32 GB ) GPU for training the mBERT models and use TPU v3-8 for training the mT5 models .", "Comments": [], "label": [[66, 70, "Hardware-device"], [73, 78, "Device-Memory"], [123, 131, "Hardware-device"]]}
{"id": 153383, "text": "Given text from a mix of high and low Webresource languages ( HRL and LRL , respectively ) , Byte Pair Encoding ( BPE ) [ Sennrich et al. , , 2016 ] and its variants like Wordpiece [ Schuster and Naka ] [ jima , 2012 ] and Sentencepiece [ Kudo and Richard ] [ son , 2018 ] prefer frequent tokens , most of those from the HRLs .", "Comments": [], "label": [[171, 180, "Software-Entity"], [223, 236, "Software-Entity"]]}
{"id": 153388, "text": "Variants like Wordpiece [ Schuster and Nakajima , 2012 ] and Sentencepiece [ Kudo and Richardson , 2018 ] either build on top of BPE or follow a very similar process . [ Kudo , 2018 ] is a variant method that chooses tokens based on unigram LM score .", "Comments": [], "label": [[14, 23, "Software-Entity"], [61, 74, "Software-Entity"]]}
{"id": 153400, "text": "We use libindic 's indictrans library [ Bhat et al. , 2015 ] for transliteration .", "Comments": [], "label": [[7, 29, "Software-Entity"]]}
{"id": 153401, "text": "MLM pre-training was done on Google v3-8 Cloud TPUs where 10K iterations required 2.1 TPU hours .", "Comments": [], "label": [[29, 51, "Hardware-device"]]}
{"id": 153415, "text": "BPE vocabulary does not capture the tokens corresponding to Punjabi as it is a LRL and will thus tokenize Niyukata into multiple tokens which do not captures its meaning whereas Niyukata when tokenized by OBPE tokenizer will contain Niyuk which captures most of the meaning of the token Niyukata whose representation will be learnt when pretraining using Punjabi monolingual data A.4 Replicability BERT configuration parameters used in our experiments are as follows : `` attention_probs_dropout_prob '' : 0.1 , `` hidden_act '' : `` gelu '' , `` hidden_dropout_prob '' : 0.1 , `` hidden_size '' : 768 , `` initializer_range '' : 0.02 , `` intermediate_size '' : 3072 , `` max_position_embeddings '' : 512 , `` num_attention_heads '' : 12 , `` num_hidden_layers '' : 12 , `` type_vocab_size '' : 2 , `` vocab_size '' : 30000 All the task-specific fine-tuning experiments are done using GPUs on Google Colaboratory where each fine-tuning experiment requires 2 GPU hours .", "Comments": [], "label": [[894, 913, "Hardware-device"]]}
{"id": 153427, "text": "We train our models on a server with four 4x Tesla V100 w/NVLink and 32GB memory .", "Comments": [], "label": [[37, 41, "Device-Count"], [42, 44, "Device-Count"], [45, 55, "Hardware-device"], [69, 80, "Device-Memory"]]}
{"id": 153428, "text": "The time efficiency experiment is conducted on the server with Intel Xeon E5-2698v4 2.2Ghz 20-core .", "Comments": [], "label": [[63, 90, "Hardware-device"]]}
{"id": 153445, "text": "For PGN-based methods , we use pretrained Chinese word vectors provided by Tencent [ 8 ] , and the vocabulary size is 10,000 .", "Comments": [], "label": [[75, 82, "Software-Entity"]]}
{"id": 153449, "text": "The metrics include traditional n-gram overlapping metrics , such as ROUGE [ Lin and Hovy , 2002 ] , BLEU [ Pap ] [ ineni et al. , , 2002 ] , and distributed representation matching metrics , including BERTScore [ Zhang et al. , 2020 ] and MoverScore [ Zhao et al. , , 2019 ] .", "Comments": [], "label": [[240, 250, "Software-Entity"]]}
{"id": 153450, "text": "We use files2rouge toolkit to calculate the F1 score of ROUGE-1 , ROUGE-2 , ROUGE-L .", "Comments": [], "label": [[7, 18, "Software-Entity"]]}
{"id": 153457, "text": "For MC dataset , we use jieba [ 14 ] tool to split sentences into words for constructing the vocabulary .", "Comments": [], "label": [[24, 29, "Software-Entity"]]}
{"id": 153459, "text": "All the PGN-based models are run on an NVIDIA TITAN Xp , and all the BERTAbs-based models are run on an NVIDIA RTX3090 .", "Comments": [], "label": [[39, 54, "Hardware-device"], [104, 118, "Hardware-device"]]}
{"id": 153460, "text": "For ROUGE metrics , we use the files2rouge [ 15 ] toolkit with the default parameters .", "Comments": [], "label": [[31, 42, "Software-Entity"]]}
{"id": 153463, "text": "For Mover-Score , we use moverscore-v2 [ 17 ] and the bert-basechinese pretrained model for obtaining representations .", "Comments": [], "label": [[25, 35, "Software-Entity"]]}
{"id": 153481, "text": "A Implementation Details For multimodal chat translation , we utilize the standard Transformer-Base architecture [ Vaswani et al. , , 2017 ] .", "Comments": [], "label": [[83, 94, "Software-Entity"]]}
{"id": 153485, "text": "The experiments are conducted using 4 NVIDIA Tesla V100 GPUs , which gives us about 4 * 4096 tokens per update .", "Comments": [], "label": [[36, 37, "Device-Count"], [38, 60, "Hardware-device"]]}
{"id": 153487, "text": "The experiments are conducted on an NVIDIA Tesla V100 GPU and the batch size is set to 64 .", "Comments": [], "label": [[36, 57, "Hardware-device"]]}
{"id": 153490, "text": "After filtering , we apply BPE [ Sennrich et al. , , 2016 ] with 32K merge operations to obtain subwords .", "Comments": [], "label": [[27, 30, "Software-Entity"]]}
{"id": 153511, "text": "We also re-run the vanilla Transformer using our Pytorch implementation .", "Comments": [], "label": [[49, 56, "Software-Entity"]]}
{"id": 153523, "text": "On one A100 GPU , training the generator takes around 24 hours and generating the samples takes roughly 35 hours for each dataset .", "Comments": [], "label": [[3, 6, "Device-Count"], [7, 15, "Hardware-device"]]}
{"id": 153536, "text": "We use the open-source packages for implementation , including HuggingFace Datasets [ 1 ] and Transformers [ 2 ] .", "Comments": [], "label": [[63, 74, "Software-Entity"], [94, 106, "Software-Entity"]]}
{"id": 153537, "text": "All the experiments are conducted on 16 GPU chips ( 32 GB V100 ) .", "Comments": [], "label": [[37, 43, "Device-Count"], [52, 57, "Device-Memory"], [58, 62, "Hardware-device"]]}
{"id": 153539, "text": "These results also verify the data efficiency of our method , TACO . 4.3 Results on BERT-Base We also compare TACO with MLM on base-sized models , which are the most commonly used models according to the download data from Huggingface [ 4 ] [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[223, 234, "Software-Entity"]]}
{"id": 153547, "text": "We then launched main annotation tasks on Amazon Mechanical Turk ( MTurk ) that were available < http : //www.crowdaq.com/ > The only change for a new domain is instructions and exams for quantity typing , which have to be domain-specific .", "Comments": [], "label": [[42, 74, "Cloud-Platform"]]}
{"id": 153549, "text": "4 Model Quantity recognition is a typical span selection problem and we use the standard token classification model based on BERT ( large , cased ) [ Devlin et al. , 2019 ] that comes with HuggingFace [ Wolf et al. , 2020 ] .", "Comments": [], "label": [[189, 200, "Software-Entity"]]}
{"id": 153564, "text": "All parameters are tuned on the development set as described in [ §5 . ] Experiments on average finish in 3 hours on a single Nvidia RTX 8000 GPU .", "Comments": [], "label": [[119, 125, "Device-Count"], [126, 145, "Hardware-device"]]}
{"id": 153586, "text": "Thus , the discriminator loss can be written as : In summary , the overall training objective function J consists of the aforementioned four losses : Here , Θ denotes all trainable parameters , while λe , λd , λr , λ are regularization weights to help Figure 3 : t-SNE visualization for the top 88 clusters of sentence semantics when threshold is 0.95 .", "Comments": [], "label": [[263, 268, "Software-Entity"]]}
{"id": 153589, "text": "Figure [ 3 ] gives a t-SNE visualization [ Van der Maaten and Hinton , 2008 ] of the top 88 clusters if setting the similarity threshold to 0.95 .", "Comments": [], "label": [[21, 26, "Software-Entity"]]}
{"id": 153593, "text": "We conduct all experiments on NVIDIA Quadro RTX 6000 GPUs . 4.3 Evaluation Metrics We conduct our evaluation from three perspectives – explanation generation performance , text–image matching performance , and rating prediction performance .", "Comments": [], "label": [[30, 57, "Hardware-device"]]}
{"id": 153602, "text": "To make these posts ready for annotating , we use HarvestText [ 3 ] to clean them and segment the resulting texts into sentences .", "Comments": [], "label": [[50, 61, "Software-Entity"]]}
{"id": 153603, "text": "We choose the doccano [ 7 ] to build up our annotation platform , and let these annotators be familiar with our task by the expert-annotated examples . 2.3 Crowd Annotation When all crowd workers are ready , we start the crowd annotation phase .", "Comments": [], "label": [[14, 21, "Software-Entity"]]}
{"id": 153612, "text": "All experiments are conducted on a single RTX 2080 Ti card at an 8-GPU server with a 14 core CPU and 128GB memory .", "Comments": [], "label": [[35, 41, "Device-Count"], [42, 53, "Hardware-device"], [65, 77, "Device-Count"], [85, 96, "Device-Count"], [101, 113, "Device-Memory"]]}
{"id": 153619, "text": "There are also mature dense retrieval libraries , such as FAISS [ Johnson et al. , , 2017 ] .", "Comments": [], "label": [[58, 63, "Software-Entity"]]}
{"id": 153634, "text": "The first stage Condenser pretraining takes roughly a week on 4 RTX 2080 Ti GPUs or 2 days on a v3-8 cloud TPU .", "Comments": [], "label": [[62, 63, "Device-Count"], [64, 80, "Hardware-device"], [96, 110, "Hardware-device"]]}
{"id": 153637, "text": "The second stage takes roughly 2 days on 4 RTX 2080 Ti GPUs or 19 hours on a v3-8 cloud TPU .", "Comments": [], "label": [[41, 42, "Device-Count"], [43, 59, "Hardware-device"], [77, 91, "Hardware-device"]]}
{"id": 153638, "text": "Our GPU implementations are based on Pytorch [ Paszke et al. , , 2019 ] and TPU implementations on JAX [ Bradbury et al. , , 2018 ] .", "Comments": [], "label": [[37, 44, "Software-Entity"], [99, 102, "Software-Entity"]]}
{"id": 153644, "text": "The authors would like to thank Google 's TPU Research Cloud ( TRC ) for access to Cloud TPUs and the anonymous reviewers for the reviews .", "Comments": [], "label": [[32, 68, "Cloud-Platform"]]}
{"id": 153647, "text": "All the experiments are conducted in Pytorch and on GeForce GTX 2080Ti GPUs .", "Comments": [], "label": [[37, 44, "Software-Entity"], [52, 75, "Hardware-device"]]}
{"id": 153662, "text": "In Fig . [ 3 , ] we show the t-SNE visualization of clusters with embeddings learned by two strongest baselines and our methods .", "Comments": [], "label": [[29, 34, "Software-Entity"]]}
{"id": 153665, "text": "In Fig . [ 6 ] and Fig . [ 7 , ] we show the t-SNE visualization of clusters on BANKING and M-CID with embeddings learned by two strongest baselines and our methods .", "Comments": [], "label": [[45, 50, "Software-Entity"]]}
{"id": 153666, "text": "Figure 6 : t-SNE visulization of embeddings on BANKING .", "Comments": [], "label": [[11, 16, "Software-Entity"]]}
{"id": 153667, "text": "Figure 7 : t-SNE visulization of embeddings on M-CID .", "Comments": [], "label": [[11, 16, "Software-Entity"]]}
{"id": 153675, "text": "Statistics for the full dataset are provided in Table [ 4 . ] Annotation Task Interface We use the Amazon Mechanical Turk ( MTurk ) crowdsourcing platform . [ 10 ] We provide Figure [ 2 ] in the Appendix to show the layout of our annotation task .", "Comments": [], "label": [[99, 131, "Cloud-Platform"]]}
{"id": 153685, "text": "[ 13 ] We use the HuggingFace Transformers library [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[18, 29, "Software-Entity"]]}
{"id": 153686, "text": "[ 14 ] [ 15 ] We use publicly available implementations for all metrics ( nltk [ 16 ] for BLEU ) .", "Comments": [], "label": [[74, 78, "Software-Entity"]]}
{"id": 153689, "text": "We compute these using scikit-learn : [ https : // ] ( https : //scikit-learn.org/stable/index.html ) [ scikit-learn.org/stable/index.html ] ( https : //scikit-learn.org/stable/index.html ) For measuring likelihood of spread , predicted and averaged values are rounded to the nearest integer .", "Comments": [], "label": [[23, 35, "Software-Entity"]]}
{"id": 153703, "text": "A.3 Experimental Setup and Model Hyperparameters All models are trained on either a single Quadro RTX 8000 or TITAN Xp GPU .", "Comments": [], "label": [[84, 90, "Device-Count"], [91, 106, "Hardware-device"], [110, 122, "Hardware-device"]]}
{"id": 153741, "text": "We also thank Google Cloud Compute , as well as OpenAI .", "Comments": [], "label": [[14, 26, "Cloud-Platform"], [48, 54, "Software-Entity"]]}
{"id": 153744, "text": "Inference is conducted on Quadro RTX 8000 GPUs and costs about 200 GPU hours in total .", "Comments": [], "label": [[26, 46, "Hardware-device"]]}
{"id": 153745, "text": "Our method is implemented with PyTorch and the Huggingface Transformers library .", "Comments": [], "label": [[31, 38, "Software-Entity"], [47, 58, "Software-Entity"]]}
{"id": 153758, "text": "3.1 Tokenizer We use a Byte-Pair Encoding ( BPE ) tokenizer for SMILES tokenization , as is shown by Chithrananda * et al .", "Comments": [], "label": [[23, 49, "Software-Entity"]]}
{"id": 153764, "text": "As a result , the final loss function is as follows , We pre-train MM-Deacon on 80 V100 GPUs for 10 epochs ( 15 hours in total ) with a 16 batch size on each GPU using AdamW optimizer with a learning rate of 10− .", "Comments": [], "label": [[80, 82, "Device-Count"], [83, 92, "Hardware-device"]]}
{"id": 153771, "text": "MM-Deacon fingerprints of paired drugs are concatenated and fed into a multi-layer perceptron ( MLP ) network implemented by scikit-learn [ Pedregosa et al. , , 2011 ] for binary classification .", "Comments": [], "label": [[125, 137, "Software-Entity"]]}
{"id": 153782, "text": "Following Dixon et al . ( 2018 ) , we express the classifier 's bias against comments or commenters harmlessly mentioning an identity term as the FPR roberta-base from HuggingFace ( Wolf et al. , 2020 ) . 3 See Appendix D for more details .", "Comments": [], "label": [[168, 179, "Software-Entity"]]}
{"id": 153787, "text": "• Fine-tuning a single model for either task takes from 4-6 hours on single NVIDIA Tesla V100 16GB GPU .", "Comments": [], "label": [[69, 75, "Device-Count"], [76, 93, "Hardware-device"], [94, 102, "Device-Memory"]]}
{"id": 153829, "text": "All experiments were run on machines with a single CPU and a single Tesla V100 GPU .", "Comments": [], "label": [[44, 54, "Device-Count"], [61, 73, "Device-Count"], [74, 82, "Hardware-device"]]}
{"id": 153831, "text": "Embedding layers were initialized with the Keras default `` uniform '' Keras initializer ( uniform random distribution in the range [ −0.05 , 0.05 ] ) .", "Comments": [], "label": [[43, 48, "Software-Entity"]]}
{"id": 153832, "text": "Dense layers were initialized also with the Keras default Glorot initializer ( uniform random distribution with mean 0 and standard deviation p 2/ ( f an_in + f an_out ) ) [ Glorot and ] [ Bengio , 2010 ] .", "Comments": [], "label": [[44, 49, "Software-Entity"]]}
{"id": 153833, "text": "While these details might not seem that important , we were unable to reproduce some of the results reported above using a re-implementation of the Transformer model in Flax , which used different defaults ( and layer normalization before each sublayer rather than after ) unless we changed these implementation details to match those of the Keras implementation .", "Comments": [], "label": [[342, 347, "Software-Entity"]]}
{"id": 153846, "text": "We use the Hugging-Face PyTorch implementation [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[11, 23, "Software-Entity"], [24, 31, "Software-Entity"]]}
{"id": 153853, "text": "We download SST-2 , MR , CR , SST-5 , and SUBJ from [ Gao et al. , 2021 ] , while the rest of the datasets are downloaded from the HuggingFace Datasets library [ Lhoest et al. , ] [ 2021b , ] [ a ] .", "Comments": [], "label": [[131, 142, "Software-Entity"]]}
{"id": 153854, "text": "Computing infrastructure We run all the experiments on one NVIDIA A100 with 40G of memory .", "Comments": [], "label": [[55, 58, "Device-Count"], [59, 70, "Hardware-device"], [76, 89, "Device-Memory"]]}
{"id": 153855, "text": "Training hyper-parameters We set the maximum sequence length based on the recommended values in the HuggingFace repository [ Wolf et al. , , 2020 ] and prior work [ Min et al. , , 2021 ] [ Schick and Schütze , 2021b ] , i.e. , we set it to 256 for SUBJ , CR , CB , RTE , and WiC , and 128 for other datasets .", "Comments": [], "label": [[100, 111, "Software-Entity"]]}
{"id": 153895, "text": "All experiments are conducted on NVIDIA RTX 2080 Ti with 11GB memory with a maximum batch size of 4 .", "Comments": [], "label": [[33, 51, "Hardware-device"], [57, 68, "Device-Memory"]]}
{"id": 153917, "text": "We use the uncased wordpiece tokenizer of BERT with 30k vocabulary .", "Comments": [], "label": [[19, 28, "Software-Entity"]]}
{"id": 153933, "text": "The models are trained on 64 V100 GPUs for 200K steps with batch size of 1024 and maximum sequence length of 512 , which takes about 2.5 days for GLMLarge .", "Comments": [], "label": [[26, 28, "Device-Count"], [29, 38, "Hardware-device"]]}
{"id": 153936, "text": "The hyperparameters for all the pre-training settings are summarized in Table [ 7 . ] A.3 Implementation Our pretraining implementation is based on Megatron-LM [ Shoeybi et al. , , 2019 ] and Deep-Speed [ Rasley et al. , , 2020 ] .", "Comments": [], "label": [[148, 159, "Software-Entity"], [192, 202, "Software-Entity"]]}
{"id": 153947, "text": "Results of T5Large on XSum are obtained by running the summarization script provided by Huggingface transformers [ 6 ] .", "Comments": [], "label": [[88, 99, "Software-Entity"]]}
{"id": 153955, "text": "Since we use WordPiece tokenization , a word can be split into several subword units .", "Comments": [], "label": [[13, 22, "Software-Entity"]]}
{"id": 153960, "text": "All claims are further annotated by crowd workers on Amazon Mechanical Turk ( Mturk ) .", "Comments": [], "label": [[53, 85, "Cloud-Platform"]]}
{"id": 153963, "text": "Substitution We perform three types of substitutions : For 1 ) Context and knowledge-based entity substitution , we first run SpaCy NER tagging [ Hon ] [ nibal and Montani , 2017 ] on a response u from WoW .", "Comments": [], "label": [[126, 131, "Software-Entity"]]}
{"id": 153968, "text": "Finally , we use two methods - SpaCy 's word2vec similarity [ 3 ] and BM25 similarity [ 4 ] to rank the top 10 evidence sentences using each method .", "Comments": [], "label": [[31, 39, "Software-Entity"]]}
{"id": 153971, "text": "1 ) * Lexical overlap * calculates the maximum word overlap between a claim and all evidence sentences after removing punctuation and stopwords using SpaCy .", "Comments": [], "label": [[150, 155, "Software-Entity"]]}
{"id": 153972, "text": "It uses the AllenNLP constituency parser [ Gardner et al. , , 2018 ] to extract potential entities from the claims .", "Comments": [], "label": [[12, 20, "Software-Entity"]]}
{"id": 153985, "text": "2.3 Annotation process The corpus was annotated with BIO encoding using Doccano [ Nakayama et al. , , 2018 ] by a native speaker of Spanish with a background in linguistic annotation ( see Appendix [ C ] .", "Comments": [], "label": [[72, 79, "Software-Entity"]]}
{"id": 153989, "text": "All models were trained using an AMD 2990WX CPU and a single RTX 2080 Ti GPU . 3.1 Conditional random field model As baseline model , we evaluated a CRF model with handcrafted features from [ Álvarez Mellado , 2020b ] .", "Comments": [], "label": [[33, 47, "Hardware-device"], [54, 60, "Device-Count"], [61, 76, "Hardware-device"]]}
{"id": 153990, "text": "The model was built using pycrfsuite [ Peng and Korobov , 2014 ] , a Python wrapper for crfsuite [ Okazaki , 2007 ] that implements CRF for labeling sequential data .", "Comments": [], "label": [[26, 36, "Software-Entity"], [88, 96, "Software-Entity"]]}
{"id": 153991, "text": "The model also uses the Token and Span utilities from spaCy library [ Honnibal and Montani , 2017 ] .", "Comments": [], "label": [[54, 59, "Software-Entity"]]}
{"id": 153992, "text": "The following handcrafted binary features from [ Álvarez Mellado , 2020b ] were used for the model : – Bias : active on all tokens to set per-class bias – Token : the string of the token – Uppercase : active if the token is all uppercase – Titlecase : active if only the first character of the token is capitalized – Character trigram : an active feature for every trigram contained in the token – Quotation : active if the token is any type of quotation mark ( ' ' `` `` `` « » ) – Suffix : last three characters of the token – POS tag : part-of-speech tag of the token provided by spaCy utilities – Word shape : shape representation of the token provided by spaCy utilities – Word embedding : provided by Spanish word2vec 300 dimensional embeddings by [ Cardellino , 2019 ] , one feature per dimension – URL : active if the token could be validated as a URL according to spaCy utilities – Email : active if the token could be validated as an email address by spaCy utilities – Twitter : active if the token could be validated as a possible Twitter special token : hashtag or @ username A window of two tokens in each direction was used for feature extraction .", "Comments": [], "label": [[583, 588, "Software-Entity"], [660, 665, "Software-Entity"], [873, 878, "Software-Entity"], [961, 966, "Software-Entity"]]}
{"id": 153993, "text": "3.2 Transformer-based models We evaluated two Transformer-based models : `` ` – BETO base cased model : a monolingual BERT model trained for Spanish ( Cañete et al. , 2020 ) `` ` – mBERT : multilingual BERT , trained on Wikipedia in 104 languages [ Devlin et al. , , 2019 ] Both models were run using the Transformers library by HuggingFace [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[329, 340, "Software-Entity"]]}
{"id": 153995, "text": "All of our BiLSTM-CRF models were built using Flair [ Akbik et al. , , 2018 ] with default hyperparameters ( hidden size = 256 , learning rate = 0.1 , mini batch size = 32 , max number of epochs = 150 ) and embeddings provided by Flair .", "Comments": [], "label": [[46, 51, "Software-Entity"], [230, 235, "Software-Entity"]]}
{"id": 153996, "text": "3.3.1 Preliminary embedding experiments We first ran exploratory experiments on the development set with different types of embeddings using Flair tuning functionalities .", "Comments": [], "label": [[141, 146, "Software-Entity"]]}
{"id": 153997, "text": "We explored the following embeddings : Transformer embeddings ( mBERT and BETO ) , fastText embeddings [ Bo ] [ janowski et al. , , 2017 ] , one-hot embeddings , byte pair embeddings [ Heinzerling and Strube , 2018 ] , and character embeddings [ Lample et al. , , 2016 ] .", "Comments": [], "label": [[83, 91, "Software-Entity"]]}
{"id": 154002, "text": "As before , we ran a BiLSTM-CRF model using Flair , but instead of using the unadapted Transformer embeddings , we used codeswitch embeddings [ Sarker , 2020 ] , fine-tuned Transformer-based embeddings pretrained for language identification on the Spanish-English section of the LinCE dataset [ Aguilar et al. , , 2020 ] .", "Comments": [], "label": [[44, 49, "Software-Entity"]]}
{"id": 154014, "text": "LIST [ Ribeiro et al. , , 2020 ] , CrossCheck [ Arendt et al. , 2021 ] , and AllenNLP Interpret [ Wallace et al. , 2019 ] have successfully been applied to tasks like machine comprehension and relation extraction [ Alt et al. , , 2020 ] .", "Comments": [], "label": [[0, 4, "Software-Entity"], [35, 45, "Software-Entity"], [77, 85, "Software-Entity"]]}
{"id": 154027, "text": "D Computational Budget The GTT ( BERT ) model on the MUC-4 dataset took 1 hour and 21 minutes to train and around 11 minutes to test on Google Colab ( GPU ) .", "Comments": [], "label": [[136, 148, "Hardware-device"]]}
{"id": 154028, "text": "The GTT ( BERT ) model on the ProMED dataset took around 24 minutes to train and 4 minutes to test , while the GTT ( SciBERT ) model on the ProMED dataset took around 13 minutes to train and 4 minutes to test , both on Google Colab ( GPU ) .", "Comments": [], "label": [[219, 239, "Hardware-device"]]}
{"id": 154029, "text": "The DyGIE++ ( BERT ) model on the ProMED dataset took around 50 minutes to train , while the DyGIE++ ( SciBERT ) model on the ProMED dataset took around 1 hour and 30 minutes to train , both on a NVIDIA V100 GPU .", "Comments": [], "label": [[196, 211, "Hardware-device"]]}
{"id": 154030, "text": "For the SciREX dataset , it took around 10-20 minutes to run the GTT ( BERT ) and GTT ( SciBERT ) models on a NVIDIA V100 GPU .", "Comments": [], "label": [[110, 125, "Hardware-device"]]}
{"id": 154031, "text": "The DyGIE++ ( BERT ) model took around 2 hours to train , while the DyGIE++ ( SciBERT ) model took around 4 hours to train , both on a NVIDIA V100 GPU .", "Comments": [], "label": [[135, 150, "Hardware-device"]]}
{"id": 154040, "text": "( * iii * ) * Selection of relevant paragraphs : * We asked crowdworkers on Amazon Mechanical Turk to label how relevant each paragraph is to its chart .", "Comments": [], "label": [[76, 98, "Cloud-Platform"]]}
{"id": 154064, "text": "A.3 Chart-to-text Baseline Models The experiments are done on our machine ( CPU : Intel ( R ) Xeon ( R ) Gold 6240 CPU @ 2.60GHz , GPU : 4 × NVIDIA GTX 2080Ti ) .", "Comments": [], "label": [[82, 128, "Hardware-device"], [137, 138, "Device-Count"], [141, 158, "Hardware-device"]]}
{"id": 154084, "text": "We consider the application-level maximum similarity score , denoted as Ns , doc , and mean similarity score generated by ElasticSearch [ NV ] as handcrafted features .", "Comments": [], "label": [[122, 135, "Software-Entity"]]}
{"id": 154086, "text": "The models are trained on a single Nvidia Quadro RTX 8000 GPU .", "Comments": [], "label": [[28, 34, "Device-Count"], [35, 61, "Hardware-device"]]}
{"id": 154087, "text": "The relatively small performance gain of using monotonic regularization may also be attributed to the compromised precision of the novelty feature due to the use of the ElasticSearch pre-filter for the sake of computational costs .", "Comments": [], "label": [[169, 182, "Software-Entity"]]}
{"id": 154098, "text": "For technical ease we require that the models use the same tokenizer , and that the pretrained checkpoints are available through the HuggingFace transformers library [ Wolf et al. , , 2019 ] .", "Comments": [], "label": [[133, 144, "Software-Entity"]]}
{"id": 154111, "text": "Pretraining ParaBLEUlarge takes ∼ 10h on a 16 A100 GPU machine .", "Comments": [], "label": [[43, 45, "Device-Count"], [46, 54, "Hardware-device"]]}
{"id": 154112, "text": "Fine-tuning takes ∼ 8h on a single A100 GPU machine . 4.1 Results ParaBLEU results on WMT17 are given in Table [ 1 , ] along with a number of baselines described in Section [ 2.3 ] .", "Comments": [], "label": [[28, 34, "Device-Count"], [35, 43, "Hardware-device"]]}
{"id": 154120, "text": "The experiments are conducted on a server with a 3090 GPU card and Ubuntu operating system .", "Comments": [], "label": [[49, 57, "Hardware-device"]]}
{"id": 154126, "text": "For bi-encoder models , we can precompute the entity embeddings and retrieve top-k entities efficiently with the help of fast similarity search tools like Faiss [ Johnson et al. , , 2021 ] .", "Comments": [], "label": [[155, 160, "Software-Entity"]]}
{"id": 154132, "text": "Models are trained with batch size 1024 on 4 V100 GPUs .", "Comments": [], "label": [[43, 44, "Device-Count"], [45, 54, "Hardware-device"]]}
{"id": 154138, "text": "5.5 Entity Visualization Figure 3 : 2-D visualization of the entity embeddings from the Wikidata5M-Trans dataset with t-SNE [ Maaten and Hinton , 2008 ] .", "Comments": [], "label": [[118, 123, "Software-Entity"]]}
{"id": 154150, "text": "This can be easily implemented by detach ( ) function of Pytorch [ Paszke et al. , , 2019 ] .", "Comments": [], "label": [[57, 64, "Software-Entity"]]}
{"id": 154151, "text": "In practice , ˜x can be easily obtained from x using the detach ( ) function of Pytorch [ Paszke et al. , , 2019 ] .", "Comments": [], "label": [[80, 87, "Software-Entity"]]}
{"id": 154160, "text": "All the experiments were conducted with a single GPU on our machine ( GPU : NVIDIA A40 ) and from single run .", "Comments": [], "label": [[42, 52, "Device-Count"], [76, 86, "Hardware-device"]]}
{"id": 154170, "text": "Following [ Liang et al. , 2021d ] , we use SacreBLEU [ 10 ] [ Post , 2018 ] and TER [ Snover et al. , 2006 ] with the statistical significance test [ Koehn , 2004 ] for fair comparison .", "Comments": [], "label": [[44, 53, "Software-Entity"]]}
{"id": 154185, "text": "All experiments in three stages are conducted utilizing 8 NVIDIA Tesla V100 GPUs , which gives us about 8 * 4096 tokens per update for all experiments .", "Comments": [], "label": [[56, 57, "Device-Count"], [58, 80, "Hardware-device"]]}
{"id": 154192, "text": "https : // [ www.sec.gov ] ( https : //www.sec.gov/edgar/ ) /edgar/ www.xbrl.us/ [ xbrl-taxonomy ] ( www.xbrl.us/xbrl-taxonomy/2020-us-gaap/ ) /2020-us-gaap/ Table 2 : finer-139 statistics , using spaCy 's tokenizer and the 139 tags of this work ( ± standard deviation ) .", "Comments": [], "label": [[197, 205, "Software-Entity"]]}
{"id": 154196, "text": "We split chronologically the remaining sentences into training , development , and test sets with an 80/10/10 ratio ( Table [ 2 ] . 4 Baseline Models spaCy [ Honnibal et al. , , 2020 ] is an open-source nlp library .", "Comments": [], "label": [[150, 155, "Software-Entity"], [207, 214, "Software-Entity"]]}
{"id": 154197, "text": "We trained spaCy 's ner from scratch on finer-139 . bilstm : This baseline uses a stacked bidirectional Long-Short Term Memory ( lstm ) network [ Graves et al. , , 2013 ] [ Lample et al. , , 2016 ] with residual connections .", "Comments": [], "label": [[11, 19, "Software-Entity"]]}
{"id": 154200, "text": "Thus , it becomes more difficult for subword models to avoid nonsensical sequences of token labels , We use nltk [ Bird et al. , , 2009 ] for sentence splitting .", "Comments": [], "label": [[108, 112, "Software-Entity"]]}
{"id": 154201, "text": "We used spaCy v.2.3 ; see https : // [ spacy.io ] ( https : //spacy.io/ ) / .", "Comments": [], "label": [[8, 13, "Software-Entity"]]}
{"id": 154208, "text": "A Experimental Setup For spaCy , we followed the recommended practices .", "Comments": [], "label": [[25, 30, "Software-Entity"]]}
{"id": 154209, "text": "All other methods were implemented in tensorflow .", "Comments": [], "label": [[38, 48, "Software-Entity"]]}
{"id": 154210, "text": "Concerning bert models , we used the implementation of huggingface [ Wolf et al. , 2020 ] .", "Comments": [], "label": [[55, 66, "Software-Entity"]]}
{"id": 154212, "text": "All final hyper-parameters are shown in Table [ 7 . ] Training was performed mainly on a dgx station with 4 nvidia v gpus and an Intel Xeon cpu e5-2698 v4 @ 2.20ghz .", "Comments": [], "label": [[106, 121, "Device-Count"], [129, 164, "Hardware-device"]]}
{"id": 154235, "text": "• Hardware : CPU : AMD Ryzen Threadripper 3970X GPU : GeForce RTX 3090 , four cards Memory : 192GB • Software : OS : Ubuntu 20.04.2 LTS ( kernel 5.4.0-80 ) CUDA : 11.1 Graphic Driver : 460.73.01 Python : 3.8.10 ( with virtualenv ) • Python libraries : PyTorch 1.8.1+cu111 transformers 4.6.1 ( for ELECTRA ) torch-optimizer 0.1.0 ( for LAMB ) ray 1.3.0 ( for hyperparameter search with ray [ tune ] ) bleurt git+https : //github.com/ google-research/bleurt Commit c6f2375 ( Oct 15th , 2021 ; for BLEURT ) tensorflow 2.7.0 ( for BLEURT ) numpy 1.21.0 scipy 1.7.0 sympy 1.8 pycocoevalcap 1.2 pycocotools 2.0.2 zss 1.2.0 discriminator .", "Comments": [], "label": [[19, 47, "Hardware-device"], [54, 70, "Hardware-device"], [93, 98, "Device-Memory"], [156, 160, "Software-Entity"], [252, 259, "Software-Entity"], [272, 284, "Software-Entity"], [307, 322, "Software-Entity"], [342, 345, "Software-Entity"], [504, 514, "Software-Entity"], [536, 541, "Software-Entity"], [549, 554, "Software-Entity"], [561, 566, "Software-Entity"], [571, 584, "Software-Entity"], [589, 600, "Software-Entity"], [607, 610, "Software-Entity"]]}
{"id": 154251, "text": "4.2 Expert Component Configurations We use a Huggingface pre-trained bert-baseuncased model as our MLM for yielding Emlm and also providing the proposal distribution in our MH MCMC sampler .", "Comments": [], "label": [[45, 56, "Software-Entity"]]}
{"id": 154252, "text": "We could have used any pre-trained attribute classifier from Huggingface for Edisc , but we keep those aside to use as external attribute classifiers for fair evaluation against baselines .", "Comments": [], "label": [[61, 72, "Software-Entity"]]}
{"id": 154258, "text": "We feed our generated test sentences to a Huggingface [ Radford et al. , , 2019 ] pre-trained GPT-2 xl model , and report its perplexity ( PPL ) , as an automatic measure of fluency .", "Comments": [], "label": [[42, 53, "Software-Entity"]]}
{"id": 154260, "text": "To have a fair comparison , we report accuracy using external classifiers from Huggingface ( textattack/bert-base-uncased- yelp-polarity [ Morris et al. , , 2020 ] for sentiment and cointegrated/robertabase-formality for formality ) .", "Comments": [], "label": [[79, 90, "Software-Entity"]]}
{"id": 154261, "text": "Clsf . * show the accuracy of the discriminator used in the energy/external discriminator from Huggingface .", "Comments": [], "label": [[95, 106, "Software-Entity"]]}
{"id": 154270, "text": "B.2 Expert Component Configurations We use a Huggingface pre-trained bert-baseuncased model as our MLM for yielding Emlm and also providing the proposal distribution in our MH MCMC sampler .", "Comments": [], "label": [[45, 56, "Software-Entity"]]}
{"id": 154271, "text": "Although we could have used any pre-trained attribute classifier from a model repository like Huggingface for Edisc , we train our own classifier for controlled empirical comparison .", "Comments": [], "label": [[94, 105, "Software-Entity"]]}
{"id": 154272, "text": "As described later , we do use pretrained Huggingface attribute classifiers as external attribute classifiers for fair evaluation against baselines .", "Comments": [], "label": [[42, 53, "Software-Entity"]]}
{"id": 154273, "text": "For experiments in which we add the BertScore [ Zhang et al. , , 2020 ] component to the energy , we download the pre-trained robertalarge_L17 models from Huggingface , respectively .", "Comments": [], "label": [[155, 166, "Software-Entity"]]}
{"id": 154274, "text": "* show the accuracy of the discriminator used in the energy/external discriminator from Huggingface .", "Comments": [], "label": [[88, 99, "Software-Entity"]]}
{"id": 154277, "text": "We feed our generated test sentences to a Huggingface [ Radford et al. , , 2019 ] pre-trained GPT-2 xl model , and report its perplexity ( PPL ) , as an automatic measure of fluency .", "Comments": [], "label": [[42, 53, "Software-Entity"]]}
{"id": 154279, "text": "To create a more fair comparison , we also report classification accuracy using external classifiers , downloaded from Huggingface .", "Comments": [], "label": [[119, 130, "Software-Entity"]]}
{"id": 154280, "text": "For sentiment classification we use textattack/bert-base-uncasedyelp-polarity [ Morris et al. , , 2020 ] , and for formality we use cointegrated/robertabase-formality .", "Comments": [], "label": [[36, 46, "Software-Entity"]]}
{"id": 154282, "text": "For BLEURT , we use pre-trained Elron/bleurt-base-512 from Huggingface .", "Comments": [], "label": [[59, 70, "Software-Entity"]]}
{"id": 154284, "text": "B.10 Human Evaluations We used Amazon Mechanical Turk for our evaluations , where each HIT was a two choice question of `` which sentence is more fluent ? '' and the providers were paid $ 0.1 per HIT .", "Comments": [], "label": [[31, 53, "Cloud-Platform"]]}
{"id": 154288, "text": "We used an in-house 4GPU server ( NVIDIA RTX2080 ) , and the samplings and hyperparameter tuning took an overall of around 10-14 full days on the 4 GPUs .", "Comments": [], "label": [[20, 24, "Device-Count"], [34, 48, "Hardware-device"]]}
{"id": 154292, "text": "B Libraries The code for the experiments is written in Python 3.8 and relies on the following libraries : keras ( 2.7.0 ) , numpy ( 1.19.5 ) , pandas ( 1.2.3 ) , scikitlearn ( 1.0.1 ) , sentence_trasformers ( 1.1.0 ) , spacy ( 3.0.5 ) , tensorflow ( 2.5.0 ) , torch ( 1.8.1 ) , transformers ( 4.5.1 ) .", "Comments": [], "label": [[106, 111, "Software-Entity"], [124, 129, "Software-Entity"], [143, 149, "Software-Entity"], [162, 173, "Software-Entity"], [186, 206, "Software-Entity"], [237, 247, "Software-Entity"], [260, 265, "Software-Entity"], [278, 290, "Software-Entity"]]}
{"id": 154293, "text": "C Infrastructure All experiments were conducted on virtual machines ( VM ) deployed on the cloud computing platform Microsoft Azure .", "Comments": [], "label": [[116, 131, "Cloud-Platform"]]}
{"id": 154304, "text": "We use * fastText * classifiers [ Joulin et al. , , 2017 ] for every dataset .", "Comments": [], "label": [[9, 17, "Software-Entity"]]}
{"id": 154311, "text": "We would also like to acknowledge the support of the NExT research grant funds , supported by the National Research Foundation , Prime Ministers Office , Singapore under its IRC @ SG Funding Initiative , and to gratefully acknowledge the support of NVIDIA Corporation with the donation of the GeForce GTX Titan XGPU used in this research .", "Comments": [], "label": [[293, 315, "Hardware-device"]]}
{"id": 154316, "text": "The statistics of the datasets are presented in Table [ 2 . ] Implementation Details We employ PyTorch [ Paszke et al. , , 2019 ] for all the experiments .", "Comments": [], "label": [[95, 102, "Software-Entity"]]}
{"id": 154317, "text": "We adopt HuggingFace Transformers [ Wolf et al. , , 2020 ] library 's implementation of pretrained T5- Large [ Raffel et al. , , 2020 ] as the base model .", "Comments": [], "label": [[9, 20, "Software-Entity"]]}
{"id": 154319, "text": "We adopt the corpus BLEU in NLTK [ Loper and ] [ Bird , 2002 ] following [ Chawla and Yang , 2020 ] .", "Comments": [], "label": [[28, 32, "Software-Entity"]]}
{"id": 154328, "text": "All our experiments are conducted on NVIDIA A100 ( 40GB ) GPUs .", "Comments": [], "label": [[37, 48, "Hardware-device"], [51, 55, "Device-Memory"]]}
{"id": 154340, "text": "Our contrastive pretraining does not require the MLM head or any additional external knowledge , and can be completed in less than one minute on 2 × 2080Ti GPUs .", "Comments": [], "label": [[145, 146, "Device-Count"], [149, 160, "Hardware-device"]]}
{"id": 154364, "text": "We conducted experiments with a V100 GPU .", "Comments": [], "label": [[32, 40, "Hardware-device"]]}
{"id": 154365, "text": "The FLOPs for our model and the baselines were calculated with Tensorflow and batch size=1 .", "Comments": [], "label": [[63, 73, "Software-Entity"]]}
{"id": 154375, "text": "As will be discussed in Section [ 3.1 , ] we propose Figure 2 : T-SNE visualization of the most frequent 500 word embeddings , of the full-precision and different 2-bit quantized models trained on PTB dataset .", "Comments": [], "label": [[64, 69, "Software-Entity"]]}
{"id": 154383, "text": "For each downstream task with our proposed method , we first fine-tune a full-precision network using the pre-trained checkpoint from huggingface [ 1 ] for both GPT-2 and BART .", "Comments": [], "label": [[134, 145, "Software-Entity"]]}
{"id": 154385, "text": "We train each task with 8 V100 GPUs based on the Pytorch framework .", "Comments": [], "label": [[24, 25, "Device-Count"], [26, 35, "Hardware-device"], [49, 56, "Software-Entity"]]}
{"id": 154425, "text": "The pre-training of dialogue generation is carried out on 32 Nvidia Telsa V100 32G GPU ( 4 nodes ) for 6 epochs , taking about 5 days to reach convergence .", "Comments": [], "label": [[58, 60, "Device-Count"], [61, 86, "Hardware-device"]]}
{"id": 154426, "text": "Mixed precision training is also adopted for efficiently training and inference , and we use the Fairseq [ Ott et al. , , 2019 ] framework to conduct all experiments .", "Comments": [], "label": [[97, 104, "Software-Entity"]]}
{"id": 154428, "text": "We used package WordPiece [ Devlin et al. , , 2019 ] for tokenization .", "Comments": [], "label": [[16, 25, "Software-Entity"]]}
{"id": 154445, "text": "Such improvements over PURE indicate the effectiveness of modeling the interrelation between the same-subject or the sameobject entity pairs in the training process . 4.4 Inference Speed In this section , we compare the models ' inference speed on an A100 GPU with a batch size of 32 .", "Comments": [], "label": [[251, 259, "Hardware-device"]]}
{"id": 154470, "text": "We run our experiments on a machine with four NVIDIA 1080-TI GPUs and two Intel Xeon E5-2620 v4 CPUs .", "Comments": [], "label": [[41, 45, "Device-Count"], [46, 65, "Hardware-device"], [70, 73, "Device-Count"], [74, 100, "Hardware-device"]]}
{"id": 154479, "text": "Suppose all the passage embeddings are fixed and stored in memory as M ∈ R × where D is the hidden dimension : For an input question q , DPR applies another BERTbased question encoder to obtain its representation Q , then it builds on FAISS [ Johnson et al. , , 2019 ] to conduct fast dot-product similarity search between Q and M , and returns N ( N ≪ N ) passages with the highest similarity scores .", "Comments": [], "label": [[235, 240, "Software-Entity"]]}
{"id": 154482, "text": "All our experiments are conducted on 8 Tesla A100 40GB GPUs .", "Comments": [], "label": [[37, 38, "Device-Count"], [39, 49, "Hardware-device"], [50, 54, "Device-Memory"]]}
{"id": 154488, "text": "Our implementation is based on the HuggingFace Transformers library [ Wolf et al. , , 2019 ] .", "Comments": [], "label": [[35, 46, "Software-Entity"]]}
{"id": 154536, "text": "The entire model has been implemented on top of PyTorch Lightning [ 6 ] .", "Comments": [], "label": [[48, 65, "Software-Entity"]]}
{"id": 154537, "text": "For all experiments , we have used an NVIDIA Quadro RTX 6000 with 24 GB of memory .", "Comments": [], "label": [[38, 60, "Hardware-device"], [66, 81, "Device-Memory"]]}
{"id": 154538, "text": "Accessible via the Hugging Face Datasets Python package : [ https : //github .", "Comments": [], "label": [[19, 31, "Software-Entity"]]}
{"id": 154555, "text": "We employ the BERT model ( bert-uncased , with 12-layer transformer ) implemented by Huggingface Transformers [ 4 ] and adopt most of its suggested hyperparameters for finetuning .", "Comments": [], "label": [[85, 96, "Software-Entity"]]}
{"id": 154557, "text": "ALL experiments were conducted in the Nvidia Ge-Force RTX-2080 Graphical Card with 11G graphical memory . 4.5 Main Results The results in BANKING and StackOverflow are presented in Table [ 1 , ] where the best results are highlighted in bold .", "Comments": [], "label": [[38, 62, "Hardware-device"]]}
{"id": 154558, "text": "5 Analysis Figure 4 : t-SNE visualization of deep learned features of some intent samples in CLINC-FULL .", "Comments": [], "label": [[22, 27, "Software-Entity"]]}
{"id": 154559, "text": "5.1 Feature Visualization To compare our method with the previous methods intuitively , we use t-SNE [ Van der Maaten and Hin ] [ ton , 2008 ] to visualize deep features of some intent samples sampled from CLINC-FULL learned by BERT , SCL , and our method .", "Comments": [], "label": [[95, 100, "Software-Entity"]]}
{"id": 154572, "text": "For fair comparison , we run all experiments on 2 Tesla V100 with 32G memory .", "Comments": [], "label": [[48, 49, "Device-Count"], [50, 60, "Hardware-device"], [66, 76, "Device-Memory"]]}
{"id": 154573, "text": "We implement GNNs [ https : //huggingface.co/microsoft/ ] ( https : //huggingface.co/microsoft/codebert-base ) [ codebert-base ] ( https : //huggingface.co/microsoft/codebert-base ) Table 1 : The statistics of datasets based on PyTorch Geometric [ Fey and Lenssen , 2019 ] .", "Comments": [], "label": [[228, 245, "Software-Entity"]]}
{"id": 154587, "text": "Implementation Details We use Hugging Face Transformers [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[30, 42, "Software-Entity"]]}
{"id": 154592, "text": "Online experiments with SQUAD , HotpotQA , NQ , and NewsQA take 2–4h each on one NVIDIA GeForce RTX 2080 Ti ; 2.5–6h for offline .", "Comments": [], "label": [[77, 80, "Device-Count"], [81, 107, "Hardware-device"]]}
{"id": 154593, "text": "For TriviaQA and SearchQA , each online simulation experiment on one NVIDIA TITAN RTX takes 4–9.5h ; 9–20h for offline . 5 Online Learning We simulate a scenario where only a limited amount of supervised data is available , and the model mainly learns from explicit user feedback on predicted answers .", "Comments": [], "label": [[65, 68, "Device-Count"], [69, 85, "Hardware-device"]]}
{"id": 154613, "text": "We thus store for every item in the memory the vector encoding by the DPR model ( whereas in the FAISS approach this dense vector is approximated instead ) .", "Comments": [], "label": [[97, 102, "Software-Entity"]]}
{"id": 154619, "text": "B Data Collection & Data Quality B.1 Data Collection & Quality Control Crowdsourced Data Collection The data collection lasted for around 6 months and in total over 1000 crowdworkers who are English-speaking annotators located in the United States were recruited and compensated through the Amazon Mechanical Turk platform .", "Comments": [], "label": [[291, 313, "Cloud-Platform"]]}
{"id": 154622, "text": "All the fine-tuned models are trained with a maximum of eight 32GB GPUs ( NVIDIA V100 ) , optimized with Adam using β = 0.9 , β = 0.999 , ϵ = 1e − 08 .", "Comments": [], "label": [[56, 61, "Device-Count"], [62, 71, "Device-Memory"], [74, 85, "Hardware-device"]]}
{"id": 154635, "text": "We first use spaCy [ Honnibal and ] [ Johnson , 2015 ] to build a dependency parse for the expression .", "Comments": [], "label": [[13, 18, "Software-Entity"]]}
{"id": 154654, "text": "Most of these preliminary experiments were performed using the area threshold mentioned in [ §4.3 . ] E.6 Description of Computing Infrastructure We primarily used a machine with Quadro RTX 8000 GPUs , Google Cloud machines with V100 GPUs , and a machine with TITAN RTX and GeForce 2080s .", "Comments": [], "label": [[179, 199, "Hardware-device"], [202, 214, "Cloud-Platform"], [229, 238, "Hardware-device"], [260, 269, "Hardware-device"], [274, 287, "Hardware-device"]]}
{"id": 154661, "text": "4.1 Data Sources For swh pre-training data we use : ( i ) the `` Language Modeling Data for Swahili '' dataset [ Shikali ] [ and Refuoe , 2019 ] hosted on Hugging Face ( which we refer to as the `` HF Swahili '' data set ) ; and ( ii ) the ALFFA speech dataset [ Gelas et al. , , 2012 ] .", "Comments": [], "label": [[155, 167, "Software-Entity"]]}
{"id": 154668, "text": "We use the Hugging Face transformers library [ Wolf et al. , 2020 ] to train all models .", "Comments": [], "label": [[11, 23, "Software-Entity"]]}
{"id": 154669, "text": "Because of the small size of the NER data set used during fine-tuning , we enabled Hugging Face 's early stopping callback for all downstream training runs .", "Comments": [], "label": [[83, 98, "Software-Entity"]]}
{"id": 154670, "text": "Average of at least three trials per experiment , calculated with the scikit-learn library . [ Pedregosa et al. , , 2011 ] Table 2 : Prediction of entity types and precise locations ( NER3 ) .", "Comments": [], "label": [[70, 82, "Software-Entity"]]}
{"id": 154671, "text": "Subword tokenization techniques , such as Byte-Pair Encodings ( BPE ) [ Sennrich et al. , , 2016 ] [ Gage , 1994 ] , or character-based word segmentation techniques might help in detecting and exploiting repeating patterns within the phonetic representation .", "Comments": [], "label": [[42, 69, "Software-Entity"]]}
{"id": 154673, "text": "In combination with [ Lhoest et al. , 2021 ] from Hugging Face , GNU Parallel significantly accelerated pre-processing and phone transcription .", "Comments": [], "label": [[50, 62, "Software-Entity"]]}
{"id": 154675, "text": "Each run lasted no more than 1-2 hoursfor finetuning , but generally much longer for pretraining ( on the order of a day ) , and only consumed one GPU resource at a time ( either an A100 or P100 ) .", "Comments": [], "label": [[143, 150, "Device-Count"], [182, 186, "Hardware-device"], [190, 194, "Hardware-device"]]}
{"id": 154676, "text": "This computation sums up to around 5-6 GPU-weeks on the A100 , about one gpu-week on the Titan RTX , and several compute-days each for the other GPUs .", "Comments": [], "label": [[56, 60, "Hardware-device"], [89, 98, "Hardware-device"]]}
{"id": 154677, "text": "Additional exploratory work and debugging consumed another few GPU-days on Google Colab .", "Comments": [], "label": [[75, 87, "Hardware-device"]]}
{"id": 154686, "text": "This requires an alternative to the ubiquitous BPE tokenization , through which exact sub-word lexical units ( i.e . morphemes ) are used .", "Comments": [], "label": [[47, 50, "Software-Entity"]]}
{"id": 154702, "text": "Table 2 : Summary of the pre-training corpus . 3.1 Pre-training details KinyaBERT model was implemented using Pytorch version 1.9 .", "Comments": [], "label": [[110, 117, "Software-Entity"]]}
{"id": 154704, "text": "Pre-training was performed using RTX 3090 and RTX 2080Ti desktop GPUs .", "Comments": [], "label": [[33, 41, "Hardware-device"], [46, 56, "Hardware-device"]]}
{"id": 154705, "text": "Each KinyaBERT model takes on average 22 hours to train for 1000 steps on one RTX 3090 GPU or 29 hours on one RTX 2080Ti GPU .", "Comments": [], "label": [[74, 77, "Device-Count"], [78, 90, "Hardware-device"], [106, 109, "Device-Count"], [110, 124, "Hardware-device"]]}
{"id": 154706, "text": "Baseline models ( BERTBP E and BERTMORP HO ) were pre-trained on cloud tensor processing units ( TPU v3-8 devices each with 128 GB memory ) using PyTorch/XLA [ 3 ] package and a TPU-optimized fairseq toolkit [ Ott et al. , , 2019 ] .", "Comments": [], "label": [[97, 105, "Hardware-device"], [124, 137, "Device-Memory"], [146, 153, "Software-Entity"], [154, 157, "Software-Entity"], [192, 199, "Software-Entity"]]}
{"id": 154708, "text": "The baselines were trained on TPU because there were no major changes needed to the existing RoBERTA ( base ) architecture implemented in fairseq and the TPU resources were available and efficient .", "Comments": [], "label": [[138, 145, "Software-Entity"]]}
{"id": 154727, "text": "Finally , all experiments are conducted on a single NVIDIA RTX A5000 24G GPU with a total time for fine-tuning all models being under 24 hours .", "Comments": [], "label": [[45, 51, "Device-Count"], [52, 68, "Hardware-device"], [69, 76, "Device-Memory"]]}
{"id": 154777, "text": "We trained all models with batches of size 8 for 20 epochs on 1 Nvidia GeForce RTX 2080 Ti or 1 Nvidia GeForce GTX 1080 Ti GPU with ≈ 11 Gb of memory , using the Adam optimizer [ Kingma and Ba , 2015 ] .", "Comments": [], "label": [[62, 63, "Device-Count"], [64, 90, "Hardware-device"], [94, 95, "Device-Count"], [96, 126, "Hardware-device"], [134, 149, "Device-Memory"]]}
{"id": 154781, "text": "We fine-tuned GPT-2 small with a batch size of 1 on 1 Nvidia GeForce RTX 2080 Ti or 1 Nvidia GeForce GTX 1080 Ti GPU with ≈ 11 Gb of memory , using the Adam optimizer [ Kingma and Ba , 2015 ] for 1 epoch with a learning rate of 5 × 10− for the GPT-2 parameters and a learning rate of 2.5 × 10− for the LTM parameters .", "Comments": [], "label": [[52, 53, "Device-Count"], [54, 80, "Hardware-device"], [84, 85, "Device-Count"], [86, 116, "Hardware-device"], [124, 139, "Device-Memory"]]}
{"id": 154784, "text": "We fine-tuned GPT-2 small with a batch size of 1 on 1 Nvidia GeForce RTX 2080 Ti or 1 Nvidia GeForce GTX 1080 Ti GPU with ≈ 11 Gb of memory , using the Adam optimizer [ Kingma and Ba , 2015 ] with a linearly decayed learning rate of 5 × 10− , for 5 epochs .", "Comments": [], "label": [[52, 53, "Device-Count"], [54, 80, "Hardware-device"], [84, 85, "Device-Count"], [86, 116, "Hardware-device"], [124, 139, "Device-Memory"]]}
{"id": 154791, "text": "We trained all models , from scratch , with batches of size 40 for 250,000 steps on 1 Nvidia Titan RTX or 1 Nvidia Quadro RTX 6000 with ≈ 24 GPU Gb of memory using the Adam optimizer [ Kingma and Ba , 2015 ] with a learning rate of 2.5 × 10− .", "Comments": [], "label": [[84, 85, "Device-Count"], [86, 102, "Hardware-device"], [106, 107, "Device-Count"], [108, 130, "Hardware-device"], [138, 157, "Device-Memory"]]}
{"id": 154798, "text": "We used the stop word list from the scikitlearn package .", "Comments": [], "label": [[36, 47, "Software-Entity"]]}
{"id": 154804, "text": "A Model Specifications All of our models are binary RoBERTa-based classifiers trained with the default settings of the Trainer module from the Huggingface library for 3 training epochs , on a Tesla V100-SXM2 GPU machine , batch size of 16 , warm-up steps of 500 and weight decay of 0.01 .", "Comments": [], "label": [[143, 154, "Software-Entity"], [192, 211, "Hardware-device"]]}
{"id": 154810, "text": "The best result for each dataset is reported . 4.2 Attack Methods and Evaluation Metrics Three well-received attack methods are leveraged via TextAttack [ Morris et al. , , 2020 ] for an extensive comparison between our proposed method and baseline algorithms .", "Comments": [], "label": [[142, 152, "Software-Entity"]]}
{"id": 154811, "text": "We train our models on NVIDIA RTX 3090 and RTX 2080Ti GPUs , depending on the volume of the dataset involved .", "Comments": [], "label": [[23, 38, "Hardware-device"], [43, 58, "Hardware-device"]]}
{"id": 154813, "text": "For a fair comparison , every model of each dataset is trained on single NVIDIA RTX 2080Ti GPU with the same batch size , among which models on SST-2 are trained with a batch size of 32 while QNLI Figure 4 : Loss and Aua % ( accuracy under attack ) of BERT trained on SST-2 under different methods .", "Comments": [], "label": [[66, 72, "Device-Count"], [73, 94, "Hardware-device"]]}
{"id": 154823, "text": "All models in [ §4.1 ] are trained with 1 NVIDIA 2080Ti , models in [ §4.2 ] are trained with 1 NVIDIA 40GB A100 GPU .", "Comments": [], "label": [[40, 41, "Device-Count"], [42, 55, "Hardware-device"], [94, 95, "Device-Count"], [103, 107, "Device-Memory"], [108, 116, "Hardware-device"]]}
{"id": 154827, "text": "Setup We use OpenNMT [ Klein et al. , , 2017 ] to build Euclidean Transformer and our Lorentz one .", "Comments": [], "label": [[13, 20, "Software-Entity"]]}
{"id": 154835, "text": "To validate this , we perform a probing on TRANSFORMER , HATT and HYBONET obtained in [ §4.2.1 . ] We use dependency parsing result of stanza [ Qi et al. , , 2020 ] on IWSLT'14 English Table 4 : The probing results on dependency tree constructed from the IWSLT'14 English corpus . corpus as our dataset .", "Comments": [], "label": [[135, 141, "Software-Entity"]]}
{"id": 154840, "text": "B.2 Machine Translation For WMT'14 , we use the preprocessing script provided by OpenNMT [ 7 ] .", "Comments": [], "label": [[81, 88, "Software-Entity"]]}
{"id": 154841, "text": "For IWSLT'14 , we clean and partition the dataset with script provided by FairSeq [ 8 ] .", "Comments": [], "label": [[74, 81, "Software-Entity"]]}
{"id": 154844, "text": "We use PyTorch as the neural networks ' framework .", "Comments": [], "label": [[7, 14, "Software-Entity"]]}
{"id": 154846, "text": "C.3 Machine Translation Our code is based on OpenNMT 's Transformer [ Klein et al. , , 2017 ] .", "Comments": [], "label": [[45, 55, "Software-Entity"]]}
{"id": 154853, "text": "For each sentence x , we use an open-source library * spaCy * [ 1 ] to identify the noun phrases , which is denoted as P = ( p i 1 , p i 2 , ... , p i ti ) , where t is the number of noun phrases in x .", "Comments": [], "label": [[54, 59, "Software-Entity"]]}
{"id": 154860, "text": "For all the above datasets , all sentences are tokenized and segmented into subwords units using byte-pair encoding ( BPE ) [ Sennrich et al. , , 2016 ] .", "Comments": [], "label": [[97, 123, "Software-Entity"]]}
{"id": 154867, "text": "Different from previous work , we use sacreBLEU [ 2 ] [ Post , 2018 ] to compute the BLEU [ Papineni et al. , , 2002 ] scores and the statistical significance of translation results with paired bootstrap resampling [ Koehn , 2004 ] for future standard comparison across papers .", "Comments": [], "label": [[38, 47, "Software-Entity"]]}
{"id": 154869, "text": "All models are trained and evaluated using 2 RTX3090 GPUs .", "Comments": [], "label": [[43, 44, "Device-Count"], [45, 57, "Hardware-device"]]}
{"id": 154870, "text": "We implement the translation model based on * fairseq * [ 4 ] [ Ott et al. , , 2019 ] .", "Comments": [], "label": [[46, 53, "Software-Entity"]]}
{"id": 154885, "text": "PLMs employ sub-word tokenization mechanisms such as WordPiece or Byte-Pair Encoding ( BPE ) for the purposes of minimizing Out-Of-Vocabulary words [ Sennrich et al. , , 2016 ] .", "Comments": [], "label": [[53, 62, "Software-Entity"], [66, 92, "Software-Entity"]]}
{"id": 154890, "text": "[ 3 ] Model We used the Transformers training framework of Huggingface [ Wolf et al. , , 2020 ] and trained two different models — a * small * model with 6 hidden layers learned from the Oscar portion of our dataset , and a * base * model with 12 hidden layers which was trained on the entire dataset .", "Comments": [], "label": [[59, 70, "Software-Entity"]]}
{"id": 154892, "text": "We trained AlephBERTbase over the entire dataset on an NVidia DGX server with 8 V100 GPUs which took 8 days .", "Comments": [], "label": [[78, 79, "Device-Count"], [80, 89, "Hardware-device"]]}
{"id": 154893, "text": "AlephBERTsmall was trained over the Oscar portion only , using 4 GTX 2080ti GPUs taking 5 days in total . 4 The Morphological Extraction Model Modern Hebrew is a Semitic language with rich morphology and complex orthography .", "Comments": [], "label": [[63, 64, "Device-Count"], [65, 80, "Hardware-device"]]}
{"id": 154917, "text": "The model was trained on a single NVIDIA Tesla V100 SXM2 16 GB GPU .", "Comments": [], "label": [[27, 33, "Device-Count"], [34, 56, "Hardware-device"], [57, 66, "Device-Memory"]]}
{"id": 154926, "text": "Each run takes 7.68 GPU hours on 4 GeFource RTX 2080 Ti GPUs . 5.2 Evaluation Evaluation of the generated definitions mainly focuses on two aspects , i.e. , accuracy and simplicity .", "Comments": [], "label": [[33, 34, "Device-Count"], [35, 60, "Hardware-device"]]}
{"id": 154933, "text": "Using these texts , we conducted a user study on Amazon Mechanical Turk .", "Comments": [], "label": [[49, 71, "Cloud-Platform"]]}
{"id": 154941, "text": "Due to GPU memory constraints ( 24GB , Quadro RTX 6000 ) , we train our model with a batch size of 1 .", "Comments": [], "label": [[32, 36, "Device-Memory"], [39, 54, "Hardware-device"]]}
{"id": 154944, "text": "As part of our crowd-sourced user study on Amazon Mechanical Turk to collect these coherence judgements , we do not collect any personal information from the participants .", "Comments": [], "label": [[43, 65, "Cloud-Platform"]]}
{"id": 154964, "text": "4.3 Implementation Our implementation is based on Pytorch [ Paszke et al. , 2019 ] and transformers .", "Comments": [], "label": [[50, 57, "Software-Entity"], [87, 99, "Software-Entity"]]}
{"id": 154965, "text": "We train NLSSum on one Tesla V100 GPU for 100,000 steps ( 2 days ) with a batch size of 4 and gradient accumulation every two steps .", "Comments": [], "label": [[19, 22, "Device-Count"], [23, 37, "Hardware-device"]]}
{"id": 154979, "text": "We choose one of its snapshots [ 4 ] , and use the pre-trained language detection model from fasttext [ Joulin et al. , , 2017 ] to filter out non-English pages .", "Comments": [], "label": [[93, 101, "Software-Entity"]]}
{"id": 154981, "text": "We initialize MarkupLM from RoBERTa and train it for 300K steps on 8 NVIDIA A100 GPUs .", "Comments": [], "label": [[67, 68, "Device-Count"], [69, 85, "Hardware-device"]]}
{"id": 154982, "text": "We also apply FP16 , gradient-checkpointing [ Chen et al. , 2016 ] , and deepspeed [ Rasley et al. , , 2020 ] to reduce GPU memory consumption and accelerate training .", "Comments": [], "label": [[73, 82, "Software-Entity"]]}
{"id": 155006, "text": "The BLEU score is calculated using the SACREBLEU toolkit [ Post , 2018 ] . [ 6 ] Baselines .", "Comments": [], "label": [[39, 48, "Software-Entity"]]}
{"id": 155009, "text": "All our models are trained on a machine with 8 RTX 3090Ti GPUs .", "Comments": [], "label": [[45, 46, "Device-Count"], [47, 62, "Hardware-device"]]}
{"id": 155014, "text": "We implement our models on top of the THUMT [ Tan et al. , , 2020 ] toolkit and the Transformers library [ Wolf et al. , , 2020 ] . 4.2 Main Results Table [ 1 ] shows the results for the Ro-En , En-De , and En-Zh translation tasks .", "Comments": [], "label": [[84, 96, "Software-Entity"]]}
{"id": 155029, "text": "The mGPT model is trained using the Megatron-LM toolkit [ Shoeybi et al. , 2019 ] [ 10 ] with the default GPT-2 configuration on the mC4 dataset [ Xue et al. , , 2021 ] , [ 11 ] which contains massive web crawled data covering 101 languages .", "Comments": [], "label": [[36, 47, "Software-Entity"]]}
{"id": 155032, "text": "Preprocessing like tokenization is done automatically with the * sentencepiece * program .", "Comments": [], "label": [[65, 78, "Software-Entity"]]}
{"id": 155046, "text": "Our code is modified based on Huggingface [ * ] with python 3.7 and pytorch 1.7.0 , run on 8 P100 GPUs , each with a 16GB memory .", "Comments": [], "label": [[30, 41, "Software-Entity"], [68, 75, "Software-Entity"], [91, 92, "Device-Count"], [93, 102, "Hardware-device"], [117, 128, "Device-Memory"]]}
{"id": 155052, "text": "Common to all models is a large shared subword vocabulary created using either BPE [ Sen ] [ nrich et al. , , 2016 ] or SentencePiece [ Kudo and ] [ Richardson , 2018 ] tokenization .", "Comments": [], "label": [[79, 82, "Software-Entity"], [120, 133, "Software-Entity"]]}
{"id": 155056, "text": "It uses a shared vocabulary consisting of 250k subwords , created using SentencePiece [ Kudo and Richardson , 2018 ] tokenization .", "Comments": [], "label": [[72, 85, "Software-Entity"]]}
{"id": 155061, "text": "All finetuning is done using the Huggingface Transformers library [ Wolf et al. , , 2020 ] with up to two Nvidia V100 GPUs .", "Comments": [], "label": [[33, 44, "Software-Entity"], [102, 105, "Device-Count"], [106, 122, "Hardware-device"]]}
{"id": 155064, "text": "Table [ 3 . ] We use fairseq [ Ott et al. , , 2019 ] to implement all translation models . [ 5 ] Translate-train For the translate-train approach , the Spanish training data provided by XNLI is translated into each target language .", "Comments": [], "label": [[21, 28, "Software-Entity"]]}
{"id": 155081, "text": "We further train a sentencepiece [ Kudo ] [ and Richardson , 2018 ] tokenizer based on such flattened code token sequences with vocabulary size 20,000 .", "Comments": [], "label": [[19, 32, "Software-Entity"]]}
{"id": 155098, "text": "The model is implemented with PyTorch-1.9.0 and Huggingface-transformer-4.12.3 [ 2 ] .", "Comments": [], "label": [[30, 37, "Software-Entity"], [48, 59, "Software-Entity"], [60, 71, "Software-Entity"]]}
{"id": 155099, "text": "DISCO is trained with Java SMALL and C SMALL for 24 hours in total with two 32GB NVIDIA Tesla V100 GPUs , using batch size of 128 with max sequence length of 256 tokens and batch [ https : //github.com/ ] ( https : //github.com/huggingface/transformers/tree/c439752482759c94784e11a87dcbf08ce69dccf3 ) [ huggingface/transformers/tree/ ] ( https : //github.com/huggingface/transformers/tree/c439752482759c94784e11a87dcbf08ce69dccf3 ) [ c439752482759c94784e11a87dcbf08ce69dccf3 ] ( https : //github.com/huggingface/transformers/tree/c439752482759c94784e11a87dcbf08ce69dccf3 ) Table 6 : Common Weakness Enumeration ( CWE ) types covered by our bug injection heuristic Table 7 : Examples for tokens and their AST node types Table 8 : Details of downstream tasks datasets .", "Comments": [], "label": [[72, 75, "Device-Count"], [76, 80, "Device-Memory"], [81, 103, "Hardware-device"]]}
{"id": 155121, "text": "Models are implemented in Pytorch and trained for 100 epochs on a 3.60GHz AMD Ryzen 7 Windows desktop with NVIDIA RTX 3090 GPU and 64GB RAM .", "Comments": [], "label": [[26, 33, "Software-Entity"], [66, 83, "Hardware-device"], [107, 126, "Hardware-device"], [131, 139, "Device-Memory"]]}
{"id": 155127, "text": "Models are implemented in Pytorch and trained for 100 epochs on a 3.60GHz AMD Ryzen 7 Windows desktop with NVIDIA RTX 3090 GPU and 64GB RAM .", "Comments": [], "label": [[26, 33, "Software-Entity"], [66, 83, "Hardware-device"], [107, 126, "Hardware-device"], [131, 139, "Device-Memory"]]}
{"id": 155137, "text": "We apply Moses tokenization [ Sennrich et al. , , 2016b ] for most languages , and for other languages , we use KyTea [ 5 ] < https : //github.com/linzehui/mRASP > < https : //commoncrawl.org/ > [ https : //www.loc.gov/standards/ ] ( https : //www.loc.gov/standards/iso639-2/php/code_list.php ) [ iso639-2/php/code_list.php ] ( https : //www.loc.gov/standards/iso639-2/php/code_list.php ) < http : //www.phontron.com/kytea/ > Table 2 : Comprehensive comparison with mRASP and mBART .", "Comments": [], "label": [[9, 14, "Software-Entity"], [112, 117, "Software-Entity"]]}
{"id": 155141, "text": "Our model is trained on 32 Nvidia V100 GPUs for 300K steps , The batch size on each GPU is 4096 tokens , and we set the value of update frequency to 8 .", "Comments": [], "label": [[24, 26, "Device-Count"], [34, 43, "Hardware-device"]]}
{"id": 155163, "text": "For BART models , we use a batch size of 8 , 50 warm-up steps , a weight decay of 0.01 , and we fine-tune up to 5 epochs ( with fine-tuning taking approximately 6 hours on a single NVIDIA V100 GPU ) .", "Comments": [], "label": [[174, 180, "Device-Count"], [181, 196, "Hardware-device"]]}
{"id": 155168, "text": "3.3 Crowdsourcing Finally , to construct actual extractive summaries for evaluation in these domains , we presented the user intents to annotators on Amazon Mechanical Turk .", "Comments": [], "label": [[150, 172, "Cloud-Platform"]]}
{"id": 155183, "text": "We use the cased variant of bert-base-cased available through HuggingFace [ Wolf et al. , , 2019 ] instead of uncased and do not lowercase the dataset during preparation .", "Comments": [], "label": [[62, 73, "Software-Entity"]]}
{"id": 155184, "text": "For fine-tuning AOSUMM on the modified CNN/DM dataset , the training completes in 8 hours on a single NVIDIA Quadro RTX 8000 .", "Comments": [], "label": [[95, 101, "Device-Count"], [102, 124, "Hardware-device"]]}
{"id": 155190, "text": "The full data splits will be published with our code . 3.2 Implementation Details All models are trained with batch size of 32 on 2 NVIDIA A100 GPUs for 6 epochs .", "Comments": [], "label": [[130, 131, "Device-Count"], [132, 148, "Hardware-device"]]}
{"id": 155200, "text": "We formalize this mining-and-pruning process as a crowdsourcing task ( on Amazon Mechanical Turk ) , where we present each turker with a batch of 200 tables and ask them to pick out suitable tables from that batch .", "Comments": [], "label": [[74, 96, "Cloud-Platform"]]}
{"id": 155201, "text": "Collecting explanations : We use the Amazon Mechanical Turk ( AMT ) platform to collect explanations for CLUES-Real .", "Comments": [], "label": [[37, 67, "Cloud-Platform"]]}
{"id": 155203, "text": "The positive correlation ( ρ = 0.17 ) suggests that students tend Statistics in Table [ 2a ] was obtained using the spacy tokenizer .", "Comments": [], "label": [[116, 121, "Software-Entity"]]}
{"id": 155208, "text": "The mutual information between the features and the label was computed using the [ scikit-learn ] ( https : //scikit-learn.org/stable/ ) package with a random state of 624 .", "Comments": [], "label": [[83, 95, "Software-Entity"]]}
{"id": 155210, "text": "E.2 Model parameters E.3 Hyper-parameter settings For all the transformer based models we use the implementation of HuggingFace library [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[116, 127, "Software-Entity"]]}
{"id": 155211, "text": "All the model based hyper-parameters are thus kept default to the settings in the HuggingFace library .", "Comments": [], "label": [[82, 93, "Software-Entity"]]}
{"id": 155213, "text": "For RoBERTa based baselines we use 'roberta-base ' checkpoint available on HuggingFace .", "Comments": [], "label": [[75, 86, "Software-Entity"]]}
{"id": 155217, "text": "E.4 Hardware and software specifications All the models are coded using Pytorch 1.4.0 [ 14 ] [ Paszke et al. , , 2019 ] and related libraries like numpy [ Harris et al. , , 2020 ] , scipy [ Jones et al. , ] [ 2001– ] etc .", "Comments": [], "label": [[72, 79, "Software-Entity"], [147, 152, "Software-Entity"], [182, 187, "Software-Entity"]]}
{"id": 155218, "text": "We run all experiments on one of the following two systems - ( 1 ) GeForce RTX 2080 GPU of size 12 GB , 256 GB RAM and 40 CPU cores ( 2 ) Tesla V100-SXM2 GPU of size 16GB , 250 GB RAM and 40 CPU cores .", "Comments": [], "label": [[67, 87, "Hardware-device"], [96, 101, "Device-Memory"], [104, 114, "Device-Memory"], [119, 131, "Device-Memory"], [138, 157, "Hardware-device"], [166, 170, "Device-Memory"], [173, 183, "Device-Memory"], [188, 200, "Device-Memory"]]}
{"id": 155227, "text": "We compare SHIELD with the following six defensive baselines : Note that due to the insufficient memory of GPU Titian Xp to simultaneously train several BERT and RoBERTa sub-models , we exclude * Ensemble * , * DT * , and * ADP * baseline for them .", "Comments": [], "label": [[111, 120, "Hardware-device"]]}
{"id": 155228, "text": "To ensure fairness and reproducibility , we use the external * TextAttack * [ Morris et al. , , 2020 ] and * OpenAttack * [ Zeng et al. , , 2021 ] . framework for adversarial text generation and evaluation .", "Comments": [], "label": [[63, 73, "Software-Entity"], [109, 119, "Software-Entity"]]}
{"id": 155236, "text": "B.4 Experimental Settings for Attack Methods Since we use external open-source * TextAttack * [ Mor ] [ ris et al. , , 2020 ] [ 3 ] and * OpenAttack * [ Zeng et al. , , 2021 ] framework for evaluating the performance of SHIELD and all defense baselines under adversarial attacks , implementation of all the attacks are publicly available .", "Comments": [], "label": [[81, 91, "Software-Entity"], [138, 148, "Software-Entity"]]}
{"id": 155237, "text": "Specifically , we use the * TextAttack * framework for evaluating all the word- and character-level attacks , and use the * OpenAttack * for evaluating the sentence-level attack SCPNA .", "Comments": [], "label": [[28, 38, "Software-Entity"], [124, 134, "Software-Entity"]]}
{"id": 155242, "text": "First , we tokenize all examples in the training set with NLTK word tokenizer [ 3 ] .", "Comments": [], "label": [[58, 62, "Software-Entity"]]}
{"id": 155250, "text": "We implement all models with the HuggingFace Transformers library [ Wolf et al. , ] Evaluation Metrics .", "Comments": [], "label": [[33, 44, "Software-Entity"]]}
{"id": 155253, "text": "All models are trained and tested on a single GeForce RTX 2080 Ti GPU . http : //www.statmt.org/lm-benchmark/ references .", "Comments": [], "label": [[39, 45, "Device-Count"], [46, 69, "Hardware-device"]]}
{"id": 155256, "text": "3 Approach 3.1 System Description For the implementation of our method , we use the open source IMS Toucan speech synthesis toolkit , first introduced in [ Lux et al. , , 2021 ] , which is in turn based on the ESPnet end-to-end speech processing toolkit [ Watanabe et al. , , 2018 ] [ Hayashi ] [ et al. , ] [ 2020 , 2021 ] .", "Comments": [], "label": [[210, 216, "Software-Entity"]]}
{"id": 155266, "text": "Computational Resources All models were trained on a single NVIDIA A6000 GPU .", "Comments": [], "label": [[53, 59, "Device-Count"], [60, 76, "Hardware-device"]]}
{"id": 155267, "text": "The speaker embedding is built according to the ECAPA-TDNN architecture [ Desplanques et al. , , 2020 ] and provided open source by SpeechBrain [ Ravanelli et al. , , 2021 ] .", "Comments": [], "label": [[132, 143, "Software-Entity"]]}
{"id": 155269, "text": "Further investigations into ( a ) 66 dimensional Featurespace ( 24 PanPhon and 42 Papercup features ) ( b ) 512 dimensional embeddingspace learned during Tacotron 2 training ( c ) 384 dimensional embeddingspace learned during FastSpeech 2 training Figure 5 : t-SNE visualizations of phoneme representations , illustrating zero-shot capabilities .", "Comments": [], "label": [[259, 264, "Software-Entity"]]}
{"id": 155276, "text": "Table 7 : Extended dataset characteristics B Models and Hyper-parameters For feature attributions : We use BERT-base with pre-trained weights from the Huggingface library [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[151, 162, "Software-Entity"]]}
{"id": 155281, "text": "All experiments are run on a single NVIDIA Tesla V100 GPU .", "Comments": [], "label": [[29, 35, "Device-Count"], [36, 57, "Hardware-device"]]}
{"id": 155294, "text": "We use the spaCy dependency parser for obtaining dependency tags .", "Comments": [], "label": [[11, 16, "Software-Entity"]]}
{"id": 155297, "text": "A.2 Hyperparameter Tuning We train and evaluate MILIE on an NVIDIA Titan RTX with 24 GB GPU RAM .", "Comments": [], "label": [[60, 76, "Hardware-device"], [82, 95, "Device-Memory"]]}
{"id": 155313, "text": "The experiments have been performed on two Tesla V100 GPUs .", "Comments": [], "label": [[39, 42, "Device-Count"], [43, 58, "Hardware-device"]]}
{"id": 155324, "text": "Using Nvidia A100 GPUs , the question answering models were first fine-tuned on SQuAD v1.1 and then on the pilot training data .", "Comments": [], "label": [[6, 22, "Hardware-device"]]}
{"id": 155331, "text": "We implement MetaDistil based on Hugging Face Transformers [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[33, 45, "Software-Entity"]]}
{"id": 155334, "text": "All experiments are conducted on a single Nvidia V100 GPU . computational overheads of MetaDistil with other methods in Table [ 2 . ] Although our approach takes more time to achieve its own peak performance , it can match up the performance of PKD [ Sun et al. , , 2019 ] with a similar time cost .", "Comments": [], "label": [[35, 41, "Device-Count"], [42, 57, "Hardware-device"]]}
{"id": 155350, "text": "For each translation direction , we use a unigram SentencePiece [ 3 ] model to learn a vocabulary on the text data from ST dataset , and use it to segment text from both ST and MT corpora into subword units .", "Comments": [], "label": [[50, 63, "Software-Entity"]]}
{"id": 155361, "text": "All models are trained on 8 Nvidia Tesla-V100 GPUs .", "Comments": [], "label": [[26, 27, "Device-Count"], [28, 50, "Hardware-device"]]}
{"id": 155362, "text": "We implement our models based on fairseq [ 7 ] [ Ott et al. , , 2019 ] .", "Comments": [], "label": [[33, 40, "Software-Entity"]]}
{"id": 155370, "text": "In Figure [ 4 , ] we take out the sequential representation of the speech ( output of acoustic encoder ) , text sequences ( output of embedding layer ) , and the STEMM sequences , average them over the sequence dimension , and apply the T-SNE dimensionality reduction algorithm to reduce the 512 dimensions to two dimensions .", "Comments": [], "label": [[237, 242, "Software-Entity"]]}
{"id": 155371, "text": "T-SNE algorithm is applied to reduce the 512-dim representations to two dimensions .", "Comments": [], "label": [[0, 5, "Software-Entity"]]}
{"id": 155395, "text": "In our experiments , we combine the original parallel data with 180 million backtranslation data as described in [ Ma et al. , , 2020 ] and call the augmented dataset WMT for short . 4.2 Experimental Setup We conduct experiments based on fairseq [ 2 ] .", "Comments": [], "label": [[238, 245, "Software-Entity"]]}
{"id": 155396, "text": "All experiments are conducted on NVIDIA V100 GPUs with 32 GB memory .", "Comments": [], "label": [[33, 49, "Hardware-device"], [55, 67, "Device-Memory"]]}
{"id": 155437, "text": "Table 8 : Number of examples in the datasets used Table 9 : Size of models used A Appendix A.1 Experimental Setup We leverage the transformers library [ Wolf et al. , , 2020 ] for accessing the LMs .", "Comments": [], "label": [[130, 142, "Software-Entity"]]}
{"id": 155438, "text": "All experiments were conducted using a single Nvidia GeForce GTX 1080 Ti Graphics Card .", "Comments": [], "label": [[39, 45, "Device-Count"], [46, 72, "Hardware-device"]]}
{"id": 155439, "text": "The model sizes are captured in Table [ 9 . ] Size of evaluation datasets is captured in Table [ 8 . ] We used the nltk pos-tagger with the universal tagset for pos-tagging .", "Comments": [], "label": [[115, 119, "Software-Entity"]]}
{"id": 155447, "text": "We used four Tesla V100 GPUs for training .", "Comments": [], "label": [[8, 12, "Device-Count"], [13, 28, "Hardware-device"]]}
{"id": 155449, "text": "We used Faiss [ Johnson et al. , , 2021 ] with the same settings as [ Khandelwal et al. , 2021 ] for fast nearest neighbor search in high-dimensional space . 3.3 Human Evaluation Settings We assessed the interpretability by human evaluation based on [ Doshi-Velez and Kim , 2017 ] .", "Comments": [], "label": [[8, 13, "Software-Entity"]]}
{"id": 155482, "text": "We pre-train the model on 4 DGX-2 machines , each having 16 NVIDIA Tesla V100 with 32GB memory .", "Comments": [], "label": [[57, 59, "Device-Count"], [60, 77, "Hardware-device"], [83, 94, "Device-Memory"]]}
{"id": 155501, "text": "E Computation Resources We train TAGOP-L2I on a NVIDIA Tesla V100 GPU with 32GB RAM . | -- | -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -| -- | -- | -- |", "Comments": [], "label": [[48, 69, "Hardware-device"], [75, 83, "Device-Memory"]]}
{"id": 155510, "text": "The sentences are tokenized with the Moses tokenizer [ 1 ] for English and Spanish and MeCab [ 2 ] for Japanese .", "Comments": [], "label": [[37, 42, "Software-Entity"], [87, 92, "Software-Entity"]]}
{"id": 155529, "text": "D Computing Infrastructure We ran the experiments on a server with a Intel ( R ) Xeon ( R ) CPU E5-2698 v4 @ 2.20GHz CPU and 10 NVIDIA TITAN Xp GPUs .", "Comments": [], "label": [[69, 120, "Hardware-device"], [125, 127, "Device-Count"], [128, 148, "Hardware-device"]]}
{"id": 155530, "text": "Each pretraining and finetuning were run with a single GPU .", "Comments": [], "label": [[48, 58, "Device-Count"]]}
{"id": 155544, "text": "The GPU ( Tesla V100-SXM2 16GB ) was warmed up for 10 repetitions before timing started , and PyTorch 's builtin GPU synchronization method was used to synchronize timing ( which occurs on the CPU ) with the training or inference running on the GPU .", "Comments": [], "label": [[10, 25, "Hardware-device"], [26, 30, "Device-Memory"], [94, 104, "Software-Entity"]]}
{"id": 155547, "text": "Experiments were all run on single Tesla V100-SXM2 16GB GPUs .", "Comments": [], "label": [[28, 34, "Device-Count"], [35, 50, "Hardware-device"], [51, 60, "Device-Memory"]]}
{"id": 155562, "text": "We use 2 bytes per token ID in our evaluation because the BERT dictionary based on WordPiece [ Wu et al. , , 2016 ] tokenizer has about 32,000 tokens .", "Comments": [], "label": [[83, 92, "Software-Entity"]]}
{"id": 155563, "text": "To evaluate latency , we uses an Amazon AWS g4dn instance with Intel Cascade Lake CPUs and an NVIDIA T4 GPU .", "Comments": [], "label": [[33, 43, "Cloud-Platform"], [44, 57, "Cloud-Platform"], [63, 86, "Hardware-device"], [94, 107, "Hardware-device"]]}
{"id": 155572, "text": "The Col-BERT code follows the original version released [ 2 ] and BERT implementation is from Huggingface [ 3 ] .", "Comments": [], "label": [[94, 105, "Software-Entity"]]}
{"id": 155574, "text": "For PQ , OPQ , RQ , and LSQ , we uses off-the-shelf implementation from Facebook 's faiss [ 5 ] library [ John ] [ son et al. , , 2017 ] .", "Comments": [], "label": [[84, 89, "Software-Entity"]]}
{"id": 155583, "text": "4.3 Model configurations Data Preprocessing We pre-process all data using Moses tools for normalization , and truecasing .", "Comments": [], "label": [[74, 79, "Software-Entity"]]}
{"id": 155587, "text": "We train all our models on two GeForce GTX 1080Ti GPUs .", "Comments": [], "label": [[27, 30, "Device-Count"], [31, 54, "Hardware-device"]]}
{"id": 155588, "text": "All models are implemented using the Fairseq toolkit .", "Comments": [], "label": [[37, 44, "Software-Entity"]]}
{"id": 155598, "text": "On WMT16 En-De translation task , our method achieves 1.80 SacreBLEU improvement over vanilla transformer model and 1.12 SacreBLEU over traditional word dropout regularization .", "Comments": [], "label": [[121, 130, "Software-Entity"]]}
{"id": 155602, "text": "English-Romanian This data set contains about 0.6M processed parallel sentence pairs tokenized by Moses toolkit [ Koehn et al. , , 2007 ] and segmented with 40K merge operations using BPE [ Sennrich et al. , , 2016 ] .", "Comments": [], "label": [[98, 103, "Software-Entity"], [184, 187, "Software-Entity"]]}
{"id": 155605, "text": "The English and Chinese sentences are tokenized with Moses toolkit and Stanford Segmenter respectively , which are further applied 32K BPE segmentation .", "Comments": [], "label": [[53, 58, "Software-Entity"]]}
{"id": 155607, "text": "5.2 Configuration To fairly compare each method , we reproduce all compared methods with transformer model [ Vaswani et al. , , 2017 ] using open-source toolkit * Fairseq * [ Ott et al. , , 2019 ] , with the same model configuration and hardware facilities .", "Comments": [], "label": [[163, 170, "Software-Entity"]]}
{"id": 155609, "text": "All experiments are conducted on 4 GeForce RTX 3090 GPUs with a distributional batch-size of 4096 tokens each GPU and an overall accumulated batch-size of 4096×8 tokens .", "Comments": [], "label": [[33, 34, "Device-Count"], [35, 56, "Hardware-device"]]}
{"id": 155615, "text": "On WMT16 En-De , PDR achieves 1.80 SacreBLEU improvement over vanilla transformer , 1.12 SacreBLEU improvement over existing word-dropout perturbation regularization , and 0.73 SacreBLEU improvement over R-Drop .", "Comments": [], "label": [[89, 98, "Software-Entity"]]}
{"id": 155625, "text": "The time measurement was conducted on a PC with an Intel Core i7 CPU , 64-GB RAM and an NVIDIA Titan X Pascal GPU .", "Comments": [], "label": [[51, 68, "Hardware-device"], [71, 80, "Device-Memory"], [88, 113, "Hardware-device"]]}
{"id": 155650, "text": "A Experimental Setup All the algorithms are implemented in Pytorch and trained on a machine with 8 NVIDIA GTX 2080Ti GPUs .", "Comments": [], "label": [[59, 66, "Software-Entity"], [97, 98, "Device-Count"], [99, 121, "Hardware-device"]]}
{"id": 155651, "text": "In this task , all the models are trained on a singe GPU with learning rate 30 , weight decay 1.2e − 6 .", "Comments": [], "label": [[47, 56, "Device-Count"]]}
{"id": 155653, "text": "During training , we use Adam optimizer with β = 0.9 , β = 0.98 , weight decay 0.01 and learning rate 5e − 4 , and apply the dynamic batching provided by fairseq [ 6 ] to train both the models with 4 GPUs .", "Comments": [], "label": [[154, 161, "Software-Entity"], [198, 204, "Device-Count"]]}
{"id": 155676, "text": "Figure 2 : T-SNE visualization [ Van der Maaten and Hinton , 2008 ] of rule-matched data that mis-classified by the model on AG News dataset .", "Comments": [], "label": [[11, 16, "Software-Entity"]]}
{"id": 155681, "text": "C Implementation Setting We test our code on the System * Ubuntu 18.04.4 LTS * with CPU : * Intel ( R ) Xeon ( R ) Silver 4214 CPU @ 2.20GHz * and * GPU : NVIDIA GeForce RTX 2080 * .", "Comments": [], "label": [[92, 140, "Hardware-device"], [155, 178, "Hardware-device"]]}
{"id": 155682, "text": "We implement our method using Python 3.6 and PyTorch 1.2 [ Paszke et al. , , 2019 ] .", "Comments": [], "label": [[45, 52, "Software-Entity"]]}
{"id": 155691, "text": "For Wikipedia , we use the WikiExtractor [ 3 ] to extract hyperlinks between Wiki articles .", "Comments": [], "label": [[27, 40, "Software-Entity"]]}
{"id": 155694, "text": "Training takes 1 day on two GeForce RTX 2080 Ti GPUs with fp16 .", "Comments": [], "label": [[24, 27, "Device-Count"], [28, 52, "Hardware-device"]]}
{"id": 155696, "text": "Training takes 4 days on four A100 GPUs with fp16 .", "Comments": [], "label": [[25, 29, "Device-Count"], [30, 39, "Hardware-device"]]}
{"id": 155698, "text": "Training takes 7 days on eight A100 GPUs with fp16 .", "Comments": [], "label": [[25, 30, "Device-Count"], [31, 40, "Hardware-device"]]}
{"id": 155703, "text": "Training takes 7 days on eight A100 GPUs with fp16 .", "Comments": [], "label": [[25, 30, "Device-Count"], [31, 40, "Hardware-device"]]}
{"id": 155705, "text": "Training takes 21 days on eight A100 GPUs with fp16 .", "Comments": [], "label": [[26, 31, "Device-Count"], [32, 41, "Hardware-device"]]}
{"id": 155719, "text": "This simple global partitioning method reduces the chance of putting all the interfering words and candidates in the same partition , while minimizing the extra computational cost in our PyTorch implementation because PyTorch supports strided index slicing without copying the variable .", "Comments": [], "label": [[187, 194, "Software-Entity"], [218, 225, "Software-Entity"]]}
{"id": 155730, "text": "We implement our models based on huggingface [ 9 ] [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[33, 44, "Software-Entity"]]}
{"id": 155737, "text": "For BERT , we perform the sentence segmentation using SpaCy [ 10 ] and input one sentence into BERT at a time .", "Comments": [], "label": [[54, 59, "Software-Entity"]]}
{"id": 155741, "text": "We use NVIDIA GeForce RTX 2080 for training * GPT-2 Small * and * BERT base * , GeForce RTX 8000 for training * GPT-2 Medium * , Tesla M40 for training * BERT large * .", "Comments": [], "label": [[7, 30, "Hardware-device"], [80, 96, "Hardware-device"], [129, 138, "Hardware-device"]]}
{"id": 155742, "text": "Since we start from the pretrained LM , we can finish training each LM within 2 weeks using 1 GPU for * GPT-2 Small * , * BERT base * , and * GPT-2 Medium * , and using 4 GPUs for training * BERT large * .", "Comments": [], "label": [[92, 97, "Device-Count"], [169, 175, "Device-Count"]]}
{"id": 155743, "text": "When testing the inference time in [ Table 1 , ] we average the time of running NVIDIA TITAN X on 10,000 batches , where each batch contains 4 sequences whose length are 200 .", "Comments": [], "label": [[80, 94, "Hardware-device"]]}
{"id": 155768, "text": "A Final Hyperparameters and Computing Environment We conduct our experiments using a NVIDIA QUADRO RTX 6000 .", "Comments": [], "label": [[85, 107, "Hardware-device"]]}
{"id": 155777, "text": "Both datasets are composed of conversations between Amazon Mechanical Turk ( AMT ) participants .", "Comments": [], "label": [[52, 82, "Cloud-Platform"]]}
{"id": 155779, "text": "5.2 Implementation Details In our work , we train and run all the models on a machine with a GeForce RTX 2080 Ti GPU .", "Comments": [], "label": [[93, 116, "Hardware-device"]]}
{"id": 155804, "text": "We implement them based on the open-source framework fairseq [ Ott et al. , , 2019 ] .", "Comments": [], "label": [[53, 60, "Software-Entity"]]}
{"id": 155810, "text": "Note that we evaluate the speedups with a single GTX 1080-Ti GPU and include the results with the same evaluating hardware for fair comparisons . ( LV-NAR , Flowseq , and CNAT ) or decoding with multiple iterations ( CMLM and LevT ) both improve non-autoregressive decoding in translation quality .", "Comments": [], "label": [[42, 48, "Device-Count"], [49, 64, "Hardware-device"]]}
{"id": 155846, "text": "Due to the resource limit , for 11B models , we adopt model parallelism [ Shoeybi et al. , , 2019 ] and store a model with 4 GPU devices .", "Comments": [], "label": [[123, 128, "Device-Count"]]}
{"id": 155847, "text": "We also use mixedprecision training [ Micikevicius et al. , , 2018 ] and ZeRO [ Rajbhandari et al. , , 2020 ] stage-1 provided in DeepSpeed [ Rasley et al. , , 2020 ] to reduce GPU memory usage .", "Comments": [], "label": [[130, 139, "Software-Entity"]]}
{"id": 155848, "text": "In this way , we train the largest 11B model with 16 NVIDIA V100 32G GPUs .", "Comments": [], "label": [[50, 52, "Device-Count"], [53, 64, "Hardware-device"], [65, 73, "Device-Memory"]]}
{"id": 155849, "text": "Since the tunable parameters are much less in PT , 8 NVIDIA V100 32G GPUs are enough for the training .", "Comments": [], "label": [[51, 52, "Device-Count"], [53, 64, "Hardware-device"], [65, 73, "Device-Memory"]]}
{"id": 155885, "text": "Three similarity scores are computed for each pair of summary sentence and document paragraph : ( 1 ) cosine similarity between the representations computed by Sentence BERT [ Reimers and Gurevych , 2019 ] for the summary sentence and the document paragraph ; ( 2 ) the percentage of unique bigrams in the summary sentence that occur in the document paragraph ; and ( 3 ) the percentage of unique named entities [ 4 ] that < https : //appen.com > We use SpaCy 3.0.3 [ Honnibal and Montani , 2017 ] with en_core_web_sm for named entity recognition .", "Comments": [], "label": [[454, 459, "Software-Entity"]]}
{"id": 155886, "text": "We then extract article content with WikiExtractor [ 10 ] from the English Wikipedia dump [ 11 ] at 2021/08/01 using the article names .", "Comments": [], "label": [[37, 50, "Software-Entity"]]}
{"id": 155888, "text": "E Sample Output We show more outputs by HIBRIDS-ENC on QSGen-Hier in Table [ 7 . ] F Details of Implementation We take the implementation of Longformer from Huggingface 4.8.1 [ Wolf et al. , , 2020 ] , which is licensed under the Apache License 2.0 [ 15 ] .", "Comments": [], "label": [[157, 168, "Software-Entity"]]}
{"id": 155889, "text": "For model training , we use Fairseq ( commit f34abcf2 ) [ Ott et al. , , 2019 ] that adopts MIT License [ 17 ] .", "Comments": [], "label": [[28, 35, "Software-Entity"]]}
{"id": 155890, "text": "Both model training and decoding are performed on the A6000 GPU with 48GB memory and the A100 GPU with 40GB memory .", "Comments": [], "label": [[54, 63, "Hardware-device"], [69, 80, "Device-Memory"], [89, 97, "Hardware-device"], [103, 114, "Device-Memory"]]}
{"id": 155894, "text": "HIBRIDS takes 2 , 2 , 5 , and 12 hours for training on QSGen-Hier , QSGen-ChildQ , WIKIBIOSUM , and GOVREPORT with 4 GPUs .", "Comments": [], "label": [[115, 121, "Device-Count"]]}
{"id": 155896, "text": "For BLEU scores , we use NLTK 3.5 [ Bird et al. , , 2009 ] .", "Comments": [], "label": [[25, 29, "Software-Entity"]]}
{"id": 155902, "text": "For the multi-lingual experiments , we use FastText [ Grave et al. , , 2018 ] .", "Comments": [], "label": [[43, 51, "Software-Entity"]]}
{"id": 155903, "text": "In all cases we preprocess using the NLTK tokenizer [ Bird et al. , , 2009 ] and stop-words list and by filtering non-alphabetic tokens .", "Comments": [], "label": [[37, 41, "Software-Entity"]]}
{"id": 155905, "text": "The model was implemented using the text vectorizer of scikit-learn [ Pedregosa et al. , , 2011 ] and uses bigrams to fivegrams .", "Comments": [], "label": [[55, 67, "Software-Entity"]]}
{"id": 155906, "text": "Cross Attention For our experiments we use pretrained models from HuggingFace [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[66, 77, "Software-Entity"]]}
{"id": 155920, "text": "The FastText baseline performs comparable to CA on average ( 26.0 vs 27.2 ) , while SN is ahead by a large margin ( 27.2 vs 32.4 ) .", "Comments": [], "label": [[4, 12, "Software-Entity"]]}
{"id": 155922, "text": "The main disadvantage of BitFit , however , is that the weight sharing it requires is much harder to implement , especially in highly optimized environments such as [ NVIDIA Triton .", "Comments": [], "label": [[174, 180, "Software-Entity"]]}
{"id": 155925, "text": "F Computing Requirements All experiments were run on a system with an AMD Ryzen Threadripper 1950X CPU and a Nvidia GeForce GTX 1080 Ti GPU .", "Comments": [], "label": [[70, 102, "Hardware-device"], [109, 139, "Hardware-device"]]}
{"id": 155957, "text": "Our near-state-of-the-art DeepPyramidion model costs us 3 days of training on 8 NVIDIA A100 GPUs .", "Comments": [], "label": [[78, 79, "Device-Count"], [80, 96, "Hardware-device"]]}
{"id": 155963, "text": "The selected k top-scoring vectors were compared to the real top-k selection using normalized Chamfer Cosine Similarity ( nCCS ) as given : Additionally , we measured an average time for processing a batch of size 16 on the NVIDIA A100 GPU , and addressed the question of how both algorithms differ in terms of speed ( Figure [ 5 ] and quality ( Figure [ 6 ] , depending on k and n choices .", "Comments": [], "label": [[224, 239, "Hardware-device"]]}
{"id": 155975, "text": "D.2 Hardware and Software Used All experiments and benchmarks were performed on a DGX-A100 server equipped with eight NVIDIA Tesla A100 GPUs .", "Comments": [], "label": [[82, 90, "Hardware-device"], [112, 117, "Device-Count"], [118, 140, "Hardware-device"]]}
{"id": 155976, "text": "We based our experiments using * fairseq * [ Ott et al. , , 2019 ] v0.9.0 , Python 3.6.10 , PyTorch 1.6.0a0+9907a3e [ Paszke et al. , , 2019 ] , CUDA Version 11.0 and NVIDIA drivers 450.51.06 .", "Comments": [], "label": [[33, 40, "Software-Entity"], [92, 99, "Software-Entity"], [145, 149, "Software-Entity"]]}
{"id": 155993, "text": "A Appendix A.1 More Details about the PET Baseline Implementation All experiments are carried out in a Linux environment with a single V100 GPU ( 32G ) .", "Comments": [], "label": [[128, 134, "Device-Count"], [135, 143, "Hardware-device"], [146, 149, "Device-Memory"]]}
{"id": 155994, "text": "In order to run each experiment in a single GPU , we fix the bottom 16 layers ' ( bottom 1/3 layers ) parameters of DeBERTa due to the limitation of GPU memory .", "Comments": [], "label": [[37, 47, "Device-Count"]]}
{"id": 156019, "text": "The self-attention operation scales quadratically with the input sequence length , Ashish Khetan Amazon AWS , USA khetan @ amazon.com Zohar Karnin Amazon AWS , Israel zkarnin @ amazon.com which is a bottleneck especially for long-sequence data .", "Comments": [], "label": [[97, 107, "Cloud-Platform"]]}
{"id": 156039, "text": "The training and inference jobs are run separately on a NVIDIA Tesla V100 GPU machine and a Intel Xeon Platinum 8000 series CPU machine respectively .", "Comments": [], "label": [[56, 77, "Hardware-device"], [92, 116, "Hardware-device"]]}
{"id": 156062, "text": "Models are trained on 64 NVIDIA V100 ( 32GB ) GPUs with FP16 and ZERO-stage-1 optimization [ Rasley et al. , , 2020 ] .", "Comments": [], "label": [[22, 24, "Device-Count"], [25, 36, "Hardware-device"], [39, 43, "Device-Memory"]]}
{"id": 156079, "text": "All experiments are conducted on a single NVIDIA RTX A6000 Graphical Card with 48G graphical memory .", "Comments": [], "label": [[35, 41, "Device-Count"], [42, 58, "Hardware-device"], [79, 82, "Device-Memory"]]}
{"id": 156088, "text": "All the experiments are conducted on a machine with 4 or 8 many 40GB A100 GPUs .", "Comments": [], "label": [[64, 68, "Device-Memory"], [69, 78, "Hardware-device"]]}
{"id": 156089, "text": "Our code is based on Huggingface Transformers [ Wolf et al. , , 2019 ] .", "Comments": [], "label": [[21, 32, "Software-Entity"]]}
{"id": 156105, "text": "However , in this case , even with 25 retrieved paths , training PATHFID+ for 10K steps with batch size of 64 using gradient accumulation takes 19 hours on 8 A100 GPUs with 40GB memory each , which is one of the most prominent limitations hurdling the progress for this line of research .", "Comments": [], "label": [[156, 157, "Device-Count"], [158, 167, "Hardware-device"], [173, 184, "Device-Memory"]]}
{"id": 156111, "text": "We implement BERT , ALBERT , and RoBERTa using the Huggingface Transformers library [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[51, 62, "Software-Entity"]]}
{"id": 156117, "text": "All models are trained on a single instance of NVIDIA RTX 3090 GPU card .", "Comments": [], "label": [[28, 34, "Device-Count"], [47, 66, "Hardware-device"]]}
{"id": 156123, "text": "We use scikit-learn v0.22 [ Pedregosa et al. , , 2011 ] for these experiments .", "Comments": [], "label": [[7, 19, "Software-Entity"]]}
{"id": 156128, "text": "We use the models provided by HuggingFace v4.2.1 [ Wolf et al. , , 2020 ] , and Pytorch v1.6.0 [ Paszke et al. , , 2019 ] for our experiments . 3.2 Tasks We instantiate our analysis of the BERT models on a diverse set of five NLP tasks , which covers syntactic and semantic predictions .", "Comments": [], "label": [[30, 41, "Software-Entity"], [80, 87, "Software-Entity"]]}
{"id": 156139, "text": "A Fine-tuning Details In this work , we fine-tune all tasks and representations using HuggingFace library .", "Comments": [], "label": [[86, 97, "Software-Entity"]]}
{"id": 156141, "text": "All the fine-tuning is run on a single Titan GPU .", "Comments": [], "label": [[32, 38, "Device-Count"], [39, 48, "Hardware-device"]]}
{"id": 156165, "text": "GeForce RTX 2080 Ti GPUs for both models .", "Comments": [], "label": [[0, 24, "Hardware-device"]]}
{"id": 156168, "text": "We use T5 [ Raffel et al. , , 2020 ] ( T5-large ) to model the knowledge composer in FAIRR and train it using the default hyperparameters available in the Hugging Face transformers library [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[155, 167, "Software-Entity"]]}
{"id": 156169, "text": "All models were trained on Nvidia Quadro RTX 8000 GPUs .", "Comments": [], "label": [[27, 54, "Hardware-device"]]}
{"id": 156170, "text": "Training a FAIRR on a single GPU takes around 20 hours on average .", "Comments": [], "label": [[22, 32, "Device-Count"]]}
{"id": 156174, "text": "We download pre-trained models from huggingface [ 4 ] .", "Comments": [], "label": [[36, 47, "Software-Entity"]]}
{"id": 156176, "text": "For TaPas , we adopt the PyTorch [ Paszke ] * et al . * , [ 2019 ] version in huggingface .", "Comments": [], "label": [[25, 32, "Software-Entity"], [78, 89, "Software-Entity"]]}
{"id": 156178, "text": "All experiments are conducted on a server with four V100 GPUs . 3.3 Results Table [ 4 ] summarizes our evaluation results .", "Comments": [], "label": [[47, 51, "Device-Count"], [52, 61, "Hardware-device"]]}
{"id": 156188, "text": "BERT-to-BERT [ Rothe ] * et al . * , [ 2020 ] A transformer encoder-decoder model [ Vaswani ] * et al . * , [ 2017 ] where the encoder and decoder are both initialized with BERT [ Devlin ] * et al . * , [ 2018 ] by loading the checkpoint named 'bert-base-uncased ' provided by the huggingface/transformers repository .", "Comments": [], "label": [[281, 305, "Software-Entity"]]}
{"id": 156189, "text": "We align model configuration with the BASE version of BART , and use the model 'facebook/bartbase ' in huggingface/transformers .", "Comments": [], "label": [[103, 127, "Software-Entity"]]}
{"id": 156192, "text": "We use the pre-trained model 't5-base ' in huggingface/transformers .", "Comments": [], "label": [[43, 67, "Software-Entity"]]}
{"id": 156197, "text": "Each header is Table 9 : Trigger Words for Functions linearized as * name * | * type * . * name * is the tokenized header string . * type * is the entity type parsed by Stanford CoreNLP , which includes `` string '' , `` number '' , `` datetime '' in our case .", "Comments": [], "label": [[169, 185, "Software-Entity"]]}
{"id": 156206, "text": "The whole pretraining phase takes about 4 days on 4 Tesla V100 GPUs . 4.2 Formula Prediction Formula prediction [ Chen ] * et al . * , [ 2021a ] facilitates spreadsheet end-users by recommending formulas since writing formulas could be time-consuming and error-prone .", "Comments": [], "label": [[50, 51, "Device-Count"], [52, 67, "Hardware-device"]]}
{"id": 156209, "text": "Since SpreadsheetCoder only published part of its code , we re-implement it in PyTorch [ Paszke ] * et al . * , [ 2019 ] based on its paper .", "Comments": [], "label": [[79, 86, "Software-Entity"]]}
{"id": 156228, "text": "All the contextual models are implemented using the transformers library [ Wolf et al. , , 2019 ] on PyTorch 1.7.1 .", "Comments": [], "label": [[52, 64, "Software-Entity"], [101, 108, "Software-Entity"]]}
{"id": 156229, "text": "All experiments are executed on a Tesla K80 GPU with 64 GB of system RAM on Ubuntu 18.04.5 LTS .", "Comments": [], "label": [[34, 47, "Hardware-device"], [53, 58, "Device-Memory"]]}
{"id": 156254, "text": "The POS tagger , lemmatizer , and stopword list are from the Natural Language Toolkit ( NLTK ) package [ Bird et al. , 2009 ] .", "Comments": [], "label": [[61, 85, "Software-Entity"], [88, 92, "Software-Entity"]]}
{"id": 156257, "text": "B Experimental Details B.1 Implementation Details We use base models from HuggingFace [ 5 ] and implement TBS based on TransferTransfo [ Wolf et al. , , 2019 ] [ 6 ] .", "Comments": [], "label": [[74, 85, "Software-Entity"]]}
{"id": 156259, "text": "Our TBS models are mostly trained on 4 Quadro RTX 8000 GPUs and take around 5 hours .", "Comments": [], "label": [[37, 38, "Device-Count"], [39, 59, "Hardware-device"]]}
{"id": 156266, "text": "NeuralCoref , an extension to the spaCy , is applied here to extract the mention clusters from the context .", "Comments": [], "label": [[0, 11, "Software-Entity"], [34, 39, "Software-Entity"]]}
{"id": 156276, "text": "We used the results from CorefRoBERTa [ Ye et al. , , 2020 ] as our baselines . 4.2 Setup Our coreference resolution was implemented in spaCy [ Honnibal and Montani , 2017 ] and Neural-Coref .", "Comments": [], "label": [[136, 141, "Software-Entity"], [178, 190, "Software-Entity"]]}
{"id": 156277, "text": "NeuralCoref is an extension for spaCy that is trained on the OntoNotes 5.0 dataset based on the training process proposed by Clark and Manning [ Clark and Manning , 2016 ] , which identifies the coreference clusters in the text as mentions .", "Comments": [], "label": [[0, 11, "Software-Entity"], [32, 37, "Software-Entity"]]}
{"id": 156278, "text": "In particular , spaCy 2.1.0 and NeuralCoref 4.0 are used , because the latest spaCy version 3.0+ has compatibility issues with NeuralCoref and extra efforts are required to solve the issues .", "Comments": [], "label": [[16, 21, "Software-Entity"], [32, 43, "Software-Entity"], [78, 83, "Software-Entity"], [127, 138, "Software-Entity"]]}
{"id": 156279, "text": "The neural network implementation was implemented in PyTorch [ Paszke et al. , , 2019 ] and Hugging Face Transformers [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[53, 60, "Software-Entity"], [92, 104, "Software-Entity"]]}
{"id": 156280, "text": "We used the embeddings of the pre-trained language model RoBERTaLARGE , with the relational graph convolutional network implemented in Deep Graph Library ( DGL ) [ Wang et al. , , 2020 ] .", "Comments": [], "label": [[135, 153, "Software-Entity"], [156, 159, "Software-Entity"]]}
{"id": 156282, "text": "Results with * , + are from [ Dasigi et al. , 2019 ] and [ Ye et al. , 2020 ] respectively . two NVIDIA TITAN RTX GPUs , each with 24GB memory . 4.3 Tasks and Datasets Our evaluation was performed on the QUOREF dataset [ Dasigi et al. , , 2019 ] .", "Comments": [], "label": [[97, 118, "Hardware-device"], [131, 142, "Device-Memory"]]}
{"id": 156284, "text": "However , the overall performance of the models is also limited by the performance of the coreference component we use , namely , NeuralCoref . 5.2 Case Studies To understand the model 's performance beyond the automated metrics , we analyze our predicted answers qualitatively .", "Comments": [], "label": [[130, 141, "Software-Entity"]]}
{"id": 156285, "text": "We make our trained metrics publicly available [ 1 ] and easily accessible via Hugging Face , to benefit the entire NLP community and in particular researchers and practitioners with limited resources .", "Comments": [], "label": [[79, 91, "Software-Entity"]]}
{"id": 156297, "text": "We finally used a BART-Base checkpoint [ 4 ] from the Fairseq library [ Ott et al. , , 2019 ] to try to reconstruct the perturbed versions of the original sequences , hence creating variants of them .", "Comments": [], "label": [[54, 61, "Software-Entity"]]}
{"id": 156301, "text": "We conducted the pretraining on a single TITAN RTX GPU ( 24GB ) .", "Comments": [], "label": [[34, 40, "Device-Count"], [41, 54, "Hardware-device"], [57, 61, "Device-Memory"]]}
{"id": 156302, "text": "We rely on the Transformers library [ Wolf et al. , , 2019 ] for all pretraining and fine-tuning experiments . 4 Experiments In this section , FrugalScore is used in inference mode to generate scores directly after pretraining , i.e. , no fine-tuning is performed ( see section [ 6 ] for fine-tuning results ) .", "Comments": [], "label": [[15, 27, "Software-Entity"]]}
{"id": 156313, "text": "For each inference operator type discussed below , we make edge connections based on the node representations we have learned , by computing similarity scores between all pairs of nodes ( using the graph node embedding - efficiently with FAISS [ Johnson et al. , , 2017 ] ) , and connecting the nodes with the top k similarity scores based on our model .", "Comments": [], "label": [[238, 243, "Software-Entity"]]}
{"id": 156314, "text": "To do this effectively , we first identify articles pairs that discuss the same event , approximated using the publication date and entity mentions overlap ( using Flair [ Akbik et al. , , 2018 ] ) in their title .", "Comments": [], "label": [[164, 169, "Software-Entity"]]}
{"id": 156332, "text": "A.2.2 Model Setup Our models are built on top of PyTorch [ Paszke et al. , 2019 ] and DGL ( Deep Graph Library ) [ Wang et al. , 2019 ] in Python .", "Comments": [], "label": [[49, 56, "Software-Entity"], [86, 89, "Software-Entity"], [92, 110, "Software-Entity"]]}
{"id": 156335, "text": "Our models were trained on a 12GB TITAN XP GPU card and training each data split for Node Classification takes approximately 4 hours .", "Comments": [], "label": [[29, 33, "Device-Memory"], [34, 46, "Hardware-device"]]}
{"id": 156339, "text": "We timed it on a single GPU GeForce GTX 1080 Ti GPU , with 6 Intel Core i5-8400 CPU @ 2.80 GHz processors over 5 runs ( all data splits averaged ) .", "Comments": [], "label": [[17, 27, "Device-Count"], [28, 51, "Hardware-device"], [61, 94, "Hardware-device"]]}
{"id": 156340, "text": "This is partly because we used FAISS [ Johnson et al. , , 2017 ] to do the embedding similarity search efficiently , as we mentioned in Sec 3.3 .", "Comments": [], "label": [[31, 36, "Software-Entity"]]}
{"id": 156352, "text": "Each is associated with a scoring function , ψ scoring users factuality assignments , and ψ scoring user pairs based on embedding similarity ( similarity comes from FAISS , as before ) .", "Comments": [], "label": [[165, 170, "Software-Entity"]]}
{"id": 156353, "text": "After receiving the top k similarity scores from the model based on FAISS , which we call candidates ( 30,000 candidates in the case of usersusers ) , DRaiL solves the constrained MAP problem of deciding which edges to add based on the weighted scores and constraints provided ( and discussed above ) .", "Comments": [], "label": [[68, 73, "Software-Entity"]]}
{"id": 156362, "text": "Since some user simulators used for implementing our MUST framework are based on the GPT model , we train Sys-MUSTadaptive on a V100 GPU and it will cost around 15 hours with the default hyperparameters above .", "Comments": [], "label": [[128, 136, "Hardware-device"]]}
{"id": 156373, "text": "G.3 Training details of user simulators We implement our GPT-based user simulators with DistilGPT2 [ Sanh et al. , , 2020 ] , a distilled version of GPT-2 by HuggingFace 's Transformers [ Wolf et al. , 2020 ] .", "Comments": [], "label": [[158, 172, "Software-Entity"]]}
{"id": 156380, "text": "The translation quality was measured by sacre-BLEU [ Post , 2018 ] and COMET [ Rei et al. , , 2020 ] .", "Comments": [], "label": [[40, 50, "Software-Entity"], [71, 76, "Software-Entity"]]}
{"id": 156381, "text": "The speed was evaluated on a single NVIDIA V100 GPU .", "Comments": [], "label": [[29, 35, "Device-Count"], [36, 51, "Hardware-device"]]}
{"id": 156382, "text": "We used FAISS [ Johnson et al. , , 2019 ] to retrieve the kNN tokens in kNN-MT and for neighbor sentence search in subset kNN-MT .", "Comments": [], "label": [[8, 13, "Software-Entity"]]}
{"id": 156383, "text": "The subset search and ADC were implemented in PYTORCH .", "Comments": [], "label": [[46, 53, "Software-Entity"]]}
{"id": 156388, "text": "We used a trained Transformer big implemented in FAIRSEQ [ Ott et al. , , 2019 ] as the base MT model .", "Comments": [], "label": [[49, 56, "Software-Entity"]]}
{"id": 156393, "text": "They searched for neighboring sentences by BM25 scores with ElasticSearch [ 4 ] , so our subset kNN-MT with BM25 can be regarded as an approximation of their method .", "Comments": [], "label": [[60, 73, "Software-Entity"]]}
{"id": 156397, "text": "Models We used the following pre-trained NMT models implemented in FAIRSEQ .", "Comments": [], "label": [[67, 74, "Software-Entity"]]}
{"id": 156398, "text": "The De-En model is included in FAIRSEQ and it is MIT-licensed .", "Comments": [], "label": [[31, 38, "Software-Entity"]]}
{"id": 156413, "text": "All the generation experiments are conducted on a machine with 8 NVIDIA GTX 2080Ti GPUs and the MIL network is trained on a GTX 3090 GPU . 4.5 Main Results Table [ 2 ] illustrates main experimental results on RealToxicityPrompts .", "Comments": [], "label": [[63, 64, "Device-Count"], [65, 87, "Hardware-device"], [124, 136, "Hardware-device"]]}
{"id": 156416, "text": "[ com/dropreg/OpenLTG- ] ( https : //github.com/dropreg/OpenLTG-MLM ) [ MLM ] Table 1 : Inference speed of each model with a single GPU ( NVIDIA A100 40GB ) .", "Comments": [], "label": [[125, 135, "Device-Count"], [138, 149, "Hardware-device"], [150, 154, "Device-Memory"]]}
{"id": 156417, "text": "The models labeled with † are implemented with the Hugging Face platform , while the rest are implemented with Fairseq . [ et al. , , 2021a ] , pre-trained AR language models can achieve promising Open-LTG .", "Comments": [], "label": [[51, 63, "Software-Entity"], [111, 118, "Software-Entity"]]}
{"id": 156418, "text": "We can see that BART [ Lewis et al. , , 2020 ] requires at least 1.3 seconds to generate a story with 200 tokens on the powerful NVIDIA A100 GPU , and extra planning [ Hua and Wang , 2020 ] can make the inference process even slower ( more than 30 seconds to create a 200-tokens story ) .", "Comments": [], "label": [[129, 144, "Hardware-device"]]}
{"id": 156430, "text": "More details are illustrated in Appendix [ A . ] Implementation & Baselines We utilize the pretrained RoBERTa base [ ‡ ] as our backbone model and implement all experiments with the open library * Fairseq * toolkit [ § ] [ Ott et al. , , 2019 ] .", "Comments": [], "label": [[197, 204, "Software-Entity"]]}
{"id": 156439, "text": "6 Analysis and Discussion 6.1 Speedup for Inference Figure [ 5 ] illustrate the generation speed with the NVIDIA A100 GPU , which all run with the batch size equal to 1 on each test dataset .", "Comments": [], "label": [[106, 121, "Hardware-device"]]}
{"id": 156442, "text": "We first use the NLTK tokenizer to split each sample into individual sentences , generally according to punctuation as a separator .", "Comments": [], "label": [[17, 21, "Software-Entity"]]}
{"id": 156443, "text": "Furthermore , we also provide the library version or link information , which is used in our paper : Transformers == v4.0.0 , NLTK == v3.5 , and evaluation scripts [ §§ ] .", "Comments": [], "label": [[101, 113, "Software-Entity"], [126, 130, "Software-Entity"]]}
{"id": 156444, "text": "We show the human evaluation interface in Figure [ 8 ] that was built using the python web library Django [ ¶¶ ] .", "Comments": [], "label": [[99, 105, "Software-Entity"]]}
{"id": 156453, "text": "Model training takes around 2 hours using a single NVIDIA A100 GPU .", "Comments": [], "label": [[44, 50, "Device-Count"], [51, 66, "Hardware-device"]]}
{"id": 156463, "text": "In order to lower computation and storage costs , recent popular lifelong learning techniques [ Madotto et al. , 2021 ] [ Ke et al. , , 2021a ] [ Zhu et al. , , 2022 ] [ Wang et al. , , 2022c ] try to solve the CF and KT leveraging parameter-efficient fine-tuning ( PEFT ) methods [ He et al. , , 2022a ] .", "Comments": [], "label": [[232, 263, "Software-Entity"], [266, 270, "Software-Entity"]]}
{"id": 156471, "text": "We train the model for 30 epochs per task with batch size 24 on 8 NVIDIA A100 GPUs .", "Comments": [], "label": [[64, 65, "Device-Count"], [66, 82, "Hardware-device"]]}
{"id": 156480, "text": "For weak signals , we apply QAFact [ Fabbri et al. , , 2022 ] , SUMMAC [ Laban et al. , , 2022 ] , and the NLI warmed up DeBERTaV3 ( NLI-warmup ) as to provide weak signals for each sample as default .", "Comments": [], "label": [[28, 34, "Software-Entity"], [64, 70, "Software-Entity"]]}
{"id": 156481, "text": "For labeling model pϕ , we follow the implementation of Snorkel [ Ratner et al. , , 2017 ] for efficiency and train it on CPUs with Adam optimizer .", "Comments": [], "label": [[56, 63, "Software-Entity"]]}
{"id": 156482, "text": "We train on 4 NVIDIA Tesla V100 GPUs , and it takes around only 5000 steps to reach the best performance .", "Comments": [], "label": [[12, 13, "Device-Count"], [14, 36, "Hardware-device"]]}
{"id": 156484, "text": "All these methods are tested on the TRUE benchmark with a single NVIDIA 32G V100 GPU and we report the relative time cost of each method comparing with WeCheck [ 6 ] .", "Comments": [], "label": [[58, 64, "Device-Count"], [72, 75, "Device-Memory"], [76, 84, "Hardware-device"]]}
{"id": 156488, "text": "Comparing concretely , our method is similar to Snorkel [ Ratner et al. , , 2017 ] .", "Comments": [], "label": [[48, 55, "Software-Entity"]]}
{"id": 156489, "text": "Following the DP paradigm , Snorkel [ Ratner et al. , , 2017 ] is proposed to for rapid training , more recent works study how to adapt label model in DP [ Ratner et al. , 2019 ] [ Awasthi et al. , , 2020 ] or modeling more complex structure between LFs [ Fu et al. , , 2020 ] .", "Comments": [], "label": [[28, 35, "Software-Entity"]]}
{"id": 156492, "text": "Data statistics are shown in Appendix [ A.1 . ] In data preprocessing , we use SPRING [ Bevilac ] [ qua et al. , , 2021 ] as the parser and LEAMR [ Blod ] [ gett and Schneider , 2021 ] as the aligner .", "Comments": [], "label": [[79, 85, "Software-Entity"]]}
{"id": 156495, "text": "AMRs are the same as the basic experiments and dependency trees are parsed by Stanford CoreNLP Toolkits [ Manning et al. , , 2014 ] . `` Without edge labels '' means all labels are the same placeholder .", "Comments": [], "label": [[78, 94, "Software-Entity"]]}
{"id": 156498, "text": "A.2 Implementation Details Preprocessing We use SPRING [ Bevilacqua et al. , 2021 ] as the parser to obtain the AMRs of input sentences and use LEAMR [ Blodgett and ] [ Schneider , 2021 ] as the AMR aligner to establish the correspondence between the AMRs and sentences .", "Comments": [], "label": [[48, 54, "Software-Entity"]]}
{"id": 156499, "text": "The total parameter size of APARN is about 130M and it takes about 8 minutes to train each epoch on a single RTX 3090 GPU with the batch size of 16 .", "Comments": [], "label": [[102, 108, "Device-Count"], [109, 121, "Hardware-device"]]}
{"id": 156503, "text": "Our implementation is based on the Hugging-Face Transformers library [ Wolf et al. , 2020 ] .", "Comments": [], "label": [[35, 47, "Software-Entity"]]}
{"id": 156508, "text": "As we can see , the number of unsafe responses is reduced again by around 20 % , which is quite effi- cient because the cost of finetuning is small , about 20 minutes on an Nvidia V100 .", "Comments": [], "label": [[173, 184, "Hardware-device"]]}
{"id": 156515, "text": "Our models are trained with NVIDIA Tesla V100 32GB GPUs using Azure Machine Learning Studio .", "Comments": [], "label": [[28, 45, "Hardware-device"], [46, 55, "Device-Memory"], [62, 91, "Cloud-Platform"]]}
{"id": 156516, "text": "Our implementation of RBE is based on Huggingface Transformers [ Wolf et al. , , 2020 ] and Sentence Transformers [ Reimers and ] [ Gurevych , 2019 ] .", "Comments": [], "label": [[38, 49, "Software-Entity"], [92, 113, "Software-Entity"]]}
{"id": 156521, "text": "In our case , we use a fine-tuned version of MPNet [ Song et al. , , 2020 ] from the Sentence Transformers library .", "Comments": [], "label": [[85, 114, "Software-Entity"]]}
{"id": 156528, "text": "All experiments were run on a single NVIDIA A100 GPU , summing to a total of roughly seven days computing time all together . 5.2 Review Score Prediction Reviewer rating behavior is heterogeneous ; the correspondence between review ratings and review texts often lacks consistency [ Wang and Shah , 2019 ] .", "Comments": [], "label": [[30, 36, "Device-Count"], [37, 52, "Hardware-device"]]}
{"id": 156533, "text": "< https : //aclanthology.org/ > Using NLTK 3.7 , see < https : //www.nltk.org/ > Metadata and Versions NLPEER includes all available revisions of a paper .", "Comments": [], "label": [[38, 42, "Software-Entity"]]}
{"id": 156534, "text": "We use the huggingface transformers implementation of RoBERTa [ 17 ] with roughly 125 million parameters , BioBERT [ 18 ] with around 110 million parameters , and SciB-ERT [ 19 ] with roughly 110 million parameters .", "Comments": [], "label": [[11, 22, "Software-Entity"]]}
{"id": 156535, "text": "We implement the training and testing pipeline in pytorch lightning [ 20 ] using huggingface transformers [ 21 ] .", "Comments": [], "label": [[50, 67, "Software-Entity"], [81, 92, "Software-Entity"]]}
{"id": 156538, "text": "To achieve this , we employ sklearn 's stratified , binary split function [ 22 ] while mapping the considered numerical stratification criterion to a discrete space by assigning the real numbers to buckets .", "Comments": [], "label": [[28, 35, "Software-Entity"]]}
{"id": 156539, "text": "Overall , the length of relevant train_test_split of sklearn 0.0 [ https : // ] ( https : //scikit-learn.org ) [ scikit-learn.org ] ( https : //scikit-learn.org ) Table 5 : Pragmatic label mapping unifying F1000-RD and the ARR review form sections .", "Comments": [], "label": [[53, 60, "Software-Entity"]]}
{"id": 156548, "text": "Our model is implemented in PyTorch [ Paszke et al. , 2019 ] .", "Comments": [], "label": [[28, 35, "Software-Entity"]]}
{"id": 156549, "text": "The batch size is set to 128/GPU , we train the model on 3 NVIDIA A100 GPUs with image size 224×224 .", "Comments": [], "label": [[57, 58, "Device-Count"], [59, 75, "Hardware-device"]]}
{"id": 156550, "text": "Our pre-training is quite efficient compared to other similar work , e.g. , 10 epochs ' pre-training in Align-Prompt [ Li et al. , , 2021a ] takes 3 days on the same 5M corpus using 16 A100 GPUs , this amounts to 16× computation cost of our pre-training .", "Comments": [], "label": [[182, 184, "Device-Count"], [185, 194, "Hardware-device"]]}
{"id": 156567, "text": "Besides , it is also more memory efficient , i.e. , its maximum allowed batch size on a single GPU is 190 while only 50 for Frozen .", "Comments": [], "label": [[88, 98, "Device-Count"]]}
{"id": 156568, "text": "Experiments conducted on a single RTX A6000 GPU with 48GB memory , training time is averaged over 8,394 DiDeMo training examples .", "Comments": [], "label": [[27, 33, "Device-Count"], [34, 47, "Hardware-device"], [53, 64, "Device-Memory"]]}
{"id": 156579, "text": "On a single A100 , this fine-tuning takes around 1.5 hours for MSRVTT , 0.5 hours for ActivityNet Captions or DiDeMo .", "Comments": [], "label": [[5, 11, "Device-Count"], [12, 16, "Hardware-device"]]}
{"id": 156582, "text": "On a single A100 , this fine-tuning takes around 4 hours for MSRVTT-QA , and 1 hour for ActivityNet-QA .", "Comments": [], "label": [[5, 11, "Device-Count"], [12, 16, "Hardware-device"]]}
{"id": 156610, "text": "To the best of our knowledge , this is the only released data that can be used to analyze hallucinations in a `` clean '' setting . 2.1 Model The model is Transformer base [ Vaswani et al. , , 2017 ] from fairseq [ Ott et al. , , 2019 ] with the standard hyperparameters setting .", "Comments": [], "label": [[205, 212, "Software-Entity"]]}
{"id": 156612, "text": "We follow the taxonomy by [ Guerreiro et al. , 2022 ] for consistency with the dataset and the evaluation framework we use and because this taxonomy is general enough for our purposes .", "Comments": [], "label": [[14, 22, "Software-Entity"]]}
{"id": 156649, "text": "A Implementation and computing All our experiments were carried out on a single server with one NVIDIA Quadro GP100 GPU .", "Comments": [], "label": [[92, 95, "Device-Count"], [96, 119, "Hardware-device"]]}
{"id": 156650, "text": "To compute BLEU and ChrF++ , we use the SacreBLEU package [ 10 ] with the default parameters .", "Comments": [], "label": [[40, 49, "Software-Entity"]]}
{"id": 156651, "text": "For COMET and COMET-QE , we use the COMET package [ 11 ] with the wmt20-comet-da and wmt20-comet-qe-da-v2 models , respectively .", "Comments": [], "label": [[36, 41, "Software-Entity"]]}
{"id": 156652, "text": "The translation hypotheses , Seq-Logprob , and LASER are computed using the Fairseq framework [ 12 ] .", "Comments": [], "label": [[76, 83, "Software-Entity"]]}
{"id": 156653, "text": "For the inference of LaBSE and the XNLI model , we use the transformers package [ 14 ] .", "Comments": [], "label": [[59, 71, "Software-Entity"]]}
{"id": 156664, "text": "We use Amazon Mechanic Turk to conduct human evaluations , asking annotators to label a random subset of our data following the standard annotation process for the NLI task .", "Comments": [], "label": [[7, 27, "Cloud-Platform"]]}
{"id": 156671, "text": "When comparing against other data augmentation approaches , e.g. , Z-aug [ Wu et al. , , 2022 ] , we used the exact code base compared with models trained on DISCO to remove any differences in implementation ( our implementation is based on the transformers library [ Wolf et al. , , 2020 ] ) .", "Comments": [], "label": [[245, 265, "Software-Entity"]]}
{"id": 156672, "text": "All experiments were performed on an NVIDIA RTX A6000 GPU .", "Comments": [], "label": [[37, 57, "Hardware-device"]]}
{"id": 156673, "text": "Screenshots of the instructions , guidelines , and annotation interface are shown in Fig [ 3 ] and Fig [ 4 . ] Annotators We recruit human workers on the Amazon Mechanical Turk [ 1 ] platform .", "Comments": [], "label": [[154, 176, "Cloud-Platform"]]}
{"id": 156674, "text": "Workers were paid $ 0.3 for each AMT hit ( consisting of 10 examples to annotate ) . https : //www.mturk.com/ Figure 3 : The annotated examples with explanations used on Amazon Mechanical Turk .", "Comments": [], "label": [[170, 192, "Cloud-Platform"]]}
{"id": 156675, "text": "Figure 4 : Instructions provided to human annotators on Amazon Mechanical Turk and the annotation interface .", "Comments": [], "label": [[56, 78, "Cloud-Platform"]]}
{"id": 156677, "text": "To construct word grounding instances , we use Stanza [ Qi et al. , , 2020 ] to parse the caption , enumerate every word in the groundable phrase , and identify those with a POS tag of NOUN or ADJ .", "Comments": [], "label": [[47, 53, "Software-Entity"]]}
{"id": 156683, "text": "We strictly follow the setup of RoBERTa to implement the MLM head with a two-layer MLP , based on the implementation of huggingface [ 3 ] .", "Comments": [], "label": [[120, 131, "Software-Entity"]]}
{"id": 156687, "text": "Specially , we select the ResNet-50 [ He et al. , , 2016 ] pretrained on ImageNet from TIMM [ 4 ] as the image encoder , and RoBERTa-base [ Liu et al. , , 2019 ] from huggingface [ 5 ] as the text encoder .", "Comments": [], "label": [[87, 91, "Software-Entity"], [167, 178, "Software-Entity"]]}
{"id": 156689, "text": "C.3 Computational Resources Our W2W models is pre-trained on 8 NVidia A40 GPUs .", "Comments": [], "label": [[61, 62, "Device-Count"], [63, 78, "Hardware-device"]]}
{"id": 156701, "text": "We use HuggingFace Transformers [ Wolf et al. , , 2019 ] to access the models .", "Comments": [], "label": [[7, 18, "Software-Entity"]]}
{"id": 156704, "text": "From the MWP templates of the SVAMP/ASDiv-A/MAWPS collection ( we consider all splits ) , we filter out the templates whose questions do not start with * How many ... * , and we use spaCy [ 7 ] to identify the subject , the object and the verbs in the sentence .", "Comments": [], "label": [[182, 187, "Software-Entity"]]}
{"id": 156706, "text": "D Computing Infrastructure & Inference Details To run our experiments , we used a single NVIDIA TITANRTX with 24GB of memory for all the versions of GPT-2 and GPT-Neo .", "Comments": [], "label": [[82, 88, "Device-Count"], [89, 104, "Hardware-device"], [110, 124, "Device-Memory"]]}
{"id": 156707, "text": "We used a single NVIDIA A100 with 40GB of memory for GPT-J-6B and a single NVIDIA A100 with 80GB of memory for GPT-NeoX and the LLaMA models ( two for the 30B version ) .", "Comments": [], "label": [[10, 16, "Device-Count"], [17, 28, "Hardware-device"], [34, 48, "Device-Memory"], [68, 74, "Device-Count"], [75, 86, "Hardware-device"], [92, 106, "Device-Memory"]]}
{"id": 156708, "text": "Experiment tracking was carried out using Weights & Biases [ 8 ] .", "Comments": [], "label": [[42, 58, "Software-Entity"]]}
{"id": 156713, "text": "We use the AWESOME aligner [ Dou and Neubig , 2021 ] to obtain : where each x and y are the lemmatized content source and target words and ↔ denotes a bidirectional word alignment .", "Comments": [], "label": [[11, 26, "Software-Entity"]]}
{"id": 156714, "text": "For these languages , we use spaCy [ Honnibal and Montani , 2017 ] and Stanza [ Qi et al. , , 2020 ] to find POS tags and detect verbs with a second-person subject in the source , and conjugated in the second ( T ) or third ( V ) person in the target .", "Comments": [], "label": [[29, 34, "Software-Entity"], [71, 77, "Software-Entity"]]}
{"id": 156715, "text": "To obtain antencedents , we use AllenNLP [ Gardner et al. , , 2017 ] 's coreference resolution module .", "Comments": [], "label": [[32, 40, "Software-Entity"]]}
{"id": 156720, "text": "Further details can be found in Appendix [ G. ] 5.2 Commercial Models To assess if commercially available machine translation engines are able to leverage context and therefore do well in MuDA , we consider two engines : [ 6 ] ( 1 ) the * Google Cloud Translation * v2 API .", "Comments": [], "label": [[239, 251, "Cloud-Platform"]]}
{"id": 156730, "text": "Then , our model using HuggingFace Transformers [ Wolf ] Table 6 : Total number of MuDA tags on TED test data . ' 0 ' indicates that the phenomenon does not apply to that language . [ et al. , , 2020 ] .", "Comments": [], "label": [[23, 34, "Software-Entity"]]}
{"id": 156735, "text": "We base our experiments on the framework * Fairseq * [ Ott et al. , , 2019 ] .", "Comments": [], "label": [[43, 50, "Software-Entity"]]}
{"id": 156754, "text": "For encoder and decoder CNN architectures , we follow the implementation provided in a public Pytorch implementation [ 5 ] by adding one more up-sampling and down-sampling layer to adjust our image size .", "Comments": [], "label": [[94, 101, "Software-Entity"]]}
{"id": 156757, "text": "Each experiment takes 10 hours in 4 NVIDIA V100 GPUs .", "Comments": [], "label": [[34, 35, "Device-Count"], [36, 52, "Hardware-device"]]}
{"id": 156762, "text": "C.4 Baselines : T5 Details We use the Huggingface [ Wolf et al. , , 2019 ] implementation T5-base model .", "Comments": [], "label": [[38, 49, "Software-Entity"]]}
{"id": 156765, "text": "A : 1 * ) } . ) . [ 7 ] We then represent Σ as a graph with an edge between each x and x where κ ( xi ) ∩ κ ( x ) ̸= ∅ ( Clark and Eyraud 's * syntactic congruence * relation ) and x and x has same part-of-speech tag according to spaCy pipeline with en-core-web-lm language model [ 8 ] .", "Comments": [], "label": [[230, 235, "Software-Entity"]]}
{"id": 156770, "text": "For each cartoon , we gather : ( iv ) 2-3 English Wikipedia links that an annotator identified as relevant , to serve as a proxy for world knowledge ( 2 per cartoon ) A random sample of annotations is shown in [ Fig ] [ ure 3 . ] We used Amazon Mechanical Turk , and paid crowdworkers a minimum of $ 15/hr .", "Comments": [], "label": [[238, 260, "Cloud-Platform"]]}
{"id": 156777, "text": "We use these three OpenAI models as both zero-shot and few-shot models .", "Comments": [], "label": [[19, 25, "Software-Entity"]]}
{"id": 156780, "text": "T5 , CLIP , and OFA were trained using 8 A100 GPUs in pytorch [ Paszke et al. , , 2019 ] .", "Comments": [], "label": [[39, 40, "Device-Count"], [41, 50, "Hardware-device"], [54, 61, "Software-Entity"]]}
{"id": 156781, "text": "We use the Transformers [ Wolf et al. , , 2020 ] implementation of T5 : T5-11B was trained with deepspeed [ Rasley ] We found that fine-tuning OFA directly was less effective .", "Comments": [], "label": [[11, 23, "Software-Entity"], [96, 105, "Software-Entity"]]}
{"id": 156783, "text": "Cartoon by Joe Dator . [ et al. , , 2020 ] ; T5-Large and CLIP were trained with Accelerate . [ 11 ] 3.1 Matching and quality ranking results [ Table 2 ] contains the results .", "Comments": [], "label": [[81, 91, "Software-Entity"]]}
{"id": 156796, "text": "References OpenAI . 2023 . [ Gpt-4 technical report . ] ( http : //arxiv.org/abs/2303.08774 ) A Crowdworking Details We use three Mechanical Turk interfaces to gather data .", "Comments": [], "label": [[130, 156, "Cloud-Platform"]]}
{"id": 156804, "text": "B.6 GPT-3 Fine-tuning We use the OpenAI fine-tuning API to fine-tune davinci , a 175B parameter language model .", "Comments": [], "label": [[33, 39, "Software-Entity"]]}
{"id": 156816, "text": "We use the official API provided by OpenAI [ 4 ] to access GPT-3 .", "Comments": [], "label": [[36, 42, "Software-Entity"]]}
{"id": 156822, "text": "Evaluation Tools : We use sklearn 1.0.2 for sequence classification evaluation , and seqeval 1.2.2 for token classification evaluation .", "Comments": [], "label": [[26, 33, "Software-Entity"], [85, 92, "Software-Entity"]]}
{"id": 156823, "text": "GPU Usage : Experiments are trained on NVIDIA RTX2080 GPUs .", "Comments": [], "label": [[39, 58, "Hardware-device"]]}
{"id": 156832, "text": "We used a tool called INCEpTION [ Klie et al. , , 2018 ] to help our coders to review and annotate MIMIC charts .", "Comments": [], "label": [[22, 31, "Software-Entity"]]}
{"id": 156843, "text": "The EffectiveCAN based models have about 17 million parameters , and each took about six hours to train on a single NVIDIA Tesla V100 16GB GPU with CO emission of about 680g .", "Comments": [], "label": [[109, 115, "Device-Count"], [116, 133, "Hardware-device"], [134, 142, "Device-Memory"]]}
{"id": 156844, "text": "E Annotation Guidelines The task is to annotate MIMIC charts with sufficient code evidence based on the documentations using an open source tool called INCEpTION .", "Comments": [], "label": [[152, 161, "Software-Entity"]]}
{"id": 156845, "text": "The general annotation process includes : Follow these instructions to annotate and export a chart in INCEpTION : 1 .", "Comments": [], "label": [[102, 111, "Software-Entity"]]}
{"id": 156867, "text": "We use the * datasets * [ Lhoest et al. , , 2021 ] library to calculate each sub-task 's scores in GLUE .", "Comments": [], "label": [[13, 21, "Software-Entity"]]}
{"id": 156868, "text": "Table [ 9 ] shows the training time taken on a single A100 GPU .", "Comments": [], "label": [[47, 53, "Device-Count"], [54, 62, "Hardware-device"]]}
{"id": 156869, "text": "Single and Multi-Task Learning on T5BASE We implement T5 based on the transformers library [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[70, 90, "Software-Entity"]]}
{"id": 156870, "text": "We use the ROUGE package [ Lin , 2004 ] for ROUGE-2 calculation and sacrebleu [ Post , 2018 ] for the BLEU score .", "Comments": [], "label": [[11, 16, "Software-Entity"], [68, 77, "Software-Entity"]]}
{"id": 156871, "text": "Table [ 10 ] shows the training time on a single A100 GPU and the detailed hyperparameters used .", "Comments": [], "label": [[42, 48, "Device-Count"], [49, 57, "Hardware-device"]]}
{"id": 156888, "text": "We train the model up to 5k steps on on 4 V100-16G GPUs and choose the hyperparameters and checkpoints on the validation set . [ 2 ] 5 Experimental Results 5.1 Main results Table [ 2 ] reports the evaluation results on AmbigQA and WebQSP .", "Comments": [], "label": [[40, 41, "Device-Count"], [42, 46, "Hardware-device"], [47, 55, "Device-Memory"]]}
{"id": 156901, "text": "We use TF-IDF with default parameters in * Scikit-learn * and S-BERT [ Reimers and Gurevych , 2019 ] to obtain the vector representations .", "Comments": [], "label": [[43, 55, "Software-Entity"]]}
{"id": 156910, "text": "Experiments are conducted on a NVidia GeForce 2080ti 11 GB .", "Comments": [], "label": [[31, 52, "Hardware-device"], [53, 58, "Device-Memory"]]}
{"id": 156911, "text": "As in [ Dasigi et al. , , 2021 ] , we employ the HuggingFace model allenai/led-base-16384 .", "Comments": [], "label": [[49, 60, "Software-Entity"]]}
{"id": 156920, "text": "Therefore , we only choose Mel-BERT and MisNet as our baselines and report the performance using their released code . 4.3 Experimental Settings We implement our framework using a pre-trained RoBERTa Base model from Huggingface .", "Comments": [], "label": [[216, 227, "Software-Entity"]]}
{"id": 156922, "text": "For metaphor detection , the results presented in Table [ 4 ] show a similar trend that each component Figure 2 : t-SNE visualization on SemEval under random setting for * in the bag * .", "Comments": [], "label": [[114, 119, "Software-Entity"]]}
{"id": 156923, "text": "Here * cts * refers to contrastive learning and * cl * refers to curriculum learning . ( a ) w/o fine-tune ( b ) w/ fine-tune ( c ) w/ cts ( d ) w/ cts and cl Figure 3 : t-SNE visualization on SemEval under typebased setting for * be my guest * .", "Comments": [], "label": [[170, 175, "Software-Entity"]]}
{"id": 156925, "text": "A Implementation Our experiments and implementation are based on the Transformers library and PyTorch .", "Comments": [], "label": [[69, 89, "Software-Entity"], [94, 101, "Software-Entity"]]}
{"id": 156926, "text": "B Experimental Details All of our experiments were conducted using two GPUs with 16GB RAM ( NVIDIA V100 ) .", "Comments": [], "label": [[67, 75, "Device-Count"], [81, 89, "Device-Memory"], [92, 103, "Hardware-device"]]}
{"id": 156948, "text": "We pre-train our SPEC-TRA model for 100 epochs on 8 Tesla-A100 GPUs with a batch size of 20 per GPU .", "Comments": [], "label": [[50, 51, "Device-Count"], [52, 67, "Hardware-device"]]}
{"id": 156953, "text": "The dataset was collected by crowdsourcing recordings through phone calls using the Appen platform [ 2 ] .", "Comments": [], "label": [[84, 98, "Software-Entity"]]}
{"id": 156964, "text": "Following the recent studies [ Shuo , 2022 ] [ Lample et al. , , 2018 ] , two BLEU scores are computed by the Natural Language Toolkit ( NLTK ) [ Bird et al. , , 2009 ] in our work : self-BLEU , which is the BLEU score between the output and input , and ref-BLEU , which is the BLEU score between the output and human reference sentences .", "Comments": [], "label": [[110, 134, "Software-Entity"], [137, 141, "Software-Entity"]]}
{"id": 156968, "text": "We train our method on one machine with 1 NVIDIA 3090 GPU .", "Comments": [], "label": [[40, 41, "Device-Count"], [42, 57, "Hardware-device"]]}
{"id": 156971, "text": "4.4 Case study In terms of the StyleTrans and StyleTrans+CTPM , we randomly sampled 20000 sentences ( 10000 sentences per style ) from Yelp and projected them in a two-dimensional space using t-SNE [ van der ] [ Maaten and Hinton , 2008 ] as shown in Figure [ 6 . ]", "Comments": [], "label": [[192, 197, "Software-Entity"]]}
{"id": 156972, "text": "Figure 6 : Visualization of Yelp dataset representations using t-SNE .", "Comments": [], "label": [[63, 68, "Software-Entity"]]}
{"id": 156981, "text": "Third , we fine-tune and publish ( 3 ) dialect-robust models on the HuggingFace Hub [ Wolf et al. , , 2020 ] , which can be used directly in downstream applications .", "Comments": [], "label": [[68, 79, "Software-Entity"]]}
{"id": 156983, "text": "For each rule , we condition the perturbation on morphosyntactic signals from POS tags , noun and verb inflection , and dependency relations using the [ spaCy 2.1.0 ] ( https : //spacy.io/ ) [ Honnibal et al. , , 2020 ] and [ inflect 5.5.2 ] ( https : //pypi.org/project/inflect/ ) libraries .", "Comments": [], "label": [[153, 164, "Software-Entity"]]}
{"id": 156990, "text": "[ 3 ] We recruit speakers from Amazon Mechanical Turk and screen them using a Dialect Assessment Survey .", "Comments": [], "label": [[31, 53, "Cloud-Platform"]]}
{"id": 156992, "text": "We use CoQA sentences for our Gold Test Sets ( [ §4.4 ] , and for added syntactic diversity , we pull sentences from three nltk corpora : Reuters [ Rose et al. , , 2002 ] , Sentiment Analysis [ Pang and Lee , 2004 ] and Movie Reviews [ Pang ] [ and Lee , 2005 ] .", "Comments": [], "label": [[123, 127, "Software-Entity"]]}
{"id": 157007, "text": "Each model was trained on an Nvidia GeForce RTX 2080 Ti for approximately 6 hours .", "Comments": [], "label": [[29, 55, "Hardware-device"]]}
{"id": 157009, "text": "We used NVIDIA A100 to train these models with T5-3b , BART-large , T5-base , and BART-base using 8 GPUs for 52 hours , 4 GPUs for 32 hours , 4 GPUs for 4 hours , 4 GPU for 13 hours respectively .", "Comments": [], "label": [[8, 19, "Hardware-device"], [98, 104, "Device-Count"], [120, 126, "Device-Count"], [142, 148, "Device-Count"], [163, 168, "Device-Count"]]}
{"id": 157012, "text": "As for the extremely large T5-3B , we configured a batch size of 64 to speed up convergence and utilised DeepSpeed to save memory .", "Comments": [], "label": [[105, 114, "Software-Entity"]]}
{"id": 157014, "text": "Evaluation was done on an Nvidia GeForce RTX 2080 Ti and takes less than 10 minutes .", "Comments": [], "label": [[26, 52, "Software-Entity"]]}
{"id": 157016, "text": "On the other hand , trained NMT models are increasingly available on platforms such as Hugginface ( < https : //huggingface.co > ) and Opus-MT ( < https : //opus.nlpl.eu/Opus-MT > ) since these models can be directly used without public access to the original training data .", "Comments": [], "label": [[87, 97, "Software-Entity"]]}
{"id": 157026, "text": "Our method is compared with the following baseline methods : 3.2 Implementation Details We use byte pair encoding ( BPE ) [ Sennrich et al. , , 2016 ] with the vocabulary size of 32k .", "Comments": [], "label": [[95, 113, "Software-Entity"], [114, 121, "Software-Entity"]]}
{"id": 157035, "text": "D Detailed Results D.1 Chinese-to-English Translation The full results for the Chinese-to-English homogeneous model setting for all configurations are shown in Table [ 13 . ] Our experiments are conducted on NVIDIA A100 GPUs .", "Comments": [], "label": [[208, 224, "Hardware-device"]]}
{"id": 157040, "text": "We implement the process of AMR re-focusing by the PENMAN package [ Goodman , 2020 ] . [ 4 ] Linearizing AMR graph .", "Comments": [], "label": [[51, 57, "Software-Entity"]]}
{"id": 157043, "text": "Specifically , we consider the GPT-2 model [ Rad ] [ ford et al. , , 2019 ] implemented by HuggingFace 's Transformers [ Wolf et al. , , 2020 ] to compute the perplexity of a candidate paraphrase .", "Comments": [], "label": [[91, 105, "Software-Entity"]]}
{"id": 157045, "text": "We used the Amazon Mechanical Turk [ 5 ] to conduct the human evaluation .", "Comments": [], "label": [[12, 34, "Cloud-Platform"]]}
{"id": 157047, "text": "It takes around 3 hours to train the SimCSE models for a single NVIDIA RTX A6000 GPU with 48GB memory .", "Comments": [], "label": [[57, 63, "Device-Count"], [64, 84, "Hardware-device"], [90, 101, "Device-Memory"]]}
{"id": 157048, "text": "for each ( * source sentence * , * paraphrase sentence * ) pair in the dataset , we treat the paraphrase sentence as the target sentence and use the Stanford CoreNLP toolkit [ Manning et al. , , 2014 ] to extract constituency parse from the paraphrase sentence as the target parse .", "Comments": [], "label": [[149, 165, "Software-Entity"]]}
{"id": 157049, "text": "It takes around 12 hours to train the SCPN model for a single NVIDIA RTX A6000 GPU with 48GB memory .", "Comments": [], "label": [[55, 61, "Device-Count"], [62, 82, "Hardware-device"], [88, 99, "Device-Memory"]]}
{"id": 157053, "text": "It takes around 5 minutes to train a fewshot classifier for a single NVIDIA RTX A6000 GPU with 48GB memory .", "Comments": [], "label": [[62, 68, "Device-Count"], [69, 89, "Hardware-device"], [95, 106, "Device-Memory"]]}
{"id": 157069, "text": "The authors employ Amazon Mechanical Turk crowd-workers to generate questions based on Wikipedia tables with cells linked to Wikipedia pages .", "Comments": [], "label": [[19, 41, "Cloud-Platform"]]}
{"id": 157070, "text": "B.3 Implementation Details MITQA is implemented using Pytorch version 1.8 and Huggingface 's transformers [ 3 ] [ Wolf et al. , , 2020 ] library .", "Comments": [], "label": [[54, 61, "Software-Entity"], [78, 92, "Software-Entity"]]}
{"id": 157071, "text": "We train our models using two NVIDIA A100 GPUs .", "Comments": [], "label": [[26, 29, "Device-Count"], [30, 46, "Hardware-device"]]}
{"id": 157073, "text": "Average Runtime : Overall training of MITQA takes approximately 24 hours on A100 gpu .", "Comments": [], "label": [[76, 84, "Hardware-device"]]}
{"id": 157082, "text": "Note that we remove all the stopwords in T , using the vocabulary provided by NLTK [ 4 ] .", "Comments": [], "label": [[78, 82, "Software-Entity"]]}
{"id": 157104, "text": "Our model has 65.9M parameters , and the implementations are based on HuggingFace 's Transformers [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[70, 84, "Software-Entity"]]}
{"id": 157105, "text": "We conduct all the experiments on an NVIDIA-3090 GTX GPU with mixed precision .", "Comments": [], "label": [[37, 56, "Hardware-device"]]}
{"id": 157106, "text": "The NLTK package version is 3.6.2 . 4 Results 4.1 NLG Results Table [ 1 ] shows the experimental results .", "Comments": [], "label": [[4, 8, "Software-Entity"]]}
{"id": 157125, "text": "For each setup , our DICL contains around 50,000 prompts ( see Table [ 8 ] and 50 dev examples per class , so the most expensive setup ( running OPT-13B on BoolQ ) costs more than 500 GPU hours on an RTXA6000 GPU .", "Comments": [], "label": [[200, 212, "Hardware-device"]]}
{"id": 157128, "text": "A.5 Implementation Details We use PyTorch and Huggingface transformers to implement in-context learning on GPT-Neo-2.7B [ Black et al. , , 2021 ] , GPTJ-6B [ Wang and Ko ] [ matsuzaki , 2021 ] , OPT-6.7B [ Zhang et al. , , 2022a ] , and OPT-13B .", "Comments": [], "label": [[34, 41, "Software-Entity"], [46, 57, "Software-Entity"]]}
{"id": 157129, "text": "We run all our evaluations on a single RTXA6000 GPU ( 48GB ) .", "Comments": [], "label": [[32, 38, "Device-Count"], [39, 51, "Hardware-device"], [54, 58, "Device-Memory"]]}
{"id": 157130, "text": "Most of our experiments can also be run on an RTX3090 GPU ( 24GB ) , except that OPT-13B model requires a GPU with larger memory .", "Comments": [], "label": [[46, 57, "Hardware-device"], [60, 64, "Device-Memory"]]}
{"id": 157131, "text": "Our data collection on DICL costs hundreds of GPU hours on an RTXA6000 .", "Comments": [], "label": [[62, 70, "Hardware-device"]]}
{"id": 157132, "text": "Once we finish the collection , our DATAMODELS method only takes about 5 seconds to train a datamodel on an i7-10700 CPU , and our CONDACC method does not involve any training , but simply calculates accuracy .", "Comments": [], "label": [[108, 120, "Hardware-device"]]}
{"id": 157147, "text": "A More Implementation Details A.1 Hyper-parameters We report the learning rates , the batch sizes , and the max sequence length for DSEE in Table [ 11 . ] The device we used for experiments are various , including NVIDIA GeForce GTX 1080 Ti , GeForce RTX 2080 Ti , Titan RTX , and A6000 .", "Comments": [], "label": [[214, 240, "Hardware-device"], [243, 262, "Hardware-device"], [265, 274, "Hardware-device"], [281, 286, "Hardware-device"]]}
{"id": 157151, "text": "To this end , we align Cognition and Affection for reSponding Empathetically ( CASE ) on coarse and fine-grained levels by fusing sentence-level commonsense knowledge from COMET [ Bosselut et al. , 2019 ] and word-level concept knowledge from ConceptNet [ Speer et al. , , 2017 ] .", "Comments": [], "label": [[172, 177, "Software-Entity"], [243, 253, "Software-Entity"]]}
{"id": 157154, "text": "Unlike commonsense knowledge that provides sentence-level commonsense expression , we adopt ConceptNet [ Speer et al. , , 2017 ] as concept knowledge , which provides word-level human knowledge and is widely used in various NLP tasks [ Zhang et al. , 2020 ] [ Zhong et al. , , 2021 ] [ Zhou et al. , , 2021 ] [ Yang et al. , , 2022 ] .", "Comments": [], "label": [[92, 102, "Software-Entity"]]}
{"id": 157155, "text": "Following [ Li et al. , 2022 ] , we use NRC_VAD [ Mohammad , 2018 ] to assign emotion intensity to concepts in ConceptNet ( processing details are in [ Li et al. , 2022 ] ) severed to extract the contextual emotional state manifested in the context , and align it with contextual cognition .", "Comments": [], "label": [[111, 121, "Software-Entity"]]}
{"id": 157159, "text": "Following [ Li et al. , 2022 ] , we use ConceptNet to infer the related concepts for each token w ∈ T , among which only the the top N′ emotional concepts ( according to the emotion intensity ω ( c ) ) are used for constructing GEC .", "Comments": [], "label": [[40, 50, "Software-Entity"]]}
{"id": 157177, "text": "Implementation Details We implemented all models with Pytorch .", "Comments": [], "label": [[54, 61, "Software-Entity"]]}
{"id": 157179, "text": "All models are trained on a GPU-P100 machine .", "Comments": [], "label": [[28, 36, "Hardware-device"]]}
{"id": 157182, "text": "Ethical Considerations In this paper , our experiments adopt the widely used EMPATHETICDIALOGUES benchmark , an open-source dataset collected from Amazon Mechanical Turk ( MTurk ) that does not contain personal information .", "Comments": [], "label": [[147, 169, "Cloud-Platform"]]}
{"id": 157189, "text": "The base model is publicly available in the Hugging-Face [ 5 ] platform , under * Apache License 2.0 * .", "Comments": [], "label": [[44, 56, "Software-Entity"]]}
{"id": 157190, "text": "Training Parameters We used a server comprised of two NVIDIA Quadro RTX 8000 GPUs for model training .", "Comments": [], "label": [[50, 53, "Device-Count"], [54, 81, "Hardware-device"]]}
{"id": 157202, "text": "Most finetuning steps take an hour or less on an A100 GPU .", "Comments": [], "label": [[49, 57, "Hardware-device"]]}
{"id": 157203, "text": "To put it all together , in the main experiment , 30 iterations with 8 contributors , 36 test sets , and 5 seeds , required approximately 4,800 A100 GPU hours and 3.2 TB of memory if all models are to be saved once .", "Comments": [], "label": [[144, 152, "Hardware-device"]]}
{"id": 157226, "text": "A.2 Training Details All models were trained on an HPC cluster , where each training run was provided with 100 GB RAM , 2 CPUs , and 1 V100 GPU .", "Comments": [], "label": [[107, 117, "Device-Memory"], [120, 126, "Device-Count"], [133, 134, "Device-Count"], [135, 143, "Hardware-device"]]}
{"id": 157236, "text": "Therefore , we did not use this external knowledge in our reimplementation of MGSKD . 4.3 Implementation Details Our code is implemented in Pytorch with the Transformers package [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[140, 147, "Software-Entity"], [157, 169, "Software-Entity"]]}
{"id": 157239, "text": "A.2 Hyperparameter Settings We run all experiments on GeForce RTX 2080 Ti GPUs .", "Comments": [], "label": [[54, 78, "Hardware-device"]]}
{"id": 157246, "text": "Concretely , we use a BERT-Base architectural backbone , as implemented on HuggingFace [ 4 ] [ Wolf et al. , , 2020 ] , with ∼ 110 million parameters .", "Comments": [], "label": [[75, 86, "Software-Entity"]]}
{"id": 157247, "text": "We use the same WordPiece tokenization as the original BERT model .", "Comments": [], "label": [[16, 25, "Software-Entity"]]}
{"id": 157250, "text": "Pre-training each model took 5 days with 8 V100 GPUs , while it took roughly a day to run GLUE fine-tuning with 8 GPUs ( 42 fine-tuning hyper-parameters for each { model , task } , [ §3.1 ] .", "Comments": [], "label": [[41, 42, "Device-Count"], [43, 52, "Hardware-device"]]}
{"id": 157255, "text": "First , The model inference is accessible via OpenAI API . we revisit and augment ELMo — which incorporates a degree of bidirectionality at fine-tuning ( albeit not at pre-training ) — with Transformers and whole model fine-tuning , facilitating a fair comparison with BERT .", "Comments": [], "label": [[46, 52, "Software-Entity"]]}
{"id": 157258, "text": "Concretely , we use the BERT codebase as implemented on Huggingface , and conduct some slight modifications in terms of removing the nextsentence prediction loss as stated above .", "Comments": [], "label": [[56, 67, "Software-Entity"]]}
{"id": 157263, "text": "Our entire codebase is based on Huggingface 's open-source Transformer implementation , which is released under the Apache-2.0 license .", "Comments": [], "label": [[32, 46, "Software-Entity"]]}
{"id": 157279, "text": "We use NVIDIA GeForce RTX 2080 , 1080 , and TITAN X , M40 GPUs for the BERTBase experiments and use GeForce RTX 8000 and Tesla M40 for the BERTLarge experiments .", "Comments": [], "label": [[7, 30, "Hardware-device"], [33, 37, "Hardware-device"], [44, 51, "Hardware-device"], [54, 62, "Hardware-device"], [100, 116, "Hardware-device"], [121, 130, "Hardware-device"]]}
{"id": 157300, "text": "Therefore , we first establish an architectural baseline by training ( i ) TFIDF-based statistical ( Multinomial Naive Bayes , Logistic Regressor , Random Forest , SVMs , and MLP network ) , ( ii ) Bi-directional GRU with Fasttext embeddings [ Gupta et al. , , 2019 ] , CNNs over character n-grams [ Shrestha et al. , , 2017 ] , ( iii ) Pre-trained BERT-base-cased [ Devlin et al. , 2019 ] , RoBERTa-base [ Liu et al. , , 2019 ] , and a DistilBERT-base-cased [ Sanh et al. , , 2019 ] sequence classifiers to identify 1,422 unique vendor accounts from 93,586 ads on the Dreams market .", "Comments": [], "label": [[222, 230, "Software-Entity"]]}
{"id": 157301, "text": "During the evaluation , we compare the performance of our transfer-BiGRU against BERT-base-cased and two-layer BiGRU ( with fasttext embeddings ) classifiers ( aka end-to-end baselines ) when trained from scratch on the LR dataset .", "Comments": [], "label": [[124, 132, "Software-Entity"]]}
{"id": 157302, "text": "While conventional neural networks such as character-based CNN and Bidirectional GRU with fasttext embeddings performed better than the statistical models , we noted a considerable increase in performance with the transformers-based architecture on our datasets .", "Comments": [], "label": [[90, 98, "Software-Entity"]]}
{"id": 157305, "text": "However , since the macro-F1 score is computed for the unweighted arithmetic mean of F1 for all class labels , the absence of previously We generate the scatter plot using [ Plotly , ] ( https : //plotly.com/python/ ) which allows us to zoom infinitely for any vendor .", "Comments": [], "label": [[174, 180, "Software-Entity"]]}
{"id": 157306, "text": "Furthermore , we also train a BERT-cased and a BiGRU classifier with fasttext embeddings from scratch to adapt new market knowledge and vendors from the emerging LR dataset .", "Comments": [], "label": [[69, 77, "Software-Entity"]]}
{"id": 157307, "text": "Furthermore , we also show the training feasibility of our transfer-BiGRU on a low-end graphic card , GeForce-MX110 , with 2 GB of GPU memory .", "Comments": [], "label": [[102, 115, "Hardware-device"], [122, 141, "Device-Memory"]]}
{"id": 157308, "text": "These estimations were conducted on Tesla V100-SXM2- 32GB ( TDP of 300W ) using the [ MachineLearn ] ( https : //mlco2.github.io/impactcompute ) [ ing Impact calculator ] ( https : //mlco2.github.io/impactcompute ) presented in [ Lacoste et al. , , 2019 ] .", "Comments": [], "label": [[36, 51, "Hardware-device"], [53, 57, "Device-Memory"]]}
{"id": 157313, "text": "Training : We perform the training and evaluation of our Neural Networks on a single Tesla V100 GPU with 32 GBs of memory .", "Comments": [], "label": [[78, 84, "Device-Count"], [85, 99, "Hardware-device"], [105, 121, "Device-Memory"]]}
{"id": 157314, "text": "The training and evaluation of statistical classifiers are performed on a server with one Intel Xeon Processor E5-2698 v4 and 512 GBs of RAM .", "Comments": [], "label": [[86, 89, "Device-Count"], [90, 121, "Hardware-device"], [126, 140, "Device-Memory"]]}
{"id": 157315, "text": "Finally , we train our distilled transfer-BiGRU model for the Low-Resource setting on a GeForce-MX110 graphic card with 2 GBs of memory .", "Comments": [], "label": [[88, 101, "Hardware-device"], [120, 135, "Device-Memory"]]}
{"id": 157317, "text": "The RNN architecture contains a two-layer Bidirectional-GRU model with two fully connected layers and fasttext embeddings .", "Comments": [], "label": [[102, 110, "Software-Entity"]]}
{"id": 157318, "text": "We first pack and pad the input sequence with variable length through a PyTorch function and then pass it to the embedding layer .", "Comments": [], "label": [[72, 79, "Software-Entity"]]}
{"id": 157319, "text": "After some experimentations , we set the All the models are implemented in python [ Van Rossum ] [ and Drake Jr , 1995 ] using [ Sklearn ] ( https : //scikit-learn.org/ ) [ Pedregosa et al. , , 2011 ] , [ PyTorch ] ( https : //pytorch.org/ ) [ Paszke et al. , , 2019 ] , and [ Hugging-face ] ( https : //huggingface.co/ ) [ Wolf et al. , , 2020 ] frameworks .", "Comments": [], "label": [[88, 94, "Software-Entity"], [129, 136, "Software-Entity"], [205, 212, "Software-Entity"], [277, 289, "Software-Entity"]]}
{"id": 157322, "text": "Evaluation Metrics : We evaluate our trained classifiers against accuracy , micro-average F1 , and macro-average F1 ( commonly known as macro-F1 and micro-F1 ) using the classification report from scikit-learn .", "Comments": [], "label": [[197, 209, "Software-Entity"]]}
{"id": 157335, "text": "A.2 Implementations We use Huggingface 's implementations of BERT and RoBERTa [ Wolf et al. , , 2020 ] [ 3 ] .", "Comments": [], "label": [[27, 41, "Software-Entity"]]}
{"id": 157336, "text": "The hyperparameters can be found in Table [ 10 . ] We use Tesla V100 GPU cards for conducting all the experiments .", "Comments": [], "label": [[58, 72, "Hardware-device"]]}
{"id": 157361, "text": "We use two NVIDIA Tesla V100 32GB SXM2 cards for the experiments . 6 Results This section presents the results obtained by the proposed model MMBV and the proposed framework dynamic UDA for the task of multilabel movie genre classification .", "Comments": [], "label": [[7, 10, "Device-Count"], [11, 28, "Hardware-device"], [29, 33, "Device-Memory"]]}
{"id": 157373, "text": "We also provide access to the benchmark through the HuggingFace Hub [ Lhoest et al. , , 2021 ] to allow for ease of use and we encourage model sharing for Bulgarian .", "Comments": [], "label": [[52, 63, "Software-Entity"]]}
{"id": 157378, "text": "The toolkit is implemented in PyTorch [ Paszke et al. , 2019 ] , using the * transformers * library [ Wolf et al. , 2020 ] .", "Comments": [], "label": [[30, 37, "Software-Entity"], [77, 89, "Software-Entity"]]}
{"id": 157379, "text": "Moreover , we integrate and release publicly all datasets ( in accordance with their licenses ; see the next paragraph ) from * bgGLUE * in the HuggingFace datasets repository [ Lhoest et al. , , 2021 ] .", "Comments": [], "label": [[144, 155, "Software-Entity"]]}
{"id": 157387, "text": "Additionally , we release the benchmark and the models on the HuggingFace Hub , which further reduces the environmental impact , as fine-tuning again is computationally costly , especially for larger models .", "Comments": [], "label": [[62, 73, "Software-Entity"]]}
{"id": 157391, "text": "To calculate the statistics , we split the texts into words using the NLTK [ Bird et al. , 2009 ] tokenizer .", "Comments": [], "label": [[70, 74, "Software-Entity"]]}
{"id": 157392, "text": "We used a custom crawler , * Beautiful-Soup * to parse the HTML , and per-site CSS selectors to extract the articles ' text .", "Comments": [], "label": [[29, 43, "Software-Entity"]]}
{"id": 157394, "text": "Amazon Mechanical Turk evaluations demonstrate the effectiveness of our framework .", "Comments": [], "label": [[0, 22, "Cloud-Platform"]]}
{"id": 157396, "text": "Discover news articles : We use GDELT , a Table 2 : Seed terms used for identifying domainspecific news articles : Each phrase is searched within the GDELT news database and the top articles that match the search terms are scraped using Beautiful-Soup [ Richardson , 2007 ] for processing .", "Comments": [], "label": [[236, 251, "Software-Entity"]]}
{"id": 157403, "text": "Models were trained on a 64-core server with 2 TITAN RTX GPUs running Question-Answering and Co-reference Resolution in tandem .", "Comments": [], "label": [[45, 46, "Device-Count"], [47, 61, "Hardware-device"]]}
{"id": 157404, "text": "B Instructions to Amazon Mechanical Turk workers For both Amazon Mechanical Turk ( AMT ) tasks described below , workers were required to be Masters-granted ( < https : //www.mturk.com/help > ) , present in the en_US locale .", "Comments": [], "label": [[18, 40, "Cloud-Platform"], [58, 88, "Cloud-Platform"]]}
{"id": 157409, "text": "The average length is calculated after word tokenization using NLTK [ Wagner , 2010 ] . with co-referenced pronouns .", "Comments": [], "label": [[63, 67, "Software-Entity"]]}
{"id": 157432, "text": "A.3 Implementation Details We run our experiments on a GTX 3090 GPU .", "Comments": [], "label": [[55, 67, "Hardware-device"]]}
{"id": 157463, "text": "The fine-tuning is conducted on Tesla V100 GPUs for approximately 10 hours on the largest corpus .", "Comments": [], "label": [[32, 47, "Hardware-device"]]}
{"id": 157464, "text": "ERNIE-mmLayout [ Wang et al. , , 2022b ] utilizes a third-party library spaCy [ 3 ] to provide external knowledge for the Common Sense Enhancement module in the system .", "Comments": [], "label": [[72, 77, "Software-Entity"]]}
{"id": 157478, "text": "Details of model sizes and compute requirements for finetuning can be found in Table [ 4 . ] All models were trained on 1 node with 2 GPUs , and the time reported is the total number of GPU hours .", "Comments": [], "label": [[132, 138, "Device-Count"]]}
{"id": 157480, "text": "In total , this project used 2,256 GPU hours across NVIDIA P100 , V100 , and A40 GPUs .", "Comments": [], "label": [[52, 63, "Hardware-device"], [66, 70, "Hardware-device"], [77, 85, "Hardware-device"]]}
{"id": 157481, "text": "For QueerNews , articles were sentence segmented using SpaCy [ Montani et al. , 2023 ] and each sentence was treated as a training datum .", "Comments": [], "label": [[55, 60, "Software-Entity"]]}
{"id": 157499, "text": "C.2 Experiment Settings Our experiments are conducted on a workstation of dual GeForce GTX 1080 Ti with 32G memory and the environment of torch 1.7.1 .", "Comments": [], "label": [[74, 78, "Device-Count"], [79, 98, "Hardware-device"], [105, 114, "Device-Memory"], [138, 149, "Software-Entity"]]}
{"id": 157555, "text": "We use the implementation for Hugging-Face [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[30, 42, "Software-Entity"]]}
{"id": 157556, "text": "All the experiments are conducted on NVIDIA RTX A6000 GPUs .", "Comments": [], "label": [[37, 58, "Hardware-device"]]}
{"id": 157559, "text": "MinIE [ Ponza et al. , , 2018 ] , and Stanford CoreNLP [ Cao et al. , , 2018 ] [ Zhang et al. , , 2021 ] for extraction .", "Comments": [], "label": [[0, 5, "Software-Entity"], [38, 54, "Software-Entity"]]}
{"id": 157560, "text": "[ Kroll et al. , 2021 ] also explicitly chooses Stanford CoreNLP and OpenIE6 for their fast extraction times ( FE ) .", "Comments": [], "label": [[48, 64, "Software-Entity"], [69, 76, "Software-Entity"]]}
{"id": 157563, "text": "We run all experiments on a Quadro RTX 5000 GPU .", "Comments": [], "label": [[28, 47, "Hardware-device"]]}
{"id": 157573, "text": "We consider four clustering models C used by [ Liu et al. , 2019 ] : a ( finite ) multinomial mixture model ( FMM ) with a Dirichlet prior over π ∼ Dir ( p , γ = 75 ) , where p is the number of clusters and each cluster distribution π is a multinomial distribution with Dirichlet priors Dir ( d , γ = 0.1 ) , where d is the size of the label space , using the bnpy library [ Hughes and Sudderth , 2013 ] , a Gaussian mixture model ( GMM ) and a K-means model ( KM ) from scikit-learn , and the Gensim implementation of Latent Dirichlet Allocation ( LDA ) ( Reh˚u ˇ [ ˇrek and Sojka , 2010 ] .", "Comments": [], "label": [[471, 483, "Software-Entity"], [494, 500, "Software-Entity"]]}
{"id": 157574, "text": "The supervised model ( CNN ) for H is a 1D convolutional neural network [ Kim , 2014 ] , with three convolution/max pool layers ( of dimension 128 ) followed by a dropout ( 0.5 ) and softmax layer implemented with TensorFlow .", "Comments": [], "label": [[214, 224, "Software-Entity"]]}
{"id": 157575, "text": "We represent language features for both our unsupervised learning and classification experiments using a state-of-the-art pre-trained paraphrase-MiniLM-L6-v2 transformer model using SBERT ( sentence-transformers ) library [ Reimers and Gurevych , 2019 ] .", "Comments": [], "label": [[190, 211, "Software-Entity"]]}
{"id": 157577, "text": "Computation : As our experiments follow a twostage setup , the first phase ( data mixing ) of it can be further optimized to run on GPUs similar to the second phase ( classification ) , which is running on GPU through the TensorFlow/Keras implementation .", "Comments": [], "label": [[222, 232, "Software-Entity"], [233, 238, "Software-Entity"]]}
{"id": 157579, "text": "B Experimental Setup Our experimental setup consists of the following configurations ; Setup - Ubuntu 18.04 , Intel i6- 7600k ( 4 cores ) at 4.20GHz , 32GB RAM , and nVidia GeForce RTX 2070 Super 8GB VRAM .", "Comments": [], "label": [[110, 149, "Hardware-device"], [151, 159, "Device-Memory"], [166, 189, "Hardware-device"], [196, 204, "Device-Memory"]]}
{"id": 157580, "text": "Setup - Debian 9.8 , Intel Xeon ( 6 cores ) at 2.2GHz , 32GB RAM , and nVidia Tesla P100 12GB VRAM .", "Comments": [], "label": [[21, 53, "Hardware-device"], [56, 64, "Device-Memory"], [71, 88, "Hardware-device"], [89, 98, "Device-Memory"]]}
{"id": 157596, "text": "To run experiments using LMs larger than two billion parameters , we use a single V100 Volta GPU with 32GB GPU memories .", "Comments": [], "label": [[75, 81, "Device-Count"], [82, 96, "Hardware-device"], [102, 110, "Device-Memory"]]}
{"id": 157602, "text": "Implementation Details : We run all our experiments using the huggingface [ Wolf et al. , , 2020 ] implementation of transformers on Nvidia V100 16GB GPUs with a batch size of 32 and learning rate ranging in { 1−5 } e−5 .", "Comments": [], "label": [[62, 73, "Software-Entity"], [133, 144, "Hardware-device"], [145, 154, "Device-Memory"]]}
{"id": 157611, "text": "Otherwise , for TableQA systems in the literature , we use the released checkpoints from huggingface .", "Comments": [], "label": [[89, 100, "Software-Entity"]]}
{"id": 157616, "text": "Bold denotes the best DA for each split . * * references a row in Table [ 4 . ] huggingface implementations , due to crossframework dependencies or any model-specific preprocessing or evaluation scripts .", "Comments": [], "label": [[80, 91, "Software-Entity"]]}
{"id": 157617, "text": "In Table [ 4 ] we compare ITR in a unified experimentation setting using the released model checkpoints in huggingface for inference-only evaluation , and the in-house fine-tuned TaPEx model that enables both training and evaluation with ITR .", "Comments": [], "label": [[107, 118, "Software-Entity"]]}
{"id": 157621, "text": "We initialize E and E using DPR weights released via huggingface , i.e. , facebook/dpr-question_encodersingle-nq-base and facebook/dprctx_encoder-single-nq-base , respectively .", "Comments": [], "label": [[53, 64, "Software-Entity"]]}
{"id": 157625, "text": "We train ITR retrieval component on an V100 machine , and the total training time for our main ITR variant is about 380 minutes .", "Comments": [], "label": [[39, 43, "Hardware-device"]]}
{"id": 157627, "text": "We initialize TableQA models with the released checkpoint from huggingface for TaPEx pretraining , i.e. , microsoft/tapex-large .", "Comments": [], "label": [[63, 74, "Software-Entity"]]}
{"id": 157628, "text": "As previously shown in Table [ 6 , ] we notice a slight difference on the performance of the released TaPEx checkpoints via huggingface and the inhouse fine-tuned TaPEx .", "Comments": [], "label": [[124, 135, "Software-Entity"]]}
{"id": 157630, "text": "A.3 Inference with ITR Models In Table [ 9 ] we report the model checkpoints from huggingface that we used as baseline when applying ITR at inference time only .", "Comments": [], "label": [[82, 93, "Software-Entity"]]}
{"id": 157631, "text": "As mentioned in § [ 5 , ] there are some differences between the performance obtained when evaluating the huggingface implementation of the baselines and the performance reported in each separate paper , mainly due to data processing and evaluation scripts .", "Comments": [], "label": [[106, 117, "Software-Entity"]]}
{"id": 157632, "text": "The authors have adjusted the code to an encoder-decoder architecture , however maintaining the tokenizer of TaBERT rather than TaPEx.This detail has not been transferred to the huggingface implementation .", "Comments": [], "label": [[178, 189, "Software-Entity"]]}
{"id": 157633, "text": "To this end , we unify the implementations in a single evaluation framework , using the dataset splits , checkpoints and evaluation methods made available in the huggingface library for all the baselines .", "Comments": [], "label": [[162, 173, "Software-Entity"]]}
{"id": 157634, "text": "Table 9 : Checkpoints released via the huggingface library for TaPEx , TaPas and OmniTab , that we use as baselines for inference only experiments with ITR .", "Comments": [], "label": [[39, 50, "Software-Entity"]]}
{"id": 157635, "text": "We train each model on an A100 machine .", "Comments": [], "label": [[26, 30, "Hardware-device"]]}
{"id": 157652, "text": "B Implementation Details For all experiments , we train models across 4 A40 GPUs ( 48GB each ) .", "Comments": [], "label": [[70, 71, "Device-Count"], [72, 80, "Hardware-device"], [83, 87, "Device-Memory"]]}
{"id": 157659, "text": "We use NLTK [ Bird et al. , 2009 ] to perform the word and sentence tokenization .", "Comments": [], "label": [[7, 11, "Software-Entity"]]}
{"id": 157676, "text": "We post-train the models on Tesla P40 GPUs with batch size of 16 .", "Comments": [], "label": [[28, 42, "Hardware-device"]]}
{"id": 157677, "text": "The full set of pre-survey questions can be found in Appendix [ D. ] 3.1.2 Paper Selection & Expert Reproduction Next , we carefully selected a small number of papers in recent ACL conferences , verifying that we could reproduce their results in a comparable , reasonable amount of time and effort using a single GPU [ 3 ] from an institution-provided computing cluster managed with Slurm .", "Comments": [], "label": [[383, 388, "Software-Entity"]]}
{"id": 157680, "text": "Students used Slurm to request and acquire sole control of resources , and all resource utilization was automatically tracked by this centralized platform .", "Comments": [], "label": [[14, 19, "Software-Entity"]]}
{"id": 157682, "text": "Time spent to reproduce results is divided into two phases : System setup time is self-reported in the postsurvey , while runtime is extracted from the centralized Slurm system using its resource usage tracking feature , which accurately reports the total GPU usage time per student .", "Comments": [], "label": [[164, 169, "Software-Entity"]]}
{"id": 157684, "text": "We focus on Python and PyTorch as these are commonly used in NLP research , including all selected papers .", "Comments": [], "label": [[23, 30, "Software-Entity"]]}
{"id": 157685, "text": "Meanwhile , students ' understanding of LSTMs [ Hochreiter and ] [ Schmidhuber , 1997 ] and transformers [ Vaswani et al. , 2017 ] is self-reported in the pre-survey based on related past homework assignments requiring students to implement them in PyTorch .", "Comments": [], "label": [[249, 256, "Software-Entity"]]}
{"id": 157687, "text": "Table 3 : Spearman correlation coefficients for how well self-reported code setup time and difficulty is predicted by skill level factors , including PyTorch and PyTorch experience , and self-reported understanding of NLP models covered in course homework assignments .", "Comments": [], "label": [[150, 157, "Software-Entity"], [162, 169, "Software-Entity"]]}
{"id": 157690, "text": "Fortunately , NLP researchers can rely on various computing environment management tools , such as pip [ 18 ] , conda [ 19 ] , Poetry , [ 20 ] and Docker . [ 21 ] Simply utilizing such tools when sharing our work can make a meaningful difference for beginners .", "Comments": [], "label": [[112, 117, "Software-Entity"], [127, 133, "Software-Entity"], [147, 153, "Software-Entity"]]}
{"id": 157693, "text": "Further , we suggest that NLP researchers should take extra care by using a centralized system like HuggingFace Datasets [ Lhoest et al. , , 2021 ] , [ 22 ] or , at a minimum , periodically verifying that important resources are still accessible .", "Comments": [], "label": [[100, 110, "Software-Entity"]]}
{"id": 157696, "text": "It is also worth noting that it is difficult to consistently calculate runtime ( as introduced in Section [ 3.2.1 ] of code on GPU hardware , as fluctuations may occur due to a number of factors , including the specific GPU hardware allocated to a student , [ 27 ] driver versions , and While experts used NVIDIA Tesla V100 GPUs with up to 16GB memory to reproduce results , NVIDIA A40 GPUs with up to 48GB memory are also available within the cluster .", "Comments": [], "label": [[306, 328, "Hardware-device"], [340, 351, "Device-Memory"], [375, 390, "Hardware-device"], [402, 413, "Device-Memory"]]}
{"id": 157698, "text": "Setup time and runtime defined in Section [ 3.2.1 . ] For reproducing all results , both experts and students used an onsite computing cluster that offers access to NVIDIA Tesla V100 GPUs with up to 16GB memory , and NVIDIA A40 GPUs with up to 48GB memory . duced results and those reported in papers .", "Comments": [], "label": [[165, 187, "Hardware-device"], [199, 210, "Device-Memory"], [217, 232, "Hardware-device"], [244, 255, "Device-Memory"]]}
{"id": 157704, "text": "More intuitively , we adopt t-sne to visualize the representation , and the results are shown in Figure [ 8 , ] which shows clear clusters for bots and humans .", "Comments": [], "label": [[28, 33, "Software-Entity"]]}
{"id": 157705, "text": "To evaluate whether BIC can capture the advanced bots after 2020 ( the TwiBot-20 published time ) , we Figure 8 : The t-sne plot of the semantic consistency representations .", "Comments": [], "label": [[118, 123, "Software-Entity"]]}
{"id": 157707, "text": "A Implementation Details We implement our framework with pytorch [ Paszke et al. , 2019 ] , PyTorch geometric [ Fey and Lenssen , 2019 ] , and the transformer library from huggingface [ Wolf et al. , , 2019 ] .", "Comments": [], "label": [[57, 64, "Software-Entity"], [92, 109, "Software-Entity"], [172, 183, "Software-Entity"]]}
{"id": 157709, "text": "A.2 Computation Our proposed method totally has 4.2M learnable parameters and 0.92 FLOPs [ 2 ] with hyperparameters presented in Table [ 4 . ] Our implementation is trained on an NVIDIA GeForce RTX 3090 GPU with 24GB memory , which takes approximately 0.06 GPU hours for training an epoch .", "Comments": [], "label": [[179, 206, "Hardware-device"], [212, 223, "Device-Memory"]]}
{"id": 157711, "text": "G Scientific Artifact The BIC model is implemented with the help of many widely-adopted scientific artifacts , including PyTorch [ Paszke et al. , , 2019 ] , NumPy [ Harris et al. , , 2020 ] , transformers [ Wolf et al. , , 2019 ] , sklearn [ Pe ] [ dregosa et al. , , 2011 ] , PyTorch Geometric [ Fey and ] [ Lenssen , 2019 ] .", "Comments": [], "label": [[121, 128, "Software-Entity"], [158, 163, "Software-Entity"], [193, 205, "Software-Entity"], [233, 240, "Software-Entity"], [278, 295, "Software-Entity"]]}
{"id": 157722, "text": "We use Hugging Face [ Wolf et al. , , 2020 ] , Py-Torch [ Paszke et al. , , 2019 ] for our experiments and use the Freebase setup specified on github [ 6 ] .", "Comments": [], "label": [[7, 19, "Software-Entity"], [47, 55, "Software-Entity"]]}
{"id": 157723, "text": "We use NVIDIA A100 GPU with 20 GB GPU memory and 60 GB RAM for training and inference of RnG-KBQA on GrailQAbility which takes 60 hours .", "Comments": [], "label": [[7, 22, "Hardware-device"], [28, 44, "Device-Memory"], [49, 58, "Device-Memory"]]}
{"id": 157727, "text": "We use NVIDIA V100 GPU with 32 GB GPU memory and 60 GB RAM for the training of ReTraCk on GrailQAbility which takes 50 hours .", "Comments": [], "label": [[7, 22, "Hardware-device"], [28, 44, "Device-Memory"], [49, 58, "Device-Memory"]]}
{"id": 157732, "text": "We use Hugging Face [ Wolf et al. , , 2020 ] , Py-Torch [ Paszke et al. , , 2019 ] for our experiments and use the Freebase setup specified on github [ 7 ] .Training configurations for schema retriver are same as mentioned in ReTraCk and training configurations for Exemplary Logical Form Retrieval is same as mentioned in Rng-KBQA .", "Comments": [], "label": [[7, 19, "Software-Entity"], [47, 55, "Software-Entity"]]}
{"id": 157733, "text": "We use NVIDIA A100 GPU with 40 GB GPU memory and 32 GB RAM for training TIARA Generator which takes around 8 hours for one model .", "Comments": [], "label": [[7, 22, "Hardware-device"], [28, 44, "Device-Memory"], [49, 58, "Device-Memory"]]}
{"id": 157734, "text": "Inference is performed parallely on 8 A100 GPUs with 40 GB GPU memory which takes around 1.5-2 hours . < https : //github.com/dki-lab/Freebase-Setup >", "Comments": [], "label": [[36, 37, "Device-Count"], [38, 47, "Hardware-device"], [53, 69, "Device-Memory"]]}
{"id": 157740, "text": "Models are trained with a batch size of 200 on one GeForce GTX 1080Ti GPU .", "Comments": [], "label": [[47, 50, "Device-Count"], [51, 73, "Hardware-device"]]}
{"id": 157751, "text": "For each journal , we downloaded full-text article PDFs from the Open-Access portion of the journal using available APIs , or scraping this content using Each experiment was run on a relatively large card with 40GB of GPU memory ( the NVIDIA A100 ) .", "Comments": [], "label": [[210, 228, "Device-Memory"], [235, 246, "Hardware-device"]]}
{"id": 157752, "text": "The dataset is available for download on the HuggingFace Datasets Hub under [ griffin/ChemSum . ] ( https : //huggingface.co/datasets/griffin/ChemSum ) Biomedical .", "Comments": [], "label": [[45, 56, "Software-Entity"]]}
{"id": 157757, "text": "For random , for each training instance , we take a random sample without replacement . 4 is the maximum number which fits in GPU memory on an A100 40GB card , even with a device batch size of one ( with gradient accumulation steps ) and half precision ( fp16 ) .", "Comments": [], "label": [[143, 147, "Hardware-device"], [148, 152, "Device-Memory"]]}
{"id": 157762, "text": "Association for Computational Linguistics . and clinical English model packages for the Stanza Python NLP library . * Journal of the American Medical Informatics Association * .", "Comments": [], "label": [[88, 94, "Software-Entity"]]}
{"id": 157763, "text": "First , following [ Goyal and Durrett , 2021 ] ; [ Lee et al. , 2022 ] , we identify all noun phrases [ 4 ] as candidates for masking using Stanza 's constituency parser [ Qi et al. , , 2020 ] .", "Comments": [], "label": [[140, 149, "Software-Entity"]]}
{"id": 157765, "text": "For the clinical corpus , we use the Stanza transformer model [ Qi et al. , , 2020 ] [ Zhang et al. , , 2021 ] trained on the i2b2 corpus [ Uzuner et al. , , 2011 ] , which learns to identify patient problems , tests , and treatments .", "Comments": [], "label": [[37, 43, "Software-Entity"]]}
{"id": 157766, "text": "Finally , for biomedical , we use the Stanza model trained on the BioNLP13CG corpus [ Pyysalo et al. , , 2015 ] , which includes a diverse set of 13 categories .", "Comments": [], "label": [[38, 44, "Software-Entity"]]}
{"id": 157767, "text": "D Evaluation Metrics D.1 Relevance For BERTScore [ Zhang et al. , , 2020b ] , we use * allenai/scibert_scivocab_uncased * weights and all default settings from HuggingFace [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[160, 171, "Software-Entity"]]}
{"id": 157772, "text": "The former is affected by missing clinical notes while the latter references are abstracts , which * should * be mostly en- google/pegasus-pubmed on the HuggingFace Transformers Hub [ Wolf et al. , , 2020 ] . tailed by the claims made in the main paper .", "Comments": [], "label": [[153, 164, "Software-Entity"]]}
{"id": 157773, "text": "F Training Details F.1 FT Training Details We fine-tune ( FT ) two state of the art longdocument summarization models for 50,000 steps : PRIMERA [ Xiao et al. , , 2022 ] ( the backbone is a Longformer Encoder-Decoder ( LED ) [ Beltagy et al. , 2020 ] model ) and LongT5 [ Guo et al. , , 2022 ] ( which incorporates the sparse attention of ETC [ Ainslie et al. , , 2020 ] into PEGASUS [ Zhang et al. , , 2020a ] ) on a single A100 40GB GPU with half precision ( FP16 ) [ 7 ] ) and a batch a size of 1 ( with 16 gradient accumulation steps ) .", "Comments": [], "label": [[418, 424, "Device-Count"], [425, 429, "Hardware-device"], [430, 438, "Device-Memory"]]}
{"id": 157786, "text": "We used V100 , RTX8000 , and RTX600 GPUS for training .", "Comments": [], "label": [[8, 12, "Hardware-device"], [15, 22, "Hardware-device"], [29, 40, "Hardware-device"]]}
{"id": 157795, "text": "Baseline KGEs are trained on NVIDIA Tesla P100 GPUs and binary classifiers are trained on AMD EPYC 7542 CPUs .", "Comments": [], "label": [[29, 51, "Hardware-device"], [90, 108, "Hardware-device"]]}
{"id": 157799, "text": "C KG Partitioning in FB15k-237 To verify the idea of relation clusters in the embedding space for KG partitioning , we show the t-SNE visualization of relation embeddings in FB15k-237 in Fig . [ 8 . ] Relations within the same cluster are assigned the same color .", "Comments": [], "label": [[128, 133, "Software-Entity"]]}
{"id": 157800, "text": "We do observe the clustering structure in the t-SNE plot .", "Comments": [], "label": [[46, 51, "Software-Entity"]]}
{"id": 157801, "text": "Figure 8 : t-SNE visualization of the KG partitioning result in FB15k-237 .", "Comments": [], "label": [[11, 16, "Software-Entity"]]}
{"id": 157804, "text": "The issue is resolved after ConvKB is implemented with PyTorch [ 3 ] , but the performance on FB15k-237 is still not as good as ConvKB originally reported in the paper .", "Comments": [], "label": [[55, 62, "Software-Entity"]]}
{"id": 157819, "text": "3.3 Implementation Details We base our implementation on the Longformer code included in the HuggingFace Transformers library [ Wolf et al. , , 2020 ] and continue training from the base model ( 110M parameters ) and large model ( 340M parameters ) checkpoints .", "Comments": [], "label": [[93, 104, "Software-Entity"]]}
{"id": 157821, "text": "Specifically we use the KILT Wikipedia snapshot [ 1 ] from 2019 [ Petroni et al. , , 2021 ] as provided by the HuggingFace Datasets library [ Lhoest et al. , 2021 ] .", "Comments": [], "label": [[111, 122, "Software-Entity"]]}
{"id": 157822, "text": "All experiments are conducted on a machine with eight A100 80GB GPUs .", "Comments": [], "label": [[48, 53, "Device-Count"], [54, 68, "Hardware-device"]]}
{"id": 157825, "text": "To construct the semantic graph G = { V , E } , we first obtain the coreference clusters from the context C by AllenNLP [ Shi ] [ and Lin , 2019 ] and build the set of initial nodes from phrases in the clusters .", "Comments": [], "label": [[111, 119, "Software-Entity"]]}
{"id": 157826, "text": "To enhance the connectedness of , we extract all named entities by * spaCy * [ 1 ] and add them as additional nodes if they are not in any clusters .", "Comments": [], "label": [[69, 74, "Software-Entity"]]}
{"id": 157833, "text": "Our human rating instructions are in Appendix [ A.9 . ] Implementation Details We fine-tune a RoBERTalarge [ Liu et al. , , 2019a ] as our binary * Question Type Classifier * with the pretrained checkpoints from fairseq [ Ott et al. , , 2019 ] on CoQA .", "Comments": [], "label": [[212, 219, "Software-Entity"]]}
{"id": 157835, "text": "The model is finetuned on a P40 Colab GPU for 10 epochs .", "Comments": [], "label": [[28, 41, "Hardware-device"]]}
{"id": 157836, "text": "Details of the input format are in Appendix [ A.5 . ] We initialise * SG-CQG * with pretrained checkpoints of T5base model [ Raffel et al. , , 2020 ] from Huggingface [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[155, 166, "Software-Entity"]]}
{"id": 157860, "text": "We also use the Stanford CoreNLP toolkit [ Manning et al. , , 2014 ] for a supplement .", "Comments": [], "label": [[16, 32, "Software-Entity"]]}
{"id": 157861, "text": "We implement our method based on the Pytorch version of Huggingface Transformer [ Wolf et al. , 2020 ] .", "Comments": [], "label": [[37, 44, "Software-Entity"], [56, 79, "Software-Entity"]]}
{"id": 157863, "text": "Our model is trained on an NVIDIA RTX 2080 GPU with 24GB memory .", "Comments": [], "label": [[27, 46, "Hardware-device"], [52, 63, "Device-Memory"]]}
{"id": 157879, "text": "A Implementation Our model is implemented based on PyTorch and HuggingFace 's Transformer [ Wolf et al. , , 2019 ] .", "Comments": [], "label": [[51, 58, "Software-Entity"], [63, 77, "Software-Entity"]]}
{"id": 157880, "text": "We train our model with a batch size of 4 for 50 epochs , which takes ~5 hours on a single A40 GPU .", "Comments": [], "label": [[84, 90, "Device-Count"], [91, 98, "Hardware-device"]]}
{"id": 157907, "text": "The number of subsequent knowledge pieces m described in Equation 1 is set to 2 for WoW and 1 for PersonaChat.3 Our models are implemented in PyTorch ( Paszke et al. , 2019 ) and trained on a NVIDIA Tesla V100 GPU.4 For decoding at inference , we use a top-k sampling scheme with k = 70 and a temperature of 0.7 .", "Comments": [], "label": [[142, 149, "Software-Entity"], [192, 213, "Hardware-device"]]}
{"id": 157925, "text": "We detect the script for each sentence and treat each language-script as a separate entity . * * 3.3 Ngram LMs and Language Divergence * * We train a 3-gram character-level language model for each language-script , using KenLM [ Heafield , 2011 ] .", "Comments": [], "label": [[221, 226, "Software-Entity"]]}
{"id": 157928, "text": "Some chunk-level filters are based on the notion of word : we use white space tokenization when possible and otherwise resort to sentencePiece [ Kudo and Richardson , 2018 ] trained by [ Costa-jussà et al. , 2022 ] .", "Comments": [], "label": [[129, 142, "Software-Entity"]]}
{"id": 157933, "text": "We take the probabilities of the ( genuinely ) new tokens directly from SentencePiece .", "Comments": [], "label": [[72, 85, "Software-Entity"]]}
{"id": 157934, "text": "We train Glot500-m on a server with eight NVIDIA RTX A6000 GPUs for two weeks .", "Comments": [], "label": [[36, 41, "Device-Count"], [42, 63, "Hardware-device"]]}
{"id": 157936, "text": "We use SimAlign [ Jalili Sabet et al. , , 2020 ] and align on the sub-word level on the Bible part of test , based on the representations of the LLM computed by transformer layer 8 as suggested in the original paper .", "Comments": [], "label": [[7, 15, "Software-Entity"]]}
{"id": 157943, "text": "We use KenLM library [ Heafield , 2011 ] to build n-gram models .", "Comments": [], "label": [[7, 12, "Software-Entity"]]}
{"id": 157955, "text": "Details We use BERT [ Devlin et al. , , 2019 ] from HuggingFace Transformers [ Wolf et al. , , 2020 ] as the PLM of our model .", "Comments": [], "label": [[52, 63, "Software-Entity"]]}
{"id": 157964, "text": "A T5 prompt-based classifier was trained using Open-Prompt [ Ding et al. , , 2022 ] to identify whether a given message would be appropriate to say to a per- This process was developed after pilot tests showed the random sample approach was unlikely to surface interesting cases , but annotators found it easier to ideate and write their own messages after being exposed to some example communication .", "Comments": [], "label": [[47, 58, "Software-Entity"]]}
{"id": 157965, "text": "Given the recent successes of promptbased models , we build models using the Open-Prompt library [ Ding et al. , , 2022 ] and , to support larger models , using the PEFT library [ Liu et al. , 2022 ] .", "Comments": [], "label": [[77, 88, "Software-Entity"], [165, 169, "Software-Entity"]]}
{"id": 157966, "text": "The OpenPrompt library was used to train t5-base and gpt2-med models using the prompt `` Is it appropriate for person1 to say `` quote '' to person2 , `` yes '' or `` no '' ?", "Comments": [], "label": [[4, 14, "Software-Entity"]]}
{"id": 157967, "text": "The PEFT library was used to train the large and xl variants of the flan-t5 model [ Chung et al. , , 2022 ] .", "Comments": [], "label": [[4, 8, "Software-Entity"]]}
{"id": 157977, "text": "Therefore , we run SpaCy NER [ Honnibal ] [ and Montani , 2017 ] on the dialog and remove all turns containing references to people , companies , countries , and nationalities in order to keep the dialog generic and maximally plausible in many different relationship contexts .", "Comments": [], "label": [[19, 24, "Software-Entity"]]}
{"id": 157981, "text": "The logistic regression model uses Scikit-learn [ Pedregosa et al. , , 2011 ] ; for each task , we adopt the evaluation metric used in the respective paper .", "Comments": [], "label": [[35, 47, "Software-Entity"]]}
{"id": 157982, "text": "Scikit-learn : Machine learning in python . * The Journal of Machine Learning Research ( JMLR ) * , 12:2825–2830 .", "Comments": [], "label": [[0, 12, "Software-Entity"]]}
{"id": 157984, "text": "A MiniLM classifier [ Wang et al. , , 2020 ] was trained using Huggingface Transformers [ Wolf et al. , 2019 ] for five epochs , keeping the model with the lowest training loss at any epoch ; Epoch 5 was selected .", "Comments": [], "label": [[63, 74, "Software-Entity"]]}
{"id": 157985, "text": "B.1 Computational resources All of our experiments were conducted on an Ubuntu 16.04.7 LTS machine installed with NVIDIA RTX A5000 and RTX A6000 GPUs having CUDA 11.3 .", "Comments": [], "label": [[114, 130, "Hardware-device"], [135, 149, "Hardware-device"], [157, 161, "Software-Entity"]]}
{"id": 157986, "text": "The Python packages used in our experiments include Pytorch 1.17.0 , Transformers 4.25.1 , PEFT 0.3.0 , OpenPrompt 1.0.1 , pandas 1.1.4 , spacy 3.3.2 , and Sci-kit learn 1.2.0 .", "Comments": [], "label": [[52, 59, "Software-Entity"], [69, 81, "Software-Entity"], [91, 95, "Software-Entity"], [104, 114, "Software-Entity"], [123, 129, "Software-Entity"], [138, 143, "Software-Entity"], [156, 169, "Software-Entity"]]}
{"id": 157988, "text": "The Label column corresponds to the label registered on the Hugging Face model repository .", "Comments": [], "label": [[60, 72, "Software-Entity"]]}
{"id": 157989, "text": "B.3 Classifiers from Sklearn For the classification of politeness and condescension tasks , we used logistic regression from sklearn with the solver as 'lbfgs ' and max_iter set to 400 .", "Comments": [], "label": [[21, 28, "Software-Entity"], [125, 132, "Software-Entity"]]}
{"id": 157991, "text": "Performance on the test set is reported in Table [ 9 . ] D Additional Prompt-based Model Details We train gpt2-base and t5-base using the OpenPrompt framework .", "Comments": [], "label": [[138, 148, "Software-Entity"]]}
{"id": 157994, "text": "We train the flan-t5-large and flan-t5-xl models using the PEFT library .", "Comments": [], "label": [[59, 63, "Software-Entity"]]}
{"id": 157996, "text": "We use PCA to capture regularity and then project relationships onto a 2D visualization using t-SNE [ van der ] [ Maaten and Hinton , 2008 ] , which is aimed at preserving local similarity in the spatial arrangement .", "Comments": [], "label": [[94, 99, "Software-Entity"]]}
{"id": 157997, "text": "If model predictions are capturing shared norms , we view t-SNE as potentially more useful than a PCA projection , as we want to visualize which re- lationships with similar judgments as being nearby ( what t-SNE does ) rather than optimizing the visualization to the global structure of distances ( what PCA does ) .", "Comments": [], "label": [[58, 63, "Software-Entity"]]}
{"id": 157998, "text": "The t-SNE projection was designed using guidance from [ Wattenberg et al. , 2016 ] ; a perplexity of 40 was used .", "Comments": [], "label": [[4, 9, "Software-Entity"]]}
{"id": 157999, "text": "While the projection is only a visual tool , and aspects such as distance are not meaningful in t-SNE visualizations , the grouping and neighbors suggest the model is sensitive to power/status and social distance in how it decides appropriateness based on the relationship .", "Comments": [], "label": [[96, 101, "Software-Entity"]]}
{"id": 158000, "text": "E.3 Per Relationship Results Table [ 11 ] shows the peformance of the flan-t5-xl model on the test set , broken down by relationship Figure 8 : A plot of relationships , projected using t-SNE from a PCA of their appropriateness judgments across the 47,801 messages we use from the PRIDE dataset .", "Comments": [], "label": [[186, 191, "Software-Entity"]]}
{"id": 158011, "text": "4.3 Implementation Details In accordance with prior work [ Bao et al. , , 2019 ] , we use pre-trained fastText [ Joulin et al. , , 2016 ] for word embedding .", "Comments": [], "label": [[102, 110, "Software-Entity"]]}
{"id": 158013, "text": "The model is implemented in PyTorch [ Paszke et al. , 2017 ] using the Adam [ Kingma and Ba , 2014 ] optimizer with a 10− learning rate .", "Comments": [], "label": [[28, 35, "Software-Entity"]]}
{"id": 158014, "text": "All experiments are conducted with NVIDIA V100 GPUs . 4.4 Comparisons The experimental results are shown in Table [ 2 ] in terms of various datasets , methods , and few-shot settings .", "Comments": [], "label": [[35, 51, "Hardware-device"]]}
{"id": 158016, "text": "We discover that λ = 0.5 yields the optimum performance , and Figure 4 : t-SNE visualization of the input representation of the classifier for a testing episode ( N = 5 , K = 5 , Q = 100 ) sampled from 20 Newsgroups .", "Comments": [], "label": [[73, 78, "Software-Entity"]]}
{"id": 158018, "text": "To illustrate that our model can generate high-quality sentence embeddings for unseen classes , we view the high-dimensional features as two-dimensional images using the t-SNE algorithm [ Van der Maaten and Hinton , 2008 ] .", "Comments": [], "label": [[170, 175, "Software-Entity"]]}
{"id": 158022, "text": "Concretely , experiments are conducted on the 8 x NVIDIA A100 GPU station .", "Comments": [], "label": [[46, 47, "Device-Count"], [50, 65, "Hardware-device"]]}
{"id": 158038, "text": "We use RL4LMs [ Ramamurthy et al. , 2022 ] to implement the reward function and use Proximal Policy Optimization ( PPO ) [ Schul ] [ man et al. , , 2017 ] for RL training .", "Comments": [], "label": [[7, 13, "Software-Entity"]]}
{"id": 158040, "text": "We use Amazon Mechanical Turk to recruit crowdsourcing workers and we pay workers over $ 15/hour on average , well above the highest state minimum wage , and engage in constructive discussions if they have concerns about the process .", "Comments": [], "label": [[7, 29, "Cloud-Platform"]]}
{"id": 158046, "text": "E Experimental and Model Details We train T5 [ Raffel et al. , , 2020 ] using Huggingface t5-trainer framework [ 7 ] .", "Comments": [], "label": [[78, 89, "Software-Entity"]]}
{"id": 158048, "text": "We use a batch size of 4 for T5-3B and train on 2 NVIDIA RTX A6000 GPUs for around 30 hours or a batch size of 8 for T5-large ( 770M ) .", "Comments": [], "label": [[48, 49, "Device-Count"], [50, 71, "Hardware-device"]]}
{"id": 158061, "text": "We run pre-training on NVIDIA A100 GPUs .", "Comments": [], "label": [[23, 39, "Hardware-device"]]}
{"id": 158069, "text": "We perform fine-tuning a single NVIDIA 3090 GPU .", "Comments": [], "label": [[25, 31, "Device-Count"], [32, 47, "Hardware-device"]]}
{"id": 158073, "text": "The selection score st , i for entity E is defined as the dot product between the context vector and the entity vector as : Then , the top-K entities are obtained by : Retrieving the top-K entities can be formulated as maximum inner product search ( MIPS ) , which can be accelerated to sub-linear time using efficient similarity search libraries such as FAISS [ Johnson et al. , 2019 ] .", "Comments": [], "label": [[355, 360, "Software-Entity"]]}
{"id": 158077, "text": "We conduct all experiments on a single 24G NVIDIA RTX 3090 GPU and select the best checkpoint based on model performance on the validation set .", "Comments": [], "label": [[32, 38, "Device-Count"], [39, 42, "Device-Memory"], [43, 62, "Hardware-device"]]}
{"id": 158093, "text": "The language model is trained on 8 Nvidia A100 GPUs for 1M steps using Adam optimizer .", "Comments": [], "label": [[33, 34, "Device-Count"], [35, 51, "Hardware-device"]]}
{"id": 158097, "text": "De-tokenized BLEU is computed using sacreBLEU on the OPUS-100 test set .", "Comments": [], "label": [[36, 45, "Software-Entity"]]}
{"id": 158121, "text": "We train the models on 8 Nvidia A100 GPUs with a batch size 8 per GPU , a learning rate of 1e-05 , and float16 enabled for 3 epochs for all the setups and datasets .", "Comments": [], "label": [[23, 24, "Device-Count"], [25, 41, "Hardware-device"], [25, 41, "Hardware-device"]]}
{"id": 158131, "text": "Figure 5 : The t-SNE visualization of modality-invariant and -specific representations from ( a ) base model , ( b ) MIR-GAN without modality discriminator and ( c ) MIR-GAN .", "Comments": [], "label": [[15, 20, "Software-Entity"]]}
{"id": 158132, "text": "Fig . [ 5 ] presents the t-SNE visualization of modality-invariant and specific representations to illustrate the principle of MIR-GAN .", "Comments": [], "label": [[25, 30, "Software-Entity"]]}
{"id": 158134, "text": "For the video clips , we detect the 68 facial keypoints using dlib toolkit [ King , 2009 ] and align the image frame to a reference face frame via affine transformation .", "Comments": [], "label": [[62, 66, "Software-Entity"]]}
{"id": 158142, "text": "The finetuning process takes ∼ 1.4 days on 4 NVIDIA-V100-32GB GPUs .", "Comments": [], "label": [[43, 44, "Device-Count"], [45, 56, "Hardware-device"], [57, 66, "Device-Memory"]]}
{"id": 158152, "text": "B Implementation Details We carry out all experiments on a single NVIDIA RTX A6000 GPU with 48GB memory .", "Comments": [], "label": [[59, 65, "Device-Count"], [66, 86, "Hardware-device"], [92, 103, "Device-Memory"]]}
{"id": 158153, "text": "Our implementation is based on Python 3.9.7 and the version of PyTorch is 1.11.0 .", "Comments": [], "label": [[63, 70, "Software-Entity"]]}
{"id": 158168, "text": "The fine-tuning was performed using a GeForce RTX 2080 Ti GPU .", "Comments": [], "label": [[38, 61, "Hardware-device"]]}
{"id": 158169, "text": "All codes were implemented in the TensorFlow framework [ Abadi et al. , , 2016 ] and are published to help replicate our results .", "Comments": [], "label": [[34, 44, "Software-Entity"]]}
{"id": 158183, "text": "To this end , we employed the FACEBOOK/BART-LARGE-MNLI checkpoint on Huggingface Transformers , which is BART [ Lewis et al. , , 2019 ] fine-tuned on the multiNLI dataset [ Williams et al. , , 2018 ] , to initialize a zero-shot classification pipeline of AGREE and DISAGREE , evaluating whether the response * entails * agreement or disagreement .", "Comments": [], "label": [[69, 80, "Software-Entity"]]}
{"id": 158185, "text": "The default hyperparameters on Huggingface Transformers are adopted if not included in Table [ 9 . ] G Computational Resources We used a GPU cluster with 16 NVIDIA A40 GPUs , 1988G memory , and 104 CPU cores for the experiments .", "Comments": [], "label": [[31, 42, "Software-Entity"], [154, 156, "Device-Count"], [157, 172, "Hardware-device"], [175, 187, "Device-Memory"], [194, 201, "Device-Count"]]}
{"id": 158186, "text": "H Scientific Artifacts We leveraged many open-source scientific artifacts in this work , including pytorch [ Paszke et al. , , 2019 ] , pytorch lightning [ Falcon and The PyTorch ] [ Lightning team , 2019 ] , HuggingFace transformers [ Wolf et al. , , 2020 ] , sklearn [ Pedregosa et al. , , 2011 ] , NumPy [ Harris et al. , , 2020 ] , NLTK [ Bird et al. , , 2009 ] , and the PushShift API [ 12 ] .", "Comments": [], "label": [[99, 106, "Software-Entity"], [136, 153, "Software-Entity"], [209, 220, "Software-Entity"], [261, 268, "Software-Entity"], [301, 306, "Software-Entity"], [336, 340, "Software-Entity"], [376, 385, "Software-Entity"]]}
{"id": 158196, "text": "All experiments were completed on 12GB 1080Ti and 48GB RTX8000 GPUs .", "Comments": [], "label": [[34, 38, "Device-Memory"], [39, 45, "Hardware-device"], [50, 54, "Device-Memory"], [55, 67, "Hardware-device"]]}
{"id": 158197, "text": "When using GPT-3 as a label generator , label generation costs several cents using the text-davinci-002 endpoint , and example generation with GPT-J 6B takes several hours on a 48GB RTX8000 GPU .", "Comments": [], "label": [[177, 181, "Device-Memory"], [182, 193, "Hardware-device"]]}
{"id": 158198, "text": "Classifier training for a bert-base-cased model takes approximately 30 minutes on a 12GB 1080 Ti GPU .", "Comments": [], "label": [[84, 88, "Device-Memory"], [89, 100, "Hardware-device"]]}
{"id": 158210, "text": "All experiments are conducted on a single 12GB NVIDIA GeForce GTX 1080Ti GPU .", "Comments": [], "label": [[35, 41, "Device-Count"], [42, 46, "Device-Memory"], [47, 76, "Hardware-device"]]}
{"id": 158220, "text": "The pre-training procedure takes around 96 hours on 4 Tesla V100- SXM2 GPUs .", "Comments": [], "label": [[52, 53, "Device-Count"], [54, 75, "Hardware-device"]]}
{"id": 158235, "text": "F Compute Resources We use NVIDIA RTX A5000 and A100 GPU to run the decoding experiments .", "Comments": [], "label": [[27, 43, "Hardware-device"], [48, 56, "Hardware-device"]]}
{"id": 158238, "text": "CD generates one continuation of length 256 tokens ( with batchsize of 1 ) in 8 seconds on NVIDIA RTX A5000 .", "Comments": [], "label": [[91, 107, "Hardware-device"]]}
{"id": 158240, "text": "This training data is representative of the degeneration Figure 4 : Human evaluation instructions and interface we post to Amazon Mechanical Turk platform .", "Comments": [], "label": [[123, 145, "Cloud-Platform"]]}
{"id": 158252, "text": "All the evaluations are performed using the official test split for each dataset , downloaded using Huggingface dataset library [ Lhoest et al. , 2021 ] .", "Comments": [], "label": [[100, 111, "Software-Entity"]]}
{"id": 158266, "text": "SUNDAE achieves BLEU of 28.46 , but requires more resources than training RoBERTa [ Liu et al. , , 2019 ] on 16 TPUs ( see Appendix [ C ] .", "Comments": [], "label": [[109, 116, "Device-Count"]]}
{"id": 158270, "text": "B Additional implementation details We run Opus experiments in table [ 1 ] on an AMD EPYC Milan with 16 cores at 2.45 GHz and 64GB of RAM ( accessible on Google Cloud - c2d-standard-16 ) .", "Comments": [], "label": [[101, 121, "Hardware-device"], [126, 137, "Device-Memory"], [154, 166, "Cloud-Platform"]]}
{"id": 158271, "text": "For the scalability experiment in figure [ 3 , ] we also used Google Cloud instances with an increasing number of cores ( referred to as c2d-standard-XX , where XX is the number of used cores ) .", "Comments": [], "label": [[62, 74, "Cloud-Platform"]]}
{"id": 158272, "text": "Experiments with MBart50 on table [ 1 , ] [ 2 ] and [ 6 ] are performed on a Desktop machine with Ubuntu 20.04.4 LTS , AMD Table 4 : Data Statistic Ryzen 9 3900X 12-Core Processor , 32GB of RAM , and a Palit Nvidia 3090 GPU .", "Comments": [], "label": [[148, 179, "Hardware-device"], [182, 193, "Device-Memory"], [208, 223, "Hardware-device"]]}
{"id": 158273, "text": "Models are implemented in Pytorch 1.11.0 [ Paszke et al. , , 2019 ] and the Huggingface Transformer library [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[26, 33, "Software-Entity"], [76, 87, "Software-Entity"]]}
{"id": 158274, "text": "We used python 3.8 and NVIDIA-SMI Drivers 510.73.05 with CUDA version 11.6 .", "Comments": [], "label": [[57, 61, "Software-Entity"]]}
{"id": 158275, "text": "For OPUS we used Huggingface models available on the hub under the tag Helsinki-NLP/opus-mt- { src } - { tgt } except for the language pair Ro-En where we used the model Helsinki-NLP/opus-mt-roa-en and the pair En-De where we used the checkpoint opus-2021-02-22 [ 4 ] .", "Comments": [], "label": [[17, 28, "Software-Entity"]]}
{"id": 158277, "text": "For each dataset , we used the official test split via the Huggingface dataset library [ Lhoest et al. , , 2021 ] .", "Comments": [], "label": [[59, 70, "Software-Entity"]]}
{"id": 158281, "text": "Did you cite the creators of artifacts you used ? * Section 4 and Appendix B * * Data is automatically downloaded with standard train/test/dev splits via the Huggingface datasets library .", "Comments": [], "label": [[158, 169, "Software-Entity"]]}
{"id": 158309, "text": "A.7 Computing Resources Experiments were done in computing nodes of a HPC cluster with specifications of 4 GPUs Nvidia Tesla V100 ( 16GB RAM , 2560 tensor cores , 10480 CUDA cores , compute capability 7.0 ) . 1 CPU Intel Xeon E5-2698v4 @ 2.2GHz ( 40 hyperthreads , RAM : 256GB ) .", "Comments": [], "label": [[105, 111, "Device-Count"], [112, 129, "Hardware-device"], [132, 140, "Device-Memory"], [143, 161, "Device-Memory"], [163, 179, "Device-Memory"], [215, 244, "Hardware-device"], [265, 276, "Device-Memory"]]}
{"id": 158319, "text": "The [ MASK ] [ MASK ] [ MASK ] 3 . '' B Environment of Experiments The experimental environment is equipped with 32 V100 GPUs , and approximately 5000 GPU hours are allocated on average to train a single model .", "Comments": [], "label": [[113, 115, "Device-Count"], [116, 125, "Hardware-device"]]}
{"id": 158322, "text": "For such experiments a NVIDIA RTX 3030 ti was used .", "Comments": [], "label": [[23, 41, "Hardware-device"]]}
{"id": 158324, "text": "We used the Shifterator software library ( see Appendix [ A ] for details ) .", "Comments": [], "label": [[12, 23, "Software-Entity"]]}
{"id": 158349, "text": "We use Nvidia V100 GPU with 32GB of GPU memory for pre-training and fine-tuning .", "Comments": [], "label": [[7, 22, "Hardware-device"], [28, 32, "Device-Memory"]]}
{"id": 158350, "text": "The training server has 104 CPU cores and 768GB of CPU memory .", "Comments": [], "label": [[24, 37, "Device-Count"], [42, 61, "Device-Memory"]]}
{"id": 158354, "text": "Interactive theorem provers To expediently verify proofs in Lean and Isabelle , we use REPL wrappers that formulate the interactions with ITPs in a REPL style .", "Comments": [], "label": [[87, 91, "Software-Entity"]]}
{"id": 158369, "text": "On our 4 nodes each consists of 8 Nvidia V100 GPUs , finding the supportive pretraining data for * each * source task in our experiment would take about a week .", "Comments": [], "label": [[32, 33, "Device-Count"], [34, 50, "Hardware-device"], [34, 50, "Hardware-device"]]}
{"id": 158384, "text": "For consistency , we use [ Fairseq ] ( https : //github.com/facebookresearch/fairseq ) to pretrain BART-large Figure 7 : Few-shot performance on QMSum test set . 6.4 General vs .", "Comments": [], "label": [[27, 34, "Software-Entity"]]}
{"id": 158391, "text": "Our experiments rely on the Huggingface Transformers library [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[28, 39, "Software-Entity"]]}
{"id": 158392, "text": "We use 8 Nvidia A100 GPUs to run the experiments described in this paper .", "Comments": [], "label": [[7, 8, "Device-Count"], [9, 25, "Hardware-device"]]}
{"id": 158403, "text": "In both cases , we use 100 of the 281 examples from the QMSum test set , and three independent annotators from the Amazon Mechanical Turk platform .", "Comments": [], "label": [[115, 137, "Cloud-Platform"]]}
{"id": 158421, "text": "G Details on Implementation We use RoBERTa from the HuggingFace library [ 4 ] , and use PyTorch to train our models .", "Comments": [], "label": [[52, 63, "Software-Entity"], [88, 95, "Software-Entity"]]}
{"id": 158422, "text": "Apart from the default parameters in the trainer module from HuggingFace , our selected hyperparameters are listed in Table [ 13 . ]", "Comments": [], "label": [[61, 72, "Software-Entity"]]}
{"id": 158424, "text": "On a single NVIDIA GeForce RTX 2080 Ti GPU , training the model for 10 epochs takes approximately 8- 12 hours , and OOD detection for a single dataset takes approximately 15 minutes .", "Comments": [], "label": [[5, 11, "Device-Count"], [12, 42, "Hardware-device"]]}
{"id": 158431, "text": "We also show the performance using another state-of-the-art AMR parser , AMRBART [ Bai et al. , , 2022 ] , in Appendix [ -A.4 . ] Besides , we use BERTbase and RoBERTalarge provided by huggingface [ 1 ] as the backbone .", "Comments": [], "label": [[185, 196, "Software-Entity"]]}
{"id": 158432, "text": "Experiments based on base models are conducted on a single Tesla T4 GPU , and large models on 4 distributed Tesla T4 GPU in parallel .", "Comments": [], "label": [[52, 58, "Device-Count"], [59, 71, "Hardware-device"], [94, 95, "Device-Count"], [108, 120, "Hardware-device"]]}
{"id": 158434, "text": "Experiments are run for one epoch on 4 same Tesla T4 GPUs .", "Comments": [], "label": [[37, 38, "Device-Count"], [44, 57, "Hardware-device"]]}
{"id": 158435, "text": "Secondly , though TARA saves up to 56 % inference time compared to the previous AMR-guided work , its entire training requires more than 7h on 4 Tesla T4 GPUs .", "Comments": [], "label": [[143, 144, "Device-Count"], [145, 158, "Hardware-device"]]}
{"id": 158437, "text": "Experiments are run for one epoch on a single Tesla T4 GPU .", "Comments": [], "label": [[39, 45, "Device-Count"], [46, 58, "Hardware-device"]]}
{"id": 158453, "text": "For ViLT models , we use GTX 1080 Ti GPU and start the batch size from 32 with a step of 8 ; for METER models , we use an A40 GPU and start the batch size from 16 with a step of 8 . 6 Experimental Results 6.1 Main Results PuMer is faster and remains accurate .", "Comments": [], "label": [[25, 40, "Hardware-device"], [122, 129, "Hardware-device"]]}
{"id": 158457, "text": "We use the Transformers [ Wolf et al. , 2020 ] and Accelerate [ Hug , 2022 ] with Deep-Speed [ Dee , 2022 ] library to implement the training tasks .", "Comments": [], "label": [[11, 23, "Software-Entity"], [51, 61, "Software-Entity"], [82, 92, "Software-Entity"]]}
{"id": 158458, "text": "We conduct training jobs on 4 Nvidia A100 GPUs .", "Comments": [], "label": [[28, 29, "Device-Count"], [30, 46, "Hardware-device"]]}
{"id": 158463, "text": "It takes around 20 hours to train with a NVIDIA RTX A6000 with 48GB memory .", "Comments": [], "label": [[41, 57, "Hardware-device"], [63, 74, "Device-Memory"]]}
{"id": 158474, "text": "A Training Details We implement the victim models using the Transformers library [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[60, 80, "Software-Entity"]]}
{"id": 158476, "text": "We ask five human annotators on Amazon Mechanical Turk ( AMT ) to classify them into human-written instances and machineedited instances .", "Comments": [], "label": [[32, 62, "Cloud-Platform"]]}
{"id": 158478, "text": "The experiments are run on a single NVIDIA RTX A6000 graphics card .", "Comments": [], "label": [[29, 35, "Device-Count"], [36, 52, "Hardware-device"]]}
{"id": 158485, "text": "Afterward , we use information extraction tools including AutoPhrase [ Shang et al. , , 2018 ] to identify concepts .", "Comments": [], "label": [[58, 68, "Software-Entity"]]}
{"id": 158494, "text": "All training and inference processes are carried out on NVIDIA GeForce RTX 3090 .", "Comments": [], "label": [[56, 79, "Hardware-device"]]}
{"id": 158499, "text": "We employ AutoPhrase [ Shang et al. , , 2018 ] , an information extraction tool to identify concepts .", "Comments": [], "label": [[10, 20, "Software-Entity"]]}
{"id": 158503, "text": "I Statistics of Evolving Concept Co-occurrence Graph We construct 240 evolving concept co-occurrence graphs ( 12 graphs per discipline/topics ) with Elasticsearch and Autophrase [ Shang et al. , , 2018 ] according to 240 essential and common queries and relevant papers .", "Comments": [], "label": [[149, 162, "Software-Entity"], [167, 177, "Software-Entity"]]}
{"id": 158538, "text": "We also utilized the ROUGE package to evaluate the performance of summarization and the NLTK library to preprocess the dataset .", "Comments": [], "label": [[21, 26, "Software-Entity"], [88, 92, "Software-Entity"]]}
{"id": 158539, "text": "We used an RTX 6000 NIVIDA GPU with a Table 6 : Performance of Multi-DYLE ( X ROG , X CES ) varying parameter initialization for extractor and generator modules on the meeting summarization task in the QMSum dataset .", "Comments": [], "label": [[11, 30, "Hardware-device"]]}
{"id": 158540, "text": "The DYLE 's extractor and generator , which are publicly available by the authors of DYLE , are used for parameter initialization . 48GB memory capacity , implemented and trained all models using the Pytorch library .", "Comments": [], "label": [[200, 207, "Software-Entity"]]}
{"id": 158553, "text": "3.2 Implementation Details and Metrics We performed all experiments on the Pytorch deep learning framework with the Intel Core i7-12700H and the NVIDIA RTX3060 GPU .", "Comments": [], "label": [[75, 82, "Software-Entity"], [116, 136, "Hardware-device"], [145, 163, "Hardware-device"]]}
{"id": 158554, "text": "The software environment includes Python 3.9 , Pytorch 1.12.1 , and CUDA 11.3 .", "Comments": [], "label": [[47, 54, "Software-Entity"], [68, 72, "Software-Entity"]]}
{"id": 158565, "text": "T-SNE representation with and without semantic information refinement components respectively on ( a ) IEMOCAP ( 4-way ) and ( b ) IEMOCAP ( 6-way ) .", "Comments": [], "label": [[0, 5, "Software-Entity"]]}
{"id": 158587, "text": "Detailed examples can be found in Appendix [ E. ] The instances within each head event are identified through syntactic parsing by using a parser from the spaCy [ 3 ] library and matching with five human-defined rules .", "Comments": [], "label": [[155, 160, "Software-Entity"]]}
{"id": 158589, "text": "Human annotations on the Amazon Mechanical Turk platform are further conducted to acquire annotations on the correctness of 131K conceptu- 3 < https : //spacy.io/ > alizations of 7K ATOMIC events .", "Comments": [], "label": [[25, 47, "Cloud-Platform"]]}
{"id": 158590, "text": "Human annotations on Amazon Mechanical Turk further verify 81K uniformly sampled abstract triples .", "Comments": [], "label": [[21, 43, "Cloud-Platform"]]}
{"id": 158594, "text": "C.1.2 Settings We use pretrained language models from the Huggingface Transformers [ 4 ] Library [ Wolf et al. , , 2020 ] to build our framework .", "Comments": [], "label": [[58, 69, "Software-Entity"]]}
{"id": 158596, "text": "Detailed ablation studies are provided in Section [ 5.3 . ] As for the computational infrastructure , the models are trained and evaluated on four NVIDIA RTX3090 ( 24G ) and four NVIDIA 1080Ti ( 12G ) graphical cards .", "Comments": [], "label": [[142, 146, "Device-Count"], [147, 161, "Hardware-device"], [164, 167, "Device-Memory"], [174, 178, "Device-Count"], [179, 192, "Hardware-device"], [195, 198, "Device-Memory"]]}
{"id": 158598, "text": "C.2 Application and Evaluation of CAT C.2.1 Settings Pretrained GPT2 models from the Huggingface Transformers Library and training codes [ 5 ] by [ Hwang et al. , 2021 ] are used as our code base .", "Comments": [], "label": [[85, 96, "Software-Entity"]]}
{"id": 158601, "text": "All the models are trained and evaluated on four NVIDIA RTX A6000 graphical cards with 48G memory .", "Comments": [], "label": [[44, 48, "Device-Count"], [49, 65, "Hardware-device"], [87, 97, "Device-Memory"]]}
{"id": 158602, "text": "Crowdsourced platforms such as Amazon Mechanical Turk are not used since experts understand conceptualization better and are more reliable for evaluation .", "Comments": [], "label": [[31, 53, "Cloud-Platform"]]}
{"id": 158611, "text": "The prompts used for performing these tasks are listed in Table [ 10 . ] Specifically , we use OpenAI 's API [ 6 ] to prompt ChatGPT and retrieve its generations .", "Comments": [], "label": [[95, 101, "Software-Entity"]]}
{"id": 158633, "text": "We use one NVIDIA RTX 3090 GPU to train our model .", "Comments": [], "label": [[7, 10, "Device-Count"], [11, 30, "Hardware-device"]]}
{"id": 158645, "text": "5.2 Implementation Our model utilizes the Pytorch-based [ Paszke et al. , 2019 ] Huggingface Transformers [ Wolf et al. , , 2020 ] packages and is designed to reconstruct sequences with a length of 5 for fMRI2text and 10 for EEG2text in * Phase 2 * .", "Comments": [], "label": [[42, 49, "Software-Entity"], [81, 92, "Software-Entity"]]}
{"id": 158646, "text": "All experiments were conducted on NVIDIA A100-80GB-PCIe GPUs . 5.3 UniCoRN Structure for fMRI2text fMRI2text across Different Splits We experiment with series length T of 10 and report the BLEU scores and ROUGE-1 scores for fMRI2text across different splits .", "Comments": [], "label": [[34, 60, "Hardware-device"]]}
{"id": 158670, "text": "We train Rel-CSKGC with 1 NVIDIA RTX 3090 Graphical Card for 5 epochs , and it takes 20 hours to finish the training . * * COMET * * ours To train * * COMET * * ours , we use the implentations provided here . [ 4 ] We use the learning rate of 1.625e-5 and the default values for other parameters .", "Comments": [], "label": [[24, 25, "Device-Count"], [26, 41, "Hardware-device"]]}
{"id": 158677, "text": "Details of the three datasets are given in Table [ 1 . ] Environments and hyperparameters We implement ShrinkE with Python 3.9 and Pytorch 1.11 , and train our model on one Nvidia A100 GPU with 40GB of VRAM .", "Comments": [], "label": [[131, 138, "Software-Entity"], [169, 172, "Device-Count"], [173, 188, "Hardware-device"], [194, 206, "Device-Memory"]]}
{"id": 158692, "text": "All inferences are performed on a single NVIDIA K80 GPU with 12GB memory as in the IWSLT Simultaneous evaluation campaigns .", "Comments": [], "label": [[34, 40, "Device-Count"], [41, 55, "Hardware-device"], [61, 72, "Device-Memory"]]}
{"id": 158693, "text": "We use sacreBLEU [ Post , 2018 ] [ 3 ] to evaluate translation quality , and Average Lagging [ Ma et al. , , 2019 ] – or AL – to evaluate latency , as in the default SimulEval evaluation setup .", "Comments": [], "label": [[7, 16, "Software-Entity"]]}
{"id": 158699, "text": "6.2 Effects of Accelerated Hardware To further investigate the computational efficiency of EDATT , we conducted experiments on all the systems described in Section [ 4.4 ] using a highly accelerated GPU , an NVIDIA A40 with 48GB memory , during simultaneous inference .", "Comments": [], "label": [[208, 218, "Hardware-device"], [224, 235, "Device-Memory"]]}
{"id": 158701, "text": "In parallel , several strategies have been developed to directly learn the best policy during training by means of * ad-hoc * architectures [ Ma et al. , , 2021 ] [ Liu et al. , ] [ 2021a , ] [ b ; ] [ Chang ] [ and Lee , 2022 ] and training procedures aimed at Figure 6 : Effect of using NVIDIA A40 GPU on MuST-C en→ { de , es } tst-COMMON considering all the systems of Section [ 4.4 . ]", "Comments": [], "label": [[289, 303, "Hardware-device"]]}
{"id": 158702, "text": "Moreover , it is also capable of achieving a Despite the benefits in terms of quality-latency tradeoff , the significantly higher costs of the A40 GPU over the K80 GPU ( 4.1 vs 0.9 USD/h in Amazon Web Services , [ https : //aws.amazon.com/it/ec2/ ] ( https : //aws.amazon.com/it/ec2/pricing/on-demand/ ) [ pricing/on-demand/ ] ( https : //aws.amazon.com/it/ec2/pricing/on-demand/ ) ) makes unlikely that such a GPU will soon be of widespread use for simultaneous inference .", "Comments": [], "label": [[143, 150, "Hardware-device"], [160, 167, "Hardware-device"]]}
{"id": 158706, "text": "* We use FBK-fairseq with its default settings unless stated otherwise , as reported in Appendix B .", "Comments": [], "label": [[9, 20, "Software-Entity"]]}
{"id": 158709, "text": "4.2 Training Settings We perform experiments using an NVIDIA RTX A5000 GPU .", "Comments": [], "label": [[54, 74, "Hardware-device"]]}
{"id": 158710, "text": "Our experiments are conducted based on PyTorch [ Paszke et al. , , 2019 ] .", "Comments": [], "label": [[39, 46, "Software-Entity"]]}
{"id": 158733, "text": "In the pre-training process , we utilize 200K steps , 25K steps , and 10K steps , respectively , for the three stages on 8 NVIDIA A100 GPUs with a batch size of 4,096 .", "Comments": [], "label": [[121, 122, "Device-Count"], [123, 139, "Hardware-device"]]}
{"id": 158766, "text": "In Figure 2 , we average the sequential representations of the image and text sequences over the sequence dimension , and apply the T-SNE dimensionality reduction algorithm to reduce the 256 dimensions to two dimensions .", "Comments": [], "label": [[132, 137, "Software-Entity"]]}
{"id": 158774, "text": "We trained our models for 100k steps on 8 NVIDIA TITAN RTX GPUs .", "Comments": [], "label": [[40, 41, "Device-Count"], [42, 63, "Hardware-device"]]}
{"id": 158796, "text": "The complete code and data is available at [ https : //github.com/M3RG-IITD/DiSCoMaT . ] ( https : //github.com/M3RG-IITD/DiSCoMaT ) A.4 Implementation details For Graph Attention Networks ( GATs ) [ Velickovi ] ˇ c´ [ et al. , , 2018 ] , we use the GAT implementation of Deep Graph Library [ Wang et al. , , 2019 ] .", "Comments": [], "label": [[272, 282, "Software-Entity"]]}
{"id": 158797, "text": "For LMs , TAPAS , we use the implementation by Transformers library [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[47, 67, "Software-Entity"]]}
{"id": 158799, "text": "We implement and train all models using PyTorch [ Paszke et al. , 2019 ] and AllenNLP [ Gardner et al. , , 2017 ] .", "Comments": [], "label": [[40, 47, "Software-Entity"], [77, 85, "Software-Entity"]]}
{"id": 158800, "text": "All experiments were run on a machine with one 32 GB V100 GPU .", "Comments": [], "label": [[43, 46, "Device-Count"], [47, 52, "Device-Memory"], [53, 61, "Hardware-device"]]}
{"id": 158807, "text": "We adopt the pypinyin toolkit5 to obtain the pinyin of each character .", "Comments": [], "label": [[13, 21, "Software-Entity"]]}
{"id": 158809, "text": "To show the effect , we employ t-SNE ( van der Maaten and Hinton , 2008 ) to visualize character representations generated by our model , with fine-tuned BERT as the baseline .", "Comments": [], "label": [[31, 36, "Software-Entity"]]}
{"id": 158810, "text": "We train our model on the combination of all the training sets and evaluate it on each test dataset . and Hutter , 2017 ) to fine-tune the model for 3 epochs on three 24G GeForce RTX 3090 GPUs .", "Comments": [], "label": [[161, 166, "Device-Count"], [167, 170, "Device-Memory"], [171, 192, "Hardware-device"]]}
{"id": 158811, "text": "Our implementation is based on Huggingface 's Transformer ( Wolf et al. , 2020 ) in PyTorch .", "Comments": [], "label": [[31, 45, "Software-Entity"], [84, 91, "Software-Entity"]]}
{"id": 158817, "text": "7 Limitations and Discussion All of our experiments have taken place by deploying conversational agents on Amazon Mechanical Turk with crowdworkers [ 2 ] , using English-language responses written by workers located in the United States .", "Comments": [], "label": [[107, 129, "Cloud-Platform"]]}
{"id": 158819, "text": "A Data Collection The data collection lasted for around 6 months and in total over 700 crowdworkers who are Englishspeaking annotators located in the United States were recruited and compensated through the Amazon Mechanical Turk platform .", "Comments": [], "label": [[207, 229, "Cloud-Platform"]]}
{"id": 158821, "text": "All the 3B fine-tuned models are trained with a maximum of eight 32GB GPUs ( NVIDIA V100 ) , optimized with Adam using β = 0.9 , β = 0.999 , ϵ = 1e − 08 .", "Comments": [], "label": [[59, 64, "Device-Count"], [65, 74, "Device-Memory"], [77, 88, "Hardware-device"]]}
{"id": 158824, "text": "From this dataset , we extracted 2,084 questions and annotated their underlying intentions . 3.1 Crowd-sourced Annotation We used Amazon Mechanical Turk ( MTurk ) to collect our annotations .", "Comments": [], "label": [[130, 162, "Cloud-Platform"]]}
{"id": 158840, "text": "Other settings conform with HuggingFace implementation .", "Comments": [], "label": [[28, 39, "Software-Entity"]]}
{"id": 158842, "text": "We conduct experiments using a P100 GPU .", "Comments": [], "label": [[31, 39, "Hardware-device"]]}
{"id": 158847, "text": "We collect five annotations for each set , and include only the rewrites that are marked as `` valid '' by a majority of annotators . 4.2 Data Quality We use the Mephisto [ 2 ] and Amazon Mechanical Turk [ 3 ] platforms to collect crowdsource data .", "Comments": [], "label": [[181, 203, "Cloud-Platform"]]}
{"id": 158867, "text": "Each model takes around 1 hour to run on 8 NVIDIA Tesla V100 Volta 32GB GPUs .", "Comments": [], "label": [[41, 42, "Device-Count"], [43, 60, "Hardware-device"], [67, 76, "Device-Memory"]]}
{"id": 158868, "text": "D.2 Classification Models For classification experiments , we finetuned the RoBERTa-large checkpoint from Huggingface [ 13 ] .", "Comments": [], "label": [[106, 117, "Software-Entity"]]}
{"id": 158871, "text": "The model takes around 1 hour to run on 1 NVIDIA Tesla V100 Volta 32GB GPU .", "Comments": [], "label": [[40, 41, "Device-Count"], [42, 59, "Hardware-device"], [66, 74, "Device-Memory"]]}
{"id": 158890, "text": "A.2 Cost of pre-training We used one A-100 80g GPU for pre-training the base/large model , which took approximately one to three days .", "Comments": [], "label": [[37, 42, "Hardware-device"], [43, 50, "Device-Memory"]]}
{"id": 158906, "text": "The total computational budget is about 50 GPU hours , using a GeForce RTX 3090 .", "Comments": [], "label": [[63, 79, "Hardware-device"]]}
{"id": 158914, "text": "First , we use a held-out dataset Dheld that contains samples for which the model M generates good quality translations according to an automatic evaluation metric ( in this work , we use COMET [ Rei et al. , , 2020 ] ) .", "Comments": [], "label": [[188, 193, "Software-Entity"]]}
{"id": 158923, "text": "It was trained with the fairseq toolkit [ Ott et al. , , 2019 ] on WMT18 DE-EN data ( excluding Paracrawl ) : the authors randomly choose 2/3 of the dataset for training and use the remaining 1/3 as a held-out set for analysis .", "Comments": [], "label": [[24, 31, "Software-Entity"]]}
{"id": 158931, "text": "All our experiments have been ran on a machine with 2 physical Intel ( R ) Xeon ( R ) Gold 6348 @ 2.60GHz CPUs ( total of 112 threads ) .", "Comments": [], "label": [[52, 53, "Device-Count"], [54, 110, "Hardware-device"]]}
{"id": 158932, "text": "E Evaluation Metrics We use scikit-learn [ Pedregosa et al. , , 2011 ] implementations of our evaluation metrics .", "Comments": [], "label": [[28, 40, "Software-Entity"]]}
{"id": 158946, "text": "A Training Details We implement all experiments with the deep learning framework PyTorch on a single NVIDIA Tesla A100 GPU ( 40GB memory ) .", "Comments": [], "label": [[81, 88, "Software-Entity"], [94, 100, "Device-Count"], [101, 122, "Hardware-device"], [125, 136, "Device-Memory"]]}
{"id": 158949, "text": "D Training Efficiency We compare the training efficiency of SimCSE and RankCSE , which are tested on a single NVIDIA Tesla A100 GPU ( 40GB memory ) .", "Comments": [], "label": [[103, 109, "Device-Count"], [110, 131, "Hardware-device"], [134, 145, "Device-Memory"]]}
{"id": 158977, "text": "A.2 Implementation Details We implement our experiments using Pytorch [ Paszke et al. , , 2019 ] on an NVIDIA RTX 3090 GPU .", "Comments": [], "label": [[62, 69, "Software-Entity"], [103, 122, "Hardware-device"]]}
{"id": 158985, "text": "Following existing dialogue summarization papers [ Feng et al. , , 2021b ] , we use py-rouge [ ‡ ] as the implementation of ROUGE score .", "Comments": [], "label": [[84, 92, "Software-Entity"]]}
{"id": 158997, "text": "We use the Transformers [ Wolf et al. , , 2020 ] for our implementation .", "Comments": [], "label": [[11, 23, "Software-Entity"]]}
{"id": 159001, "text": "We have provided more details in the appendix appendix [ A . ] Adversarial Attacks We use the open source toolkit TextAttack [ Morris et al. , ] [ 2020a , ] [ b ] to evaluate all models on five black-box adversarial attacks .", "Comments": [], "label": [[114, 124, "Software-Entity"]]}
{"id": 159010, "text": "We experimented with several different versions of both PyTorch and the transformers libraries but were still unsuccessful .", "Comments": [], "label": [[56, 63, "Software-Entity"]]}
{"id": 159012, "text": "Results shown for the SST-2 dataset . fit the GPU ( for example on the 16GB V100 ) , we use gradient accumulation to have the effective batch size of 16 .", "Comments": [], "label": [[71, 75, "Device-Memory"], [76, 80, "Hardware-device"]]}
{"id": 159013, "text": "For training t5-3b , we needed to use DeepSpeed [ 7 ] for our experiments .", "Comments": [], "label": [[38, 47, "Software-Entity"]]}
{"id": 159014, "text": "A.3 Reproducibility Details Dataset Splits We use the dataset splits from the Huggingface datasets repository . [ 8 ] .", "Comments": [], "label": [[78, 89, "Software-Entity"]]}
{"id": 159016, "text": "Hardware We run most of our experiments using the Nvidia V100 ( 32 GB ) GPU .", "Comments": [], "label": [[50, 61, "Hardware-device"], [64, 69, "Device-Memory"]]}
{"id": 159017, "text": "Some of the later experiments with T5-3b required even larger GPU RAM and therefore , I was able to use Tesla A100 ( 40 GB VRAM ) for last few experiments .", "Comments": [], "label": [[104, 114, "Hardware-device"], [117, 127, "Device-Memory"]]}
{"id": 159018, "text": "Additionally , the servers had CPU : AMD EPYC 7513 32-Core Processor with CPU RAM 512 GB .", "Comments": [], "label": [[37, 68, "Hardware-device"], [74, 88, "Device-Memory"]]}
{"id": 159027, "text": "Each model is trained using one GPU ( NVIDIA_A100 ) , which takes 1.5 hours on average .", "Comments": [], "label": [[28, 35, "Device-Count"], [38, 49, "Hardware-device"]]}
{"id": 159028, "text": "The DRAGON model undergoes MLM training on the BookCorpus dataset and requires training on 8 A100 GPUs for 7 days .", "Comments": [], "label": [[91, 92, "Device-Count"], [93, 102, "Hardware-device"]]}
{"id": 159043, "text": "Specifically , we use DINO [ Schick and Schütze , 2021 ] as a data augmentation framework , that leverages the generative abilities of pre-trained language models ( PLMs ) to generate task-specific data by using prompts .", "Comments": [], "label": [[22, 26, "Software-Entity"]]}
{"id": 159046, "text": "Specifically , we use TextRank [ Mihalcea and Ta ] [ rau , 2004 ] , such that sentences receive a higher ranking if they have a high similarity score to all other sentences .", "Comments": [], "label": [[22, 30, "Software-Entity"]]}
{"id": 159072, "text": "The DINO [ Schick and Schütze , 2021 ] is trained on a single NVIDIA Tesla 32G V100 GPU , with each run taking up to twelve hours .", "Comments": [], "label": [[4, 8, "Software-Entity"], [55, 61, "Device-Count"], [75, 78, "Device-Memory"], [79, 87, "Hardware-device"]]}
{"id": 159074, "text": "B.3 Experimental parameters for KPG We train the model for a total of 15 epochs on two NVIDIA Tesla A100 80GB GPUs with and batch Figure 3 : Data distribution of the data augmented dataset size of 16 , limiting input length to 512 .", "Comments": [], "label": [[83, 86, "Device-Count"], [87, 104, "Hardware-device"], [105, 114, "Device-Memory"]]}
{"id": 159089, "text": "4.4 Qualitative Analysis To better illustrate the quality of word embeddings inferred by GRM , we select six pairs of words from the family part of the Google dataset for the word analogy task and visualize the results by reducing dimension through t-SNE ( van der Maaten and Hinton , 2008 ) .", "Comments": [], "label": [[249, 254, "Software-Entity"]]}
{"id": 159091, "text": "B.1 Downstream Models We use the gensim ( Reh˚ ˇ uˇrek and Sojka , 2010 ) package for intrinsic tasks .", "Comments": [], "label": [[33, 39, "Software-Entity"]]}
{"id": 159094, "text": "B.2 Pre-training Wordpiece Embeddings To obtain the pre-trained embeddings of wordpiece nodes , we tokenized the corpus of background model using WordPiece ( Wu et al. , 2016 ) and put the processed corpus into the skipgram model ( Mikolov et al. , 2013 ) in the case where the background word embedding model is Word2Vec .", "Comments": [], "label": [[146, 155, "Software-Entity"]]}
{"id": 159095, "text": "We trained the skipgram model with gensim ( version 4.1.2 ) ( Reh˚ ˇ uˇrek and Sojka , 2010 ) .", "Comments": [], "label": [[35, 41, "Software-Entity"]]}
{"id": 159105, "text": "Each epoch consumes 1.8 hours on a workstation equipped with an Intel ( R ) Xeon ( R ) Silver 4214R CPU @ 2.40GHz and an Nvidia RTX 1080-Ti GPU .", "Comments": [], "label": [[64, 113, "Hardware-device"], [121, 143, "Hardware-device"]]}
{"id": 159116, "text": "The error types are categorized under two categories Fluency ( Flu . ) and Accuracy ( Acc. ) . from Microsoft Azure Cognitive Services API [ 3 ] and Google translation API [ 4 ] ( additional details in Appendix [ B.1 ] .", "Comments": [], "label": [[100, 138, "Cloud-Platform"], [149, 171, "Cloud-Platform"]]}
{"id": 159138, "text": "We use the same training process and hyper-parameters as COMET for a fair comparison ( additional details in Appendix [ B.2 ] .", "Comments": [], "label": [[57, 62, "Software-Entity"]]}
{"id": 159152, "text": "B Additional details B.1 MT systems Considered For the mBART we use the Huggingface Transformers [ Wolf et al. , , 2020 ] for generating the outputs for the various languages .", "Comments": [], "label": [[72, 83, "Software-Entity"]]}
{"id": 159156, "text": "A cumulative of 10 hours of computation was performed on a single RTX A4000 GPU .", "Comments": [], "label": [[59, 65, "Device-Count"], [66, 79, "Hardware-device"]]}
{"id": 159168, "text": "In addition to the standard fine-tuning approach [ Devlin et al. , , 2019 ] , we also experiment with three parameter-efficient fine-tuning ( PEFT ) approaches as – in the few-shot setting – they have been shown to achieve comparable or even better performance than fine-tuning all parameters [ Peters et al. , 2019 ] [ Logan IV et al. , , 2022 ] [ Liu et al. , , 2022 ] .", "Comments": [], "label": [[108, 139, "Software-Entity"], [142, 146, "Software-Entity"]]}
{"id": 159173, "text": "We use the Py-Torch framework [ 11 ] to implement all approaches discussed in the paper .", "Comments": [], "label": [[11, 19, "Software-Entity"]]}
{"id": 159174, "text": "Hugging Face [ Wolf et al. , , 2020 ] is used for downloading and training the RoBERTa-base model .", "Comments": [], "label": [[0, 12, "Software-Entity"]]}
{"id": 159175, "text": "AdapterHub [ Pfeiffer et al. , , 2020 ] is used for implementing parameter-efficient fine-tuning .", "Comments": [], "label": [[0, 10, "Software-Entity"]]}
{"id": 159177, "text": "Computing infrastructure and training cost We use Nvidia V100-32 GPUs for training deep learning models .", "Comments": [], "label": [[50, 61, "Hardware-device"], [62, 69, "Device-Memory"]]}
{"id": 159206, "text": "All experiments were conducted on one node with 4 32GB V100 GPUs .", "Comments": [], "label": [[48, 49, "Device-Count"], [50, 54, "Device-Memory"], [55, 64, "Hardware-device"]]}
{"id": 159212, "text": "All the experiments were conducted on one node with a single A100 80GB GPU .", "Comments": [], "label": [[54, 60, "Device-Count"], [61, 65, "Hardware-device"], [66, 74, "Device-Memory"]]}
{"id": 159234, "text": "Compute Infrastructure : All our experiments are conducted on a single NVIDIA A100 GPU .", "Comments": [], "label": [[64, 70, "Device-Count"], [71, 86, "Hardware-device"]]}
{"id": 159239, "text": "Implementation Software and Packages : We implement all our models in PyTorch [ 7 ] and use the HuggingFace [ 8 ] implementations of mBART50 and XLM-RoBERTA ( base and large ) .", "Comments": [], "label": [[70, 77, "Software-Entity"], [96, 107, "Software-Entity"]]}
{"id": 159240, "text": "We use the FLAIR toolkit [ Akbik et al. , , 2019 ] to fine-tune all our NER models .", "Comments": [], "label": [[11, 16, "Software-Entity"]]}
{"id": 159246, "text": "We first use spaCy API for sentence segmentation . [ 6 ] Then , we utilize the finetuned cross-segment BERT [ Lukasik et al. , , 2020 ] to obtain coherent uni-modal segments .", "Comments": [], "label": [[13, 18, "Software-Entity"]]}
{"id": 159264, "text": "References A Training Detail All model fine-tuning and inference ( in Section [ 5 ] and Section [ 5.5 ] were conducted on an NVIDIA Tesla V100 32GB GPU .", "Comments": [], "label": [[125, 142, "Hardware-device"], [143, 151, "Device-Memory"]]}
{"id": 159265, "text": "We also ran all of the models with shared training settings , including the number of training steps , optimizers , and token batch sizes ; we set other related training parameters as the settings in Huggingface Trainer .", "Comments": [], "label": [[200, 211, "Software-Entity"]]}
{"id": 159276, "text": "We use an NVIDIA A100 to train the model for an average of 45 minutes .", "Comments": [], "label": [[10, 21, "Hardware-device"]]}
{"id": 159280, "text": "We test them on an Nvidia GeForce Rtx 2080 Ti GPU and show the average results of 1000 examples .", "Comments": [], "label": [[19, 49, "Hardware-device"]]}
{"id": 159288, "text": "Results marked with symbol ∗ were obtained on an Nvidia GeForce Rtx 2080 Ti GPU over 100 examples .", "Comments": [], "label": [[49, 79, "Hardware-device"]]}
{"id": 159299, "text": "All the experiments are run on a single Nvidia 1080Ti GPU . 4.2 Main Results To show the effectiveness of our DA2LM approach , we consider the following competitive domain adaptation comparison systems for the crossdomain ABSA task .", "Comments": [], "label": [[33, 39, "Device-Count"], [40, 57, "Hardware-device"]]}
{"id": 159301, "text": "To visually verify the superiority of our DA2LM framework , we further utilize t-SNE [ Van der Maaten and Hinton , 2008 ] to perform a visualization of the sentence representations obtained by a pre-trained language model BERT [ Kenton ] [ and Toutanova , 2019 ] .", "Comments": [], "label": [[79, 84, "Software-Entity"]]}
{"id": 159303, "text": "It tokenizes the input text with the byte-level Byte-Pair Encoding ( BPE ) [ Sennrich et al. , , 2016 ] [ Radford et al. , , 2019 ] and adds [ < s > ] and [ < /s > ] tokens to the start and end of the sequence , respectively .", "Comments": [], "label": [[48, 74, "Software-Entity"]]}
{"id": 159313, "text": "We measure the average inference time of processing 1 VQA instance over 10K runs on 1 NVIDIA TITAN V GPU .", "Comments": [], "label": [[84, 85, "Device-Count"], [86, 104, "Hardware-device"]]}
{"id": 159330, "text": "Our models are based on fairseq [ Ott et al. , , 2019 ] implementations .", "Comments": [], "label": [[24, 31, "Software-Entity"]]}
{"id": 159347, "text": "All results are generated with beam size 5 , batch size 256 , max decoding length 10 on a NVIDIA Quadro RTX A6000 .", "Comments": [], "label": [[90, 113, "Hardware-device"]]}
{"id": 159367, "text": "To create valid training data for * loaded language * generation , we first use SpaCy to perform part-of-speech tagging and dependency parsing , and then keep the examples where there exists an adverb pointing to a verb or an adjective pointing to a noun through dependency parsing edges .", "Comments": [], "label": [[80, 85, "Software-Entity"]]}
{"id": 159368, "text": "In total , there are 4,535 news articles in TIMELINE17 . 3.2 Crowdsourcing for Data Curation We use Amazon 's Mechanical Turk ( AMT ) to verify the quality and the correctness of the generated disinformation .", "Comments": [], "label": [[100, 133, "Cloud-Platform"]]}
{"id": 159373, "text": "To evaluate the quality of our generation approach , we asked Amazon Mechanical Turk ( AMT ) workers to rate the plausibility of 100 generated articles from PROPANEWS and to determine the degree by which their answer to this question is influenced by the generated propaganda .", "Comments": [], "label": [[62, 92, "Cloud-Platform"]]}
{"id": 159377, "text": "All experiments are conducted on an Ubuntu 18.04 machine with NVIDIA Tesla V100 .", "Comments": [], "label": [[62, 79, "Hardware-device"]]}
{"id": 159378, "text": "We use PyTorch 1.10.0 and Transformers 4.3.0 for constructing all models and loading pre-trained weights , except for GROVER , which operates on Tensorflow 1.13.1 .", "Comments": [], "label": [[7, 14, "Software-Entity"], [26, 38, "Software-Entity"], [145, 155, "Software-Entity"]]}
{"id": 159379, "text": "Among the software we use , SpaCy ( `` en_core_web_md '' ) is licensed under the MIT License , Transformers and Stanza are licensed under the Apache License 2.0 , and PyTorch is released under the Modified BSD License .", "Comments": [], "label": [[28, 33, "Software-Entity"], [95, 107, "Software-Entity"], [112, 118, "Software-Entity"], [167, 174, "Software-Entity"]]}
{"id": 159392, "text": "D Implementation Details We implement our method based on the ALBEF [ Li et al. , 2021 ] framework and we pretrain the SRCL for 30 epochs with the total batch size of 512 on 8 NVIDIA V100 GPUs .", "Comments": [], "label": [[62, 67, "Software-Entity"], [174, 175, "Device-Count"], [176, 192, "Hardware-device"]]}
{"id": 159414, "text": "B Experiment Hyper-Parameters We perform all the computational experiments on a Google Colab instance with a single Nvidia V100 GPU and 50 Gigabytes of RAM .", "Comments": [], "label": [[80, 92, "Cloud-Platform"], [109, 115, "Device-Count"], [116, 131, "Hardware-device"], [136, 155, "Device-Memory"]]}
{"id": 159419, "text": "All models are trained and evaluated using the NVIDIA A100 Tensor Core GPU .", "Comments": [], "label": [[47, 58, "Hardware-device"]]}
{"id": 159421, "text": "Figure 6 : T-SNE visualization of the contextualized features with or without topic features .", "Comments": [], "label": [[11, 16, "Software-Entity"]]}
{"id": 159450, "text": "A sample implementation for FAIRSEQ [ Ott et al. , , 2019 ] is given in Appendix [ A . ]", "Comments": [], "label": [[28, 35, "Software-Entity"]]}
{"id": 159457, "text": "We report both CHRF [ Popovic´ , 2015 ] and SPBLEU [ Goyal et al. , 2022 ] , a SENTENCEPIECE-based BLEU computed using the Flores-101 tokenizer , with sacreBLEU [ 5 ] version 2.3.1 .", "Comments": [], "label": [[80, 92, "Software-Entity"], [151, 160, "Software-Entity"]]}
{"id": 159458, "text": "We also provide COMET scores [ Rei et al. , , 2020 ] [ 6 ] for selected models in Appendix [ G. ] Tokenization We use SENTENCEPIECE [ Kudo ] [ and Richardson , 2018 ] , with a vocabulary size of 250k , and a character coverage of 0.9995 .", "Comments": [], "label": [[118, 131, "Software-Entity"]]}
{"id": 159464, "text": "Hyperparameters All experiments are implemented using FAIRSEQ [ Ott et al. , , 2019 ] .", "Comments": [], "label": [[54, 61, "Software-Entity"]]}
{"id": 159475, "text": "A LSLs in FAIRSEQ Listing [ 1 ] shows our implementation of LSL in FAIRSEQ .", "Comments": [], "label": [[10, 17, "Software-Entity"], [67, 74, "Software-Entity"]]}
{"id": 159476, "text": "The implementation is straightforward , and consists of a dictionary that selects the appropriate language depending on the lang_pair attribute , which FAIRSEQ dynamically sets , and is guaranteed to match that of the input .", "Comments": [], "label": [[152, 159, "Software-Entity"]]}
{"id": 159478, "text": "Listing 1 : Sample implementation of a Language-Specific Transformer Layer in FAIRSEQ .", "Comments": [], "label": [[78, 85, "Software-Entity"]]}
{"id": 159480, "text": "Our latency measurements were collected using a single NVIDIA V100 GPU ( Speed GPU ) or a single-threaded Intel Xeon Platinum 8275CL CPU @ 3.00GHz ( Speed CPU ) , both with batch Table 12 : COMET , CHRF , and SPBLEU scores for the ( non-LSL ) baseline , our LSL models , and the best adapter model for the separate decoder and shared decoder architectures .", "Comments": [], "label": [[48, 54, "Device-Count"], [55, 70, "Hardware-device"], [90, 96, "Device-Count"], [106, 146, "Hardware-device"]]}
{"id": 159488, "text": "We conduct experiments on one RTX 6000 GPU .", "Comments": [], "label": [[30, 42, "Hardware-device"]]}
{"id": 159499, "text": "More case shown in Appendix [ H. ] Figure 3 : Stylistic features visualization of the golden texts ( -Golden ) and generated texts ( -Trans ) on the Chinese test set using t-SNE [ Hinton and Roweis , 2002 ] .", "Comments": [], "label": [[172, 177, "Software-Entity"]]}
{"id": 159508, "text": "Concretely , we frst delete all stop words in a text and then adopt an openresource toolkit SPACY [ 2 ] to convert the input text into a syntax dependency tree .", "Comments": [], "label": [[92, 97, "Software-Entity"]]}
{"id": 159516, "text": "We implement all methods based on Huggingface Transformers [ Wolf et al. , 2020 ] .", "Comments": [], "label": [[34, 45, "Software-Entity"]]}
{"id": 159534, "text": "To verify it , we report the mean training and inference latency per batch on the CSQA-3k dataset of our method and baselines on RTX3090 GPU , where all these methods utilize RoBERTa-base as the backbone .", "Comments": [], "label": [[129, 140, "Hardware-device"]]}
{"id": 159538, "text": "4.2 Fine-tuned Evaluation For this setting , we create a training data called * Base * ( see Section [ 4.2.1 ] on which we fine-tune the following models : FLAN-large , FLAN-base , T5-base , BART-base and NT5 accessed from Huggingface [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[223, 234, "Software-Entity"]]}
{"id": 159539, "text": "We use NVIDIA V100 GPU nodes with a 32G memory .", "Comments": [], "label": [[7, 22, "Hardware-device"], [36, 46, "Device-Memory"]]}
{"id": 159546, "text": "The rest of the hyperparameters were as the default setting in Huggingface .", "Comments": [], "label": [[63, 74, "Software-Entity"]]}
{"id": 159547, "text": "The resource used was an Nvidia Tesla V100 with 32G .", "Comments": [], "label": [[25, 42, "Hardware-device"], [48, 51, "Device-Memory"]]}
{"id": 159552, "text": "For all experiments , we use NVIDIA A100 and RTX 2080 Ti GPUs .", "Comments": [], "label": [[29, 40, "Hardware-device"], [45, 61, "Hardware-device"]]}
{"id": 159553, "text": "We implement DecT with PyTorch ( Paszke et al. , 2019 ) , HuggingFace Tansformers ( Wolf et al. , 2020 ) , and OpenPrompt ( Ding et al. , 2022b ) .", "Comments": [], "label": [[23, 30, "Software-Entity"], [58, 69, "Software-Entity"], [111, 121, "Software-Entity"]]}
{"id": 159560, "text": "Other datasets are downloaded using Huggingface Datasets ( Lhoest et al. , 2021 ) .", "Comments": [], "label": [[36, 47, "Software-Entity"]]}
{"id": 159583, "text": "The implementation of mBART-50 is based on the Transformers [ Wolf et al. , 2020 ] library with default settings ( 12 encoder layers , 12 decoder layers and 1024 hidden states ) .", "Comments": [], "label": [[47, 59, "Software-Entity"]]}
{"id": 159584, "text": "The cross-lingual pre-training and task-specific pretraining stages are conducted on 8 NVIDIA Tesla V100 GPUs with 32GB memory .", "Comments": [], "label": [[85, 86, "Device-Count"], [87, 109, "Hardware-device"], [115, 126, "Device-Memory"]]}
{"id": 159585, "text": "In the finetuning stage , we fine-tune the PISCES model on 8 NVIDIA Tesla V100 GPUs ( 32G ) with 4 batch size , 10 epochs , 2K warmup steps , 3e-5 learning rate , and set the maximum number of tokens for input sequences to 1024 .", "Comments": [], "label": [[59, 60, "Device-Count"], [61, 83, "Hardware-device"], [86, 89, "Device-Memory"]]}
{"id": 159609, "text": "For the Perseus dataset , this is a TnT tagger [ Brants , 2000 ] , while for PROIEL , it is Stanza [ Qi et al. , , 2020 ] .", "Comments": [], "label": [[92, 98, "Software-Entity"]]}
{"id": 159621, "text": "GRεBERTA and PHILBERTA were trained on an NVIDIA A100- PCIE-40GB , GRεTA and PHILTA on a Google TPU v2-8 .", "Comments": [], "label": [[42, 59, "Hardware-device"], [60, 64, "Device-Memory"], [89, 104, "Hardware-device"]]}
{"id": 159622, "text": "Further details in Table [ 7 . ] A.2 Fine-tuning Details We train every Greek model for 50 epochs on an NVIDIA GeForce GTX 1080 Ti , evaluating the model after every epoch on the validation set and using early stopping with a stopping patience of 5 .", "Comments": [], "label": [[104, 130, "Hardware-device"]]}
{"id": 159630, "text": "All experiments are conducted on NVIDIA T4 GPU ( 16GB ) .", "Comments": [], "label": [[33, 46, "Hardware-device"], [49, 53, "Device-Memory"]]}
{"id": 159635, "text": "NVIDIA A100 has 80G memory , which means it can handle a document of 100,000 words with a conventional CR model .", "Comments": [], "label": [[0, 11, "Hardware-device"], [16, 26, "Device-Memory"]]}
{"id": 159638, "text": "Annotation Tool We developed a web-based annotation tool with a server developed with Flask [ 7 ] as shown in Figure [ 8 . ] The entities are listed in the top of the web page , shown in the blue boxes with the texts from the character list .", "Comments": [], "label": [[86, 91, "Software-Entity"]]}
{"id": 159642, "text": "All methods utilize the preprocessing script of a shared BPE model with 32k tokens based on the Sentencepiece library [ 4 ] .", "Comments": [], "label": [[96, 109, "Software-Entity"]]}
{"id": 159645, "text": "We implement all models based on the open-source toolkit fairseq [ 5 ] [ Ott et al. , , 2019 ] .", "Comments": [], "label": [[57, 64, "Software-Entity"]]}
{"id": 159647, "text": "We report the detokenized casesensitive BLEU of models by the SacreBLEU evaluation script [ Post , 2018 ] [ 6 ] .", "Comments": [], "label": [[62, 71, "Software-Entity"]]}
{"id": 159655, "text": "All incremental models are trained on 2 NVIDIA A100 GPUs .", "Comments": [], "label": [[38, 39, "Device-Count"], [40, 56, "Hardware-device"]]}
{"id": 159656, "text": "Figure 3 : The training time of various methods in incremental learning for MNMT . ( a ) Adapter ( b ) KT ( Ours ) Figure 4 : T-SNE visualizations of encoder representations of original and incremental languages on xx-to-English translation directions by Adapter and our method .", "Comments": [], "label": [[126, 131, "Software-Entity"]]}
{"id": 159657, "text": "We use `` FLoRes '' and reduce the 1024-dim representations to 2-dim with t-SNE [ Van der Maaten ] [ and Hinton , 2008 ] for visualization .", "Comments": [], "label": [[74, 79, "Software-Entity"]]}
{"id": 159686, "text": "E Infrastructure and Reproducibility We trained 220M-parameter T5-base model on a single NVIDIA Tesla A100 GPU machine .", "Comments": [], "label": [[82, 88, "Device-Count"], [89, 110, "Hardware-device"]]}
{"id": 159687, "text": "We used PyTorch [ Paszke et al. , , 2019 ] and Huggingface Transformers [ Wolf et al. , , 2020 ] for implementing and training T5-base models .", "Comments": [], "label": [[8, 15, "Software-Entity"], [47, 58, "Software-Entity"]]}
{"id": 159688, "text": "We use OpenAI 's API [ 27 ] for querying GPT3 .", "Comments": [], "label": [[7, 13, "Software-Entity"]]}
{"id": 159699, "text": "Interestingly , this neither happens for the generator of the ELECTRA model nor for the Note that since the tokenizer is a sentencepiece tokenzier , there are extremely few UNK words in the low-resource languages .", "Comments": [], "label": [[123, 136, "Software-Entity"]]}
{"id": 159701, "text": "For the base and large models , we train on 128 Nvidia A100-40GB GPU cards , and for the XL model , we use 512 Nvidia A100-80GB GPU cards .", "Comments": [], "label": [[48, 68, "Hardware-device"], [111, 131, "Hardware-device"]]}
{"id": 159709, "text": "As for prompt search space , 200 words with top frequency in English [ 1 ] are gathered and grouped according to part-of-speech tag with NLTK package [ Loper and Bird , 2002 ] into nouns , verbs , prepositions , adjectives and adverbs .", "Comments": [], "label": [[137, 141, "Software-Entity"]]}
{"id": 159714, "text": "All experiments are conducted on NVIDIA A100 and GeForce RTX 3090 GPUs with CUDA .", "Comments": [], "label": [[33, 44, "Hardware-device"], [49, 70, "Hardware-device"], [76, 80, "Software-Entity"]]}
{"id": 159715, "text": "Our method is developed by OpenPrompt [ Ding et al. , , 2022 ] , an open-source prompt-learning framework based on PyTorch [ Paszke et al. , , 2019 ] .", "Comments": [], "label": [[27, 37, "Software-Entity"], [115, 122, "Software-Entity"]]}
{"id": 159716, "text": "The models are obtained from the Huggingface Transformers library [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[33, 44, "Software-Entity"]]}
{"id": 159717, "text": "Since it would require up to 4 hours with a single NVIDIA A100 , we seek to optimize the process by pruning the search space .", "Comments": [], "label": [[44, 50, "Device-Count"], [51, 62, "Hardware-device"]]}
{"id": 159719, "text": "Our method is run for 5 times on a single NVIDIA A100 and the mean time cost is reported .", "Comments": [], "label": [[35, 41, "Device-Count"], [42, 53, "Hardware-device"]]}
{"id": 159735, "text": "Our model is trained on a GeForce RTX 3090Ti GPU and parameters are optimized through an AdamW optimizer . 3.4 Main Results on the MERMC task We report the results of different methods on the MERMC task in Table [ 2 ] and Table [ 4 . ] The results of baselines are retrieved from previous studies .", "Comments": [], "label": [[26, 48, "Hardware-device"]]}
{"id": 159741, "text": "The detailed steps are as follows : 1 ) using the FFmpeg [ 4 ] tool to sample frame-level images from a video ; 2 ) using the OpenFace [ 5 ] library to detect all the people in the frame-level images , and obtain different FaceID , confidence , 68 feature landmarks , and aligned facial images ; 3 ) using the FFmpeg tool to extract the audio from the video ; 4 ) determining the number of possible speakers in the current video based on three rules as follows : https : //ffmpeg.org/ https : //github.com/TadasBaltrusaitis/OpenFace A.2 Pseudo-code of MARIO We provide the pseudocode for training the proposed MARIO model , where θSwin , θ , θself−attn , and θCMT represent the parameters of Swin-Transformer , the text encoder , self-attention Transformer , and Cross-Modal Transformer , respectively .", "Comments": [], "label": [[126, 134, "Software-Entity"]]}
{"id": 159750, "text": "Among the existing TKGE methods , TeLM obtains SOTA results on ICEWS14 and ICEWS05- 15 and BoxTE achieves SOTA results on GDELT dataset . 5.4 Experimental Setup We implement our proposed model TeAST via pytorch based on TNTComplEx [ Lacroix et al. , , 2020 ] training framework [ 1 ] .", "Comments": [], "label": [[203, 210, "Software-Entity"]]}
{"id": 159751, "text": "All experiments are trained on a single NVIDIA Tesla V100 with 32GB memory .", "Comments": [], "label": [[33, 39, "Device-Count"], [40, 57, "Hardware-device"], [63, 74, "Device-Memory"]]}
{"id": 159753, "text": "Furthermore , we utilize t-SNE [ Van der Maaten ] [ and Hinton , 2008 ] to visualize the trained timestamp embeddings of TeAST , which with and without the temporal spiral regularizer .", "Comments": [], "label": [[25, 30, "Software-Entity"]]}
{"id": 159755, "text": "All experiments are trained on a single NVIDIA Tesla V100 with 32GB memory .", "Comments": [], "label": [[33, 39, "Device-Count"], [40, 57, "Hardware-device"], [63, 74, "Device-Memory"]]}
{"id": 159764, "text": "All of our training was done on a single machine with 8GB GPU and an Interl i9 processor , with very limited environmental cost .", "Comments": [], "label": [[54, 61, "Device-Count"], [69, 88, "Device-Memory"]]}
{"id": 159766, "text": "All experiments done on a single NVIDIA ( R ) GeForce ( R ) RTX 2070 SUPER ( TM ) 8GB GDDR6 and 10th Gen Intel ( R ) Core ( TM ) i9-10900K processor .", "Comments": [], "label": [[26, 32, "Device-Count"], [33, 68, "Hardware-device"], [82, 85, "Device-Memory"], [105, 148, "Hardware-device"]]}
{"id": 159773, "text": "The results of using different semantics of target anchors are reported in [ Section A.4 . ] 4 Evaluation 4.1 Experimental Setup Our experiments are conducted in Python 3.8 with PyTorch 1.13.1 and CUDA 11.4 on an Ubuntu 20.04 machine equipped with six GeForce RTX 6000 GPUs .", "Comments": [], "label": [[178, 185, "Software-Entity"], [197, 201, "Software-Entity"], [248, 251, "Device-Count"], [252, 273, "Hardware-device"]]}
{"id": 159775, "text": "All the PLMs we use are obtained from Huggingface [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[38, 49, "Software-Entity"]]}
{"id": 159828, "text": "But , in addition to the costs necessary for fine-tuning this model , the resultant weights would not be accessible to run locally in any case ; one would have access to it only via the OpenAI interface , which motivated our decision to fine-tune the smaller and open-source Flan T5 instead .", "Comments": [], "label": [[186, 192, "Software-Entity"]]}
{"id": 159829, "text": "Ethics Statement Our work required an extensive manual annotation and evaluation process which involved using Amazon Mechanical Turk .", "Comments": [], "label": [[110, 132, "Cloud-Platform"]]}
{"id": 159832, "text": "The final values for the ones that were manually tuned are provided in Table [ 4 . ] We perform all experiments with a single NVIDIA Quadro RTX 8000 with 64GB of RAM on an Intel Xeon E502680v4 ( 2.4GHz ) .", "Comments": [], "label": [[119, 125, "Device-Count"], [126, 148, "Hardware-device"], [154, 165, "Device-Memory"], [172, 203, "Hardware-device"]]}
{"id": 159835, "text": "To achieve a more accurate characterization of how LLMs perform on generative RE tasks , we hired human annotators on Amazon Mechanical Turk [ 5 ] to manually re-assess all ostensible FPs and FNs from each of our datasets .", "Comments": [], "label": [[118, 140, "Cloud-Platform"]]}
{"id": 159847, "text": "We implement our experiments based on an open-source toolkit OpenPrompt [ Ding et al. , , 2021 ] , which aims to conduct prompt learning easily .", "Comments": [], "label": [[61, 71, "Software-Entity"]]}
{"id": 159848, "text": "All experiments are conducted on Nvidia A6000 GPUs and more details can be found in Appendix [ A.1 . ] 5.2 Baselines We evaluate the following baseline methods .", "Comments": [], "label": [[33, 50, "Hardware-device"]]}
{"id": 159850, "text": "Particularly , we use all-mpnet-base-v2 from Hugging Face as the sentence embedding model , and the descriptions for each class can be found in Appendix [ A.1 . ]", "Comments": [], "label": [[45, 57, "Software-Entity"]]}
{"id": 159851, "text": "We show the template we use in Appendix [ A.1 . ] ManualVerb Manual verbalizers are defined by human experts with domain knowledge and we simply use the label words provided by OpenPrompt [ Ding et al. , , 2021 ] .", "Comments": [], "label": [[177, 187, "Software-Entity"]]}
{"id": 159932, "text": "B Experimental Details B.1 General Information Implementation We implemented all models within the same general framework based on Py-Torch [ Paszke et al. , , 2019 ] .", "Comments": [], "label": [[131, 139, "Software-Entity"]]}
{"id": 159934, "text": "Datasets are downloaded directly from HuggingFace Datasets [ Lhoest et al. , , 2021 ] .", "Comments": [], "label": [[38, 49, "Software-Entity"]]}
{"id": 159936, "text": "We run our experiments on single-GPU servers available to us as part of a computation grid , ranging between GeForce GTX Titan X and RTX 3090 .", "Comments": [], "label": [[26, 36, "Device-Count"], [109, 128, "Hardware-device"], [133, 141, "Hardware-device"]]}
{"id": 159940, "text": "B.3 Time per Example Due to the lack of reliable software to measure FOPs in PyTorch , we calculate these numbers manually .", "Comments": [], "label": [[77, 84, "Software-Entity"]]}
{"id": 159964, "text": "The training objective of each model with R-Drop is defined in Appendix [ C. ] We implement our models based on the Fairseq toolkit [ Ott et al. , , 2019 ] [ Wang et al. , , 2020 ] .", "Comments": [], "label": [[116, 123, "Software-Entity"]]}
{"id": 159970, "text": "We use the sacrebleu toolkit [ Post , 2018 ] to calculate the BLEU scores . 4 Experimental results In this section , we present the experimental results on three corpora .", "Comments": [], "label": [[11, 20, "Software-Entity"]]}
{"id": 159985, "text": "When initializing the text decoder in two-pass direct S2ST models randomly , we build vocabularies of 1k , 6k , and 2k unigram subword units [ Kudo , 2018 ] with the SentencePiece toolkit [ Kudo and Richardson , 2018 ] for the Fisher , CVSS-C , and multi-domain corpora , respectively .", "Comments": [], "label": [[166, 179, "Software-Entity"]]}
{"id": 160005, "text": "H Training details We optimized all models with the mixed precision training using 32GB V100 GPUs [ Micikevicius et al. , 2018 ] .", "Comments": [], "label": [[83, 87, "Device-Memory"], [88, 97, "Hardware-device"]]}
{"id": 160018, "text": "The system was implemented using PyTorch and the SpeechBrain toolkit [ Ra ] [ vanelli et al. , , 2021 ] .", "Comments": [], "label": [[33, 40, "Software-Entity"], [49, 60, "Software-Entity"]]}
{"id": 160019, "text": "Training took ∼ 8 hours on an NVIDIA A100 GPU .", "Comments": [], "label": [[30, 45, "Hardware-device"]]}
{"id": 160042, "text": "We implement our model using the Pytorch library .", "Comments": [], "label": [[33, 40, "Software-Entity"]]}
{"id": 160043, "text": "The bidirectional masked language model used in our work is RoBERTabase , which is initialized with the pretrained checkpoint from Huggingface .", "Comments": [], "label": [[131, 142, "Software-Entity"]]}
{"id": 160051, "text": "We conduct all experiments on a single Tesla P40 GPU with 24GB memory .", "Comments": [], "label": [[32, 38, "Device-Count"], [39, 52, "Hardware-device"], [58, 69, "Device-Memory"]]}
{"id": 160057, "text": "The SOTA setting is based on the setting in [ Haq et al. , 2021 ] using BioBERT-base-cased-v1.1 as the pretrained model . 4.2 Metrics and settings We use an NVIDIA Titan RTX GPU cluster of 7 nodes for fine-tuning experiments through HuggingFace 's Transformer API [ Wolf et al. , , 2020 ] version 4.13.0 .", "Comments": [], "label": [[157, 177, "Hardware-device"], [233, 247, "Software-Entity"]]}
{"id": 160058, "text": "We leverage the run_glue.py pytorch version as our fine-tuning script .", "Comments": [], "label": [[28, 35, "Software-Entity"]]}
{"id": 160066, "text": "Unless otherwise specified , we use default values of the hyperparameters in Huggingface .", "Comments": [], "label": [[77, 88, "Software-Entity"]]}
{"id": 160081, "text": "We use two NVIDIA V100 GPUs for training .", "Comments": [], "label": [[7, 10, "Device-Count"], [11, 27, "Hardware-device"]]}
{"id": 160089, "text": "We use the SentEval toolkit [ Conneau ] [ and Kiela , 2018 ] for evaluation .", "Comments": [], "label": [[11, 19, "Software-Entity"]]}
{"id": 160092, "text": "This takes a few seconds on a single V100 GPU .", "Comments": [], "label": [[30, 36, "Device-Count"], [37, 45, "Hardware-device"]]}
{"id": 160093, "text": "The first step takes the same inference time as BERT-base ( 0.07 seconds for a given sentence on a single V100 GPU ) as RankEncoder uses BERTbase .", "Comments": [], "label": [[99, 105, "Device-Count"], [106, 114, "Hardware-device"]]}
{"id": 160100, "text": "And while switching to contextualized word embeddings , such as * Flair * , leads to significant improvements , pre-trained transformers perform best .", "Comments": [], "label": [[66, 71, "Software-Entity"]]}
{"id": 160108, "text": "A Implementation and Training Details A.1 Generating Embeddings All claim embeddings were generated using the flair library , [ 4 ] via DocumentPoolEmbeddings for non-transformer-based models , such as Glove and Flair , or TransformerDocumentEmbeddings for BERT and ELECTRA embeddings .", "Comments": [], "label": [[110, 115, "Software-Entity"]]}
{"id": 160111, "text": "A.2 Training SVM models For faster convergence when dealing with a large number of samples , we use a SVM with a linear kernel , specifically , LinearSVC , as implemented in the sklearn library .", "Comments": [], "label": [[178, 185, "Software-Entity"]]}
{"id": 160112, "text": "A.3 Fine-tuning Transformer-based models We used the * bert-base-cased * pre-trained BERT version ( 110M parameters ) , the * electra-basediscriminator * pre-trained ELECTRA version ( 110M parameters ) , and the * deberta-base * pretrained DeBERTA version ( 140M parameters ) as implemented in the huggingface library .", "Comments": [], "label": [[298, 309, "Software-Entity"]]}
{"id": 160116, "text": "The training time on one RTX 2080Ti GPU was 80–160 minutes , depending on the chosen setup ( with or without context information ) .", "Comments": [], "label": [[25, 39, "Hardware-device"]]}
{"id": 160125, "text": "We paid crowd workers on Amazon MTurk to annotate our data .", "Comments": [], "label": [[25, 37, "Cloud-Platform"]]}
{"id": 160129, "text": "Some prior user studies [ Nguyen et al. , , 2018 ] [ Pen ] [ nycook and Rand , 2019 ] [ Shabani et al. , , 2021 ] have also shown laypeople ( e.g. , Amazon Mechanical Turk workers ) can be good at judging the veracity of claims or reliability of news articles .", "Comments": [], "label": [[148, 171, "Cloud-Platform"]]}
{"id": 160130, "text": "B Annotation Interface for Stance Classification Our stance data collection procedures ( [ §4.1.2 ] on Amazon MTurk were approved by an ethics board .", "Comments": [], "label": [[103, 115, "Cloud-Platform"]]}
{"id": 160131, "text": "C Approved Treatments Table [ 8 ] shows the approved treatments that were used for filtering in [ §4.1.3 . ] D Implementation Details All experiments are performed with NVIDIA A40 GPUs .", "Comments": [], "label": [[169, 184, "Hardware-device"]]}
{"id": 160134, "text": "`` * Figure 5 : Amazon MTurk ethics statement which was shown to annotators before stance labeling task Figure 6 : Amazon MTurk interface for stance annotation towards extracted claims in tweets .", "Comments": [], "label": [[16, 28, "Cloud-Platform"], [115, 127, "Cloud-Platform"]]}
{"id": 160137, "text": "In our experiments , we modify the public PyTorch implementation [ 3 ] of SimCSE to support our proposed augmentation and subsampling methods .", "Comments": [], "label": [[42, 49, "Software-Entity"]]}
{"id": 160140, "text": "We employ the modified SentEval [ 5 ] [ Conneau and Kiela , 2018 ] package accompanying the source code of SimCSE for fair comparison with other works .", "Comments": [], "label": [[23, 31, "Software-Entity"]]}
{"id": 160141, "text": "In fact , on a single NVIDIA A100 GPU ( 40GB ) , our model can finish training in under 15 minutes .", "Comments": [], "label": [[15, 21, "Device-Count"], [22, 37, "Hardware-device"], [40, 44, "Device-Memory"]]}
{"id": 160142, "text": "We employ UMAP [ McInnes et al. , , 2018 ] with cosine distance as the metric to preserve local and global topological neighborhoods .", "Comments": [], "label": [[10, 14, "Software-Entity"]]}
{"id": 160150, "text": "For * text * input , all sentences in ST and external MT datasets are tokenized and segmented into subwords using SentencePiece [ 3 ] .", "Comments": [], "label": [[114, 127, "Software-Entity"]]}
{"id": 160157, "text": "We implement our model with * fairseq * [ 7 ] [ Ott et al. , , 2019 ] .", "Comments": [], "label": [[30, 37, "Software-Entity"]]}
{"id": 160158, "text": "All models are trained on 4 Nvidia RTX 3090 GPUs .", "Comments": [], "label": [[26, 27, "Device-Count"], [28, 48, "Hardware-device"]]}
{"id": 160161, "text": "Second , our proposed CRESS significantly outperforms MTL in both settings , with 1.8 BLEU improvement in the base set- < https : //github.com/mjpost/sacrebleu > sacreBLEU signature : nrefs:1 | bs:1000 | seed:12345 | case : mixed | eff : no | tok:13a | smooth : exp | version:2.0.0 < https : //github.com/pytorch/fairseq > For example , the pre-training of SpeechUT takes 3 days with 32 V100 GPUs .", "Comments": [], "label": [[384, 386, "Device-Count"], [387, 396, "Hardware-device"]]}
{"id": 160166, "text": "We also release the code implemented with a wellknown framework fairseq .", "Comments": [], "label": [[64, 71, "Software-Entity"]]}
{"id": 160181, "text": "We also evaluate fewshot PROMPTRANK on the 2WikiMQA dataset [ Ho et al. , , 2020 ] in Appendix [ D.1 . ] Compute Infrastructure All our reranking experiments are run on a workstation with a single Nvidia A40 GPU and 256GB of RAM .", "Comments": [], "label": [[190, 196, "Device-Count"], [197, 211, "Hardware-device"], [216, 228, "Device-Memory"]]}
{"id": 160182, "text": "Our QA experiments in [ §3.2 ] are run on a workstation with two Nvidia Quadro RTX 8000 GPUs and 128GB of RAM .", "Comments": [], "label": [[61, 64, "Device-Count"], [65, 92, "Hardware-device"], [97, 109, "Device-Memory"]]}
{"id": 160183, "text": "Models We use HuggingFace implementations [ Wolf et al. , , 2020 ] of GPT2-XL ( 1.5B ) [ Brown et al. , 2020 ] , T5-Base ( 220M ) , T5-Large ( 770M ) and T5-XL ( 3B ) [ Raffel et al. , , 2020 ] in our experiments .", "Comments": [], "label": [[14, 25, "Software-Entity"]]}
{"id": 160214, "text": "Inference was run over a single Nvidia Quadro RTX 8000 GPU .", "Comments": [], "label": [[25, 31, "Device-Count"], [32, 58, "Hardware-device"]]}
{"id": 160235, "text": "Our entire codebase is implemented in PyTorch . [ 7 ] The implementations of the transformer-based models are extended from the Huggingface [ 8 ] codebase [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[38, 45, "Software-Entity"], [128, 139, "Software-Entity"]]}
{"id": 160236, "text": "All the models in this work are trained on NVIDIA A6000 GPUs on a Ubuntu 20.04.2 operating system .", "Comments": [], "label": [[43, 60, "Hardware-device"]]}
{"id": 160245, "text": "TextAttack [ Morris et al. , , 2020 ] provides a comprehensive categorization of such methods and a framework to implement them .", "Comments": [], "label": [[0, 10, "Software-Entity"]]}
{"id": 160246, "text": "In this work , we introduce augmentations in the textual part of multimodal data using TextAttack methods and consider them as * baseline * augmentations .", "Comments": [], "label": [[87, 97, "Software-Entity"]]}
{"id": 160258, "text": "We utilize the TextAttack [ Mor ] [ ris et al. , , 2020 ] [ 1 ] framework for implementing all the baseline perturbation strategies .", "Comments": [], "label": [[15, 25, "Software-Entity"]]}
{"id": 160260, "text": "We use TextAttack 's default fast implementation of CLARE . 4.4 XMAI Implementation Details We choose k = 3 for the number of top-k predicted BERT words for each [ MASK ] token and flair/pos-english-fast for PoS tagging of text .", "Comments": [], "label": [[7, 20, "Software-Entity"]]}
{"id": 160261, "text": "Next , to compare the nouns in the text with the objects identified in the image , we use word embeddings produced by a Transformer-based model ( bert-base-nli-mean-tokens on HuggingFace [ Wolf et al. , , 2020 ] ) .", "Comments": [], "label": [[175, 186, "Software-Entity"]]}
{"id": 160262, "text": "For [ MASK ] filling , we use the bert-base-cased model on HuggingFace and the list of stopwords is adopted from NLTK .", "Comments": [], "label": [[59, 70, "Software-Entity"], [113, 117, "Software-Entity"]]}
{"id": 160263, "text": "[ 2 ] To compute the similarity between attributes detected in I and BERT predictions , we employ SpaCy 's pretrained tok2vec model ( en_core_web_md ) , which contains 300 dimensional embeddings for ∼ 500k words [ Hon ] [ nibal et al. , , 2020 ] .", "Comments": [], "label": [[98, 103, "Software-Entity"]]}
{"id": 160265, "text": "Similarly , we report BLEU [ Pa ] [ pineni et al. , , 2002 ] and METEOR [ Banerjee and ] [ Lavie , 2005 ] , using NLTK to further compare the texts ( considering n-grams of size upto 4 in the case of BLEU ) .", "Comments": [], "label": [[114, 118, "Software-Entity"]]}
{"id": 160267, "text": "5.1 Human Assessment of Augmentations We recruit annotators using Amazon Mechanical Turk ( AMT ) to answer the following two questions : * ( i ) * do cross-modal attribute insertions lead to better text augmentations than the most competitive baseline ( i.e. , CLARE ) , and * ( ii ) * are cross-modal Novel insertions in ′ mean more 'false positives ' with respect to T , indicating lower precision and BLEU scores .", "Comments": [], "label": [[66, 96, "Cloud-Platform"]]}
{"id": 160269, "text": "We release the code for our experiments to aid reproducibility and enable future research on this topic . * Annotations , IRB approval , and datasets * : The annotators for evaluations done in this study were recruited via Amazon Mechanical Turk .", "Comments": [], "label": [[223, 245, "Cloud-Platform"]]}
{"id": 160270, "text": "A Appendix A.1 Human Evaluation Details For our annotation tasks , we recruited annotators using Amazon Mechanical Turk .", "Comments": [], "label": [[97, 119, "Cloud-Platform"]]}
{"id": 160275, "text": "However , XMAI can augment the text a magnitude faster than CLARE , even after using the fast implementation of CLARE from TextAttack .", "Comments": [], "label": [[123, 133, "Software-Entity"]]}
{"id": 160276, "text": "A.5 Compute Resources Our experiments were split between a single Tesla V100 for object detection ( ∼ 1 hour ) and NVIDIA Tesla T4 GPUs for our augmentation ( ∼ 3 hours ) .", "Comments": [], "label": [[59, 65, "Device-Count"], [66, 76, "Hardware-device"], [115, 135, "Hardware-device"]]}
{"id": 160289, "text": "Our experiments were done using GeForce RTX 2080 Ti GPU .", "Comments": [], "label": [[32, 55, "Hardware-device"]]}
{"id": 160299, "text": "By utilizing a single GPU ( RTX2080ti in our experiments ) , for instance , with a batch size of 16 , the reference time reduces to around 14.20s .", "Comments": [], "label": [[15, 25, "Device-Count"], [28, 37, "Hardware-device"]]}
{"id": 160306, "text": "In our experiments , one epoch training on one GeForce RTX2080Ti took about two and a half hours .", "Comments": [], "label": [[43, 46, "Device-Count"], [47, 64, "Hardware-device"]]}
{"id": 160311, "text": "3.3 Pre-processing step The supplied text data has been split into subword units using SentencePiece [ Kudo and Richard ] [ son , 2018 ] , an extension of Byte-Pair encoding ( BPE ) [ Sennrich et al. , , 2016 ] and WordPiece [ Wu et al. , 2016 ] that does not require pre-tokenization ( at the word or token level ) , thereby avoiding the requirement for language-specific tokenizers .", "Comments": [], "label": [[87, 100, "Software-Entity"], [215, 224, "Software-Entity"]]}
{"id": 160314, "text": "Language modeling We train the models on the Masked Language Modeling ( MLM ) task using HuggingFace library [ Wolf et al. , , 2019 ] .", "Comments": [], "label": [[89, 100, "Software-Entity"]]}
{"id": 160315, "text": "Models are trained on 128 Nvidia V100 32 GB GPUs for 20 hours on Jean Zay supercomputer .", "Comments": [], "label": [[22, 25, "Device-Count"], [26, 37, "Hardware-device"], [38, 48, "Device-Memory"]]}
{"id": 160323, "text": "We utilize the open source text annotation tool Doccano [ 5 ] to Table 5 : Fleiss ' Kappa for different granularities . facilitate the labeling process .", "Comments": [], "label": [[48, 55, "Software-Entity"]]}
{"id": 160330, "text": "All experiments are conducted using a GeForce RTX 3090 GPU .", "Comments": [], "label": [[38, 58, "Hardware-device"]]}
{"id": 160365, "text": "B.2 ASR models We use ASR models publicly released on Hugging-Face to transcribe the generated speech in order to calculate WER or BLEU scores in comparison with ground truth texts .", "Comments": [], "label": [[54, 66, "Software-Entity"]]}
{"id": 160370, "text": "Each bilingual model is trained on 16 A100 GPUs for 3 days on average .", "Comments": [], "label": [[35, 37, "Device-Count"], [38, 47, "Hardware-device"]]}
{"id": 160379, "text": "In a batch , token sizes of 1500 and 9000 with update frequency of 15 and 2 are used for V100 and A100 training respectively .", "Comments": [], "label": [[89, 93, "Hardware-device"], [98, 102, "Hardware-device"]]}
{"id": 160384, "text": "Textless models used 32 A100 GPUs , the 70M model was trained for 3 days , the 260M model was for 5 days , and the 424M model was for 6 days .", "Comments": [], "label": [[21, 23, "Device-Count"], [24, 33, "Hardware-device"]]}
{"id": 160385, "text": "It took 2 days to train XM Transformer on 32 A100 GPUs for Slavic-to-English translation .", "Comments": [], "label": [[42, 44, "Device-Count"], [45, 54, "Hardware-device"]]}
{"id": 160392, "text": "There is one expert in each GPU and we used 64 GPUs in our experiments , which means we have 64 Base Layer experts in total .", "Comments": [], "label": [[44, 51, "Hardware-device"]]}
{"id": 160397, "text": "It took 3 days to train dense XM Transformer for all-to-English with 32 A100 GPUs .", "Comments": [], "label": [[69, 71, "Device-Count"], [72, 81, "Hardware-device"]]}
{"id": 160398, "text": "It took 5 days to train the GShard counterpart with 64 A100 GPUs .", "Comments": [], "label": [[52, 54, "Device-Count"], [55, 64, "Hardware-device"]]}
{"id": 160408, "text": "For document simplification , a GPU with at least 24 GB of memory is required to reproduce our results with mBART and at least 16 GB for sentence simplification respectively .", "Comments": [], "label": [[50, 65, "Device-Memory"], [127, 132, "Device-Memory"]]}
{"id": 160409, "text": "For the evaluation of all alignment methods , we required less than 1 GPU hour on an NVIDIA RTX A5000 with 24 GB .", "Comments": [], "label": [[85, 101, "Hardware-device"], [107, 112, "Device-Memory"]]}
{"id": 160410, "text": "The experiments with document and sentence simplification , overall , took less than 18 GPU hours on a NVIDIA RTX A5000 with 24 GB .", "Comments": [], "label": [[103, 119, "Hardware-device"], [125, 130, "Device-Memory"]]}
{"id": 160411, "text": "We used Python 3 and the Python Package Beautiful Soup for the HTML data and pymupdf for the PDF data .", "Comments": [], "label": [[40, 54, "Software-Entity"], [77, 84, "Software-Entity"]]}
{"id": 160433, "text": "We did experiments with all three options , for the word vectors we used the German embeddings of fasttext [ 35 ] [ Athiwaratkun et al. , , 2018 ] .", "Comments": [], "label": [[98, 106, "Software-Entity"]]}
{"id": 160436, "text": "Considering the benefits of the transformer architectures on parallelization and learning long-term dependencies over recurrent models [ Vaswani et al. , , 2017 ] , we have implemented the baseline models using transformer architecture .", "Comments": [], "label": [[211, 222, "Software-Entity"]]}
{"id": 160437, "text": "We obtained the best model by fine-tuning uncased BERT-large , with a learning rate of 1e-5 , batch size of 16 , and after 6 epochs at the 1029th step on a single GPU .", "Comments": [], "label": [[156, 166, "Device-Count"]]}
{"id": 160438, "text": "All the experiments have been conducted on a single GeForce RTX 2080 Ti GPU .", "Comments": [], "label": [[45, 51, "Device-Count"], [52, 75, "Hardware-device"]]}
{"id": 160439, "text": "Only the ALBERT classifier model has been trained with 8 TPU cores on Google Cloud .", "Comments": [], "label": [[55, 56, "Device-Count"], [57, 66, "Hardware-device"], [70, 82, "Cloud-Platform"]]}
{"id": 160440, "text": "The 50 Technotes were obtained by issuing a query to an instance of Elasticsearch [ 5 ] that indexes the 801 , 998 Technotes .", "Comments": [], "label": [[68, 81, "Software-Entity"]]}
{"id": 160441, "text": "Submitted systems will run on a machine with 128 GB of memory and two 16G V100 GPUs , with 64 GB local disk space available for temporary files or logs .", "Comments": [], "label": [[45, 61, "Device-Memory"], [66, 69, "Device-Count"], [70, 83, "Hardware-device"], [91, 113, "Device-Memory"]]}
{"id": 160442, "text": "Our experiments are conducted with 4 NVIDIA P40 GPU .", "Comments": [], "label": [[35, 36, "Device-Count"], [37, 51, "Hardware-device"]]}
{"id": 160443, "text": "Byte Pair Encoding ( BPE ) [ Sennrich et al. , , 2016 ] word segmentation is used for data pre-processing .", "Comments": [], "label": [[0, 18, "Software-Entity"], [21, 24, "Software-Entity"]]}
{"id": 160444, "text": "2 Model In order to design a neural network that is efficient to train and that exploits syntactic information while producing high-quality translations , we base our model on the Transformer architecture [ Vaswani et al. , , 2017 ] and upgrade its encoder with * parent-scaled self-attention * ( PASCAL ) heads at layer ls .", "Comments": [], "label": [[180, 191, "Software-Entity"]]}
{"id": 160445, "text": "We rely on Stanford CoreNLP [ Man ] [ ning et al. , , 2014 ] to parse source sentences .", "Comments": [], "label": [[11, 27, "Software-Entity"]]}
{"id": 160446, "text": "[ 1 ] Training We implement our models in PyTorch on top of the Fairseq toolkit .", "Comments": [], "label": [[42, 49, "Software-Entity"], [64, 71, "Software-Entity"]]}
{"id": 160447, "text": "Training details All experiments are based on the base Transformer architecture and optimized following the learning schedule of [ Vaswani et al. , 2017 ] with 8 , 000 warm-up steps .", "Comments": [], "label": [[55, 66, "Software-Entity"]]}
{"id": 160448, "text": "We use a batch size of 32K tokens and run experiments on a cluster of 4 machines , each having 4 Nvidia P100 GPUs .", "Comments": [], "label": [[95, 96, "Device-Count"], [97, 113, "Hardware-device"]]}
{"id": 160449, "text": "We use the SACREBLEU [ Post , 2018 ] tool to compute case-sensitive BLEU scores .", "Comments": [], "label": [[11, 20, "Software-Entity"]]}
{"id": 160450, "text": "[ 7 ] When evaluating En-Ja translations , we follow the procedure employed at WAT by computing BLEU scores after tokenizing target sentences using KyTea .", "Comments": [], "label": [[148, 153, "Software-Entity"]]}
{"id": 160451, "text": "We train all models on 2 NVIDIA 2080Ti GPUs using Adam optimizer with β = 0.9 , β = 0.98 , ε = 10− and following the same learning rate schedule in [ Vaswani et al. , , 2017 ] .", "Comments": [], "label": [[23, 24, "Device-Count"], [25, 43, "Hardware-device"]]}
{"id": 160452, "text": "We implement them on top of OpenNMT [ Klein et al. , , 2017 ] .", "Comments": [], "label": [[28, 35, "Software-Entity"]]}
{"id": 160453, "text": "We conduct all experiments on the TRANSFORMER-BIG with four V100 GPUs . 4.1 Effect of τ in HeadXL SANs Fig . [ 3 ] reports the results of different τ for Head XL SANs .", "Comments": [], "label": [[55, 59, "Device-Count"], [60, 69, "Hardware-device"]]}
{"id": 160454, "text": "On the other hand , contextualized word representations and language models have been developed using both featurebased architectures , the most notable examples being ELMo and Flair [ Peters et al. , , 2018 ] [ Akbik et al. , 2018 ] , and transformer based architectures , that are commonly used in a fine-tune setting , as is the case of GPT-1 , GPT-2 [ Radford et al. , ] [ 2018 , 2019 ] , BERT and its derivatives [ Devlin et al. , , 2018 ] [ Liu et al. , , 2019 ] [ Lan et al. , , 2019 ] and more recently T5 [ Raffel et al. , , 2019 ] .", "Comments": [], "label": [[177, 182, "Software-Entity"]]}
{"id": 160455, "text": "More precisely , language classification was performed using the * fastText * linear classifier [ Joulin et al. , ] [ 2016 , 2017 ] , which was trained by [ Grave et al. , 2018 ] to recognize 176 languages and was shown to have an extremely good accuracy to processing time trade-off .", "Comments": [], "label": [[67, 75, "Software-Entity"]]}
{"id": 160456, "text": "In our set-up we used two different machines , each one having 4 NVIDIA GeForce GTX 1080 Ti graphic cards and 128GB of RAM , the difference between the machines being that one uses a single Intel Xeon Gold 5118 processor , while the other uses two Intel Xeon E5-2630 v4 processors .", "Comments": [], "label": [[63, 64, "Device-Count"], [65, 91, "Hardware-device"], [110, 122, "Device-Memory"], [183, 189, "Device-Count"], [190, 210, "Hardware-device"], [244, 247, "Device-Count"], [248, 269, "Hardware-device"]]}
{"id": 160457, "text": "One GeForce GTX 1080 Ti card is rated at around Table 7 : Average power draw ( Watts ) , training times ( in both hours and days ) , mean power consumption ( KWh ) and CO emissions ( kg ) for each ELMo model trained .", "Comments": [], "label": [[4, 23, "Hardware-device"]]}
{"id": 160458, "text": "250 W , [ 17 ] the Xeon Gold 5118 processor is rated at 105 W , [ 18 ] while one Xeon E5-2630 v4 is rated at 85 W. [ 19 ] For the DRAM we can use the work of [ Desrochers et al. , 2016 ] to estimate the total power draw of 128GB of RAM at around 13W .", "Comments": [], "label": [[19, 33, "Hardware-device"], [77, 80, "Device-Count"], [81, 96, "Hardware-device"], [223, 235, "Device-Memory"]]}
{"id": 160459, "text": "We do not report the power consumption or the carbon footprint of training the UDPipe 2.0 architecture , as each model took less than 4 hours to train on a machine using a single NVIDIA Tesla V100 card .", "Comments": [], "label": [[172, 178, "Device-Count"], [179, 196, "Hardware-device"]]}
{"id": 160460, "text": "We use the Hugging Face implementation of [ Wolf et al. , , 2019 ] for GPT-2 small ( GPT-2S ) , medium ( GPT-2M ) and large ( GPT-2L ) .", "Comments": [], "label": [[11, 23, "Software-Entity"]]}
{"id": 160461, "text": "Fine-tuning converges after 6 epochs , which takes just a few hours on a V100 GPU [ 1 ] .", "Comments": [], "label": [[73, 81, "Hardware-device"]]}
{"id": 160462, "text": "We compute BLEU [ Papineni et al. , , 2002 ] using SacreBLEU [ Ma et al. , , 2019 ] .", "Comments": [], "label": [[51, 60, "Software-Entity"]]}
{"id": 160463, "text": "Secondly , we utilize FLAIR [ 1 ] [ Akbik et al. , , 2019 ] , an off-theshelf state-of-the-art syntactic chunker that leverages contextual embeddings , to shallow parse each review r in corpus C .", "Comments": [], "label": [[22, 27, "Software-Entity"]]}
{"id": 160464, "text": "Specifically , we first shallow parse y using FLAIR , obtaining a list of chunks Cy , each of which is removed with probability p .", "Comments": [], "label": [[46, 51, "Software-Entity"]]}
{"id": 160465, "text": "Our model was trained on a single GeForce GTX 1080 Ti GPU and is implemented using PyTorch . [ 4 ] Comparison Systems We compared DENOIS-ESUM to several unsupervised extractive and abstractive methods .", "Comments": [], "label": [[27, 33, "Device-Count"], [34, 57, "Hardware-device"], [83, 90, "Software-Entity"]]}
{"id": 160466, "text": "Human Evaluation We also conducted two judgment elicitation studies using the Amazon Mechanical Turk ( AMT ) crowdsourcing platform .", "Comments": [], "label": [[78, 108, "Cloud-Platform"]]}
{"id": 160467, "text": "- UDA [ Xie et al. , , 2019 ] : Since we do not have access to TPU and need to use smaller amount of unlabeled data , we implemented Unsupervised Data Augmentation ( UDA ) using pytorch by ourselves .", "Comments": [], "label": [[178, 185, "Software-Entity"]]}
{"id": 160468, "text": "We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan V GPU used for this research .", "Comments": [], "label": [[85, 96, "Hardware-device"]]}
{"id": 160469, "text": "Figs [ 3 ] & [ 4 ] took 1 week on a cluster of 15 NVidia 1080Tis .", "Comments": [], "label": [[47, 49, "Device-Count"], [50, 64, "Hardware-device"]]}
{"id": 160470, "text": "All experiments are implemented in PyTorch .", "Comments": [], "label": [[35, 42, "Software-Entity"]]}
{"id": 160471, "text": "Our models are trained using NVIDIA Tesla P100 GPUs . 4.4 Results and Discussions Table [ 2 ] shows the results on the * TExEval-2 task * Evaluation on * science * and * environment * domains .", "Comments": [], "label": [[29, 51, "Hardware-device"]]}
{"id": 160472, "text": "The final block uses pretrained fastText embeddings to initialize our * Graph2Taxo * model , and then * fine tunes * based on our training data .", "Comments": [], "label": [[32, 40, "Software-Entity"]]}
{"id": 160473, "text": "Specifically , we initialize the input matrix H of our * Graph2Taxo * model with pre-trained fastText [ 5 ] embeddings .", "Comments": [], "label": [[93, 101, "Software-Entity"]]}
{"id": 160474, "text": "Our model using fastText embeddings improves upon Row 1 by a margin of 4 % in precision values for the Environment ( Eurovoc ) domain , but unfortunately has no significant effect on the F-score .", "Comments": [], "label": [[16, 24, "Software-Entity"]]}
{"id": 160475, "text": "For fair comparison , all the above models share the same vocabulary and BPE tokenizer [ Sennrich et al. , 2015 ] .", "Comments": [], "label": [[73, 86, "Software-Entity"]]}
{"id": 160476, "text": "We train each model on 8 GPUs for 10 epochs and perform early stopping based on accuracy on the test set .", "Comments": [], "label": [[23, 29, "Device-Count"]]}
{"id": 160477, "text": "The CPU inference time is tested on Intel Xeon E5-2698 v4 with batch size 128 .", "Comments": [], "label": [[36, 57, "Hardware-device"]]}
{"id": 160478, "text": "The GPU inference time is tested on NVIDIA Quadro P100 with batch size ∈ { 128 , 256 , 384 } .", "Comments": [], "label": [[36, 54, "Hardware-device"]]}
{"id": 160479, "text": "Note that the P100 will be out of memory when batch size is 384 for Roberta-Large .", "Comments": [], "label": [[14, 18, "Hardware-device"]]}
{"id": 160480, "text": "Our implementation of DeeBERT is adapted from the HuggingFace Transformers Library [ Wolf et al. , , 2019 ] .", "Comments": [], "label": [[50, 61, "Software-Entity"]]}
{"id": 160481, "text": "Inference runtime measurements are performed on a single NVIDIA Tesla P100 graphics card .", "Comments": [], "label": [[50, 56, "Device-Count"], [57, 74, "Hardware-device"]]}
{"id": 160482, "text": "We use the AllenNLP framework [ Gardner et al. , , 2018 ] to implement our methods .", "Comments": [], "label": [[11, 19, "Software-Entity"]]}
{"id": 160483, "text": "Finally , we acknowledge the support of NVIDIA Corporation with the donation of a Titan Xp GPU used for the study .", "Comments": [], "label": [[82, 94, "Hardware-device"]]}
{"id": 160484, "text": "BERT encoder The BERT encoder uses a pretrained model checkpoint and tokenizer , specifically * bert-base-uncased * , provided by the HuggingFace Transformer library [ Wolf et al. , , 2019 ] .", "Comments": [], "label": [[134, 145, "Software-Entity"]]}
{"id": 160485, "text": "All of our models were built using PyTorch [ Paszke et al. , , 2019 ] .", "Comments": [], "label": [[35, 42, "Software-Entity"]]}
{"id": 160486, "text": "We also thank the NVIDIA Corporation for the donation of the Titan X Pascal GPU used in this research .", "Comments": [], "label": [[61, 79, "Hardware-device"]]}
{"id": 160487, "text": "Training and Implementation We implement our model in AllenNLP [ Gardner et al. , , 2018 ] .", "Comments": [], "label": [[54, 62, "Software-Entity"]]}
{"id": 160488, "text": "We train the model on a single Titan V GPU ( 12G memory ) for 2 epochs , with batch size of 4 ( the maximum that fit in our GPU memory ) and use gradient accumulation for an effective batch size of 32 .", "Comments": [], "label": [[24, 30, "Device-Count"], [31, 42, "Hardware-device"], [45, 55, "Device-Memory"]]}
{"id": 160489, "text": "Visualization Figure [ 2 ] shows t-SNE [ van der ] [ Maaten , 2014 ] projections of our embeddings ( SPECTER ) compared with the SciBERT baseline We experimented with both concatenating authors with the title and abstract and also considering them as an additional field .", "Comments": [], "label": [[33, 38, "Software-Entity"]]}
{"id": 160490, "text": "We trained Doc2Vec on our training subset using Gensim ( Reh ˇ [ u˚ˇrek and Sojka , 2010 ] , and chose the hyperparameter grid using suggestions from [ Lau ] [ and Baldwin , 2016 ] .", "Comments": [], "label": [[48, 54, "Software-Entity"]]}
{"id": 160491, "text": "We trained our own 300 dimensional fasttext embeddings [ Bojanowski et al. , , 2017 ] on a corpus of around 3.1B tokens from scientific papers which is similar in size to the SciBERT corpus [ Beltagy et al. , 2019 ] .", "Comments": [], "label": [[35, 43, "Software-Entity"]]}
{"id": 160492, "text": "SIF The SIF method of [ Arora et al. , 2017 ] is a strong text representation baseline that takes a weighted sum of pretrained word vectors ( we use fasttext embeddings described above ) , then computes the first principal component of the document embedding matrix and subtracts out each document embedding 's projection to the first principal component .", "Comments": [], "label": [[149, 157, "Software-Entity"]]}
{"id": 160493, "text": "When computing term-frequency values for SIF , we used scikit-learn 's TfidfVectorizer with the same parameters as enumerated in the preceding section . sublinear_tf , binary , use_idf , smooth_idf were all set to False .", "Comments": [], "label": [[55, 70, "Software-Entity"]]}
{"id": 160494, "text": "Since SIF is a sum of pretrained fasttext vectors , the resulting dimensionality is 300 . 5 .", "Comments": [], "label": [[33, 41, "Software-Entity"]]}
{"id": 160495, "text": "We used the 768-dimensional pretrained ELMo model in AllenNLP [ Gardner et al. , , 2018 ] . 6 .", "Comments": [], "label": [[53, 61, "Software-Entity"]]}
{"id": 160496, "text": "[ 18 ] During hyperparameter optimization we chose how to compute TF and IDF values weights by taking the following non-redundant combinations of scikit-learn 's TfidfVectorizer [ Pedregosa et al. , , 2011 ] parameters : sublinear_tf , binary , use_idf , smooth_idf .", "Comments": [], "label": [[146, 161, "Software-Entity"]]}
{"id": 160497, "text": "For training of fasttext , we used all default parameters with the exception of setting dimension to 300 and minCount was set to 25 due to the large corpus .", "Comments": [], "label": [[16, 24, "Software-Entity"]]}
{"id": 160498, "text": "Acknowledgments The first author would like to gratefully thank the NVIDIA Corporation for the donation of a TITAN Xp GPU that was used in this research .", "Comments": [], "label": [[109, 121, "Software-Entity"]]}
{"id": 160499, "text": "One epoch takes ≈ 50 min with a TitanXp/1080Ti .", "Comments": [], "label": [[32, 46, "Hardware-device"]]}
{"id": 160500, "text": "The NLTK [ 4 ] package was used for sentence splitting and word tokenization .", "Comments": [], "label": [[4, 8, "Software-Entity"]]}
{"id": 160501, "text": "Our model was trained on two Nvidia RTX 2080Ti graphics cards .", "Comments": [], "label": [[25, 28, "Device-Count"], [29, 46, "Hardware-device"]]}
{"id": 160502, "text": "It takes approximately one hour to train the base MultiQT model on one NVIDIA GeForce GTX 1080 Ti GPU card .", "Comments": [], "label": [[67, 70, "Device-Count"], [71, 101, "Hardware-device"]]}
{"id": 160503, "text": "To assess the real-time capability of MultiQT , we test it on an average emergency call using an NVIDIA GTX 1080 Ti GPU card .", "Comments": [], "label": [[97, 119, "Hardware-device"]]}
{"id": 160504, "text": "Batching further increases the real-time factor and enables a larger number of ongoing calls to be processed in parallel on a single GPU .", "Comments": [], "label": [[126, 136, "Device-Count"]]}
{"id": 160505, "text": "For the decoder , we use a Transformer [ Vaswani et al. , , 2017 ] ( 3 layers , 8 heads ) .", "Comments": [], "label": [[27, 38, "Software-Entity"]]}
{"id": 160506, "text": "We train the models on a single Nvidia Table 4 : Average numbers of parameters and time costs for training .", "Comments": [], "label": [[25, 31, "Device-Count"]]}
{"id": 160507, "text": "GTX 1080 GPU and report the training time until the convergence of each model .", "Comments": [], "label": [[0, 12, "Hardware-device"]]}
{"id": 160508, "text": "We use one NVIDIA Tesla v100 for the experiments .", "Comments": [], "label": [[7, 10, "Device-Count"], [11, 28, "Hardware-device"]]}
{"id": 160509, "text": "C Experimental Settings We present hyperparameters in Tables [ 1 ] and [ 2 ] and Figure [ 5 . ] We conduct all GloVe and ELMo experiments using PyTorch 1.3.0 with CUDA 10.0 and cuDNN 7.6.3 , running on NVIDIA Titan RTX , Titan V , and RTX 2080 Ti graphics accelerators .", "Comments": [], "label": [[144, 151, "Software-Entity"], [163, 167, "Software-Entity"], [177, 182, "Software-Entity"], [202, 218, "Hardware-device"], [221, 228, "Hardware-device"], [235, 246, "Hardware-device"]]}
{"id": 160510, "text": "Our MLP and LSTM experiments use PyTorch 0.4.1 with CUDA 9.2 and cuDNN 7.1.4 , running on RTX 2080 Ti 's .", "Comments": [], "label": [[33, 40, "Software-Entity"], [52, 56, "Software-Entity"], [65, 70, "Software-Entity"], [90, 101, "Hardware-device"]]}
{"id": 160511, "text": "This took approximately seven days on 16 P100 GPUs .", "Comments": [], "label": [[38, 40, "Device-Count"], [41, 50, "Hardware-device"]]}
{"id": 160512, "text": "We take Transformer-base [ Vaswani et al. , , 2017 ] as our model architecture , and follow [ Ma et al. , 2019 ] to train our model with wait-k policies for integer 1 ≤ k ≤ 10 .", "Comments": [], "label": [[8, 19, "Software-Entity"]]}
{"id": 160513, "text": "These methods are tested on one GeForce GTX TITAN-X GPU for ZH→EN test set .", "Comments": [], "label": [[28, 31, "Device-Count"], [32, 55, "Hardware-device"]]}
{"id": 160514, "text": "We measure the speedup of model inference in each task on a single NVIDIA P40 GPU with the batch size 1 .", "Comments": [], "label": [[60, 66, "Device-Count"], [67, 81, "Hardware-device"]]}
{"id": 160515, "text": "[ 5 ] Training Details We use PyTorch as our neural network toolkit and run the code on a NVIDIA GeForce GTX Titan Xp GPU and Intel Xeon E5- 2603 v4 CPU .", "Comments": [], "label": [[30, 37, "Software-Entity"], [90, 121, "Hardware-device"], [126, 152, "Hardware-device"]]}
{"id": 160516, "text": "Cython '' stands for using Cython to optimize the python code . `` w/o tree inference '' stands for evaluating without tree inference .", "Comments": [], "label": [[0, 6, "Software-Entity"], [27, 33, "Software-Entity"]]}
{"id": 160517, "text": "We visualize its K-Means clustering results using t-SNE [ Maaten and ] [ Hinton , 2008 ] in Figure [ 2 ( b ) . ] Similar results can be observed for the word * penalty * , as shown in Figure [ 2 ( c ) . ] These examples demonstrate how our document contextualization works for each word .", "Comments": [], "label": [[50, 55, "Software-Entity"]]}
{"id": 160518, "text": "It is the only model in our comparison that uses external word embedding ( in our experiments , we use ELMo [ Peters et al. , , 2018 ] for English and fastText [ Grave et al. , , 2018 ] for Japanese ) .", "Comments": [], "label": [[151, 159, "Software-Entity"]]}
{"id": 160519, "text": "We tune PRPN and URNNG with the same time budget of 5 days on a GPU cluster with TITAN V GPUs .", "Comments": [], "label": [[81, 93, "Hardware-device"]]}
{"id": 160520, "text": "4.3 Metrics We first report attack results both in terms of charlevel BLEU ( * chrBLEU * ) of perturbed source by the origin to indicate modification rate , and relative decrease in target BLEU ( * RD * ) : We adopt sacreBLEU [ Post , 2018 ] to test caseinsensitive BLEU on detokenized targets .", "Comments": [], "label": [[216, 225, "Software-Entity"]]}
{"id": 160521, "text": "* * Tokens embedding * * * xt xt * * xt * −1 * X Xemb * * * ( a ) Overview ( b ) Discriminator ( c ) Agent * * src bi-GRU Figure 2 : Time consumption of different methods : we limit memory usage to 2.5G on single Nvidia 1080 , and generate adversarial examples for the same 800 inputs in Zh→En MT with different methods , our method significantly outclasses the state-of-the-art search paradigm ( GS ) .", "Comments": [], "label": [[198, 202, "Device-Memory"], [206, 212, "Device-Count"], [213, 224, "Hardware-device"]]}
{"id": 160522, "text": "Our agent takes around 30 hours to converge on a single Nvidia 1080ti .", "Comments": [], "label": [[49, 55, "Device-Count"], [56, 69, "Hardware-device"]]}
{"id": 160523, "text": "The former is from GloVe vectors with 840B tokens [ Pennington et al. , , 2014 ] , and the latter is trained on a large domain-specific corpus using fastText and published by [ Xu et al. , 2018 ] .", "Comments": [], "label": [[149, 157, "Software-Entity"]]}
{"id": 160524, "text": "We run three models on the Restaurant 2014 dataset with the same batch size 8 in a single 1080Ti GPU , and present the results in Table [ 6 . ]", "Comments": [], "label": [[83, 89, "Device-Count"], [90, 100, "Hardware-device"]]}
{"id": 160525, "text": "For each dataset , we tokenize the sentences by Moses [ Koehn et al. , , 2007 ] and segment each word into subwords using Byte-Pair Encoding ( BPE ) [ Sennrich et al. , , 2015 ] , resulting in a 32k vocabulary shared by source and target languages .", "Comments": [], "label": [[48, 53, "Software-Entity"], [122, 148, "Software-Entity"]]}
{"id": 160526, "text": "Our model is based on the Transformer [ Vaswani et al. , , 2017 ] architecture , with multi-head positional attention proposed in [ Gu et al. , , 2017 ] .", "Comments": [], "label": [[26, 37, "Software-Entity"]]}
{"id": 160527, "text": "5.1.5 Training and Inference We train the model with 8/1 Nvidia 1080Ti GPUs on the WMT datasets and IWSLT14 dataset respectively , and we utilize the Adam optimizer while following the same settings used in the original Transformer .", "Comments": [], "label": [[57, 75, "Hardware-device"]]}
{"id": 160528, "text": "We also report the clock time of inference latency on a single Nvidia 1080Ti GPU in our experiments , where we set the batch size to 1 and calculate the average per sentence translation time on newstest2014 for the WMT14 En-De task to keep consistence with previous works .", "Comments": [], "label": [[56, 62, "Device-Count"], [63, 80, "Hardware-device"]]}
{"id": 160529, "text": "Our implementation is based on fairseq [ Ott et al. , , 2019 ] and is avaliable at [ https : //github.com/ ] ( https : //github.com/lemmonation/jm-nat ) [ lemmonation/jm-nat ] ( https : //github.com/lemmonation/jm-nat ) .", "Comments": [], "label": [[31, 38, "Software-Entity"]]}
{"id": 160530, "text": "The adaptation takes about 5 hours to complete on one single NVIDIA P100 GPU . 3.4 Adversarial Training The post-training procedure injects target domain knowledge and brings domain-awareness to BERT .", "Comments": [], "label": [[54, 60, "Device-Count"], [61, 76, "Hardware-device"]]}
{"id": 160531, "text": "The figure shows t-SNE visualization of the BERT 's hidden state for the B → E task .", "Comments": [], "label": [[17, 22, "Software-Entity"]]}
{"id": 160532, "text": "As shown in Figure [ 1 , ] the graphs are obtained by applying t-SNE on the set of all representation of source and target data points .", "Comments": [], "label": [[63, 68, "Software-Entity"]]}
{"id": 160533, "text": "Every sample is mapped into a 768-dimensional feature space through BERT and projected back into a two-dimensional plane by the t-SNE .", "Comments": [], "label": [[128, 133, "Software-Entity"]]}
{"id": 160534, "text": "We conducted experiments based on the Neutron implementation [ Xu and Liu , 2019 ] of the Transformer translation model .", "Comments": [], "label": [[38, 60, "Software-Entity"]]}
{"id": 160535, "text": "Our experiments run on 2 GTX [ https : //github.com/tensorflow/ ] ( https : //github.com/tensorflow/tensor2tensor/blob/v1.6.5/tensor2tensor/layers/common_hparams.pyL110-L112 ) [ tensor2tensor/blob/v1.6.5/tensor2tensor/ ] ( https : //github.com/tensorflow/tensor2tensor/blob/v1.6.5/tensor2tensor/layers/common_hparams.pyL110-L112 ) [ layers/common_hparams.pyL110-L112 ] ( https : //github.com/tensorflow/tensor2tensor/blob/v1.6.5/tensor2tensor/layers/common_hparams.pyL110-L112 ) .", "Comments": [], "label": [[23, 24, "Device-Count"], [25, 28, "Hardware-device"]]}
{"id": 160536, "text": "1080 Ti GPUs , and a batch size of 25k target tokens is achieved through gradient accumulation of small batches .", "Comments": [], "label": [[0, 12, "Hardware-device"]]}
{"id": 160537, "text": "Experiments on WMT are conducted on 8 P100 GPUs .", "Comments": [], "label": [[36, 37, "Device-Count"], [38, 47, "Hardware-device"]]}
{"id": 160538, "text": "Following [ Ott et al. , 2018 ] , we accumulate the gradient 8 iterations and then update to simulate a 64-GPU environment with a batch-size of 65K tokens per step .", "Comments": [], "label": [[104, 110, "Device-Count"]]}
{"id": 160539, "text": "For a fair comparison , we implemented all methods on the same Transformer backbone as well as model settings . with the depth of MSC increasing , which is consistent with observation of [ Wang et al. , 2019a ] .", "Comments": [], "label": [[63, 74, "Software-Entity"]]}
{"id": 160540, "text": "For a fair comparison , we implemented all models on the same Transformer backbone as well as model settings .", "Comments": [], "label": [[62, 73, "Software-Entity"]]}
{"id": 160541, "text": "3.2 Implementation Details We implement all models in TensorFlow 1.15 [ Abadi et al. , , 2015 ] based on the original BERT [ Devlin et al. , , 2019 ] and the XLNet [ Yang et al. , , 2019 ] codebases .", "Comments": [], "label": [[54, 64, "Software-Entity"]]}
{"id": 160542, "text": "We perform all experiments on one TPU v3-8 node ( 8 cores , 128GB memory ) with * bfloat16 * format enabled .", "Comments": [], "label": [[30, 33, "Device-Count"], [34, 47, "Hardware-device"], [60, 72, "Device-Memory"]]}
{"id": 160543, "text": "We measure the FLOPs and memory consumption through the TensorFlow Profiler [ 4 ] .", "Comments": [], "label": [[56, 66, "Software-Entity"]]}
{"id": 160544, "text": "For DeFormer models , we tune the hyperparameters for weighting different losses using bayesian optimizaiton libray [ Nogueira , Fernando , 2019 ] with 50 iterations on the tune split ( 10 % of the original training sets ) and report the performance numbers on the original dev sets .", "Comments": [], "label": [[87, 108, "Software-Entity"]]}
{"id": 160545, "text": "We used default hyperparameters provided by the open-source AllenNLP framework [ Gardner et al. , , 2018 ] .", "Comments": [], "label": [[60, 68, "Software-Entity"]]}
{"id": 160546, "text": "Experimental cost Experiments were run for 20 GPU hours on Quadro RTX 8000 .", "Comments": [], "label": [[59, 74, "Hardware-device"]]}
{"id": 160547, "text": "Pretraining details Each model is a Transformer [ Vaswani et al. , , 2017 ] with 8 layers , 12 heads and GELU activiation functions [ Hendrycks and Gim ] [ pel , 2016 ] .", "Comments": [], "label": [[36, 47, "Software-Entity"]]}
{"id": 160548, "text": "For each model , we train it with 8 NVIDIA V100 GPUs with 32GB of memory and mixed precision .", "Comments": [], "label": [[34, 35, "Device-Count"], [36, 52, "Hardware-device"], [58, 72, "Device-Memory"]]}
{"id": 160549, "text": "As a baseline , we use the first 200k embeddings of fastText [ Bojanowski et al. , , 2017 ] and learn the mapping using the same procedure as [ §5.1 . ] Note we use a subset of 200k vocabulary of fastText , the same as BERT , to get a comparable number .", "Comments": [], "label": [[52, 60, "Software-Entity"], [196, 204, "Software-Entity"]]}
{"id": 160550, "text": "Figure [ 4b ] shows embeddings matrix with the average of all contextual embeddings of each word can also be aligned to obtain a decent quality bilingual dictionary , although underperforming fastText .", "Comments": [], "label": [[192, 200, "Software-Entity"]]}
{"id": 160551, "text": "All experiments were run on a single NVIDIA 1080Ti .", "Comments": [], "label": [[30, 36, "Device-Count"], [37, 50, "Hardware-device"]]}
{"id": 160552, "text": "ELMo , Flair and BERTLARGE refer to the pre-trained language models .", "Comments": [], "label": [[7, 12, "Software-Entity"]]}
{"id": 160553, "text": "For example , of coherence , score 2 means the response with good coherence without illogical expression and continues the dialogue history reasonably ; score 1 means the result is acceptable but with a slight flaw ; score 0 means the statement of result illogically or the result improper to the dialog context . 3.4 Implement Detail We implement our model over Tensorflow framework [ Abadi et al. , , 2016 ] .", "Comments": [], "label": [[363, 373, "Software-Entity"]]}
{"id": 160554, "text": "We train our model using Adagrad [ Duchi et al. , 2011 ] optimizer with a mini-batch size of 128 and learning rate 0.1 at most 130k iterations ( 70k iterations on Wizard-of-Wikipedia ) on a GPU-P100 machine .", "Comments": [], "label": [[190, 198, "Hardware-device"]]}
{"id": 160555, "text": "We make the following contributions : 2 Previous work 2.1 Contextual Language Models From non-contextual to contextual word embeddings The first neural word vector representations were non-contextualized word embeddings , most notably word2vec [ Mikolov et al. , , 2013 ] , GloVe [ Pennington et al. , , 2014 ] and fastText [ Mikolov et al. , , 2018 ] , which were designed to be used as input to task-specific neural architectures .", "Comments": [], "label": [[315, 323, "Software-Entity"]]}
{"id": 160556, "text": "Contextualized word representations such as ELMo [ Peters et al. , , 2018 ] and flair [ Akbik et al. , , 2018 ] , improved the representational power of word embeddings by taking context into account .", "Comments": [], "label": [[80, 85, "Software-Entity"]]}
{"id": 160557, "text": "It follows the same approach as [ Grave et al. , , 2018 ] by using a language classification model based on the fastText linear classifier [ Grave et al. , , 2017 ] [ Joulin et al. , 2016 ] pretrained on Wikipedia , Tatoeba and SETimes , which supports 176 languages .", "Comments": [], "label": [[112, 120, "Software-Entity"]]}
{"id": 160558, "text": "We use a non-shuffled version of the French data , which amounts to 138GB of raw text and 32.7B tokens after subword tokenization . 4.2 Pre-processing We segment the input text data into subword units using SentencePiece [ Kudo and Richardson , 2018 ] .", "Comments": [], "label": [[207, 220, "Software-Entity"]]}
{"id": 160559, "text": "SentencePiece is an extension of Byte-Pair encoding ( BPE ) [ Sennrich et al. , , 2016 ] and WordPiece [ Kudo , 2018 ] that does not require pre-tokenization ( at the word or token level ) , thus removing the need for language-specific tokenisers .", "Comments": [], "label": [[0, 13, "Software-Entity"], [33, 59, "Software-Entity"], [93, 102, "Software-Entity"]]}
{"id": 160560, "text": "CamemBERT is very similar to RoBERTa , the main difference being the use of whole-word masking and the usage of SentencePiece tokenization [ Kudo and Richardson , 2018 ] instead of WordPiece [ Schuster and Nakajima , 2012 ] .", "Comments": [], "label": [[112, 125, "Software-Entity"]]}
{"id": 160561, "text": "Since we use SentencePiece to tokenize our corpus , the input tokens to the model are a mix of whole words and subwords .", "Comments": [], "label": [[13, 26, "Software-Entity"]]}
{"id": 160562, "text": "Pretraining We use the RoBERTa implementation in the fairseq library [ Ott et al. , , 2019 ] .", "Comments": [], "label": [[53, 60, "Software-Entity"]]}
{"id": 160563, "text": "Unless otherwise specified , our models use the BASE architecture , and are pretrained for 100k backpropagation steps on 256 Nvidia V100 GPUs ( 32GB each ) for a day .", "Comments": [], "label": [[121, 124, "Device-Count"], [125, 141, "Hardware-device"], [144, 148, "Device-Memory"]]}
{"id": 160564, "text": "The POS tagging , dependency parsing , and NER experiments are run using Hugging Face 's Transformer library extended to support CamemBERT and dependency parsing [ Wolf et al. , , 2019 ] .", "Comments": [], "label": [[73, 88, "Software-Entity"]]}
{"id": 160565, "text": "The NLI experiments use the fairseq library following the RoBERTa implementation .", "Comments": [], "label": [[28, 35, "Software-Entity"]]}
{"id": 160566, "text": "In that spirit , we make the models used in our experiments available via our website and via the huggingface and fairseq APIs , in addition to the base CamemBERT model .", "Comments": [], "label": [[98, 109, "Software-Entity"], [114, 121, "Software-Entity"]]}
{"id": 160567, "text": "In this paper , we find that this trend holds in our multilingual settings , although our results show lower overall numbers to those reported in [ Gillick et al. , 2016 ] . [ 4 ] 3.4 Hyperparameters All experiments are run on GeForce RTX 2080 Ti GPUs , using Tensorflow [ Abadi et al. , , 2016 ] .", "Comments": [], "label": [[227, 251, "Hardware-device"], [260, 270, "Software-Entity"]]}
{"id": 160568, "text": "The TensorFlow implementation of the representation Figure 6 : The web interface for annotators to label action phrase spans in an ANDROIDHOWTO instruction . is available on Github [ 6 ] .", "Comments": [], "label": [[4, 14, "Software-Entity"]]}
{"id": 160569, "text": "All the models were trained for 1 million steps with a batch size of 128 on a single Tesla V100 GPU , which took 28 to 30 hours .", "Comments": [], "label": [[78, 84, "Device-Count"], [85, 99, "Hardware-device"]]}
{"id": 160570, "text": "4.2 Implementation We use Transformer as the underlying architecture for the translation and captioning modules .", "Comments": [], "label": [[26, 37, "Software-Entity"]]}
{"id": 160571, "text": "We use 4 Titan Xp GPUs with 1,000 tokens in each mini-batch for training .", "Comments": [], "label": [[7, 8, "Device-Count"], [9, 22, "Hardware-device"]]}
{"id": 160572, "text": "Evaluation and Model selection For evaluation , we report BLEU scores by multi-bleu.pl [ 1 ] in Moses and METEOR [ 2 ] scorea on the Multi30K testing set .", "Comments": [], "label": [[96, 101, "Software-Entity"]]}
{"id": 160573, "text": "The components of our method are described in the rest of this section . 3.1 Indexing and initial candidates retrieval We index the FAQ pairs using the ElasticSearch [ 4 ] search engine .", "Comments": [], "label": [[152, 165, "Software-Entity"]]}
{"id": 160574, "text": "To measure the runtime , we use an Intel ( R ) Core ( TM ) i7-6850K CPU ( 3.60 GHz ) and the TensorFlow 1.12.0 library with Python 3.6.8 on Ubuntu 16.04.06 LTS .", "Comments": [], "label": [[35, 84, "Hardware-device"], [93, 103, "Software-Entity"]]}
{"id": 160575, "text": "We use the efficient spatial pyramid network ( ESPNet ) toolkit [ Watanabe et al. , , 2018 ] for this implementation .", "Comments": [], "label": [[11, 44, "Software-Entity"], [47, 54, "Software-Entity"]]}
{"id": 160576, "text": "B.2 Runtimes on a GPU Additionally , we similarly measure the runtimes in a GPU-augmented environment ( using GeForce GTX 1080 Ti ) .", "Comments": [], "label": [[110, 129, "Software-Entity"]]}
{"id": 160577, "text": "3.1 Data SNLI : The Stanford NLI dataset [ Bowman et al. , , 2015 ] contains samples of premise and hypothesis pairs with human annotations , using Amazon Mechanical Turk .", "Comments": [], "label": [[148, 170, "Cloud-Platform"]]}
{"id": 160578, "text": "The explanations were crowdsourced using Amazon Mechanical Turk .", "Comments": [], "label": [[41, 63, "Cloud-Platform"]]}
{"id": 160579, "text": "In this work , we leverage the implementation of transformer architectures and pre-trained models provided by [ Wolf et al. , 2019 ] .", "Comments": [], "label": [[49, 60, "Software-Entity"]]}
{"id": 160580, "text": "We would also like to thank HuggingFace for providing a state-of-the-art Transformers library for natural language understanding .", "Comments": [], "label": [[28, 39, "Software-Entity"]]}
{"id": 160581, "text": "We ran our experiments on GeForce GTX 1080 Ti GPUs .", "Comments": [], "label": [[26, 50, "Hardware-device"]]}
{"id": 160582, "text": "We adjust the batch size to be the largest multiple of 16 to fit on the GPU memory ( ∼12GB ) .", "Comments": [], "label": [[86, 90, "Device-Memory"]]}
{"id": 160583, "text": "The backbone of our infrastructure is inspired by the transformer blocks in [ Dong et al. , , 2019 ] , which supports both bi-directional encoding and uni-directional decoding flexibly via specific selfattention masks .", "Comments": [], "label": [[54, 72, "Software-Entity"]]}
{"id": 160584, "text": "The pretraining of dialogue generation was carried out on 8 Nvidia Telsa V100 32G GPU cards for 3.5M steps , taking about two weeks to reach convergence .", "Comments": [], "label": [[58, 59, "Device-Count"], [60, 77, "Hardware-device"], [78, 85, "Device-Memory"]]}
{"id": 160585, "text": "To model this one-to-many relationship , CVAE [ Zhao et al. , , 2017 ] employs Gaussian distri- Given a dialogue context and background knowledge , our model is able to generate K diverse responses .", "Comments": [], "label": [[41, 45, "Software-Entity"]]}
{"id": 160586, "text": "[ 13 ] Our code is based on HuggingFace 's implementation of GPT2-small ( 117M parameters ) .", "Comments": [], "label": [[28, 42, "Software-Entity"]]}
{"id": 160587, "text": "We trained on EC2 P3.8x machines which had 4 NVidia Tesla v100 GPUs each .", "Comments": [], "label": [[14, 17, "Cloud-Platform"], [42, 44, "Device-Count"], [45, 62, "Hardware-device"]]}
{"id": 160590, "text": "Each model was trained for 30 epochs using 4 NVIDIA Tesla V100 GPUs , each with 16 GB memory , which lasted two weeks .", "Comments": [], "label": [[43, 44, "Device-Count"], [45, 62, "Hardware-device"], [80, 85, "Device-Memory"]]}
{"id": 160592, "text": "In practice , the proposed framework can process 26 and 11 sentences per second in PTB and CTB 5.1 test sets respectively , by a single NVIDIA RTX GPU .", "Comments": [], "label": [[136, 146, "Hardware-device"]]}
{"id": 160595, "text": "When choosing the pre-train models [ Wolf et al. , , 2020 ] , `` bert-largecased '' is utilized for English with a single RTX 3090 , `` bert-base-chinese '' is utilized for Chinese with a single RTX 1080TI .", "Comments": [], "label": [[115, 121, "Device-Count"], [122, 130, "Hardware-device"], [188, 194, "Device-Count"], [195, 205, "Hardware-device"]]}
{"id": 160596, "text": "The average processing speed in PTB test set is 26 sentences per second with a single RTX 3090 , and the one in CTB 5.1 test set is 11 sentences per second with a single RTX 1080TI ( or 20 sentences per second with single RTX 3090 ) .", "Comments": [], "label": [[79, 85, "Device-Count"], [86, 94, "Hardware-device"], [163, 169, "Device-Count"], [170, 180, "Hardware-device"], [215, 221, "Device-Count"], [222, 230, "Hardware-device"]]}
{"id": 160597, "text": "We conducted each experiment on a single NVIDIA GeForce RTX 3090 24GB .", "Comments": [], "label": [[34, 40, "Device-Count"], [41, 64, "Hardware-device"], [65, 69, "Device-Memory"]]}
{"id": 160599, "text": "D Main Experimental Environments and Other Parameters Settings D.1 Experimental Environments We deploy all models on a server with 250GB of memory and 4 TITAN Xp GPUs .", "Comments": [], "label": [[131, 136, "Device-Memory"], [150, 152, "Device-Count"], [153, 161, "Hardware-device"]]}
{"id": 160600, "text": "Specifically , the configuration environment of the server is ubuntu 16.04 , and our framework mainly depends on python 3.6.0 and PyTorch 1.0 .", "Comments": [], "label": [[130, 137, "Software-Entity"]]}
{"id": 160601, "text": "BART HEPOS indicates encoder variants with head-wise positional strides ( HEPOS ) encoder-decoder attention . 4.4 Implementation Details We fit all models into a single RTX A6000 GPU with a 48 GiB memory .", "Comments": [], "label": [[162, 168, "Device-Count"], [169, 182, "Hardware-device"], [189, 196, "Device-Memory"]]}
{"id": 160602, "text": "We adopt the fairseq [ 3 ] implementation for BART .", "Comments": [], "label": [[13, 20, "Software-Entity"]]}
{"id": 160603, "text": "A Hyperparameters for Authorship Attribution Model B Hyperparameters for User Embedding Model C Running Times Authorship attribution models are trained on an NVIDIA GeForce RTX-2080Ti GPU and take 2.5 hours for K = 100 anchors and 4 hours for K = 10 , 000 anchors .", "Comments": [], "label": [[158, 187, "Hardware-device"]]}
{"id": 160604, "text": "Training a new language model for weighted fine-tuning as described in Section [ 6.1 ] takes about 2.5 hours to train a model on a dataset on an NVIDIA Tesla V100 GPU .", "Comments": [], "label": [[145, 166, "Hardware-device"]]}
{"id": 160605, "text": "The user embedding models are trained on an NVIDIA GeForce RTX-2080Ti GPU .", "Comments": [], "label": [[44, 73, "Hardware-device"]]}
{"id": 160606, "text": "The model is trained with a batch size of 2,560 on 32x Tesla V100 GPUs .", "Comments": [], "label": [[51, 54, "Device-Count"], [55, 70, "Hardware-device"]]}
{"id": 160607, "text": "Both models are trained for 100k steps on 32 GPUs of NVIDIA V100 Tensor Core with a bach size of 25,000 .", "Comments": [], "label": [[42, 49, "Device-Count"], [53, 64, "Hardware-device"]]}
{"id": 160608, "text": "Specifically , each configuration is inferred using a data center GPU NVIDIA V100 Tensor Core , and the GPU is fully occupied by one model .", "Comments": [], "label": [[66, 81, "Hardware-device"]]}
{"id": 160609, "text": "We implement the presented approach in Py-Torch and fine-tune the downstream tasks on multiple Nvidia Tesla V100 GPUs .", "Comments": [], "label": [[39, 47, "Software-Entity"], [95, 118, "Hardware-device"]]}
{"id": 160610, "text": "The basic architecture of PLMs and pre-trained parameters are provided by Huggingface [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[74, 85, "Software-Entity"]]}
{"id": 160611, "text": "This is because MWA must calculate aligned attention weights token by token , and it can not benefit from CUDNN parallelization , resulting in terrible operating efficiency .", "Comments": [], "label": [[106, 111, "Software-Entity"]]}
{"id": 160612, "text": "Therefore , HLG could be maximally accelerated through the optimized matrix operation of CUDNN primitive , which only produces a negligible impact on time efficiency .", "Comments": [], "label": [[89, 94, "Software-Entity"]]}
{"id": 160613, "text": "As shown in Table [ 1 , ] we compare both translation performance and speed between a pre-trained NMT model [ Ng et al. , , 2019 ] with 270M parameters and the adaptive kNN-MT [ Zheng et al. , , 2021a ] system originated from the former on the same hardware ( a P100-16GB GPU with 18 cores Intel Xeon Gold 6240 CPU @ 2.60GHz ) , where the later is the most advanced Equal contribution .", "Comments": [], "label": [[262, 275, "Hardware-device"], [281, 289, "Device-Memory"], [290, 314, "Hardware-device"], [317, 324, "Device-Memory"]]}
{"id": 160614, "text": "In our experiments , We adopted the scikit-learn clustering implements . [ 4 ] 4.3 Baselines We adopted the following models as our baselines . • Base NMT .", "Comments": [], "label": [[36, 48, "Software-Entity"]]}
{"id": 160615, "text": "4.4 Evaluation All experiments were conducted on a P100-16GB GPU with 18 cores Intel ( R ) Xeon ( R ) Gold 6240 CPU @ 2.60GHz except for the experiments in [ https : //github.com/facebookresearch/faiss/ ] [ https : //scikit-learn.org/stable/modules/clustering.html ] [ http : //www.statmt.org/wmt19/ ] Table 4 : The translation BLEU comparison in different domains .", "Comments": [], "label": [[51, 55, "Hardware-device"], [56, 64, "Device-Count"], [70, 78, "Device-Memory"], [79, 115, "Hardware-device"], [118, 125, "Device-Memory"]]}
{"id": 160616, "text": "Subsection [ 4.5.2 ] where we used 2 GPU cards to load a larger datastore .", "Comments": [], "label": [[35, 40, "Device-Count"]]}
{"id": 160617, "text": "All translation results were evaluated in case-sensitive detokenized BLEU with SacreBLEU [ Post , 2018 ] .", "Comments": [], "label": [[79, 88, "Software-Entity"]]}
{"id": 160618, "text": "B Hyper-parameters of Faiss We followed the default implementation setting of [ Zheng et al. , , 2021a ] .", "Comments": [], "label": [[22, 27, "Software-Entity"]]}
{"id": 160619, "text": "For computing resources , we use NVIDIA Tesla V100 GPUs with 32 GB memory for all experiments , and utilize the mixed-precision ( FP16 ) to improve the computational efficiency .", "Comments": [], "label": [[33, 55, "Hardware-device"], [61, 66, "Device-Memory"]]}
{"id": 160620, "text": "Our model takes around 4-5 hours for training , and 30 minutes for decoding on V100 GPUs .", "Comments": [], "label": [[79, 88, "Hardware-device"]]}
{"id": 160621, "text": "We use NLTK [ 2 ] to implement BLEU and METEOR , and the ROUGE_SCORE package [ 3 ] to implement ROUGE .", "Comments": [], "label": [[7, 11, "Software-Entity"]]}
{"id": 160622, "text": "For WMT14 , we reported Tokenized BLEU and SacreBLEU results , arranged on the left and right . `` † '' indicates that the results of the model are our implementations .", "Comments": [], "label": [[43, 52, "Software-Entity"]]}
{"id": 160623, "text": "Especially , compared with Reformer and Routing Transformer , the Tokenized BLEU score of Cluster-Former respectively has 4.2 % and 12.8 % improvement and the sacreBLEU score respectively has 4.3 % and 12.3 % improvement . 4.2 Text Classification We validate our model on five text classification tasks .", "Comments": [], "label": [[159, 168, "Software-Entity"]]}
{"id": 160624, "text": "We test our model on the 20NEWS dataset of text classification tasks with a NVIDIA V100 ( 16GB ) GPU .", "Comments": [], "label": [[76, 87, "Hardware-device"], [90, 94, "Device-Memory"]]}
{"id": 160625, "text": "We test the memory and time cost on a NVIDIA V100 GPU .", "Comments": [], "label": [[38, 53, "Hardware-device"]]}
{"id": 160626, "text": "We tested them on text classification of the 20NEWS dataset and trained them on a NVIDIA V100 GPU .", "Comments": [], "label": [[82, 98, "Hardware-device"]]}
{"id": 160627, "text": "A Dataset and Model Details Datasets Statistics of the datasets are presented in Table [ 5 . ] Table 5 : Data Statistics Models All models are implemented based on the PyTorch [ 1 ] library .", "Comments": [], "label": [[168, 176, "Software-Entity"]]}
{"id": 160628, "text": "All experiments are conducted on NVIDIA GeForce GTX 1080 Ti GPUs .", "Comments": [], "label": [[33, 64, "Hardware-device"]]}
{"id": 160629, "text": "B Interpretation Methods Details For VaGrad , GradInp , VaPGD , PGDInp , and Ing-Grad , we use the automatic differentiation mechanism of PyTorch .", "Comments": [], "label": [[138, 145, "Software-Entity"]]}
{"id": 160630, "text": "For DeepLIFT , we use the implementation in Captum [ 2 ] .", "Comments": [], "label": [[44, 50, "Software-Entity"]]}
{"id": 160631, "text": "For Certify , we modify the code in auto_LiRPA [ 3 ] .", "Comments": [], "label": [[36, 46, "Software-Entity"]]}
{"id": 160632, "text": "For all the three translation tasks , we train our models on Tesla V-100 GPU where we batch sentence pairs of similar lengths together containing roughly 32000 tokens .", "Comments": [], "label": [[61, 76, "Hardware-device"]]}
{"id": 160633, "text": "When we benchmark POKI in an Nvidia 2080Ti GPU , in Table [ 6 , ] we see that knowledge generation ( or retrieval ) could be a computational bottleneck for POKI .", "Comments": [], "label": [[29, 46, "Hardware-device"]]}
{"id": 160634, "text": "For diversity calculation ( in automatic evaluation ) , we use NLTK [ 11 ] to extract n-grams .", "Comments": [], "label": [[63, 67, "Software-Entity"]]}
{"id": 160635, "text": "For each sampled documentrevision R , we extract its full edit actions using * latexdiff * . [ 7 ] We provide both the paragraph-level and sentence-level revisions where the latter is constructed by applying a sentence segmentation tool , [ 8 ] and aligning each sentence to each revision .", "Comments": [], "label": [[79, 88, "Software-Entity"], [210, 236, "Software-Entity"]]}
{"id": 160636, "text": "In [ §4.2.2 , ] we then use Amazon Mechanical Turk ( AMT ) to crowdsource edit intention annotations for each edit action according to our proposed editintention taxonomy ( [ §4.2.1 ] .", "Comments": [], "label": [[28, 50, "Cloud-Platform"]]}
{"id": 160637, "text": "D Details on Computational Experiments For all computational experiments in this work , we deploy them on a single Quadro RTX 4000 ( 16GB ) GPU .", "Comments": [], "label": [[108, 114, "Device-Count"], [115, 130, "Hardware-device"], [133, 137, "Device-Memory"]]}
{"id": 160638, "text": "We leverage the RoBERTa-large model from Huggingface transformers [ Wolf et al. , , 2020 ] , which has 354 million parameters .", "Comments": [], "label": [[41, 52, "Software-Entity"]]}
{"id": 160639, "text": "We use the sklearn package [ Pedregosa et al. , , 2011 ] to calculate the precision , recall and f1 score .", "Comments": [], "label": [[11, 18, "Software-Entity"]]}
{"id": 160640, "text": "We leverage the BART-large ( with 400 million parameters ) and PEGASUS-large ( with 568 million parameters ) from Huggingface transformers [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[114, 125, "Software-Entity"]]}
{"id": 160641, "text": "We use the metrics package from Huggingface transformers to calculate the SARI , BLEU , ROUGE-1/2/L score .", "Comments": [], "label": [[32, 43, "Software-Entity"]]}
{"id": 160642, "text": "Among them are Deep Averaging Networks ( DAN ) [ Iyyer et al. , 2015 ] , a deep Multi-Layer Perceptron ( MLP ) model with n layers that relies on averaging the BoW , Simple Word Embedding Models ( SWEM ) [ Shen et al. , , 2018 ] that explores different pooling strategies for pretrained word embeddings , and fastText [ Bojanowski et al. , , 2017 ] , which uses a linear layer on top of pretrained word embed- dings .", "Comments": [], "label": [[309, 317, "Software-Entity"]]}
{"id": 160643, "text": "In fastText [ Bojanowski et al. , , 2017 ] [ Joulin et al. , 2017 ] a linear layer on top of pretrained embeddings is used for classification .", "Comments": [], "label": [[3, 11, "Software-Entity"]]}
{"id": 160644, "text": "We consider fastText , SWEM , and a DAN-like deeper MLP in our comparison .", "Comments": [], "label": [[12, 20, "Software-Entity"]]}
{"id": 160645, "text": "Note that those approaches that rely on logistic regression on top of pretrained word embeddings , e. g. , fastText , share a similar architecture as an MLP with one hidden layer .", "Comments": [], "label": [[107, 116, "Software-Entity"]]}
{"id": 160646, "text": "The authors report that their approach outperforms existing models like fastText on two of the five English datasets and both Chinese datasets .", "Comments": [], "label": [[72, 80, "Software-Entity"]]}
{"id": 160647, "text": "We list the numbers for fastText , SWEM , and logistic regression from [ Ding et al. , 2020 ] in our comparison .", "Comments": [], "label": [[24, 32, "Software-Entity"]]}
{"id": 160648, "text": "We further observe that both pure BoW and TF-IDF weighted BoW lead to better results than approaches that exploit pretrained word embeddings such as GloVe-MLP , fastText , and SWEM .", "Comments": [], "label": [[161, 169, "Software-Entity"]]}
{"id": 160649, "text": "Runtime Performance of the Models We provide the * total * running times in Table [ 5 ] as observed while conducting the experiments on a single NVIDIA A100-SXM4-40GB card .", "Comments": [], "label": [[138, 144, "Device-Count"], [145, 161, "Hardware-device"], [162, 166, "Device-Memory"]]}
{"id": 160650, "text": "We train Transformer-base models [ Vaswani et al. , , 2017 ] using Fairseq [ Ott et al. , , 2019 ] .", "Comments": [], "label": [[67, 74, "Software-Entity"]]}
{"id": 160651, "text": "Afterwards , we compute SacreBLEU scores on the 'devtest ' set [ Post , 2018 ] , using beam search ( beam size = 5 ) , yielding scores of 20.6±.4 , 24.4±.3 and 25.8±.1 for the small , medium and full datasets , respectively .", "Comments": [], "label": [[24, 33, "Software-Entity"]]}
{"id": 160652, "text": "Appendix A Semi-natural templates The semi-natural data that we use in our test sets is generated with the library DiscoDOP , [ 12 ] developed for data-oriented parsing [ Van Cranenburgh et al. , , 2016 ] .", "Comments": [], "label": [[115, 123, "Software-Entity"]]}
{"id": 160653, "text": "Generate a treebank using the disco-dop library and the discodop parser en ptb command .", "Comments": [], "label": [[30, 39, "Hardware-device"]]}
{"id": 160654, "text": "Compute tree fragments from the resulting treebank ( discodop fragments ) .", "Comments": [], "label": [[53, 61, "Software-Entity"]]}
{"id": 160655, "text": "Extract sentences matching the eight fragments ( discodop treesearch ) .", "Comments": [], "label": [[49, 57, "Software-Entity"]]}
{"id": 160656, "text": "Preprocessing We tokenise the data using the tokenisation script [ 13 ] from the SMT library Moses . [ 14 ] Following the number of subwords suggested by [ Tiedemann , 2020 ] , we generate a subword vocabulary applying 60k BPE merge-operations .", "Comments": [], "label": [[93, 98, "Software-Entity"]]}
{"id": 160657, "text": "To compute BLEU scores , we tokenised the data with the Moses tokenisation script mentioned above , and then used the commandline script fairseq-generate to compute scores .", "Comments": [], "label": [[56, 61, "Software-Entity"], [137, 144, "Software-Entity"]]}
{"id": 160658, "text": "E.2 Architecture and training As reported in the main text , we focus on English-Dutch translation , and all our models are Transformerbase models , as implemented in Fairseq [ Ott et al. , , 2019 ] .", "Comments": [], "label": [[167, 174, "Software-Entity"]]}
{"id": 160659, "text": "Any other hyperparameters involved follow the Fairseq default .", "Comments": [], "label": [[46, 53, "Software-Entity"]]}
{"id": 160660, "text": "We provide the BLEU scores per model seed in Table [ 9 . ] E.3 Compute All experiments were ran using Tesla V100 GPUs on an internal SLURM-based cluster .", "Comments": [], "label": [[102, 117, "Hardware-device"]]}
{"id": 160661, "text": "Training a transformer-base model on our small , medium and full dataset takes on average 3.5 , 17 and 113 minutes per epoch , respectively ( numbers are rounded ) on 32 GPUs .", "Comments": [], "label": [[167, 174, "Device-Count"]]}
{"id": 160662, "text": "This makes the total training time for these models , which are trained for around 160 , 60 and 30 epochs , 10 , 17 and 56 hours , respectively ( again , spread over 32 GPUs ) .", "Comments": [], "label": [[166, 173, "Device-Count"]]}
{"id": 160663, "text": "Besides , the vocabulary size of both datasets is the same as BERT [ Devlin et al. , , 2018 ] setting . 4.2 Implementation Details Our experiments are implemented in Tensorflow [ Abadi et al. , , 2016 ] on an NVIDIA Tesla P100 GPU .", "Comments": [], "label": [[166, 176, "Software-Entity"], [209, 231, "Hardware-device"]]}
{"id": 160664, "text": "Our implementation is based on the Huggingface Library [ Wolf et al. , 2019a ] .", "Comments": [], "label": [[35, 46, "Software-Entity"]]}
{"id": 160665, "text": "To quantify the importance of different pre-training data , we pre-train The latency of each model is measured on a single Nvidia V100 GPU with a batch size of 1 .", "Comments": [], "label": [[116, 122, "Device-Count"], [123, 138, "Hardware-device"]]}
{"id": 160666, "text": "We train each model with 8 NVIDIA 32GB V100 GPUs and use total batch size 2048 with gradient accumulation strategy .", "Comments": [], "label": [[25, 26, "Device-Count"], [27, 48, "Hardware-device"]]}
{"id": 160667, "text": "We use 32 NVIDIA V100 GPUs for pretraining and 8 GPUs for adaptation .", "Comments": [], "label": [[7, 9, "Device-Count"], [10, 26, "Hardware-device"], [47, 53, "Device-Count"]]}
{"id": 160668, "text": "Experiments with generative settings are conducted on a V100 GPU . 4.2 Comparison to state-of-the-art Methods In this section , we compare our model with previous state-of-the-art methods .", "Comments": [], "label": [[56, 64, "Hardware-device"]]}
{"id": 160669, "text": "Specifically , given a math problem consisting of a textual description d and several formulas { f1 , f2 , · · · , fm } , we first utilize the open-source toolkit TangentS [ 1 ] to convert each formula into an https : //github.com/BehroozMansouri/TangentCFT OPT , and Stanza [ 2 ] to convert the textual description into a syntax dependency tree .", "Comments": [], "label": [[163, 171, "Software-Entity"], [268, 274, "Software-Entity"]]}
{"id": 160670, "text": "For all PLM-related models , we implement them based on HuggingFace Transformers [ 4 ] [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[56, 67, "Software-Entity"]]}
{"id": 160671, "text": "It costs about 40 hours to perform the continual pre-training on 4 Tesla-V100-PCIE-32G GPUs .", "Comments": [], "label": [[65, 66, "Device-Count"], [67, 82, "Hardware-device"], [83, 86, "Device-Memory"]]}
{"id": 160672, "text": "Technical details We train all our experiments on one RTX8000 ( 48GB ) or RTX3090 ( 24GB ) GPUs .", "Comments": [], "label": [[50, 53, "Device-Count"], [54, 61, "Hardware-device"], [64, 68, "Device-Memory"], [74, 81, "Hardware-device"], [84, 88, "Device-Memory"]]}
{"id": 160673, "text": "Our PReasM-Base and PReasM-Large models training time was 5-6 and 8-9 days on one RTX8000 GPU , respectively .", "Comments": [], "label": [[78, 81, "Device-Count"], [82, 93, "Hardware-device"]]}
{"id": 160674, "text": "The whole pre-training procedure takes about 1.7 days on 64 Nvidia A100 GPU cards .", "Comments": [], "label": [[57, 60, "Device-Count"], [60, 75, "Hardware-device"]]}
{"id": 160675, "text": "Results were obtained on an Intel i9-7920X machine with an NVIDIA GTX 2080Ti .", "Comments": [], "label": [[28, 43, "Hardware-device"], [59, 77, "Hardware-device"]]}
{"id": 160676, "text": "Models are trained for 0.5M steps on a machine with 4 2080Ti GPUs .", "Comments": [], "label": [[52, 53, "Device-Count"], [54, 65, "Hardware-device"]]}
{"id": 160677, "text": "For the simultaneous scenario ( IWSLT 2020 simultaneous text- to-text ) , it is truecased and tokenized using Moses .", "Comments": [], "label": [[110, 115, "Software-Entity"]]}
{"id": 160678, "text": "A popular approach for reducing such variance is model ensemble , where the weights or predictions of a set of models are aggregated to produce the Work was done during an internship at Microsoft Azure AI .", "Comments": [], "label": [[186, 201, "Cloud-Platform"]]}
{"id": 160679, "text": "Our implementation is based on the fairseq code-base and follows the training and hyper-parameters settings from [ Ott et al . ] [ 2018 , 2019 ] .", "Comments": [], "label": [[35, 43, "Software-Entity"]]}
{"id": 160680, "text": "Table [ 3 ] shows the BLEU scores on the IWSLT test set and the SacreBLEU scores [ Post , 2018 ] with compound splitting on the WMT test set [ 7 ] .", "Comments": [], "label": [[64, 73, "Software-Entity"]]}
{"id": 160681, "text": "In particular , virtual adversarial https : //wit3.fbk.eu/ http : //data.statmt.org/wmt16/translation-task/ We evaluate the SacreBLEU score on the average of last 10 checkpoints .", "Comments": [], "label": [[124, 133, "Software-Entity"]]}
{"id": 160682, "text": "Table 3 : Test set scores on ensembled Transformer-base on IWSLT tasks ( BLEU ) and WMT tasks ( SacreBLEU ) . `` Single '' denotes single model performance .", "Comments": [], "label": [[96, 105, "Software-Entity"]]}
{"id": 160683, "text": "All experiments are conducted on Nvidia V100 GPUs .", "Comments": [], "label": [[33, 50, "Hardware-device"]]}
{"id": 160684, "text": "All experiments are conducted on Nvidia V100 GPUs .", "Comments": [], "label": [[33, 49, "Hardware-device"]]}
{"id": 160685, "text": "For evaluation on WMT datasets , we average the last 10 checkpoints , decode with a beam size of 4 and length penalty of 0.6 , then report the SacreBLEU scores after compound splitting .", "Comments": [], "label": [[143, 152, "Software-Entity"]]}
{"id": 160686, "text": "We run the pretraining on NVIDIA 's PyTorch Docker container 19.02 hosted on a server with two Intel Xeon Platinum 8168 CPUs and 16 NVIDIA Tesla V100 GPUs .", "Comments": [], "label": [[36, 43, "Software-Entity"], [44, 50, "Cloud-Platform"], [91, 94, "Device-Count"], [95, 124, "Hardware-device"], [129, 131, "Device-Count"], [132, 154, "Hardware-device"]]}
{"id": 160687, "text": "We run the fine-tuning on a server with a Intel ( R ) Core ( TM ) i7-6950X CPU and 4 NVIDIA GeForce RTX 3090 GPUs .", "Comments": [], "label": [[42, 78, "Hardware-device"], [83, 84, "Device-Count"], [85, 113, "Hardware-device"]]}
{"id": 160688, "text": "We set the batch size as 96 and train LiLTBASE for 5 epochs on the IIT-CDIP dataset using 4 NVIDIA A40 48GB GPUs .", "Comments": [], "label": [[90, 91, "Device-Count"], [92, 102, "Hardware-device"], [103, 112, "Device-Memory"]]}
{"id": 160689, "text": "Additionally , we remove short lines < 10 characters and those > 5000 characters . [ 6 ] The wiki data is extracted from the 05-21-2020 dump using WikiExtractor [ 7 ] .", "Comments": [], "label": [[147, 160, "Software-Entity"]]}
{"id": 160690, "text": "[ Left ] : The in-batch negative sampling in a single core ; [ Right ] : * Synchronized multi-accelerator negative sampling * using n TPU cores and batch size 8 per core with examples from other cores all treated as negatives .", "Comments": [], "label": [[134, 143, "Hardware-device"]]}
{"id": 160691, "text": "[ 11 ] Our models are trained on Cloud TPU V3 with 32-cores using a global batch size of 4096 with a maximum sequence length of 128 , using the AdamW [ Loshchilov and Hutter , 2019 ] optimizer with initial learning rate 1e-3 , and linear weight decay .", "Comments": [], "label": [[39, 45, "Hardware-device"], [51, 59, "Device-Memory"]]}
{"id": 160692, "text": "For example , a 4096 batch run across 32 cores results in a local batch size of 128 , with each example then only receiving 127 negatives .", "Comments": [], "label": [[38, 46, "Device-Memory"]]}
{"id": 160693, "text": "Pre-training uses TPUv3 with 512-cores and a batch size of 8192 .", "Comments": [], "label": [[18, 23, "Hardware-device"], [29, 38, "Device-Memory"]]}
{"id": 160694, "text": "These in-passage negatives function as hard negative samples in our contrastive learning framework . 3.3 Retrieval For retrieval , we first use FAISS [ Johnson et al. , , 2019 ] to calculate the matching scores between the question and all the contextual sentence indexes .", "Comments": [], "label": [[144, 149, "Software-Entity"]]}
{"id": 160695, "text": "Score Normalization After getting the scores for each contextual sentences to each question by FAISS , we first use a Softmax operation to normalize all these similarity scores into probabilities .", "Comments": [], "label": [[95, 100, "Software-Entity"]]}
{"id": 160696, "text": "We use 8 Tesla V100 GPUs to train the Bi-Encoder with a batch size of 16 on each GPU .", "Comments": [], "label": [[7, 8, "Device-Count"], [9, 24, "Hardware-device"]]}
{"id": 160697, "text": "Under 8 V100 GPUs with a batch size of 16 on each GPU , the validation process could be viewed as a tiny retrieval process for both DPR and DCSR .", "Comments": [], "label": [[6, 7, "Device-Count"], [8, 17, "Hardware-device"]]}
{"id": 160698, "text": "We train each model for 8,000 steps with a batch size of 4 on a Google Cloud virtual machine with one NVIDIA Tesla T4 GPU , using the AdamW optimizer [ Loshchilov and ] [ Hutter , 2019 ] .", "Comments": [], "label": [[64, 92, "Cloud-Platform"], [98, 101, "Device-Count"], [102, 121, "Hardware-device"]]}
{"id": 160699, "text": "SCRL is roughly 4000× faster than HC with T = On a Google Colab Notebook with a Tesla P-100 GPU Figure 4 : Distribution of summary lengths and compression ratios .", "Comments": [], "label": [[51, 72, "Cloud-Platform"], [80, 95, "Hardware-device"]]}
{"id": 160700, "text": "`` ` 8https : //huggingface.co/ distilroberta-base 9https : //huggingface.co/distilgpt2 `` ` Algorithm 1 Stochastic First-Choice Hill Climbing input objective function R ( x , y ) , source sentence x , summary length L , number of steps T , initialization function Init ( x , L ) , sampling function S ( y ) D.2 Tokenization We use the NLTK [ 10 ] Punkt Tokenizer for several purposes in this work : The involved transformer models ( SCRL , Rf luency , Rsim ) internally tokenize sentences based on Byte Pair Encoding .", "Comments": [], "label": [[336, 340, "Software-Entity"]]}
{"id": 160701, "text": "We initialize our models with the pre-trained T5 model , available in the HuggingFace Transformers library1 .", "Comments": [], "label": [[74, 85, "Software-Entity"]]}
{"id": 160702, "text": "We fine-tune the models on each dataset independently using AdamW ( Loshchilov and Hutter , 2019 ) and conducted experiments on 4 NVIDIA-V100-32GB .", "Comments": [], "label": [[128, 129, "Device-Count"], [130, 141, "Hardware-device"], [142, 146, "Device-Memory"]]}
{"id": 160703, "text": "We optimized our models on NVIDIA A40 GPU by AdamW [ Loshchilov and Hutter , 2019 ] with β = 0.9 , β = 0.999 , ϵ = 1e − 8 , and 10 % warmup steps .", "Comments": [], "label": [[27, 41, "Hardware-device"]]}
{"id": 160704, "text": "In this paper , we use the `` generate '' functionality implemented in Huggingface transformers library that leverages heuristic generation rules to reduce repeated words and determine when to stop .", "Comments": [], "label": [[71, 82, "Software-Entity"]]}
{"id": 160705, "text": "We utilise open-source models that are implemented in the Huggingface library to form the sets of F and A .", "Comments": [], "label": [[58, 69, "Software-Entity"]]}
{"id": 160706, "text": "Training is conducted using a single NVIDIA A100 GPU . 5.2 Compared Approaches We consider different configurations for BERT classifiers based on the input representations IB , I or IB+F , and the prompts used P1 , P2 , P3 or P1+P2 described in Section 4.1.1 .", "Comments": [], "label": [[30, 36, "Device-Count"], [37, 52, "Hardware-device"]]}
{"id": 160707, "text": "Each of the LLMs is an open source implementation hosted on the Huggingface , we provide the link to the fine-tuned model .", "Comments": [], "label": [[64, 75, "Software-Entity"]]}
{"id": 160708, "text": "In addition , we reduce the batch size and use gradient accumulation to make sure our experiments run on limited compute hardware ( i.e . a single Nvidia 40 GB A100 GPU ) .", "Comments": [], "label": [[140, 146, "Device-Count"], [147, 153, "Hardware-device"], [154, 159, "Device-Memory"], [160, 168, "Hardware-device"]]}
{"id": 160709, "text": "The total computational budget for this study is about 390 hours on a 40 GB A100 GPU ( 160 fine-tuning runs of roughly 2 hours each , and pretraining runs of roughly 70 hours ) .", "Comments": [], "label": [[70, 75, "Device-Memory"], [76, 84, "Hardware-device"]]}
{"id": 160710, "text": "We perform all experiments using the HuggingFace Transformers library , version 4.24.0 [ Wolf et al. , , 2020 ] . 4 Experimental Setup For each of the languages , we use varying amounts of training data for fine-tuning the multilingual XLS-R model .", "Comments": [], "label": [[37, 48, "Software-Entity"]]}
{"id": 160711, "text": "Our method uses a NeuPSL formulation ; however , we introduce a novel variation to the soft logic formulation , develop theory for unsupervised tasks , introduce the whole system in Tensorflow , and apply it to dialog structure induction .", "Comments": [], "label": [[182, 192, "Software-Entity"]]}
{"id": 160712, "text": "The experiments are run in Google TPU V4 , and the total GPU hours for all finetuning are 326 GPU hours .", "Comments": [], "label": [[27, 40, "Hardware-device"]]}
{"id": 160713, "text": "We use fairseq [ 5 ] [ Ott et al. , , 2019 ] for implementation .", "Comments": [], "label": [[7, 14, "Software-Entity"]]}
{"id": 160714, "text": "All models are trained on 4 Nvidia GeForce RTX 3090 GPUs .", "Comments": [], "label": [[26, 28, "Device-Count"], [28, 56, "Hardware-device"]]}
{"id": 160715, "text": "We evaluate case-sensitive detokenized BLEU on MuST-C tst-COMMON set using sacreBLEU [ 7 ] [ Post , 2018 ] .", "Comments": [], "label": [[75, 84, "Software-Entity"]]}
{"id": 160716, "text": "We use sacreBLEU to test the significance of the results .", "Comments": [], "label": [[7, 16, "Software-Entity"]]}
{"id": 160717, "text": "Baselines We compare our method with the Fairseq ST [ Wang et al. , , 2020a ] baseline implemented with the same framework [ Ott et al. , , 2019 ] .", "Comments": [], "label": [[41, 48, "Software-Entity"]]}
{"id": 160718, "text": "For hyperparameter , see sensitivity analysis in Section [ 5 . ] All reported results are based on first-order meta learning approximation and experiments are performed on multiple NVIDIA A40 GPUs .", "Comments": [], "label": [[181, 196, "Hardware-device"]]}
{"id": 160719, "text": "5.1.5 Hyperparameters and Enviroment The model was trained for 300 epochs using the Adam optimizer with a batch size of 1024 examples across 1 GeForce GTX 1080Ti on each dataset .", "Comments": [], "label": [[141, 142, "Device-Count"], [143, 162, "Hardware-device"]]}
{"id": 160720, "text": "HAHE and all its variants have been trained on a single 11G 1080Ti GPU .", "Comments": [], "label": [[49, 55, "Device-Count"], [56, 59, "Device-Memory"], [60, 70, "Hardware-device"]]}
{"id": 160721, "text": "For Transformer , the implementation by HuggingFace [ 3 ] is utilized , where the hyperparameters follow the default settings in the original Transformer [ Vaswani et al. , , 2017 ] .", "Comments": [], "label": [[40, 51, "Software-Entity"], [142, 153, "Software-Entity"]]}
{"id": 160722, "text": "Our experiments are conducted on the workstation with an Intel Xeon E5 2.40 GHz CPU , 128 GB memory , an NVIDIA A100 GPU , and CentOS 7.2 .", "Comments": [], "label": [[57, 83, "Hardware-device"], [86, 92, "Device-Memory"], [105, 120, "Hardware-device"]]}
{"id": 160723, "text": "All experiments are conducted with 4 Tesla V100-32GB GPUs . 4.2 Comparison with Base LLMs APPS-dev & APPS-test .", "Comments": [], "label": [[35, 36, "Device-Count"], [37, 47, "Hardware-device"], [48, 57, "Device-Memory"]]}
{"id": 160724, "text": "5 Discussion 5.1 Time Cost compared with Post-processing Baseline For the specific issue of time cost , we use * Google Colab * [ 3 ] with a Tesla T4 GPU to build a demo and conduct evaluations over APPS-test dataset .", "Comments": [], "label": [[141, 153, "Hardware-device"]]}
{"id": 160725, "text": "KS is run on a fine-tuned BERT model using 200 samples per instance ( approx . 3.47s per instance on average using a single A100 GPU , more than 150 times slower than one forward inference of the BERT model ) .", "Comments": [], "label": [[117, 123, "Device-Count"], [124, 132, "Hardware-device"]]}
{"id": 160726, "text": "The set of tokens with high explanation scores Work done during full-time work at AWS AI Figure 2 : Illustration of our proposed Amortized Model .", "Comments": [], "label": [[82, 85, "Cloud-Platform"]]}
{"id": 160727, "text": "For example , it takes about 183 seconds to interpret each instance in Yelp-Polarity using the KS Captum implementation [ Kokhlikyan et al. , , 2020 ] on an A100 GPU .", "Comments": [], "label": [[98, 104, "Software-Entity"], [157, 165, "Hardware-device"]]}
{"id": 160728, "text": "On Yelp-Polarity dataset and using A100 GPU , we compare with typical KS running with 200 samples .", "Comments": [], "label": [[35, 43, "Hardware-device"]]}
{"id": 160729, "text": "[ 3 ] We use the publicly available finetuned BERT-base-uncased checkpoints [ 4 ] [ Morris et al. , 2020 ] as the target models to interpret and use the implementation of Captum [ Kokhlikyan et al. , 2020 ] to compute the explanation scores for both KS and SVS .", "Comments": [], "label": [[171, 177, "Software-Entity"]]}
{"id": 160730, "text": "We take more than 2,000 hours on a single A100 GPU for all experiments in this section .", "Comments": [], "label": [[35, 41, "Device-Count"], [42, 50, "Hardware-device"]]}
{"id": 160731, "text": "The inference cost is thus amortized by training on a set of pre-computed For SVS , the recommended number of perturbation samples is 25 in Captum .", "Comments": [], "label": [[140, 146, "Software-Entity"]]}
{"id": 160732, "text": "We find it non-trivial to adapt Fast-SHAP to the text domain .", "Comments": [], "label": [[32, 41, "Software-Entity"]]}
{"id": 160733, "text": "We thank the generous support from AWS AI on computational resources and external collaborations .", "Comments": [], "label": [[35, 38, "Cloud-Platform"]]}
{"id": 160734, "text": "Association for Computational Linguistics . [ and generic model interpretability library for pytorch . ] ( https : //arxiv.org/abs/2009.07896 ) * ArXiv preprint * , abs/2009.07896 .", "Comments": [], "label": [[93, 100, "Software-Entity"]]}
{"id": 160735, "text": "However , this requires us to train the model on a single A100 GPU for 3 days to wait for FastSHAP to converge .", "Comments": [], "label": [[51, 57, "Device-Count"], [58, 66, "Hardware-device"]]}
{"id": 160736, "text": "For implementation , we mainly use Captum [ Kokhlikyan et al. , , 2020 ] and Thermostat [ Feldhus et al. , , 2021 ] .", "Comments": [], "label": [[35, 41, "Software-Entity"]]}
{"id": 160737, "text": "Captum is open-sourced under BSD 3-Clause `` New '' or `` Revised '' License and Thermostat is open-sourced under Apache License 2.0 .", "Comments": [], "label": [[0, 6, "Software-Entity"]]}
{"id": 160738, "text": "All neural models were trained on a single Nvidia A100 GPU .", "Comments": [], "label": [[36, 42, "Device-Count"], [43, 58, "Hardware-device"]]}
{"id": 160739, "text": "If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE , etc . ) ?", "Comments": [], "label": [[182, 186, "Software-Entity"], [189, 194, "Software-Entity"], [197, 202, "Software-Entity"]]}
{"id": 160740, "text": "We annotate content words using the spaCy tokenizer and part-of-speech ( POS ) tagger [ Honnibal et al. , , 2017 ] , defining content words as those with an open class Universal POS tag ( nouns , verbs , adjectives , adverbs , and interjections ; [ Nivre et al. , , 2020 ] and excluding stop words in spaCy .", "Comments": [], "label": [[36, 41, "Software-Entity"], [301, 306, "Software-Entity"]]}
{"id": 160741, "text": "As with our vocabulary drift metric , we annotate POS tags using the spaCy tokenizer and POS tagger . 3.3 Semantic Drift Finally , we consider semantic drift , defined as any divergence in semantic meaning between two text samples .", "Comments": [], "label": [[69, 74, "Software-Entity"]]}
{"id": 160742, "text": "All models are finetuned using one Tesla V100 GPU , taking about two hours per model .", "Comments": [], "label": [[31, 34, "Device-Count"], [35, 49, "Hardware-device"]]}
{"id": 160743, "text": "C Implementation Details The implementation of all models was based on the Transformer library [ 3 ] , in addition , the Pytorch-Lightning [ 4 ] library was used for training control .", "Comments": [], "label": [[75, 94, "Software-Entity"], [121, 138, "Software-Entity"]]}
{"id": 160744, "text": "All models were fine-tuned on an Nvidia Quadro RTX 8000 GPU .", "Comments": [], "label": [[33, 59, "Hardware-device"]]}
{"id": 160745, "text": "We use 6 NVIDIA RTX A6000 , 3 Titan RTX , and 8 GeForce GTX 2080 Ti for our computations .", "Comments": [], "label": [[7, 8, "Device-Count"], [9, 25, "Hardware-device"], [28, 29, "Device-Count"], [30, 39, "Hardware-device"], [46, 47, "Device-Count"], [48, 67, "Hardware-device"]]}
{"id": 160746, "text": "ViLT uses the default ViLT feature extractor and BERT tokenizer , based on the Hugging Face implementation ( Wolf et al. , 2020 ) .", "Comments": [], "label": [[79, 91, "Software-Entity"]]}
{"id": 160747, "text": "4.4 Run-Time Comparison We compare the training time of the five baselines , Doc2EDAG , DE-PPN , PTPCG , GIT , and ReDEE , with ProCNet on a GPU server with NVIDIA Quadro RTX 6000 and the same setting .", "Comments": [], "label": [[157, 179, "Hardware-device"]]}
{"id": 160748, "text": "We run the model 3 times with a maximal number of epochs 100 selecting the best checkpoint and with one NVIDIA Quadro RTX 6000 GPU .", "Comments": [], "label": [[100, 103, "Device-Count"], [104, 130, "Hardware-device"]]}
{"id": 160749, "text": "B.2 Implementation Details We implement our models using the Transformers library [ Wolf et al. , , 2020 ] [ 5 ] .", "Comments": [], "label": [[61, 81, "Software-Entity"]]}
{"id": 160750, "text": "We use Nvidia Tesla A100 with 40 GB of GPU memory for training and inference .", "Comments": [], "label": [[7, 24, "Hardware-device"], [30, 35, "Device-Memory"]]}
{"id": 160751, "text": "On a single GPU , one training epoch of the exercise generator takes about 30 minutes , and that of DKT takes about 7 minutes when they are separately trained .", "Comments": [], "label": [[5, 15, "Device-Count"]]}
{"id": 160752, "text": "In Appendix [ D.3 , ] we also benchmark the models ' computational efficiency and find that BIC performs favorably compared to pipelined approaches and can process 18.9 documents per second on a single GPU , demonstrating another benefit of joint modeling .", "Comments": [], "label": [[195, 205, "Device-Count"]]}
{"id": 160753, "text": "Entity Overlap uses spaCy 's NER model [ Honnibal et al. , , 2020 ] to extract named entities from both revisions and computes the Jaccard index between the entity sets as a score , with the assumption that newly introduced entities can be a signal of new and unaligned information .", "Comments": [], "label": [[20, 25, "Software-Entity"]]}
{"id": 160754, "text": "The training of the final model took roughly 1 hour on a single A100 GPU , and roughly 50 runs were conducted in iterations of model training .", "Comments": [], "label": [[57, 63, "Device-Count"], [64, 72, "Hardware-device"]]}
{"id": 160755, "text": "The training of the final model took roughly 25 minutes on a single A100 GPU , and roughly 20 training runs were conducted in iterations of model training .", "Comments": [], "label": [[61, 67, "Device-Count"], [68, 76, "Hardware-device"]]}
{"id": 160756, "text": "The training of the final model took approximately 20 minutes on a single A100 GPU , and roughly 10 training runs were conducted in iterations of model training .", "Comments": [], "label": [[67, 73, "Device-Count"], [74, 82, "Hardware-device"]]}
{"id": 160757, "text": "The training of the final model took approximately 20 minutes on a single A100 GPU , and roughly 15 training runs were conducted in iterations of model training .", "Comments": [], "label": [[67, 73, "Device-Count"], [74, 82, "Hardware-device"]]}
{"id": 160758, "text": "All models were benchmarked by the time they took to identify edits in the entire validation set ( i.e. , roughly 500 document pairs ) , using a single A-100 GPU on the same server , and we report normalized documents per second throughput ( Doc/s ) .", "Comments": [], "label": [[145, 151, "Device-Count"], [152, 161, "Hardware-device"]]}
{"id": 160759, "text": "The Keep-it-Simple model was implemented using the original author 's public model release on the HuggingFace model hub [ 5 ] .", "Comments": [], "label": [[98, 109, "Software-Entity"]]}
{"id": 160760, "text": "Training required 6-10 hours for each model , on a single A-100 GPU , and 5 runs were completed in the development of the models .", "Comments": [], "label": [[51, 57, "Device-Count"], [58, 67, "Hardware-device"]]}
{"id": 160761, "text": "Besides , the paired t-test [ Kim , 2015 ] is used to verify the statistical significance of the differences between the two approaches . 3.4 Implementation Details All experiments are conducted on a single NVIDIA Tesla V100 32GB card .", "Comments": [], "label": [[200, 206, "Device-Count"], [207, 224, "Hardware-device"], [225, 234, "Device-Memory"]]}
{"id": 160762, "text": "The SIL is implemented using PyTorch 1.9.0 with CUDA 10.0 and cudnn 7.6.5 .", "Comments": [], "label": [[29, 36, "Software-Entity"], [48, 52, "Software-Entity"], [62, 67, "Software-Entity"]]}
{"id": 160763, "text": "All the experiments are conducted on a workstation with two NVIDIA GeForce RTX 2080Ti GPU .", "Comments": [], "label": [[56, 59, "Device-Count"], [60, 89, "Hardware-device"]]}
{"id": 160764, "text": "The model is implemented with PyTorch [ Paszke et al. , 2019 ] .", "Comments": [], "label": [[30, 37, "Software-Entity"]]}
{"id": 160765, "text": "All the experiments are conducted on a server that has an Intel ( R ) Core ( TM ) i9-10900K @ 3.70GHz CPU and a 24-GB Nvidia RTX 3090 GPU .", "Comments": [], "label": [[58, 105, "Hardware-device"], [112, 117, "Device-Memory"], [118, 137, "Hardware-device"]]}
{"id": 160766, "text": "We calculate the inference time of a single batch on A100 GPU . pretrain-then-transfer models leverage additional data , we introduce LMRec+agnostic , a more robust representation learning method using additional corpus for language modeling .", "Comments": [], "label": [[53, 61, "Hardware-device"]]}
{"id": 160767, "text": "We leverage the automatic mixedprecision [ Micikevicius et al. , , 2018 ] package in Pytorch [ Paszke et al. , , 2019 ] to reduce training time and GPU memory usage .", "Comments": [], "label": [[85, 92, "Software-Entity"]]}
{"id": 160768, "text": "Since they did not release pretrained M6 [ Lin et al. , , 2021 ] , we used Huggingface RoBERTa [ Liu et al. , , 2019 ] to implement it .", "Comments": [], "label": [[75, 86, "Software-Entity"]]}
{"id": 160769, "text": "We calculate and gather top-5 Hessian eigenvalues by PyHessian [ Yao et al. , , 2020 ] , and resulting max eigenvalues are visualized using kernel density estimation in Scikit-learn [ Pedregosa et al. , , 2011 ] .", "Comments": [], "label": [[169, 181, "Software-Entity"]]}
{"id": 160770, "text": "To this end , we first identify and normalize all nouns in the context and mention using NLTK [ 2 ] , and then recall types that exactly nltk.tag package < https : //www.nltk.org > Figure 4 : Recall from MLM using prompts . match these nouns .", "Comments": [], "label": [[89, 93, "Software-Entity"]]}
{"id": 160771, "text": "We conduct the speed test using NVIDIA TITAN RTX for all the models and the inference batch size is 4 .", "Comments": [], "label": [[32, 48, "Hardware-device"]]}
{"id": 160772, "text": "We train this on a single V100 GPU .", "Comments": [], "label": [[19, 25, "Device-Count"], [26, 34, "Hardware-device"]]}
{"id": 160773, "text": "The rest of the hyperparameters are set to their default settings in the Transformers library [ Wolf et al. , , 2019 ] .", "Comments": [], "label": [[73, 93, "Software-Entity"]]}
{"id": 160774, "text": "Model configuration The SVM is a linear SVM in one-vs-rest mode from scikit-learn [ Pedregosa et al. , 2011 ] with TF-IDF document vectors of the word 1–3-grams with a minimum document frequency of 5 as features , tokenized by the bertbase-uncased tokenizer from Hugging Face .", "Comments": [], "label": [[69, 81, "Software-Entity"], [263, 275, "Software-Entity"]]}
{"id": 160775, "text": "Model training was done on a single A100 GPU .", "Comments": [], "label": [[29, 35, "Device-Count"], [36, 44, "Hardware-device"]]}
{"id": 160776, "text": "All of the models were trained on a single machine equipped with a 12-core processor , 64 GB of RAM , and a GPU with 24 GB of VRAM . [ 4 ] Training details can be found in Appendix [ B . ] 8 Results 8.1 Does noise injection improve selective rationalization ?", "Comments": [], "label": [[36, 42, "Device-Count"], [67, 84, "Device-Memory"], [87, 99, "Device-Memory"], [117, 130, "Device-Memory"]]}
{"id": 160777, "text": "Our code is based on Huggingface Transformers [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[21, 32, "Software-Entity"]]}
{"id": 160778, "text": "B Computing Infrastructure All experiments are carried out on a single Tesla V100 GPU with 32GB memory .", "Comments": [], "label": [[64, 70, "Device-Count"], [71, 85, "Hardware-device"], [91, 95, "Device-Memory"]]}
{"id": 160779, "text": "All experiments are conducted on NVIDIA A100 GPU with PyTorch 1.11 .", "Comments": [], "label": [[33, 48, "Hardware-device"], [54, 61, "Software-Entity"]]}
{"id": 160780, "text": "For multi-task pre-training , we initialize from BART-large , and train the model on 16 GPUs with 300,000 steps , batch size of 32 , learning rate of 1.5e-5 , and warm-up with 4,000 steps .", "Comments": [], "label": [[85, 92, "Cloud-Platform"]]}
{"id": 160781, "text": "For few-shot tuning , we prefix-tune the model on 4 GPUs with 100 and 1000 steps for 10-shot and 100 shot , respectively , with batch size of 32 , learning rate of 1.5e-4 , and warm-up with 10 % of the training steps .", "Comments": [], "label": [[50, 56, "Device-Count"]]}
{"id": 160782, "text": "A.6 Implementation Details Implementation We implement our T0 baseline and METRO-T0 based on fairseq [ 1 ] .", "Comments": [], "label": [[93, 100, "Software-Entity"]]}
{"id": 160783, "text": "The prompt templates to format the finetuing data are from the promptsource [ 2 ] library [ Bach et al. , , 2022 ] .", "Comments": [], "label": [[63, 75, "Software-Entity"]]}
{"id": 160784, "text": "Pretraining METRO-T5 in the * base * setting takes 20.8 hours on 64x NVIDIA A100 ( 40GB ) GPUs .", "Comments": [], "label": [[65, 67, "Device-Count"], [69, 80, "Hardware-device"], [83, 87, "Device-Memory"]]}
{"id": 160785, "text": "Pretraining METRO-T5 in the * base++ * setting takes 159 hours on 128x NVIDIA A100 ( 40GB ) GPUs .", "Comments": [], "label": [[66, 69, "Device-Count"], [71, 82, "Hardware-device"], [85, 89, "Device-Memory"]]}
{"id": 160786, "text": "Pretraining METRO-T5 in the * large++ * setting takes 289 hours on 256x NVIDIA A100 ( 40GB ) GPUs .", "Comments": [], "label": [[67, 70, "Device-Count"], [72, 83, "Hardware-device"], [86, 90, "Device-Memory"]]}
{"id": 160787, "text": "Prompt-finetuning each * base/base++ * model takes about 22 hours on 64x NVIDIA V100 ( 16GB ) GPUs .", "Comments": [], "label": [[69, 71, "Device-Count"], [73, 84, "Hardware-device"], [87, 91, "Device-Memory"]]}
{"id": 160788, "text": "Prompt-finetuning each * large++ * model takes about 70 hours on 64x NVIDIA V100 ( 16GB ) GPUs .", "Comments": [], "label": [[65, 67, "Device-Count"], [69, 80, "Hardware-device"], [83, 87, "Device-Memory"]]}
{"id": 160789, "text": "For BERTBASE and BERTLARGE , the raw model checkpoints are obtained from Huggingface Transformers [ Wolf et al. , , 2020 ] platform .", "Comments": [], "label": [[73, 84, "Software-Entity"]]}
{"id": 160790, "text": "The training of a 4-layer student model on one RTX 3090 Ti GPU costs approximately 6.5 hours for our method .", "Comments": [], "label": [[43, 46, "Device-Count"], [47, 62, "Hardware-device"]]}
{"id": 160791, "text": "They hire crowdsorce workers on Amazon Mechanical Turk to annotate dialogue summary .", "Comments": [], "label": [[32, 54, "Cloud-Platform"]]}
{"id": 160792, "text": "We evaluate the performance of DIONYSUS by conducting human evaluation experiments on Amazon Mechanical Turk .", "Comments": [], "label": [[86, 108, "Cloud-Platform"]]}
{"id": 160793, "text": "Additionally , we utilize 16 Nvidia V100 GPUs to train our models , which may not be accessible or reproducible for all researchers .", "Comments": [], "label": [[26, 28, "Device-Count"], [29, 45, "Hardware-device"]]}
{"id": 160794, "text": "Additionally , we thank the Mechanical Turk workers for conducting the human evaluation .", "Comments": [], "label": [[28, 51, "Cloud-Platform"]]}
{"id": 160795, "text": "DIONYSUS is implemented with Huggingface Pytorch Transformers [ 8 ] [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[29, 40, "Software-Entity"], [41, 48, "Software-Entity"]]}
{"id": 160796, "text": "Models are pre-trained with batch size 8 and learning rate 0.00001 on 16 Nvidia V100 GPUs until we observe no progress on validation data or up to 5 epochs .", "Comments": [], "label": [[70, 72, "Device-Count"], [73, 89, "Hardware-device"]]}
{"id": 160797, "text": "Figure 8 : A screenshot of the human evaluation on Amazon Mechanical Turk .", "Comments": [], "label": [[51, 73, "Cloud-Platform"]]}
{"id": 160798, "text": "Chosen LRs are presented in table Table [ 2 ] All experiments were run on a single NVIDIA A5000 GPU .", "Comments": [], "label": [[76, 82, "Device-Count"], [83, 99, "Hardware-device"]]}
{"id": 160799, "text": "We implement all methods using the HuggingFace Transformer library [ Wolf et al. , 2020 ] .", "Comments": [], "label": [[35, 46, "Software-Entity"]]}
{"id": 160800, "text": "We then study what is encoded < https : //huggingface.co/embedding-data > Figure 3 : The t-SNE visualization of the real ( * Re * ) and imaginary ( * Im * ) text embeddings and the kernel density estimate plot of the real and imaginary distance between text pairs in the saturation zone of the STS-B test .", "Comments": [], "label": [[89, 94, "Software-Entity"]]}
{"id": 160801, "text": "We train the models on the STS-B dataset for one epoch using a single GPU ( Nvidia GeForce RTX3090 Ti ) .", "Comments": [], "label": [[63, 73, "Device-Count"], [76, 101, "Hardware-device"]]}
{"id": 160802, "text": "We train and do experiments on one NVIDIA A100 GPU , and the performance and time consumption are in Table [ 12 . ] With the increase in model size , both accuracy and F1-score show upward trends , while the time increase is linear , which is reasonable .", "Comments": [], "label": [[31, 34, "Device-Count"], [35, 50, "Hardware-device"]]}
{"id": 160803, "text": "All the above models are obtained from huggingface transformers [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[39, 50, "Software-Entity"]]}
{"id": 160804, "text": "One of the best RoBERTa checkpoints trained on all PoliClaimgold and PoliClaimsilver is available on HuggingFace [ 5 ] . 6 Related Work Claim Detection : The term `` claim detection '' has different definitions in various research fields [ Boland et al. , , 2022 ] .", "Comments": [], "label": [[101, 112, "Software-Entity"]]}
{"id": 160805, "text": "Results are presented in Table [ 4 . ] For both models , we use the official checkpoints on huggingface and conduct greedy decoding when inference .", "Comments": [], "label": [[92, 103, "Software-Entity"]]}
{"id": 160806, "text": "We use the huggingface checkpoints of `` roberta-base '' and `` distilbert-base-uncased '' .", "Comments": [], "label": [[11, 22, "Software-Entity"]]}
{"id": 160807, "text": "All experiments are conducted on a node with 4 32G V100 GPUs .", "Comments": [], "label": [[45, 46, "Device-Count"], [47, 50, "Device-Memory"], [51, 60, "Hardware-device"]]}
{"id": 160808, "text": "In this work , we always use Sci-kit Learn for score computing .", "Comments": [], "label": [[29, 42, "Software-Entity"]]}
{"id": 160809, "text": "Scipy 's implementations for Student-t test and Fisher 's Method are used .", "Comments": [], "label": [[0, 8, "Software-Entity"]]}
{"id": 160810, "text": "Specifically , we fine-tune the base model on 8 32G Tesla V100 for 5 epochs , with the batch size as 8 and learning rate as 5e-6 , the parameter β as 0.1 .", "Comments": [], "label": [[46, 47, "Device-Count"], [48, 51, "Device-Memory"], [52, 62, "Hardware-device"]]}
{"id": 160811, "text": "Finally , we fine-tune the model on 8 32G Tesla V100 for 1 epoch , with the batch size as 8 and learning rate as 5e-7 .", "Comments": [], "label": [[36, 37, "Device-Count"], [38, 41, "Device-Memory"], [42, 52, "Hardware-device"]]}
{"id": 160812, "text": "Our ADM model is implemented with PyTorch .", "Comments": [], "label": [[34, 41, "Software-Entity"]]}
{"id": 160813, "text": "We run all experiments on two NVIDIA Tesla A100 GPUs with 40GB memory .", "Comments": [], "label": [[26, 29, "Device-Count"], [30, 52, "Hardware-device"], [58, 69, "Device-Memory"]]}
{"id": 160814, "text": "For instance , GPTQ [ Frantar et al. , , 2022 ] uses only an A100-80G GPU to quantize BLOOM-175B [ Work ] [ shop et al. , , 2022 ] within 4 hours .", "Comments": [], "label": [[61, 73, "Hardware-device"]]}
{"id": 160815, "text": "Using a batch size of 1 , all processes are performed on one NVIDIA A100-40G GPU .", "Comments": [], "label": [[57, 60, "Device-Count"], [61, 72, "Hardware-device"], [73, 80, "Device-Memory"]]}
{"id": 160816, "text": "Furthermore , we have optimized TaxoLLaMA to be lightweight through 4-bit quantization [ Dettmers et al. , 2023 ] and the application of LoRA [ Hu et al. , 2022 ] , making it feasible to run on GPU devices with only 4.8Gb of GPU for forward pass and 5.5Gb for fine-tuning , ensuring its accessibility for widespread use , e.g .", "Comments": [], "label": [[216, 228, "Device-Memory"]]}
{"id": 160817, "text": "The experiments utilized Nvidia A100 or Quadro RTX 8000 GPUs .", "Comments": [], "label": [[25, 36, "Hardware-device"], [40, 60, "Hardware-device"]]}
{"id": 160818, "text": "For LLAMA2 , we access it via the Huggingface Library [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[34, 45, "Software-Entity"]]}
{"id": 160819, "text": "The models are hosted on sixteen NVIDIA-V100 GPUs , and the time required to distill the entire dataset is approximately one month .", "Comments": [], "label": [[25, 32, "Device-Count"], [33, 49, "Hardware-device"]]}
{"id": 160820, "text": "First , we use the Huggingface [ 4 ] Library [ Wolf et al. , 2020 ] to build all models .", "Comments": [], "label": [[19, 30, "Software-Entity"]]}
{"id": 160821, "text": "Event . stands for event conceptualization discrimination and Triple . stands for triple conceptualization discrimination . we access it through Microsoft Azure APIs [ 5 ] .", "Comments": [], "label": [[145, 160, "Cloud-Platform"]]}
{"id": 160822, "text": "All experiments are conducted on sixteen NVIDIA-V100 ( 32G ) GPUs .", "Comments": [], "label": [[33, 40, "Device-Count"], [41, 52, "Hardware-device"], [55, 58, "Device-Memory"]]}
{"id": 160823, "text": "Our method can achieve fine-tuning results comparable to those obtained with larger memory capacities , even when operating under more limited resources such as a 24GB memory single GPU setup , with acceptable loss in training efficiency .", "Comments": [], "label": [[163, 174, "Device-Memory"], [175, 185, "Device-Count"]]}
{"id": 160824, "text": "The orange part denotes that the model has reached its fine-tuning limit with a 24GB GPU .", "Comments": [], "label": [[80, 88, "Device-Memory"]]}
{"id": 160825, "text": "Concretely , for achieving optimal results , approximately 10 % of the parameters are updated , which corresponds to more than 24G of GPU memory usage ( the orange part [ Corresponding author ] in the figure ) .", "Comments": [], "label": [[127, 144, "Device-Memory"]]}
{"id": 160826, "text": "All experiments are implemented using PyTorch and the Hugging Face Trainer , on machines with RTX-3090 or A40 GPUs .", "Comments": [], "label": [[38, 45, "Software-Entity"], [54, 66, "Software-Entity"], [94, 102, "Hardware-device"], [106, 114, "Hardware-device"]]}
{"id": 160827, "text": "Each experiment is conducted on an RTX 3090 , taking approximately 12 GPU hours , and the results are reported from a single run .", "Comments": [], "label": [[35, 43, "Hardware-device"]]}
{"id": 160828, "text": "5.3 Efficiency Results To analyze the efficiency of our method , Figure [ 5 ] illustrates the training latency ( wall time ) per batch , using a server with an RTX 3090 GPU and a 32 core CPU that supports AVX .", "Comments": [], "label": [[160, 172, "Hardware-device"], [179, 190, "Hardware-device"]]}
{"id": 160829, "text": "Due to the absence of detailed memory management and optimization of data transfer between the CPU and GPU ( such as custom CUDA streams ) , the final efficiency of our method is currently not optimal .", "Comments": [], "label": [[124, 128, "Software-Entity"]]}
{"id": 160830, "text": "We primarily use Python and PyTorch to implement functions including model forwarding , data transmission , and CPU retrieval of relevant key-value pairs .", "Comments": [], "label": [[28, 35, "Software-Entity"]]}
{"id": 160831, "text": "Deepspeed 's Zero [ Ren et al. , , 2021 ] uses offload technology to exchange parameters between the CPU and GPU , which is similar to our method .", "Comments": [], "label": [[0, 17, "Software-Entity"]]}
{"id": 160832, "text": "Decomposition is carried out on a CPU machine and rank search is done on a single L4 GPU with 24GB of VRAM for faster evaluations during search .", "Comments": [], "label": [[75, 81, "Device-Count"], [82, 88, "Hardware-device"], [94, 106, "Device-Memory"]]}
{"id": 160833, "text": "These numbers are reported on a 48 Core 128GB CPUonly machine , thus eliminating the need for a GPU altogether for decomposition .", "Comments": [], "label": [[32, 49, "Hardware-device"]]}
{"id": 160834, "text": "In the case of the rank search mechanism , we employ a single 24GB L4 GPU which is 2-3× slower and runs at a lower batch size than a 40/80GB A100 machine .", "Comments": [], "label": [[55, 61, "Device-Count"], [62, 66, "Device-Memory"], [67, 73, "Hardware-device"]]}
{"id": 160835, "text": "We apply MiniGPT-4 and InstructBLIP with one Tesla V100 .", "Comments": [], "label": [[41, 44, "Device-Count"], [45, 55, "Hardware-device"]]}
{"id": 160836, "text": "In case of BioBERT [ Lee et al. , , 2019 ] , we use the checkpoint of pre-trained model from huggingface [ 4 ] .", "Comments": [], "label": [[93, 104, "Software-Entity"]]}
{"id": 160837, "text": "We run ConNER and BioBERT on eight NVIDIA RTX A5000 GPUs .", "Comments": [], "label": [[29, 34, "Device-Count"], [35, 56, "Hardware-device"]]}
{"id": 160838, "text": "For example , we utilize two NVIDIA RTX A5000 GPUs to train the classifiers using our proposed approach whereas the single classifier based methods that we explored as our baselines are trained with a single GPU .", "Comments": [], "label": [[25, 28, "Device-Count"], [29, 50, "Hardware-device"]]}
{"id": 160839, "text": "We use the huggingface [ 3 ] implementation for both models .", "Comments": [], "label": [[11, 22, "Software-Entity"]]}
{"id": 160840, "text": "We use a single NVIDIA RTX A5000 GPU for the experiments involving a single model training ( e.g. , DBST , FixMatch etc . ) , and two NVIDIA RTX A5000 GPU for the dual model training experiments ( e.g. , co-training , weighted co-training , coteaching ) .", "Comments": [], "label": [[9, 15, "Device-Count"], [16, 36, "Hardware-device"], [130, 133, "Device-Count"], [134, 154, "Hardware-device"]]}
{"id": 160841, "text": "All experiments were conducted on an NVIDIA A800 GPU . 4.1.1 Baslines To comprehensively evaluate the effectiveness of SirLLM , we utilized three baseline models : StreamLLM : StreamLLM [ Xiao et al. , , 2023 ] preserves the key-value states of only the attention sink tokens and recent tokens .", "Comments": [], "label": [[37, 52, "Hardware-device"]]}
{"id": 160842, "text": "The experiments are conducted on two NVIDIA A100 GPUs .", "Comments": [], "label": [[33, 36, "Device-Count"], [37, 53, "Hardware-device"]]}
{"id": 160843, "text": "We use Python-MIP [ 5 ] package and Gurobi as our MIP solver .", "Comments": [], "label": [[7, 17, "Software-Entity"], [36, 42, "Software-Entity"]]}
{"id": 160844, "text": "Fine-tuning of DTR models as well as computing coarse- and fine-grained relevance scores are performed on one Tesla V100 GPU .", "Comments": [], "label": [[106, 109, "Device-Count"], [110, 124, "Hardware-device"]]}
{"id": 160845, "text": "B Implementation Details We use ANCE provided by Huggingface as the base model [ 1 ] .", "Comments": [], "label": [[49, 60, "Software-Entity"]]}
{"id": 160846, "text": "We have utilized various open-source models such as LLaMA-2 , CodeLLaMA , Mistral , and Mixtral-8x7B , as well as open-source software such as Hugging Face and PyTorch .", "Comments": [], "label": [[143, 155, "Software-Entity"], [160, 167, "Software-Entity"]]}
{"id": 160847, "text": "E Finetuning Details In this work , we finetune all models using the HuggingFace library .", "Comments": [], "label": [[69, 80, "Software-Entity"]]}
{"id": 160848, "text": "The 70B and 34B models are fine-tuned on 32 NVIDIA A800 80GB GPUs .", "Comments": [], "label": [[41, 43, "Device-Count"], [44, 65, "Hardware-device"]]}
{"id": 160849, "text": "Mistral-8x7B is fine-tuned on 16 NVIDIA A800 80GB GPUs , while the 7B , 13B , and 20B models are all fine-tuned on 8 NVIDIA A800 80GB GPUs .", "Comments": [], "label": [[30, 32, "Device-Count"], [33, 44, "Hardware-device"], [45, 54, "Device-Memory"], [115, 116, "Device-Count"], [117, 128, "Hardware-device"], [129, 138, "Device-Memory"]]}
{"id": 160850, "text": "Therefore , we use LLaMA-2-Chat in our main experiments and further investigation . [ 16 ] For all backbone models , we used their publicly available checkpoints on Huggingface .", "Comments": [], "label": [[165, 176, "Software-Entity"]]}
{"id": 160851, "text": "The finetuning process was implemented using PyTorch and Colossal-AI frameworks [ Li et al. , , 2023 ] .", "Comments": [], "label": [[45, 52, "Software-Entity"], [57, 68, "Software-Entity"]]}
{"id": 160852, "text": "To optimize memory usage and accelerate training , we applied Deepspeed ZeRO stage 2 [ Rasley et al. , , 2020 ] and BFloat16 mixed precision techniques .", "Comments": [], "label": [[62, 76, "Software-Entity"]]}
{"id": 160853, "text": "Additionally , Flash attention [ Dao et al. , , 2022 ] was used to further improve training efficiency .", "Comments": [], "label": [[15, 30, "Software-Entity"]]}
{"id": 160854, "text": "All models were trained on 8 Tesla A100-40G GPUs .", "Comments": [], "label": [[27, 28, "Device-Count"], [29, 48, "Hardware-device"]]}
{"id": 160855, "text": "We use the NLTK tool [ 2 ] to split the fact description into sentences , * i.e . * , f = { s1 , s2 , s3 , · · · } , and generate the sentence embeddings using an averagepooling layer : where wi , j is the j-th word in sentence s , hwi , j is its word embedding in Hd , and hs is the sentence embedding of s .", "Comments": [], "label": [[11, 15, "Software-Entity"]]}
{"id": 160856, "text": "The implementation is based on Pytorch and trained on a Tesla V100 GPU with AdamW [ Loshchilov and Hutter , 2019 ] optimizer for 20 epochs .", "Comments": [], "label": [[31, 38, "Software-Entity"], [56, 70, "Hardware-device"]]}
{"id": 160857, "text": "A.2.2 Implementation Details During the training process , we conduct the training experiments of the Vec2Text model on four Nvidia A100 40G GPUs .", "Comments": [], "label": [[120, 124, "Device-Count"], [125, 136, "Hardware-device"], [137, 145, "Device-Memory"]]}
{"id": 160858, "text": "The dense retrieval is performed using Faiss [ Johnson et al. , , 2021 ] .", "Comments": [], "label": [[39, 44, "Software-Entity"]]}
{"id": 160859, "text": "B Experiment Settings The experiments are done on 8 Tesla V100 and 4 Tesla A100 GPUs , taking up a total of around 500 GPU hours .", "Comments": [], "label": [[50, 51, "Device-Count"], [52, 62, "Hardware-device"], [67, 68, "Device-Count"], [69, 84, "Hardware-device"]]}
{"id": 160860, "text": "All the experiments were performed on a GPU cluster with NVIDIA A40 48GB PCIe 4.0 .", "Comments": [], "label": [[57, 67, "Hardware-device"], [68, 72, "Device-Memory"]]}
{"id": 160861, "text": "The model is trained on 64 NVIDIA A100 GPUs , exceeding a total of 1,000 GPU days , employing a cosine learning rate schedule with 1 % of warm-up steps .", "Comments": [], "label": [[24, 26, "Device-Count"], [27, 43, "Hardware-device"]]}
{"id": 160862, "text": "Then the train loss is defined as L = L + L2 . 6 Evaluation 6.1 Evaluation Setup We implement ViC using Pytorch 2.1 [ Paszke et al. , , 2019 ] .", "Comments": [], "label": [[104, 111, "Software-Entity"]]}
{"id": 160863, "text": "We use IDA Pro 8.3 [ Hex-rays , 2023b ] to disassemble and extract the functions from the binary executable file in all of the experiments .", "Comments": [], "label": [[7, 14, "Software-Entity"]]}
{"id": 160864, "text": "The CPU setup is 128 cores with 2TB RAM for each server .", "Comments": [], "label": [[17, 26, "Device-Memory"], [32, 39, "Device-Memory"]]}
{"id": 160865, "text": "The total GPU setup is 32 NVIDIA Tesla A100 . 6.2 Evaluation of Virtual Compiler We endeavor to appraise the quality of assembly code yielded by the virtual compiler , with an emphasis on the model 's capability to generate assembly code for augmenting the code search dataset .", "Comments": [], "label": [[22, 25, "Device-Count"], [26, 43, "Hardware-device"]]}
{"id": 160866, "text": "All models and datasets are downloaded from Huggingface . [ 3 ] All models are fine-tuned on NVIDIA A800 GPUs .", "Comments": [], "label": [[44, 55, "Software-Entity"], [93, 109, "Hardware-device"]]}
{"id": 160867, "text": "3.1 Experimental Setups Our experiments are conducted on 8 Nvidia A100 GPUs , each with 80GB of memory , and we use PyTorch in Python .", "Comments": [], "label": [[57, 58, "Device-Count"], [59, 75, "Hardware-device"], [88, 102, "Device-Memory"], [116, 123, "Software-Entity"]]}
{"id": 160868, "text": "To efficiently train the computationally intensive models , we simultaneously employ DeepSpeed [ Rajbhan ] [ dari et al. , , 2020 ] and Flash Attention 2 [ Dao , 2023 ] .", "Comments": [], "label": [[85, 94, "Software-Entity"], [136, 153, "Software-Entity"]]}
{"id": 160869, "text": "On 32 NVIDIA A800 80GB GPUs , StarCoder-1B , StarCoder-15B , Code Llama 7B , and Code Llama 13B take 14 hours , 140 hours , 75 hours , and 138 hours , respectively . 5.2 Results Baselines .", "Comments": [], "label": [[3, 5, "Device-Count"], [6, 17, "Hardware-device"], [18, 27, "Device-Memory"]]}
{"id": 160870, "text": "We use spaCy sentencizer [ Honnibal and Montani , 2017 ] . a constrained decoding technique to maintain exact lexical matching to the highlighted content .", "Comments": [], "label": [[7, 24, "Software-Entity"]]}
{"id": 160871, "text": "For our human evaluation we used the Amazon Mechanical Turk [ 13 ] ( MTurk ) crowdsourcing platform .", "Comments": [], "label": [[37, 59, "Cloud-Platform"]]}
{"id": 160872, "text": "We then used whisper [ Radford et al. , , 2023 ] to transcribe the program 's speech into text .", "Comments": [], "label": [[13, 20, "Software-Entity"]]}
{"id": 160873, "text": "We use a 32GB NVIDIA V100 GPU to train our model .", "Comments": [], "label": [[9, 13, "Device-Memory"], [14, 29, "Hardware-device"]]}
{"id": 160874, "text": "Specifically , we utilize the Python package * pdfminer * to convert the content of literature files into plain text .", "Comments": [], "label": [[47, 56, "Software-Entity"]]}
{"id": 160875, "text": "`` ` 1 https : //huggingface.co/datasets/zjunlp/ OceanBench `` ` 4.1 Implementation Details and Baselines For the pre-training stage , we pre-train our OCEANGPT [ 2 ] based on the LLaMA-2 [ Touvron et al. , 2023b ] for seven days with six A800 Nvidia GPUs .", "Comments": [], "label": [[235, 238, "Device-Count"], [239, 255, "Hardware-device"]]}
{"id": 160876, "text": "We studied six widely-used repositories , namely : Fairseq-ST [ Wang et al. , , 2020 ] , ESPnet-ST [ In ] [ aguma et al. , , 2020 ] , NeMo [ Kuchaiev et al. , , 2019 ] , SpeechBrain [ Ravanelli et al. , , 2021 ] , an open source codebase named `` Conformer '' , [ 8 ] and TorchAudio [ Yang et al. , , 2021 ] .", "Comments": [], "label": [[51, 61, "Software-Entity"], [89, 98, "Software-Entity"], [134, 138, "Software-Entity"], [170, 181, "Software-Entity"], [247, 256, "Software-Entity"], [272, 282, "Software-Entity"]]}
{"id": 160877, "text": "All the implementations but one ( NeMo ) are affected by .", "Comments": [], "label": [[34, 38, "Software-Entity"]]}
{"id": 160878, "text": "Also , all are affected by and , except for TorchAudio , whose implementation neither includes relative positional encodings in the attention nor the initial sub-sampling convolutional layers .", "Comments": [], "label": [[45, 54, "Software-Entity"]]}
{"id": 160879, "text": "Evaluation is performed on the tst-COMMON , by computing word error rate ( WER ) for ASR and BLEU with SacreBLEU [ Post , 2018 ] [ 10 ] for ST .", "Comments": [], "label": [[103, 112, "Software-Entity"]]}
{"id": 160880, "text": "Detailed experimental settings are reported in Appendix [ B . ] Trainings and inferences were performed on , respectively , two and one A40 GPU ( s ) .", "Comments": [], "label": [[124, 127, "Device-Count"], [132, 135, "Device-Count"], [136, 143, "Hardware-device"]]}
{"id": 160881, "text": "On Ampere GPUs , PyTorch computes convolutions and matrix multiplications with TensorFloat-32 [ 11 ] ( TF32 ) tensor cores by default .", "Comments": [], "label": [[17, 24, "Software-Entity"]]}
{"id": 160882, "text": "Built upon the widely used PyTorch library [ Paszke et al. , , 2019 ] , pangoliNN offers a collection of pre-defined tests that enforce specific behaviors of the modules .", "Comments": [], "label": [[27, 42, "Software-Entity"]]}
{"id": 160883, "text": "All trainings are performed with 40k tokens as batch size and 4 as update frequency on two GPUs .", "Comments": [], "label": [[87, 95, "Device-Count"]]}
{"id": 160884, "text": "All other settings are the default of Fairseq-ST [ Wang et al. , , 2020 ] , which we forked as a base of our implementation .", "Comments": [], "label": [[38, 48, "Software-Entity"]]}
{"id": 160885, "text": "Furthermore , we observe consistent trends and similar improvements as in the previous section when applying joint CTC rescoring and employing the AS-BLEU , computed with SubER tool [ AppTek , 2022 ] , is calculated with the popular sacreBLEU metric [ Post , 2018 ] after aligning blocks of the hypothesis and reference with the minimum Levenshtein distance method [ Matusov et al. , , 2005 ] .", "Comments": [], "label": [[233, 242, "Software-Entity"]]}
{"id": 160886, "text": "To collect this information , we used ELAN [ Wittenburg et al. , , 2006 ] , an audio/video annotation tool commonly employed in the literature [ Sloetjes and Witten ] [ burg , 2008 ] .", "Comments": [], "label": [[38, 42, "Software-Entity"]]}
{"id": 160887, "text": "A.2 Model and Training Our systems are implemented with fairseq-ST [ Wang et al. , , 2020a ] using the default settings unless specified otherwise .", "Comments": [], "label": [[56, 66, "Software-Entity"]]}
{"id": 160888, "text": "Utterance-level Cepstral Mean and Variance Normalization ( CMVN ) and SpecAugment [ Park et al. , , 2019 ] are applied during training and segments longer than 30 seconds are filtered out ( fairseq-ST default ) to avoid excessive VRAM requirements .", "Comments": [], "label": [[190, 200, "Software-Entity"]]}
{"id": 160889, "text": "All trainings are executed on 4 NVIDIA Ampere GPU A100 ( 64GB VRAM ) .", "Comments": [], "label": [[30, 31, "Device-Count"], [32, 54, "Hardware-device"], [57, 66, "Device-Memory"]]}
{"id": 160890, "text": "* In addition , specific guidelines on the usage of the annotation tool , ELAN , were provided ( Figure [ 2 ] shows the interface ) .", "Comments": [], "label": [[74, 78, "Software-Entity"]]}
{"id": 160891, "text": "All our models are implemented in fairseq-s2t [ Wang et al. , , 2020 ] .", "Comments": [], "label": [[34, 45, "Software-Entity"]]}
{"id": 160892, "text": "We use sacreBLEU [ Post , 2018 ] [ 6 ] for translation quality , and LAAL – for AlignAtt – and StreamLAAL ( Section [ 4 ] for latency .", "Comments": [], "label": [[7, 16, "Software-Entity"]]}
{"id": 160893, "text": "Inferences are executed on a NVIDIA K80 GPU with 12GB VRAM . 6 Results In this section , we first compare our proposed StreamAtt policy for StreamST with the streaming baseline ( Section [ 6.1 ] and then with the state-of-the-art SimulST policy ( Section [ 6.2 ] .", "Comments": [], "label": [[29, 43, "Hardware-device"], [49, 58, "Device-Memory"]]}
{"id": 160894, "text": "Trainings are performed on 4 NVIDIA A40 GPUs with 40GB RAM .", "Comments": [], "label": [[27, 28, "Device-Count"], [29, 44, "Hardware-device"], [50, 58, "Device-Memory"]]}
{"id": 160895, "text": "Crayon : Customized On-Device LLM via Instant Adapter Blending and Edge-Server Hybrid Inference A Details on Experiments A.1 Training setting We implemented the proposed and baseline methods based on the Huggingface PEFT library [ Man ] [ grulkar et al. , , 2022 ] .", "Comments": [], "label": [[204, 215, "Software-Entity"], [216, 228, "Software-Entity"]]}
{"id": 160896, "text": "All the proposed and baseline methods are implemented with PyTorch 2.0.1 and executed on a single NVIDIA A5000 GPU .", "Comments": [], "label": [[59, 72, "Software-Entity"], [91, 97, "Device-Count"], [98, 115, "Hardware-device"]]}
{"id": 160897, "text": "After these adjustments , the total number of dialogues prepared for annotation was 4 , 876 . 3.3 Human Annotation We established our annotation platform using Label Studio [ 3 ] .", "Comments": [], "label": [[160, 172, "Cloud-Platform"]]}
{"id": 160898, "text": "Our analysis utilizes a multi-class sentiment classification model , specifically the Distilbert-base-uncasedemotion model from Hugging Face , to determine the dominant emotion within each dialogue .", "Comments": [], "label": [[128, 140, "Software-Entity"]]}
{"id": 160899, "text": "Moreover , Figure [ 5 ' ] s left panel showcases a t-SNE visualization of Sentence Transformer embeddings for both manipulative and non-manipulative dialogues within MENTALMANIPcon , using the all-MiniLM-L12-v2 model from Hugging Face .", "Comments": [], "label": [[51, 56, "Software-Entity"], [222, 234, "Software-Entity"]]}
{"id": 160900, "text": "All experiments were performed on three Quadro RTX 6000 GPUs .", "Comments": [], "label": [[34, 39, "Device-Count"], [40, 60, "Hardware-device"]]}
{"id": 160901, "text": "We used t-SNE visualization of Sentence Transformer embeddings to analyze the semantic distributions of these dialogues , which are presented in Figure [ 7 . ]", "Comments": [], "label": [[8, 13, "Software-Entity"]]}
{"id": 160902, "text": "We use 8 Nvidia A100 for the experiments .", "Comments": [], "label": [[7, 8, "Device-Count"], [9, 20, "Hardware-device"]]}
{"id": 160903, "text": "B.4 Training Details The default experiment setting for both semiparametric token-sequence co-supervision and the baselines is set to train the pre-trained Llama2 7B [ Touvron et al. , , 2023b ] provided from huggingface [ Wolf et al. , , 2019 ] as the initial model for both Gen and Embseq .", "Comments": [], "label": [[209, 220, "Software-Entity"]]}
{"id": 160904, "text": "We use 8 Nvidia A100 with 80GB memory for our experiments .", "Comments": [], "label": [[7, 8, "Device-Count"], [9, 20, "Hardware-device"], [26, 37, "Device-Memory"]]}
{"id": 160905, "text": "We use FSDP [ Zhao et al. , , 2023 ] to conduct multi-GPU distributed training .", "Comments": [], "label": [[7, 11, "Software-Entity"]]}
{"id": 160906, "text": "We run inference of our trained models using 1 A6000 GPU with 48GB memory .", "Comments": [], "label": [[45, 46, "Device-Count"], [47, 56, "Hardware-device"], [62, 73, "Device-Memory"]]}
{"id": 160907, "text": "Llama2-7B models are initialized with Llama2-7B and are further trained with the training dataset whereas contriever-msmarco is the released model from huggingface which is trained via msmarco .", "Comments": [], "label": [[152, 163, "Software-Entity"]]}
{"id": 160908, "text": "This approach was notably adopted by Code Alpaca [ Chaudhary , 2023 ] , based on Stanford Alpaca [ Taori et al. , , 2023 ] , transforming 21 basic code prompts into 20,000 high-quality instructions .", "Comments": [], "label": [[37, 48, "Software-Entity"], [81, 96, "Software-Entity"]]}
{"id": 160909, "text": "For fine-tuning , models were trained with a batch size of 256 over 2 epochs , a learning rate of 2e-5 , a cosine learning rate scheduler with 10 % warm-up steps , and under bf16 precision on 4 × 8 NVIDIA A100 40G GPUs .", "Comments": [], "label": [[192, 197, "Device-Count"], [198, 209, "Hardware-device"], [210, 218, "Device-Memory"]]}
{"id": 160910, "text": "All experiments are conducted on 4 Tesla V100 GPUs and averaged over 3 runs . < https : //huggingface.co/bert-base-uncased > Table 2 : The main results on three datasets under various imbalance ratios γ ( γ = 1 is the balanced NID setting ) .", "Comments": [], "label": [[33, 34, "Device-Count"], [35, 50, "Hardware-device"]]}
{"id": 160911, "text": "All experiments are conducted on 4 Tesla V100 GPUs and averaged over 3 runs .", "Comments": [], "label": [[33, 34, "Device-Count"], [35, 50, "Hardware-device"]]}
{"id": 160912, "text": "Implementation Details We implement the baseline settings and our MIDI-Tuning using the Huggingface PEFT [ Mangrulkar et al. , , 2022 ] library , and we incorporate DeepSpeed [ Rasley et al. , , 2020 ] to improve the training efficiency .", "Comments": [], "label": [[88, 104, "Software-Entity"], [165, 174, "Software-Entity"]]}
{"id": 160913, "text": "We will consider improving the compute efficiency of our framework by employing advanced acceleration techniques , such as FLASHATTENTION [ Dao et al. , , 2022 ] .", "Comments": [], "label": [[123, 137, "Software-Entity"]]}
{"id": 160914, "text": "We experiment on one server equipped with 4 NVIDIA V100 GPUs .", "Comments": [], "label": [[42, 43, "Device-Count"], [44, 60, "Hardware-device"]]}
{"id": 160915, "text": "For a 40B token corpus , the * * 490 * * n-gram training procedure ( with n = 4 ) requires * * 491 * * 4 CPU cores for 5 hours , followed by computing * * 492 * * data commonness using 4 CPU cores in 2 hours .", "Comments": [], "label": [[103, 114, "Device-Count"], [185, 196, "Device-Count"]]}
{"id": 160916, "text": "* * 493 * * Compared to the substantial costs of GPU con- * * 494 * * servation ( at least 930 V100 GPU hours in our * * 495 * * experiments ) , these expenses can be considered * * 496 * * negligible .", "Comments": [], "label": [[95, 103, "Hardware-device"]]}
{"id": 160917, "text": "F Computational Costs For the QA evaluations across the 28 datasets , we use an RTX 3090 GPU .", "Comments": [], "label": [[80, 93, "Hardware-device"]]}
{"id": 160918, "text": "In the iterative self-SFT method employed for fine-tuning the Vicuna model , we use an A100 GPU ( 80G ) .", "Comments": [], "label": [[87, 95, "Hardware-device"], [98, 101, "Device-Memory"]]}
{"id": 160919, "text": "We embed the utterances using HuggingFace [ Wolf et al. , , 2020 ] transformer model * all-MiniLM-L6-v2 * .", "Comments": [], "label": [[30, 41, "Software-Entity"]]}
{"id": 160920, "text": "We use a single A100 GPU for the experiments .", "Comments": [], "label": [[9, 15, "Device-Count"], [16, 24, "Hardware-device"]]}
{"id": 160921, "text": "Whenever possible , we use bilingual lexicons sourced from [ Conneau et al. , 2017 ] to facilitate the English replacement , relying on the spaCy tagger [ Honnibal et al. , , 2020 ] to predict POS tags for each word .", "Comments": [], "label": [[140, 152, "Software-Entity"]]}
{"id": 160922, "text": "For languages lacking these resources , we use NLLB [ Costajussà et al. , , 2022 ] to translate each sentence word by word into English and borrow the English POS tags .", "Comments": [], "label": [[47, 51, "Software-Entity"]]}
{"id": 160923, "text": "We used a single 80GB GPU in the NVIDIA DGX A100 GPU cluster , to run experiments on both the models simultaneously .", "Comments": [], "label": [[10, 16, "Device-Count"], [17, 25, "Device-Memory"], [33, 52, "Hardware-device"]]}
{"id": 160924, "text": "The training utilizes 8 A100 GPUs , with a global < https : //github.com/EleutherAI/gpt-neox/tree/v2.0 > batch size of 256 and 2 gradient steps accumulation , taking approximately 96 hours for 2 epochs .", "Comments": [], "label": [[22, 23, "Device-Count"], [24, 33, "Hardware-device"]]}
{"id": 160925, "text": "All experiments are conducted with 32 A100 GPUs , setting a per-device batch size to 8 without gradient accumulation .", "Comments": [], "label": [[35, 37, "Device-Count"], [38, 47, "Hardware-device"]]}
{"id": 160926, "text": "The experiments are conducted on 32 A100 GPUs , with a per-device batch size of 1 and no gradient accumulation .", "Comments": [], "label": [[33, 35, "Device-Count"], [36, 45, "Hardware-device"]]}
{"id": 160927, "text": "Experimental results indicate that Roformer+CoCA only demands approximately 60GB of GPU memory during inference with a sequence length of 32k , aligning closely with the memory consumption of the vanilla self-attention mechanism .", "Comments": [], "label": [[76, 94, "Device-Memory"]]}
{"id": 160928, "text": "We carry out all the experiments in one run with 4 NVIDIA Tesla V100 GPUs .", "Comments": [], "label": [[49, 50, "Device-Count"], [51, 73, "Hardware-device"]]}
{"id": 160929, "text": "The UMR framework is trained on a single A800 GPU . 4.3 Evaluation Metrics For the evaluation of Hateful memes , we adopt the methodology outlined in [ Kiela et al. , , 2020 ] , employing the Area Under the Receiver Operating Characteristic curve ( AUROC ) and accuracy ( Acc . ) as evaluation metrics .", "Comments": [], "label": [[33, 40, "Device-Count"], [41, 49, "Hardware-device"]]}
{"id": 160930, "text": "We used Surge AI [ 2 ] as the annotation tool .", "Comments": [], "label": [[8, 16, "Cloud-Platform"]]}
{"id": 160931, "text": "LLaMA 2 experiments were implemented using the Huggingface transformers library [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[47, 58, "Software-Entity"]]}
{"id": 160932, "text": "All prompting experiments wit LLaMA 2 models are executed on A100 GPUs .", "Comments": [], "label": [[61, 70, "Hardware-device"]]}
{"id": 160933, "text": "To create rewrites that are highly representative of a dimension , we use the appropriateness corpus , sentence transformers [ Reimers and Gurevych , 2019 ] , and PageRank [ Lawrence , 1998 ] ( details in Appendix [ B ] .", "Comments": [], "label": [[103, 124, "Software-Entity"]]}
{"id": 160934, "text": "Training a single model took around two days on four A100 GPUs .", "Comments": [], "label": [[48, 52, "Device-Count"], [53, 62, "Hardware-device"]]}
{"id": 160935, "text": "The fine- tuning process was conducted on a device with eight NVIDIA A100 80G GPUs , with the global batch size set to 64 .", "Comments": [], "label": [[56, 61, "Device-Count"], [62, 82, "Hardware-device"]]}
{"id": 160936, "text": "Richard Eckart de Castilho for his invaluable assistance in creating the cross-document annotation environment within INCEpTION .", "Comments": [], "label": [[118, 127, "Software-Entity"]]}
{"id": 160937, "text": "We augment the original ITG documents with sentence nodes , employing an assembled sentence segmentation methodology using spaCy [ 4 ] and ScispaCy [ 5 ] [ Neumann et al. , , 2019 ] .", "Comments": [], "label": [[123, 128, "Software-Entity"], [139, 147, "Software-Entity"]]}
{"id": 160938, "text": "In our preliminary testing , we discovered that neither spaCy nor ScispaCy sentence splitters are infallible for segmentation , with neither consistently outperforming the other .", "Comments": [], "label": [[56, 61, "Software-Entity"], [66, 74, "Software-Entity"]]}
{"id": 160939, "text": "The similarity thresholds * t0 * , and * t1 * are optimized in a pilot study on 20 document pairs , where the ideal configuration was determined to be * t0 * =40 , We use fuzzywuzzy 0.18.0 at : [ https : //github.com/ ] ( https : //github.com/seatgeek/fuzzywuzzy ) [ seatgeek/fuzzywuzzy ] ( https : //github.com/seatgeek/fuzzywuzzy ) Figure 9 : INCEpTION enables cross-document annotation in the context of two full documents ( doc1-doc2 ) .", "Comments": [], "label": [[345, 354, "Software-Entity"]]}
{"id": 160940, "text": "D.4 Computational details In our classification tasks with Llama2-70B , covering revision alignment , edit intent classification , and review request extraction , we employed two RTX™ A6000 GPUs , each equipped with 48GB of memory .", "Comments": [], "label": [[175, 178, "Device-Count"], [179, 194, "Hardware-device"], [216, 220, "Device-Memory"]]}
{"id": 160941, "text": "Setup We pretrain each version of the model for 20 epochs on the encoded books3 dataset for up to five days on four Tesla V100-SXM2-32GB .", "Comments": [], "label": [[111, 115, "Device-Count"], [116, 131, "Hardware-device"], [132, 136, "Device-Memory"]]}
{"id": 160942, "text": "By default , to encode the input chunks into vectors , we use the MiniLM-L6 model from the Sentence Transformers library [ 1 ] .", "Comments": [], "label": [[91, 120, "Software-Entity"]]}
{"id": 160943, "text": "We use the chapter-level version from the Huggingface Hub [ 3 ] and recast the task as a Semantic Textual Similarity one by positing that each chapter should be most similar to its respective summaries rather than to other summaries .", "Comments": [], "label": [[42, 57, "Software-Entity"]]}
{"id": 160944, "text": "Table 8 : Training data statistics . `` Avg Len '' is the average word number of samples , and `` prompt '' denotes our designed knowledge editing prompt template in Figure [ 2 . ] B Implementation Details The training procedure was executed on 4 NVIDIA A100 GPUs , each equipped with 80GB of memory .", "Comments": [], "label": [[245, 246, "Device-Count"], [247, 263, "Hardware-device"], [285, 299, "Device-Memory"]]}
{"id": 160945, "text": "TRUE [ Honovich et al. , , 2022 ] and SummaC [ Laban et al. , , 2022 ] aim to detect compatibility between the generated output and the reference .", "Comments": [], "label": [[38, 44, "Software-Entity"]]}
{"id": 160946, "text": "SummaC [ Laban et al. , , 2022 ] generates NLI scores from the sentences of compared texts and calculates an overall score .", "Comments": [], "label": [[0, 6, "Software-Entity"]]}
{"id": 160947, "text": "To account for this , our Llama 2-Chat experiments were run on-site , taking approximately ∼30 hours on a single NVIDIA A100 GPU with 80GB memory , and are strictly reproducible .", "Comments": [], "label": [[106, 112, "Device-Count"], [113, 128, "Hardware-device"], [134, 138, "Device-Memory"]]}
{"id": 160948, "text": "On the other hand , baseline SummaC scores are better than both model outputs .", "Comments": [], "label": [[29, 35, "Software-Entity"]]}
{"id": 160949, "text": "Last two rows : BLEURT , TRUE and SummaC .", "Comments": [], "label": [[34, 40, "Software-Entity"]]}
{"id": 160950, "text": "NLI-based metrics , however , do capture the difference : particularly for TRUE , the results are more pronounced in favor of GPT 3.5 ; a similar pattern can be observed for SummaC .", "Comments": [], "label": [[174, 180, "Software-Entity"]]}
{"id": 160951, "text": "WC : word count , Sum : SummaC .", "Comments": [], "label": [[24, 30, "Software-Entity"]]}
{"id": 160952, "text": "TRUE and SummaC are less correlated with each other compared to conventional metrics .", "Comments": [], "label": [[9, 15, "Software-Entity"]]}
{"id": 160953, "text": "SummaC , on the other hand , processes paragraphs at the sentence level and produces an overall score by convolution – decreasing its sensitivity but also leading to smaller differences between prompt configurations .", "Comments": [], "label": [[0, 6, "Software-Entity"]]}
{"id": 160954, "text": "Table [ 5 ] shows that ROGUE , SummaC and TRUE have no significant correlation with human scores .", "Comments": [], "label": [[31, 37, "Software-Entity"]]}
{"id": 160955, "text": "We gratefully acknowledge the support of Microsoft with a grant for access to OpenAI GPT models via the Azure cloud ( Accelerate Foundation Model Academic Research ) .", "Comments": [], "label": [[104, 116, "Cloud-Platform"]]}
{"id": 160956, "text": "* Custom instance prompt : * Main paper abstract : { Citing paper abstract } Relevant paper abstract : { Cited paper abstract } Intent : { Intent of the paragraph } Example : { Example citation sentence } * C Main Experimentation Details We have obtained Llama 2-Chat weights [ 9 ] and converted the checkpoints to the Huggingface format .", "Comments": [], "label": [[319, 330, "Software-Entity"]]}
{"id": 160957, "text": "We utilized the Huggingface framework [ Wolf et al. , , 2020 ] for inference .", "Comments": [], "label": [[16, 27, "Software-Entity"]]}
{"id": 160958, "text": "We used a single NVIDIA A100 GPU with 80GB memory , batch size 8 and maximum sequence length of 1024 , with greedy decoding .", "Comments": [], "label": [[10, 16, "Device-Count"], [17, 32, "Hardware-device"], [38, 42, "Device-Memory"]]}
{"id": 160959, "text": "For NLI-based measurements , we use TRUE model based on T5-XXL [ 10 ] and the best reported model for SummaC [ 11 ] .", "Comments": [], "label": [[102, 108, "Software-Entity"]]}
{"id": 160960, "text": "To confirm , we run paired bootstrapping significance test [ Koehn , 2004 ] on ROUGE-L , BERTScore , BLEURT , and SummaC .", "Comments": [], "label": [[114, 121, "Software-Entity"]]}
{"id": 160961, "text": "We show two models here llama-2-7b-chat from Huggingface and llama-2-7b-InBedder that is our fine-tuned model from llama-2-7b .", "Comments": [], "label": [[45, 56, "Software-Entity"]]}
{"id": 160962, "text": "We further show that increasing prompt length in certain task can improve the performance in Appendix [ D. ] We train and evaluate these models with at most 4×A100 ( PCIe ) .", "Comments": [], "label": [[157, 158, "Device-Count"], [159, 163, "Hardware-device"]]}
{"id": 160963, "text": "LLMs like GPT-4 [ Ope ] [ nAI , 2023b ] and Gemini [ Google , 2023b ] are so powerful that they can now serve as programming Figure 1 : Llama-30B attention inference latency w.r.t . system prompt length ( A40 GPU , batch size 32 ) .", "Comments": [], "label": [[204, 212, "Hardware-device"]]}
{"id": 160964, "text": "Specifically , while the system prompt is shared by all requests , its hidden states ( * i.e * . , key-value pairs ) are read from DRAM multiple times by existing attention algorithms such as PagedAttention [ Kwon et al. , , 2023 ] and FlashAttention [ Dao et al. , , 2022 ] [ Dao , 2023 ] , each for an individual request in the batch .", "Comments": [], "label": [[236, 250, "Software-Entity"]]}
{"id": 160965, "text": "FlashAttention [ Dao et al. , , 2022 ] [ Dao , 2023 ] is another technique to optimize LLMs ' throughputs on GPUs by avoiding redundant write/read of attention probability matrix into/from DRAM .", "Comments": [], "label": [[0, 14, "Software-Entity"]]}
{"id": 160966, "text": "The speed of modern GPUs far outpaces the bandwidth of its memory ( * i.e * . , tc 1 ) , and thus it typically requires a high arithmetic intensity to achieve full utilization of the computing capability ( * e.g * . , A100-SXM4 GPU requires at least 38.2 ) .", "Comments": [], "label": [[218, 231, "Hardware-device"]]}
{"id": 160967, "text": "Algorithm [ 1 ] summarizes the algorithm in Pytorch-like pseudo code .", "Comments": [], "label": [[44, 51, "Software-Entity"]]}
{"id": 160968, "text": "We plot the speedup w.r.t . the length of the system prompt under two context lengths ( 128 and 256 ) and four batch sizes ( 4 , 8 , 16 , and 32 ) , on an A40 GPU .", "Comments": [], "label": [[155, 162, "Hardware-device"]]}
{"id": 160969, "text": "Our experiments involve three GPUs : A40 , A100-PCIE-40GB , and A100-SXM4-80GB .", "Comments": [], "label": [[24, 34, "Device-Count"], [37, 40, "Hardware-device"], [43, 52, "Hardware-device"], [53, 57, "Device-Memory"], [64, 73, "Hardware-device"], [74, 78, "Device-Memory"]]}
{"id": 160970, "text": "However , A40 is used unless stated otherwise .", "Comments": [], "label": [[10, 13, "Hardware-device"]]}
{"id": 160971, "text": "In the case of the 5-shot test , our vLLM-RA provides a 76 % reduction of processing time on both A40 and A100-SXM4-80GB GPUs . 4.3 Interactive Serving An important LLM application is chatbots [ Ope ] [ nAI , 2022 ] [ Google , 2023a ] , in which interactive LLM services are typically provided .", "Comments": [], "label": [[98, 101, "Hardware-device"], [106, 115, "Hardware-device"], [116, 125, "Device-Memory"]]}
{"id": 160972, "text": "Following PagedAttention [ Kwon et al. , , 2023 ] , we sample 1000 requests from the ShareG- Table 4 : Throughput ( req/s ) of different models during the batch processing of the ShareGPTv3 dataset . † : the 30B model is hosted on two A100-SXM4-80GB GPUs .", "Comments": [], "label": [[231, 234, "Device-Count"], [235, 244, "Hardware-device"], [245, 254, "Device-Memory"]]}
{"id": 160973, "text": "For the system attention involving the system prompt of non-growing static length , we use off-the-shelf FlashAttention kernels [ Dao , 2023 ] , which natively return the logsum-exp required for computation of combination coefficients in Eq . [ 7 ] .", "Comments": [], "label": [[105, 119, "Software-Entity"]]}
{"id": 160974, "text": "We use greedy decoding , and task the model with generating up to 682 tokens . ( max context length of BLOOMZ 2048 // 3 ) . [ 12 ] We used a g5.48xlarge instance from AWS , which has 8 NVIDIA A10G GPUs ( 24 * 8 GB=192GB vRAM ) .", "Comments": [], "label": [[141, 161, "Hardware-device"], [167, 170, "Cloud-Platform"], [183, 184, "Device-Count"], [185, 201, "Hardware-device"], [214, 225, "Device-Memory"]]}
{"id": 160975, "text": "The implementation is done with the DeepSpeed-Chat framework [ Yao et al. , , 2023 ] and the transformers library [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[36, 60, "Software-Entity"], [93, 113, "Software-Entity"]]}
{"id": 160976, "text": "The context is truncated for display purposes . 4.3.3 Hardware All experiments in this evaluation were conducted on a server with 4 * A100 80GB GPUs .", "Comments": [], "label": [[130, 131, "Device-Count"], [134, 138, "Hardware-device"], [139, 148, "Device-Memory"]]}
{"id": 160977, "text": "For models with a scale of 7B and 13B , we use one GPU ; for models around 30B , we use two GPUs ; and for models at the 70B scale , we use four GPUs .", "Comments": [], "label": [[88, 96, "Device-Count"], [140, 149, "Device-Count"]]}
{"id": 160978, "text": "Despite these advancements , our analysis reveals that StripedHyena underperforms in detailed long context question answering compared to traditional transformers and does not reduce memory usage effectively , even with advanced attention mechanisms like Flash Attention 2 [ Dao , 2023 ] .", "Comments": [], "label": [[255, 272, "Software-Entity"]]}
{"id": 160979, "text": "Qwen-14B-chat [ Bai et al. , , 2023a ] represents the 14-billion parameter iteration of the Qwen large language model series , abbreviated as Tongyi Qianwen , developed by Alibaba Cloud .", "Comments": [], "label": [[172, 185, "Cloud-Platform"]]}
{"id": 160980, "text": "We experimented with various word embedding models , both without context ( Word2Vec [ Mikolov et al. , , 2013 ] , GloVe [ Pennington et al. , , 2014 ] and FastText [ Bo ] [ janowski et al. , , 2017 ] ) and with context ( BERT [ De ] [ vlin et al. , , 2019 ] , and RoBERTa [ Liu et al. , , 2019 ] ) We found FastText to perform best , and use it for all future embedding components .", "Comments": [], "label": [[156, 164, "Software-Entity"], [308, 316, "Software-Entity"]]}
{"id": 160981, "text": "For answers with multiple tokens , we use the average of the FastText embeddings to represent those answer .", "Comments": [], "label": [[61, 69, "Software-Entity"]]}
{"id": 160982, "text": "We used the implementation from pyclustering [ Novikov , 2019 ] .", "Comments": [], "label": [[32, 44, "Software-Entity"]]}
{"id": 160983, "text": "For finetuning , we used the ProtoQA pre-trained model and also trained GPT2 Large with a similar task format for 3 epochs on an Nvidia M40 GPU , denoted as GPT2-L FT in our results .", "Comments": [], "label": [[129, 143, "Hardware-device"]]}
{"id": 160984, "text": "Figure 7 : The screen shot for the the dataset collection page in Amazon MTurk .", "Comments": [], "label": [[66, 78, "Cloud-Platform"]]}
{"id": 160985, "text": "Over the past year , many advanced LLMs [ Touvron et al. , , 2023 ] [ Zheng et al. , 2023 ] [ Dettmers et al. , , 2023 ] [ Zeng et al. , , 2022 ] have been open-sourced on model-sharing platforms such as HuggingFace [ HuggingFace , 2023a ] .", "Comments": [], "label": [[204, 215, "Software-Entity"]]}
{"id": 160986, "text": "Attacker 's background knowledge and capability We assume the attacker can download opensourced LLMs from model-sharing platforms ( e.g. , Hugging Face ) .", "Comments": [], "label": [[139, 151, "Software-Entity"]]}
{"id": 160987, "text": "We provide a snapshot of AdvBench as well as discuss the selection of datasets in Appendix [ A.3 . ] Large language models We evaluate EnDec on open-sourced LLMs downloaded from Hugging Face .", "Comments": [], "label": [[178, 190, "Software-Entity"]]}
{"id": 160988, "text": "A Additional Experimental Setup A.1 Basic Experimental Setup We use PyTorch [ Paszke et al. , , 2019 ] as the deep learning framework for implementations .", "Comments": [], "label": [[68, 75, "Software-Entity"]]}
{"id": 160989, "text": "LLMs in our experiments are provided by Hugging-Face [ HuggingFace , 2023a ] .", "Comments": [], "label": [[40, 52, "Software-Entity"]]}
{"id": 160990, "text": "All experiments are executed on 1 NVIDIA A-100 GPU .", "Comments": [], "label": [[32, 33, "Device-Count"], [34, 50, "Hardware-device"]]}
{"id": 160991, "text": "We ran GCG on two A-100 GPUs with 80GB memory .", "Comments": [], "label": [[14, 17, "Device-Count"], [18, 28, "Hardware-device"], [34, 38, "Device-Memory"]]}
{"id": 160992, "text": "The only personal information we collect is the worker IDs from Amazon Mechanical Turk , which we will not release .", "Comments": [], "label": [[64, 86, "Cloud-Platform"]]}
{"id": 160993, "text": "Turk workers and pay rate : Our participants were recruited on the Amazon Mechanical Turk platform .", "Comments": [], "label": [[67, 89, "Cloud-Platform"]]}
{"id": 160994, "text": "I Training hyperparameters The fine-tuning of the Digital Socrates DS-7B and DS-13B critique models were done on NVIDIA RTX A6000 GPUs , using the open-instruct code base .", "Comments": [], "label": [[113, 134, "Hardware-device"]]}
{"id": 160995, "text": "The hyperparameters included : batch size = 32 , lora rank = 64 , lora alpha = 16 , lora dropout = 0.1 , learning rate = 1e-4 , lr schedule = linear , warmup ratio = 0.03 , weight decay = 0 , flash attention .", "Comments": [], "label": [[192, 207, "Software-Entity"]]}
{"id": 160996, "text": "Specifically , the TULU models with 7B and 13B parameters were tested using a single NVIDIA SXM4 with 80GB RAM .", "Comments": [], "label": [[78, 84, "Device-Count"], [85, 96, "Hardware-device"], [102, 110, "Device-Memory"]]}
{"id": 160997, "text": "The 30B model utilized two of these NVIDIA SXM4 80GB GPU , while the largest , the 65B model , was evaluated using eight RTX A6000 with 48GB RAM each .", "Comments": [], "label": [[23, 26, "Device-Count"], [36, 47, "Hardware-device"], [48, 56, "Device-Memory"], [115, 120, "Device-Count"], [121, 130, "Hardware-device"], [136, 144, "Device-Memory"]]}
{"id": 160998, "text": "On the hardware side , a total of 4 × A100-80G GPUs were used for the experiments .", "Comments": [], "label": [[34, 35, "Device-Count"], [38, 51, "Hardware-device"]]}
{"id": 160999, "text": "Meanwhile , we can further accelerate supervised fine-tuning by deepspeed or other approaches .", "Comments": [], "label": [[64, 73, "Software-Entity"]]}
{"id": 161000, "text": "When we used the Deepspeed , we set 4 processes and zero-stage is 2 .", "Comments": [], "label": [[17, 26, "Software-Entity"]]}
{"id": 161001, "text": "The model is trained for 1 epoch taking 7 hours on an NVIDIA A100 machine .", "Comments": [], "label": [[54, 65, "Hardware-device"]]}
{"id": 161002, "text": "Training for each model takes 3 hours on an NVIDIA V100 graphical processing unit .", "Comments": [], "label": [[44, 55, "Hardware-device"]]}
{"id": 161003, "text": "The model is trained for 1 epoch taking 30 minutes on an NVIDIA A100 machine .", "Comments": [], "label": [[57, 68, "Hardware-device"]]}
{"id": 161004, "text": "Additionally , the average GPU expenditure for single model evaluations on NVIDIA A100 GPUs is provided in Table [ 10 . ] Financially , deploying GPT-4 in both interactor and evaluator roles within KIEval incurs a cost of around 27 USD for each model evaluation , comprising 1000 interaction rounds .", "Comments": [], "label": [[75, 91, "Hardware-device"]]}
{"id": 161005, "text": "We use Huggingface Transformers [ Wolf et al. , , 2019 ] library and the Deepspeed [ Rasley et al. , , 2020 ] Zero-3 optimizer [ Ra ] [ jbhandari et al. , , 2021 ] , forms the backbone of our computational experiments .", "Comments": [], "label": [[7, 18, "Software-Entity"], [73, 82, "Software-Entity"]]}
{"id": 161006, "text": "Our hardware setup consists of 4 NVIDIA A100 GPUs , and we 've set per-device batch size to 1 , coupled with a gradient accumulation step of 4 .", "Comments": [], "label": [[31, 32, "Device-Count"], [33, 49, "Hardware-device"]]}
{"id": 161007, "text": "For others , we directly employed their pre-defined interfaces , either through their online API or the CHAT function in the Transformers library [ 3 ] .", "Comments": [], "label": [[125, 145, "Software-Entity"]]}
{"id": 161008, "text": "All of our experiments were run on single A100 80GB GPUs . 5 Results and Findings Our obtained results are provided in Tables [ 1 ] and [ 2 . ] Overall , GPT-4 significantly outperformed the other LLMs in both tasks .", "Comments": [], "label": [[35, 41, "Device-Count"], [42, 56, "Hardware-device"]]}
{"id": 161009, "text": "For example , loading the Mixtral 8x7B model in bf16 format requires at least two A100-80G GPUs .", "Comments": [], "label": [[78, 81, "Device-Count"], [82, 86, "Hardware-device"], [87, 95, "Device-Memory"]]}
{"id": 161010, "text": "As shown in Fig . [ 1 , ] we halve the number of GPUs needed , allowing deployment on a single 80G GPU and achieving a 1.2× inference speedup .", "Comments": [], "label": [[88, 94, "Device-Count"], [95, 102, "Device-Memory"]]}
{"id": 161011, "text": "Researchers in [ Chen et al. , , 2022 ] provide a dropping-while-training method that progressively drops the non-professional experts for target downstream tasks , and experiments are carried out on Switch Transformers models [ Fedus et al. , 2022 ] .", "Comments": [], "label": [[207, 219, "Software-Entity"]]}
{"id": 161012, "text": "After removing the insignificant experts , the pruned model can be easily loaded using existing packages ( e.g. , Huggingface Transformers [ Wolf et al. , , 2020 ] ) with just a change of the model configuration .", "Comments": [], "label": [[114, 125, "Software-Entity"]]}
{"id": 161013, "text": "Especially , with 2 experts pruned , the deployment budget is reduced to a single 80G GPU for loading the Mixtral 8x7B ( Instruct ) model with bf16 data type .", "Comments": [], "label": [[75, 81, "Device-Count"], [82, 89, "Device-Memory"]]}
{"id": 161014, "text": "The memory usage statistics are shown in Fig . [ 1 . ] It takes 2 A100-80G GPUs to load and forward the We revise the script provided in [ https : //github.com/ ] ( https : //github.com/AutoGPTQ/AutoGPTQ/ ) [ AutoGPTQ/AutoGPTQ/ ] ( https : //github.com/AutoGPTQ/AutoGPTQ/ ) to test token generating speed .", "Comments": [], "label": [[64, 65, "Device-Count"], [66, 70, "Hardware-device"], [71, 79, "Hardware-device"]]}
{"id": 161015, "text": "After pruning 2 and 4 experts , only one 80G GPU is needed for the inference process .", "Comments": [], "label": [[37, 40, "Device-Count"], [41, 48, "Device-Memory"]]}
{"id": 161016, "text": "The training is conducted on 16 A100-80G GPUs .", "Comments": [], "label": [[29, 31, "Device-Count"], [32, 36, "Hardware-device"], [37, 45, "Device-Memory"]]}
{"id": 161017, "text": "Subsequently , it acquires the corresponding image segment for each entity using a combination of language grounding by Grounding DINO [ Liu et al. , , 2023b ] and image segmentation by SAM [ Kirillov et al. , 2023 ] .", "Comments": [], "label": [[120, 134, "Software-Entity"], [186, 189, "Software-Entity"]]}
{"id": 161018, "text": "The model is trained from scratch at 256x256 using the AdamW optimizer with a weight decay of 0.01 , a learning rate of 5e-5 , and a batch size of 40x256 for 100K steps , which costs about 300 A100 GPU days .", "Comments": [], "label": [[193, 201, "Hardware-device"]]}
{"id": 161019, "text": "The process , depicted in Figure [ 3 , ] involves : ( 1 ) Generation of captions and extraction of entity tokens from captions by the MLLM ; ( 2 ) Identifying entity detection boxes via Grounding DINO [ Liu et al. , , 2023b ] ; ( 3 ) Segmentation and extraction of regions corresponding to each entity by SAM [ Kirillov et al. , , 2023 ] ; ( 4 ) Randomly substitution of entity tokens with their corresponding image segments .", "Comments": [], "label": [[186, 200, "Software-Entity"], [305, 308, "Software-Entity"]]}
{"id": 161020, "text": "This process totally costs about 800 A100 GPU days .", "Comments": [], "label": [[37, 45, "Hardware-device"]]}
{"id": 161021, "text": "To ensure data quality , we request human listeners to participate in a listening test conducted on the Amazon Mechanical Turk platform .", "Comments": [], "label": [[104, 135, "Cloud-Platform"]]}
{"id": 161022, "text": "This result can be explained in two ways : 1 ) From the objective evaluation of response text , the performance of Spoken-LLM and Text-LLM ( cascade ) is similar , so the human listeners might not differentiate the content difference , and 2 ) for the response style , it is possible to respond with more than one response style but still Score calculated by [ Hugging face Evaluate package ] ( https : //huggingface.co/docs/evaluate/ ) Table 3 : Different training strategy and data usages on Spoken LLM- * chunk * method .", "Comments": [], "label": [[361, 373, "Software-Entity"]]}
{"id": 161023, "text": "Real speech with diverse and mixed styles : The speech data in StyleTalk is synthesized from the Azure TTS system with style control .", "Comments": [], "label": [[97, 113, "Cloud-Platform"]]}
{"id": 161024, "text": "All experiments are run with a single A40 48G GPU .", "Comments": [], "label": [[31, 37, "Device-Count"], [38, 41, "Hardware-device"], [42, 49, "Device-Memory"]]}
{"id": 161025, "text": "All models are downloaded from HuggingFace , and training and inference are based on the transformers library .", "Comments": [], "label": [[31, 42, "Software-Entity"], [89, 101, "Software-Entity"]]}
{"id": 161026, "text": "Each item in our experiment is done on a single NVIDIA A800 80G GPU .", "Comments": [], "label": [[41, 47, "Device-Count"], [48, 67, "Hardware-device"]]}
{"id": 161027, "text": "C Hyperparamter Tuning We present the hyperparameters for our experiments in Table [ 5 . ] We carry out the experiments over 3 seeds on a A6000 GPU with early stopping with patience of 5 over the validation set for all experiments .", "Comments": [], "label": [[138, 147, "Hardware-device"]]}
{"id": 161028, "text": "We implement the entire experiments in Python , with help of the Pytorch library and use the pre-trained models as specified in Huggingface under the agreed upon license agreements .", "Comments": [], "label": [[65, 72, "Software-Entity"], [128, 139, "Software-Entity"]]}
{"id": 161029, "text": "These linguistic features were obtained using spaCy [ Honnibal et al. , , 2020 ] .", "Comments": [], "label": [[46, 51, "Software-Entity"]]}
{"id": 161030, "text": "B.1 Hyperparameters Our target and frame identification models are built using PyTorch with the Hugging Face library . [ 3 ] We use Hugging Face 's implementation of the pretrained BERT and RoBERTa models .", "Comments": [], "label": [[79, 86, "Software-Entity"], [96, 108, "Software-Entity"], [132, 147, "Software-Entity"]]}
{"id": 161031, "text": "We trained our models using several GPUs , including GTX 1070 , GTX 1080 Ti , GTX 2080 Ti , and NVIDIA Tesla T4 .", "Comments": [], "label": [[53, 61, "Hardware-device"], [64, 75, "Hardware-device"], [78, 90, "Hardware-device"], [96, 111, "Hardware-device"]]}
{"id": 161032, "text": "Such training lasts for ~24 hours on 8×A100 GPUs , making use of the DeepSpeed library [ Rasley et al. , , 2020 ] and the stage 3 of ZeRO optimizer [ Rajbhan ] [ dari et al. , , 2020 ] .", "Comments": [], "label": [[37, 38, "Device-Count"], [39, 48, "Hardware-device"], [69, 86, "Software-Entity"], [133, 147, "Software-Entity"]]}
{"id": 161033, "text": "The fine-tuning process for MixLoRA ( E=16 , r=4 ) takes approximately 20 hours on 4 A100 GPUs , with an effective batch size of 8 per GPU and a gradient accumulation step of 4 .", "Comments": [], "label": [[83, 84, "Device-Count"], [85, 94, "Hardware-device"]]}
{"id": 161034, "text": "All implementations involving ChatGPT are using * gpt-3.5 turbo-0613 * from Azure OpenAI . [ 5 ] We retrieve results from Claude2 by posting requests to their webpage [ 6 ] , and for Bard , we use * chat-bison-001 * from PaLM2 API [ 7 ] .", "Comments": [], "label": [[76, 81, "Cloud-Platform"]]}
{"id": 161035, "text": "For the open-source models LLaMA2-70B and DeepSeekMath , we use four RTX A6000 GPUs , each with 48GB memory to generate output from them .", "Comments": [], "label": [[64, 68, "Device-Count"], [69, 83, "Hardware-device"], [96, 107, "Device-Memory"]]}
{"id": 161036, "text": "For both timeline extraction and summarization tasks , we fine-tuned Llama2-13B on an A100 80GB GPU .", "Comments": [], "label": [[86, 90, "Hardware-device"], [91, 99, "Device-Memory"]]}
{"id": 161037, "text": "For topic timeline summarization , we utilized an advanced library named vllm [ Kwon et al. , , 2023 ] , designed specifically for efficient Large Language Model ( LLM ) inference .", "Comments": [], "label": [[73, 77, "Software-Entity"]]}
{"id": 161038, "text": "The vllm library takes advantage of PagedAttention , a technique that optimizes LLM serving by managing memory more Table 8 : GPU hours for LLM-TLS experiments using Llama2-13B .", "Comments": [], "label": [[4, 8, "Software-Entity"], [36, 50, "Software-Entity"]]}
{"id": 161039, "text": "Each trial in our topic TLS experiments was run on a single A100 40GB GPU .", "Comments": [], "label": [[53, 59, "Device-Count"], [60, 64, "Hardware-device"], [65, 73, "Device-Memory"]]}
{"id": 161040, "text": "All models are implemented with Huggingface transformers and evaluations are conducted using the evaluate library .", "Comments": [], "label": [[32, 43, "Software-Entity"]]}
{"id": 161041, "text": "All the experiments are conducted on a single Nvidia RTX8000 GPU with 49GB of GPU memory and 128GB of CPU memory for maximum 3 days on each experiment .", "Comments": [], "label": [[39, 45, "Device-Count"], [46, 64, "Software-Entity"], [70, 88, "Device-Memory"], [93, 112, "Device-Memory"]]}
{"id": 161042, "text": "In addition , we include FastText [ Joulin et al. , , 2017 ] , which uses linguistic statistics as features .", "Comments": [], "label": [[25, 33, "Software-Entity"]]}
{"id": 161043, "text": "On the other hand , FastText faces significant challenges in detecting texts from various domains ( * Arbitrary-domains * ) , despite its robustness on texts sourced by different language models .", "Comments": [], "label": [[20, 28, "Software-Entity"]]}
{"id": 161044, "text": "The performance of FastText further degrades , with AU-ROC dropping from 0.83 to 0.74 .", "Comments": [], "label": [[19, 27, "Software-Entity"]]}
{"id": 161045, "text": "To accomplish this , we employ Stanza [ Qi et al. , 2020 ] to extract the distribution of various linguistic patterns such as named entities , part-ofspeech tags , and constituents .", "Comments": [], "label": [[31, 37, "Software-Entity"]]}
{"id": 161046, "text": "All models are finetuned for 5 epochs on 8 V100 GPUs .", "Comments": [], "label": [[41, 42, "Device-Count"], [43, 52, "Hardware-device"]]}
{"id": 161047, "text": "FastText .", "Comments": [], "label": [[0, 8, "Software-Entity"]]}
{"id": 161048, "text": "Following [ Pu et al. , 2022 ] , we use GPT-2-XL [ Radford et al. , 2019 ] as the language model and use scikitlearn [ Pedregosa et al. , , 2011 ] to train regression models .", "Comments": [], "label": [[105, 116, "Software-Entity"]]}
{"id": 161049, "text": "We further use Stanza , a linguistics analysis tool [ Qi et al. , , 2020 ] , to gain a more systematic understanding of the linguistic components in both sources , with results shown in Figure [ 13 . ]", "Comments": [], "label": [[15, 21, "Software-Entity"]]}
{"id": 161050, "text": "On a machine equipped with four A100 80GB GPUs , training a 9B model takes less than four hours when the encoder layers are frozen , and under five hours when the entire encoder is trainable .", "Comments": [], "label": [[27, 31, "Device-Count"], [32, 36, "Hardware-device"], [37, 46, "Device-Memory"]]}
{"id": 161051, "text": "We test using a single A6000 GPU on an idle server .", "Comments": [], "label": [[16, 22, "Device-Count"], [22, 32, "Hardware-device"]]}
{"id": 161052, "text": "Our development environment was PyTorch , and the experiments were conducted on a Ubuntu machine equipped with four 40GB NVIDIA A100 GPUs . https : //github.com/IDEA-FinAI/Simple-HHEA Table 1 : The detailed statistics of the datasets . * Temporal * denotes whether the dataset contains temporal information .", "Comments": [], "label": [[32, 39, "Software-Entity"], [116, 120, "Device-Memory"], [121, 137, "Hardware-device"]]}
{"id": 161053, "text": "We access these models via HuggingFace [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[27, 38, "Software-Entity"]]}
{"id": 161054, "text": "We run all our experiments on a server with three NVIDIA RTX A6000 and 48GB of RAM .", "Comments": [], "label": [[44, 49, "Device-Count"], [50, 66, "Hardware-device"], [71, 82, "Device-Memory"]]}
{"id": 161055, "text": "For the ZS HTC and the clustering part we follow [ Bongiovanni et al. , 2023 ] and use as embedder mpnet-all [ Reimers and Gurevych , 2019 ] [ Song et al. , 2020 ] from HuggingFace Sentence Transformers library .", "Comments": [], "label": [[169, 180, "Software-Entity"]]}
{"id": 161056, "text": "All experiments are performed on a single Tesla T4 GPU [ 6 ] . 5 Results 5.1 Label augmentation results In our experiments , we did not specify a target number of generated labels , but the LLM always produced at least three labels .", "Comments": [], "label": [[35, 41, "Device-Count"], [42, 54, "Hardware-device"]]}
{"id": 161057, "text": "Our experiments were run on 8×A100 GPUs , combining datasets from Spider and Bird with GPT-4-generated data for supervised fine-tuning using the AdamW optimizer with a learning rate of 2e-5 and a cosine warmup scheduler over three epochs .", "Comments": [], "label": [[28, 29, "Device-Count"], [30, 39, "Hardware-device"]]}
{"id": 161058, "text": "For NER , we employ FLAIR [ Akbik et al. , , 2019 ] with a starting lr of 1e −5 and constant decay .", "Comments": [], "label": [[20, 25, "Software-Entity"]]}
{"id": 161059, "text": "Compute Infrastructure : All our experiments are conducted on a single NVIDIA A100 GPU .", "Comments": [], "label": [[64, 70, "Device-Count"], [71, 86, "Hardware-device"]]}
{"id": 161060, "text": "Implementation Software and Packages : We implement all our models in PyTorch [ 4 ] and use the HuggingFace [ 5 ] implementations of BERTbase and BARTlarge .", "Comments": [], "label": [[70, 77, "Software-Entity"], [96, 107, "Software-Entity"]]}
{"id": 161061, "text": "The reported results are formulated by 'hh : mm : ss ' , which refer to hours , minutes and seconds respectively . ( The computational device is Nvidia A100-40G . ) MR [ Maas et al. , , 2011 ] contains movie reviews representing positive or negative sentiment .", "Comments": [], "label": [[144, 156, "Hardware-device"], [157, 160, "Device-Memory"]]}
{"id": 161062, "text": "Moreover , we solve these text classification tasks on Nvidia A100-40G platform , and employ LLaMA2-7B [ Touvron et al. , , 2023b ] as the lightweight LLM backbone throughout all of our main experiments .", "Comments": [], "label": [[55, 66, "Hardware-device"], [67, 70, "Device-Memory"]]}
{"id": 161063, "text": "( The computational device is Nvidia A100-40G . )", "Comments": [], "label": [[30, 41, "Hardware-device"], [42, 45, "Device-Memory"]]}
{"id": 161064, "text": "Our model is trained on 8 NVIDIA A800 GPUs .", "Comments": [], "label": [[24, 25, "Device-Count"], [26, 42, "Hardware-device"]]}
{"id": 161065, "text": "Our model is trained on 8 NVIDIA A800 GPUs .", "Comments": [], "label": [[24, 25, "Device-Count"], [26, 42, "Hardware-device"]]}
{"id": 161066, "text": "For different control intentions , we use different calculation methods to get Scorectl list : Finally , we get Rlist , which measures the overall merits of the recommendation list : 2.3.3 RL Implementation Notes We follow the Transformer Reinforcement Learning [ 2 ] package to implement the reinforcement learning stage .", "Comments": [], "label": [[227, 238, "Software-Entity"]]}
{"id": 161067, "text": "The dataset statistics can be found in [ Table 1 ] and [ Table 7 . ] For the SL stage , we use 4 A100 GPUs ( 40GB GPU memory ) for training .", "Comments": [], "label": [[95, 96, "Device-Count"], [97, 106, "Hardware-device"], [109, 124, "Hardware-device"]]}
{"id": 161068, "text": "For the RL stage , we use 2 A100 GPUs ( 40GB GPU memory ) for training .", "Comments": [], "label": [[26, 27, "Device-Count"], [28, 37, "Hardware-device"], [40, 55, "Device-Memory"]]}
{"id": 161069, "text": "Once the training process is finished , we use vllm [ 5 ] for inference and serving .", "Comments": [], "label": [[47, 51, "Software-Entity"]]}
{"id": 161070, "text": "On a single A100 GPU ( 40GB memory ) , when responding to the top-10 recommendation request , our implementation can process 20 requests per second ( on average each request involves newly generated 112 tokens ) .", "Comments": [], "label": [[5, 11, "Device-Count"], [12, 20, "Hardware-device"], [23, 34, "Device-Memory"]]}
{"id": 161071, "text": "For additional information about the dataset , please refer to the GitHub repository and Huggingface .", "Comments": [], "label": [[89, 100, "Software-Entity"]]}
{"id": 161072, "text": "Models are trained on two A100 GPUs for a single epoch with batch size set to 4 .", "Comments": [], "label": [[22, 25, "Device-Count"], [26, 35, "Hardware-device"]]}
{"id": 161073, "text": "Huggingface [ Wolf et al. , , 2019 ] is used for fine-tuning the models , and VLLM [ Kwon et al. , , 2023 ] is used for inference of the models for efficiency .", "Comments": [], "label": [[0, 11, "Software-Entity"], [78, 82, "Software-Entity"]]}
{"id": 161074, "text": "We release our datasets on GitHub and Huggingface . [ 1 ] A natural extension of our research involves investigating whether trial-and-error information is beneficial for more general mathematical theoremproving settings .", "Comments": [], "label": [[38, 49, "Software-Entity"]]}
{"id": 161075, "text": "We run inference on the open source models on a single A100 GPU , and query the closed-source models using their API .", "Comments": [], "label": [[48, 54, "Device-Count"], [55, 63, "Hardware-device"]]}
{"id": 161076, "text": "All the experiments are conducted using 8 A800 GPUs .", "Comments": [], "label": [[40, 41, "Device-Count"], [42, 51, "Hardware-device"]]}
{"id": 161077, "text": "Software , Hardware and Hyperparameters We use PyTorch [ Paszke et al. , , 2019 ] with Hugging-Face 's TRL framework [ von Werra et al. , , 2020 ] for all fine-tuning experiments across t `` 0 , . . . , T .", "Comments": [], "label": [[47, 54, "Software-Entity"], [87, 102, "Software-Entity"]]}
{"id": 161078, "text": "All experiments are conducted on 8x A6000 GPUs .", "Comments": [], "label": [[33, 34, "Device-Count"], [36, 46, "Hardware-device"]]}
{"id": 161079, "text": "We use vLLM [ Kwon et al. , , 2023 ] for inference .", "Comments": [], "label": [[7, 11, "Software-Entity"]]}
{"id": 161080, "text": "The comparison on efficiency includes the API cost of GPT-3.5 and local inference time and consumed tokens for FNCTOD-LLAMA2-13B on a single Nvidia A6000 GPU , where FS stands for Function Selection ( the first step ) and AG stands for Argument Generation ( the second step ) .", "Comments": [], "label": [[134, 140, "Device-Count"], [141, 157, "Hardware-device"]]}
{"id": 161081, "text": "We utilized checkpoints available on Huggingface [ 4 ] .", "Comments": [], "label": [[37, 48, "Software-Entity"]]}
{"id": 161082, "text": "All inferences were executed on a cluster equipped with eight 48G NVIDIA RTX A6000 GPUs .", "Comments": [], "label": [[56, 61, "Device-Count"], [62, 65, "Device-Memory"], [66, 87, "Hardware-device"]]}
{"id": 161083, "text": "Further details about the fine-tuning hyperparameters can be found in Table [ 7 . ] The fine-tuning was conducted on 4 A6000 48GB GPUs .", "Comments": [], "label": [[119, 124, "Hardware-device"], [125, 134, "Device-Memory"]]}
{"id": 161084, "text": "However , current language identification tools such as fastText [ Joulin et al. , , 2016 ] are prone to error [ Kreutzer et al. , , 2022 ] .", "Comments": [], "label": [[56, 64, "Software-Entity"]]}
{"id": 161085, "text": "Training Frameworks For MC2XLMR-large , we use Transformers [ Wolf et al. , , 2020 ] for training .", "Comments": [], "label": [[47, 59, "Software-Entity"]]}
{"id": 161086, "text": "For MC2Llama-13B , we use Megatron [ Shoeybi et al. , 2019 ] for training .", "Comments": [], "label": [[26, 34, "Software-Entity"]]}
{"id": 161087, "text": "Computing Infrastructure We continually pretrain MC2XLMR-large on one A100 GPU , and an epoch takes 17 hours .", "Comments": [], "label": [[66, 69, "Device-Count"], [70, 78, "Hardware-device"]]}
{"id": 161088, "text": "We continually pretrain MC2Llama-13B on eight A100 GPUs , and an epoch takes 20 hours .", "Comments": [], "label": [[40, 45, "Device-Count"], [46, 55, "Hardware-device"]]}
{"id": 161089, "text": "In our experiment , for a fair comparison , we assume that there is only one checkpoint . 4.2 Experimental Setting All experiments are run on a server of Ubuntu 20.04.6 LTS with 2 H100 GPUs .", "Comments": [], "label": [[178, 179, "Device-Count"], [180, 189, "Hardware-device"]]}
{"id": 161090, "text": "The CPUs are dual Intel ( R ) Xeon ( R ) Gold 6438N and the memory is 1.48TB .", "Comments": [], "label": [[18, 51, "Hardware-device"], [70, 76, "Device-Memory"]]}
{"id": 161091, "text": "RapidInMP used 2 GPUs , and T = 1 for caching , T = 8 for retrieval . tioned above .", "Comments": [], "label": [[15, 21, "Device-Count"]]}
{"id": 161092, "text": "We execute all the experiments on a Linux server with 2 H100 GPUs .", "Comments": [], "label": [[54, 55, "Device-Count"], [56, 65, "Hardware-device"]]}
{"id": 161093, "text": "The CPUs are dual Intel ( R ) Xeon ( R ) Gold 6438N 3.60GHz , 32 cores , 64 threads and the memory is 1.48TB .", "Comments": [], "label": [[13, 59, "Hardware-device"], [62, 70, "Device-Memory"], [102, 109, "Device-Memory"]]}
{"id": 161094, "text": "We leverage Huggingface Transformers [ Wolf et al. , , 2020 ] , PEFT [ Man ] [ grulkar et al. , , 2022 ] , and bitsandbytes [ 4 ] to implement finetuning and inference for llama-2 7b and 70b with QLoRA adapters [ Hu et al. , , 2022 ] [ Dettmers et al. , , 2023 ] .", "Comments": [], "label": [[12, 23, "Software-Entity"], [64, 68, "Software-Entity"], [111, 123, "Software-Entity"]]}
{"id": 161095, "text": "The length is computed using the NLTK tokenizer .", "Comments": [], "label": [[33, 37, "Software-Entity"]]}
{"id": 161096, "text": "The length is computed using the NLTK tokenizer .", "Comments": [], "label": [[33, 37, "Software-Entity"]]}
{"id": 161097, "text": "It takes about 16 hours to train one 13B model on 4 A100 GPUs .", "Comments": [], "label": [[50, 51, "Device-Count"], [52, 61, "Hardware-device"]]}
{"id": 161098, "text": "We use the huggingface package [ Wolf et al. , 2020 ] for the specific implementation .", "Comments": [], "label": [[11, 22, "Software-Entity"]]}
{"id": 161099, "text": "C.3 Computational Budget For all the experiments mentioned in this paper , we use one Nvidia A100-SXM4 GPU with 80GB memory .", "Comments": [], "label": [[82, 85, "Device-Count"], [86, 106, "Hardware-device"], [112, 123, "Device-Memory"]]}
{"id": 161100, "text": "We run each experiment on a single NVIDIA V100 GPU .", "Comments": [], "label": [[28, 34, "Device-Count"], [35, 50, "Hardware-device"]]}
{"id": 161101, "text": "All the LoRA parameters are fine-tuned on an NVIDIA A100 GPU with 80GB memory .", "Comments": [], "label": [[45, 60, "Hardware-device"], [66, 77, "Device-Memory"]]}
{"id": 161102, "text": "We train the model on 8 A100-80G GPUs with a total batch size of 32 and accumulate the gradients for 8 steps .", "Comments": [], "label": [[22, 23, "Device-Count"], [24, 28, "Hardware-device"], [29, 37, "Device-Memory"]]}
{"id": 161103, "text": "After instruction tuning , we conduct CaPO using DPO with 30K constructed positive-negative response pairs , utilizing 8 A100-80G GPUs with an effective batch size of 32 and a learning rate of 1e-5 .", "Comments": [], "label": [[119, 120, "Device-Count"], [121, 125, "Hardware-device"], [126, 134, "Device-Memory"]]}
{"id": 161104, "text": "Specifically , we use BLEURT-20 [ 7 ] model in Pytorch implementation [ 8 ] for BLEURT metric .", "Comments": [], "label": [[47, 54, "Software-Entity"]]}
{"id": 161105, "text": "A Implementation Details We implement POMP based on an open-source translation framework Fairseq [ 11 ] and LLM framework GoT [ 12 ] [ Besta et al. , , 2023 ] .", "Comments": [], "label": [[89, 96, "Software-Entity"], [123, 125, "Software-Entity"]]}
{"id": 161106, "text": "We perform POMP 's training on 1 GeForce RTX 3090 GPU with 24GB memory and testing on 1 Tesla V100 GPU with 32GB memory .", "Comments": [], "label": [[31, 32, "Device-Count"], [33, 53, "Hardware-device"], [59, 71, "Device-Memory"], [86, 87, "Device-Count"], [88, 102, "Hardware-device"], [108, 119, "Device-Memory"]]}
{"id": 161107, "text": "C Implementation Details We access open-source language models using Transformers [ Wolf et al. , , 2020 ] and fine-tune them on 8 NVIDIA A100 ( 80G ) GPUs .", "Comments": [], "label": [[69, 81, "Software-Entity"], [129, 130, "Device-Count"], [131, 142, "Hardware-device"], [145, 148, "Device-Memory"]]}
{"id": 161108, "text": "We use 1-8 40GB A100 GPUs for the experiments .", "Comments": [], "label": [[7, 10, "Device-Count"], [11, 15, "Device-Memory"], [16, 25, "Hardware-device"]]}
{"id": 161109, "text": "We use the model weights from HuggingFace Transformers [ Wolf et al. , , 2020 ] and use full precision for LLaMA-2 7B and 13B and half-precision for 70B .", "Comments": [], "label": [[30, 41, "Software-Entity"]]}
{"id": 161110, "text": "Each LLM is run with HuggingFace .", "Comments": [], "label": [[21, 32, "Software-Entity"]]}
{"id": 161111, "text": "A.2 LLM Inference For LLaMA , Falcon , and Mixtral we allocated 24 GPU hours for almost every prompt format and dataset , and used 8 NVIDIA RTXA5000 GPUs .", "Comments": [], "label": [[131, 132, "Device-Count"], [133, 153, "Hardware-device"]]}
{"id": 161112, "text": "Phi required around 60 total GPU hours to run all prompt formats per dataset , and used a single NVIDIA RTXA4000 GPU .", "Comments": [], "label": [[90, 96, "Device-Count"], [97, 116, "Hardware-device"]]}
{"id": 161113, "text": "A.6 Metrics Accuracy was computed with numpy . [ 7 ] Cohen 's κ was computed with scikit-learn . [ 8 ] All metrics are reported from a single run .", "Comments": [], "label": [[39, 44, "Software-Entity"], [82, 94, "Software-Entity"]]}
{"id": 161114, "text": "We also examined GPT-3.5 turbo and GPT-4 ( version 1106 ) through Azure OpenAI API .", "Comments": [], "label": [[66, 71, "Cloud-Platform"]]}
{"id": 161115, "text": "Using GPT-4 from Azure OpenAI service , we constructed a synthetic repository called `` Coffee Company '' , which contains documents ( in .md format ) , codes ( in various programming languages ) , and database files ( in .csv format ) .", "Comments": [], "label": [[17, 22, "Cloud-Platform"]]}
{"id": 161116, "text": "Table 3 : Hyperparameters of experiments E Experiment Details Figure 8 : Illustration of our environment We use NVIDIA RTX 4090 , RTX A6000 , and Tesla A40 for the training and evaluation of our proposed Reflect-RL method .", "Comments": [], "label": [[112, 127, "Hardware-device"], [130, 139, "Hardware-device"], [146, 155, "Hardware-device"]]}
{"id": 161117, "text": "Python , PyTorch , HuggingFace PEFT , and AutoGen are used throughout the project .", "Comments": [], "label": [[9, 16, "Software-Entity"], [19, 35, "Software-Entity"], [42, 49, "Software-Entity"]]}
{"id": 161118, "text": "Specifically , we use LLaMA 13B at half-precision running on a single NVIDIA A100 GPU .", "Comments": [], "label": [[63, 69, "Device-Count"], [70, 85, "Hardware-device"]]}
{"id": 161119, "text": "A.2 Training Details Training and evaluation were done on NVIDIA H100 , A100 , and RTX A6000 GPUs , depending on the compute requirements of the job .", "Comments": [], "label": [[58, 69, "Hardware-device"], [72, 76, "Hardware-device"], [83, 97, "Hardware-device"]]}
{"id": 161120, "text": "These hyperparameters apply to training all models across all tasks , with two notable exceptions : ( 1 ) when training parity baselines , we used a slightly higher learning rate of 5e-4 for better stability , and ( 2 ) the scratchpad training job for the dynamic programming problem on LLaMA 13B used a batch size of 64 , along with 64 gradient accumulation steps , so that the training job could be done on a single A100 GPU .", "Comments": [], "label": [[411, 426, "Hardware-device"]]}
{"id": 161121, "text": "Early work in this area demonstrated some potential for smaller models [ Üstün ] [ and Cooper Stickland , 2022 ] , and the accessibility that PEFT provides designers in terms of finetuning on low-to-mid performance hardware setups renders it desirable .", "Comments": [], "label": [[142, 146, "Software-Entity"]]}
{"id": 161122, "text": "While fully fine-tuned NMT LLMs tend to suffer from some level of catastrophic-forgetting [ Kirkpatrick et al. , , 2017 ] , intuitively , PEFT-based NMT LLMs should not suffer from any loss of off-task performance , as adapters can be loaded or detached depending on whether or not a given user is prompting for a translation .", "Comments": [], "label": [[138, 142, "Software-Entity"]]}
{"id": 161123, "text": "To facilitate the rapid development of solutions for SimulMT LLMs ( fine-tuned for SimulMT ) or NMT LLMs ( finetuned for NMT ) adapted for SimulMT , we choose to develop and provide an open-source framework written in PyTorch for researchers to actively employ for future experiments .", "Comments": [], "label": [[218, 225, "Software-Entity"]]}
{"id": 161124, "text": "Additionally , Simul-LLM by default engages in PEFT [ Mangrulkar et al. , , 2022 ] via a passed configuration , although it does also support full model fine-tuning .", "Comments": [], "label": [[47, 51, "Software-Entity"]]}
{"id": 161125, "text": "This includes the capability to measure computationally aware text-based latency via the specification of an oracle , zero-latency transcription agent ( effectively required for latency modeling , elaborated on in our Appendix ) and support for COMET [ Rei et al. , , 2020 ] translation quality evaluations ( default model employed is COMET-DA [ Rei et al. , , 2021 ] ) .", "Comments": [], "label": [[245, 250, "Software-Entity"]]}
{"id": 161126, "text": "Some additional experiments are provided for the en-es language pair that validate fundamental SimulMT concepts and display BLEU scores , gathered via sacreBLEU [ Post , 2018 ] , with respect to samples observed during fine-tuning .", "Comments": [], "label": [[151, 160, "Software-Entity"]]}
{"id": 161127, "text": "Simul-LLM seamlessly integrates with the fine-tuning and generation tools of the popular transformers library as well as with SimulEval , the preeminent SimulMT evaluation framework .", "Comments": [], "label": [[89, 109, "Software-Entity"]]}
{"id": 161128, "text": "A Appendix A.1 Licensing Information Fairseq [ Ott et al. , , 2019 ] is MIT-licensed .", "Comments": [], "label": [[37, 44, "Software-Entity"]]}
{"id": 161129, "text": "A.2 Training , Fine-Tuning , and Evaluation Hyperparameters and Hardware Details All classical models were trained on two NVIDIA 32GB V100s and validated on a single V100 .", "Comments": [], "label": [[118, 121, "Device-Count"], [129, 133, "Device-Memory"], [134, 139, "Hardware-device"], [159, 165, "Device-Count"], [166, 170, "Hardware-device"]]}
{"id": 161130, "text": "All LLMs were fine-tuned via PEFT [ Mangrulkar et al. , , 2022 ] on a single NVIDIA 40GB A40 in bfloat16 and evaluated on a single V100 in float32 .", "Comments": [], "label": [[70, 76, "Device-Count"], [84, 88, "Device-Memory"], [89, 92, "Hardware-device"], [124, 130, "Device-Count"], [131, 135, "Hardware-device"]]}
{"id": 161131, "text": "Fairseq [ Ott et al. , , 2019 ] , an easily extensible sequence to sequence modeling toolkit written in PyTorch .", "Comments": [], "label": [[0, 7, "Software-Entity"], [104, 111, "Software-Entity"]]}
{"id": 161132, "text": "Classical models were trained with typical hyperparameters provided in Fairseq examples .", "Comments": [], "label": [[71, 78, "Software-Entity"]]}
{"id": 161133, "text": "For training optimization , DeepSpeed [ Rasley et al. , , 2020 ] is employed , while inference tasks are accelerated using vLLM [ Kwon et al. , , 2023 ] .", "Comments": [], "label": [[28, 37, "Software-Entity"], [123, 127, "Software-Entity"]]}
{"id": 161134, "text": "Llama-2-13b is trained on four NVIDIA A100 80GB GPUs , the other experiments are conducted on two NVIDIA A100 80GB GPUs .", "Comments": [], "label": [[26, 30, "Device-Count"], [31, 42, "Hardware-device"], [43, 52, "Device-Memory"], [94, 97, "Device-Count"], [98, 109, "Hardware-device"], [110, 119, "Device-Memory"]]}
{"id": 161135, "text": "We employ DeepSpeed [ Rasley et al. , 2020 ] to efficiently fine-tune open-source LLMs .", "Comments": [], "label": [[10, 19, "Software-Entity"]]}
{"id": 161136, "text": "All fine-tuning runs are done with 4 NVIDIA A100/A6000 GPUs .", "Comments": [], "label": [[35, 36, "Device-Count"], [36, 48, "Hardware-device"], [49, 59, "Hardware-device"]]}
{"id": 161137, "text": "B Implementation Details The implementation of our LLMs is based on Pytorch and Transformers toolkit .", "Comments": [], "label": [[68, 75, "Software-Entity"], [80, 100, "Software-Entity"]]}
{"id": 161138, "text": "In particular , for Llama2-13B-Chat [ 5 ] and Llama2-70B-Chat [ 6 ] , we adopt the official version in Huggingface .", "Comments": [], "label": [[103, 114, "Software-Entity"]]}
{"id": 161139, "text": "All of our experiments are conducted on two NVIDIA A100 GPUs .", "Comments": [], "label": [[40, 43, "Device-Count"], [44, 60, "Hardware-device"]]}
{"id": 161140, "text": "During training , we train Make-A-Voice for 100K steps using 8 NVIDIA V100 GPUs with a batch size of 6000 tokens for each GPU on the publicly-available * fairseq * framework [ Ott et al. , , 2019 ] .", "Comments": [], "label": [[61, 62, "Device-Count"], [63, 79, "Hardware-device"], [154, 161, "Software-Entity"]]}
{"id": 161141, "text": "S model is optimized with a segment size of 8192 and a learning rate of − until 500K steps using 4 NVIDIA V100 GPUs .", "Comments": [], "label": [[97, 98, "Device-Count"], [99, 116, "Hardware-device"]]}
{"id": 161142, "text": "FLARE [ Jiang et al. , , 2023c ] and Self-RAG [ Asai et al. , 2023 ] are dedicated to training models to have the capability of actively retrieving and filtering retrieval content on their own .", "Comments": [], "label": [[0, 5, "Software-Entity"]]}
{"id": 161143, "text": "We use one NVIDIA-A100-40G to train 5 epochs given the question and 5 retrieval passages .", "Comments": [], "label": [[7, 10, "Device-Count"], [11, 22, "Hardware-device"], [23, 26, "Device-Memory"]]}
{"id": 161144, "text": "We also use one NVIDIA-A100-80G to generate the answer given the question and compression results .", "Comments": [], "label": [[12, 15, "Device-Count"], [16, 27, "Hardware-device"], [28, 31, "Device-Memory"]]}
{"id": 161145, "text": "The model is trained on 32 Tesla V100 32GB GPUs .", "Comments": [], "label": [[24, 26, "Device-Count"], [27, 37, "Hardware-device"], [38, 47, "Device-Memory"]]}
{"id": 161146, "text": "We implement our framework with Python 3.7 , PyTorch 1.13 , and HuggingFace , and train our framework on Nvidia RTX 3090 and Nvidia A100 GPU .", "Comments": [], "label": [[45, 52, "Software-Entity"], [64, 75, "Software-Entity"], [105, 120, "Hardware-device"], [125, 140, "Hardware-device"]]}
{"id": 161147, "text": "D Computing Infra For mT5 fine-tuning runs , we use a combination of 4 to 64 TPU-V3 , and for PaLM-2 we use up to 256 TPU-V4 .", "Comments": [], "label": [[69, 76, "Device-Count"], [77, 83, "Hardware-device"], [114, 117, "Device-Count"], [118, 124, "Hardware-device"]]}
{"id": 161148, "text": "We use NVIDIA-V100 16GB GPUs for evaluating open-source models like BLOOM , BLOOMZ , Airavata on INDICGEN-BENCH .", "Comments": [], "label": [[7, 18, "Hardware-device"], [19, 28, "Device-Memory"]]}
{"id": 161149, "text": "C Experiment Setup We use GPT-4 for the entire process of training and evaluating * SPUR * , and * SPUR * -Lin-ada is trained and tested on an NVIDIA A100 instance .", "Comments": [], "label": [[143, 154, "Hardware-device"]]}
{"id": 161150, "text": "We also show that our method could effectively integrate with other memory-saving techniques . 3.1 Generation Throughput We test our method with 1.1B , 7B , and 30B parameters on an NVIDIA GeForce RTX 3090 ( 24GB ) GPU and an NVIDIA A100 ( 80GB ) GPU respectively .", "Comments": [], "label": [[182, 205, "Hardware-device"], [208, 212, "Device-Memory"], [226, 237, "Hardware-device"], [240, 244, "Device-Memory"]]}
{"id": 161151, "text": "Our implementation is based on HuggingFace Transformers [ Wolf et al. , 2020 ] with kernel replacement with FlashAttention 2 [ Dao , 2023 ] , fused RMS norm , fused cross-entropy and fused SwiGLU .", "Comments": [], "label": [[31, 42, "Software-Entity"], [108, 124, "Software-Entity"]]}
{"id": 161152, "text": "Table 1 : Maximum generation batch size and throughput on an RTX 3090 ( 24GB ) and an A100 ( 80GB ) GPU respectively with different sequence lengths .", "Comments": [], "label": [[61, 69, "Hardware-device"], [72, 76, "Device-Memory"], [86, 90, "Hardware-device"], [93, 97, "Device-Memory"]]}
{"id": 161153, "text": "Figure [ 4 ] shows the throughput of 7B Llama and our model on an A100 GPU w.r.t . the batch size .", "Comments": [], "label": [[66, 74, "Hardware-device"]]}
{"id": 161154, "text": "The models are trained on 128 NVIDIA A800 ( 80GB ) GPUs .", "Comments": [], "label": [[26, 29, "Device-Count"], [30, 41, "Hardware-device"], [44, 48, "Device-Memory"]]}
{"id": 161155, "text": "The throughput is tested on an RTX 3090 GPU with prompt length 5 and generation length 2043. vs. syntactic ) and it makes more sense to warm up both than just one of them . 4.2 Number of Warmup Layers Warmup layers serve as a bridge between the standard transformer and our model .", "Comments": [], "label": [[31, 43, "Hardware-device"]]}
{"id": 161156, "text": "The models are tested on an A100 ( 80GB ) GPU with prompt length 2048 and generation length 2048 .", "Comments": [], "label": [[28, 32, "Hardware-device"], [35, 39, "Device-Memory"]]}
{"id": 161157, "text": "Implementation Details Our code is implemented using DeepSpeed [ Rasley et al. , , 2020 ] on eight NVIDIA A800-SXM4-80GB GPUs .", "Comments": [], "label": [[53, 62, "Software-Entity"], [93, 98, "Device-Count"], [99, 115, "Hardware-device"], [116, 125, "Device-Memory"]]}
{"id": 161158, "text": "We then apply t-SNE [ Van der Maaten and Hinton , 2008 ] to reduce the 4096-dim representations to 2-dim for visualization .", "Comments": [], "label": [[14, 19, "Software-Entity"]]}
{"id": 161159, "text": "Figure 2 : t-SNE visualizations of output representations by LLaMA-2 before and after applying SDRRL .", "Comments": [], "label": [[11, 16, "Software-Entity"]]}
{"id": 161160, "text": "A Implementation Details The signature of SacreBLEU we use in this work is `` nrefs:1 | case : mixed | eff : no | tok : flores200 | smooth : exp | version:2.0.0 '' .", "Comments": [], "label": [[42, 51, "Software-Entity"]]}
{"id": 161161, "text": "When utilizing NLLB for machine translation , we set the beam size to 4 , with the remaining configurations adopting the default parameters from Huggingface Transformers [ Wolf et al. , 2020 ] .", "Comments": [], "label": [[15, 19, "Software-Entity"], [145, 156, "Software-Entity"]]}
{"id": 161162, "text": "] ( http : //arxiv.org/abs/2307.09288 ) A Computational Resource All experiments are performed on platforms with 20 Intel Xeon Gold 6248 CPUs , 236 GB ROM , and 4 Nvidia Tesla v100 32 GB GPUs .", "Comments": [], "label": [[113, 115, "Device-Count"], [116, 141, "Hardware-device"], [144, 154, "Device-Memory"], [161, 162, "Device-Count"], [163, 180, "Hardware-device"], [181, 191, "Device-Memory"]]}
{"id": 161163, "text": "The responses were tokenized using the NLTK [ Loper and Bird , 2002 ] .", "Comments": [], "label": [[39, 43, "Software-Entity"]]}
{"id": 161164, "text": "Based on our approach , running the fine-tuned student model ( LLaMA-7B ) on machines with normal computational power ( such as RTX 3090 ) is also a form of progress .", "Comments": [], "label": [[128, 136, "Hardware-device"]]}
{"id": 161165, "text": "The trainable LoRA parameters are fine-tuned on NVIDIA A100-40GB GPUs , and the training duration is approximately 15 hours .", "Comments": [], "label": [[48, 59, "Hardware-device"], [60, 69, "Device-Memory"]]}
{"id": 161166, "text": "This model is trained on a large amount of unlabeled data , making it well suited for fine-tuning a variety of tasks , and can be run on a single V100 GPU [ 5 ] .", "Comments": [], "label": [[139, 145, "Device-Count"], [146, 154, "Hardware-device"]]}
{"id": 161167, "text": "The ETR framework 's training is performed on 1 × A100 GPU .", "Comments": [], "label": [[46, 47, "Device-Count"], [50, 58, "Hardware-device"]]}
{"id": 161168, "text": "The inference is conducted on 8 × A100 GPUs as it requires serving LLMs simultaneously .", "Comments": [], "label": [[30, 31, "Device-Count"], [34, 43, "Hardware-device"]]}
{"id": 161169, "text": "The experimental results are as shown in Table [ 5 . ] Table 5 : Time Cost of LLM Switching The experiment runs with CUDA 12.1 , Pytorch 2.1 and Flash-Attention 2 .", "Comments": [], "label": [[117, 121, "Software-Entity"], [129, 136, "Software-Entity"], [145, 162, "Software-Entity"]]}
{"id": 161170, "text": "Each LLM is served on an A100-SXM4-80G GPU , utilizing bf16 quantization .", "Comments": [], "label": [[25, 34, "Hardware-device"], [35, 42, "Device-Memory"]]}
{"id": 161171, "text": "Generations were conducted on either A100 GPUs with 80Gb memory or on V100 with 32Gb memory .", "Comments": [], "label": [[37, 46, "Hardware-device"], [52, 63, "Device-Memory"], [70, 74, "Hardware-device"], [80, 91, "Device-Memory"]]}
{"id": 161172, "text": "Models were run with a bfloat16 precision on A100 and with float32 precisions on V100 , except for the PYTHIA models , which have been ran with float16 mixed precision , following the original setup [ Biderman et al. , , 2023 ] .", "Comments": [], "label": [[45, 49, "Hardware-device"], [81, 85, "Hardware-device"]]}
{"id": 161173, "text": "For models with ≈ 7B parameters , generating 4000 samples takes approximately 5 hours on 2 A100s , it takes about 10 hours for models with ≈ 13B parameters on 2 A100 , and about 40 hours for models with ≈ 70B parameters on 4 A100 .", "Comments": [], "label": [[89, 90, "Device-Count"], [91, 96, "Hardware-device"], [159, 160, "Device-Count"], [161, 165, "Hardware-device"], [223, 224, "Device-Count"], [225, 229, "Hardware-device"]]}
{"id": 161174, "text": "Code is built on PyTorch [ Paszke et al. , , 2019 ] , with the HuggingFace library [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[17, 24, "Software-Entity"], [63, 74, "Software-Entity"]]}
{"id": 161175, "text": "Every experiment is conducted with a maximum of 8 NVIDIA A100 80GB VRAM GPUs for up to 90 hours .", "Comments": [], "label": [[48, 49, "Device-Count"], [50, 61, "Hardware-device"], [62, 76, "Device-Memory"]]}
{"id": 161176, "text": "The baselines are implemented based on the Transformers [ 7 ] , PEFT [ 8 ] and TRL [ 9 ] library .", "Comments": [], "label": [[43, 55, "Software-Entity"], [64, 68, "Software-Entity"], [79, 82, "Software-Entity"]]}
{"id": 161177, "text": "For sequence similarity , we employ MMseqs2 [ Steineg ] [ ger and Söding , 2017 ] to calculate the editing distance d * s * ( · , · ) ( see Appendix [ B.1.1 ] .", "Comments": [], "label": [[36, 43, "Software-Entity"]]}
{"id": 161178, "text": "We use mmseqs2 to cluster proteins with an identity surpassing the 70 % threshold , then remove the clusters containing the proteins in the test set , for a total of 19,455 sequences .", "Comments": [], "label": [[7, 14, "Software-Entity"]]}
{"id": 161179, "text": "( a ) We visualize the pLDDT of generated sequences predicted by AlphaFold2 to assess the protein foldability .", "Comments": [], "label": [[65, 75, "Software-Entity"]]}
{"id": 161180, "text": "We sample 100 sequences from each class and assess the foldability of individual sequences by predicting their corresponding structures using ColabFold [ Mirdita et al. , 2022 ] [ Jumper et al. , , 2021 ] and computing the average predicted local distance difference test ( pLDDT ) across the whole structure ( Figure [ 4 ] ( a ) ) .", "Comments": [], "label": [[142, 151, "Software-Entity"]]}
{"id": 161181, "text": "In Figure [ 5 , ] we present the docking result ( docked by DiffDock [ Corso et al. , , 2023 ] ) , the binding affinity ( predicted by Smina [ Koes et al. , , 2013 ] [ Trott ] [ and Olson , 2010 ] , the lower the better ) , and the pLDDT score ( predicted by ColabFold ; the higher the absolute value , the better ) .", "Comments": [], "label": [[60, 68, "Software-Entity"], [135, 140, "Software-Entity"], [259, 268, "Software-Entity"]]}
{"id": 161182, "text": "scoring with smina from the csar 2011 benchmarking exercise .", "Comments": [], "label": [[13, 18, "Software-Entity"]]}
{"id": 161183, "text": "For sequence similarity , we use mmseqs2 ( GPL-3.0 license ) with –cov-mode 0 –min-seq-id 0.8 parameter .", "Comments": [], "label": [[33, 40, "Software-Entity"]]}
{"id": 161184, "text": "We utilize Pytorch to conduct experiments with 8 32G V100 GPUs .", "Comments": [], "label": [[11, 18, "Software-Entity"], [47, 48, "Device-Count"], [49, 52, "Device-Memory"], [53, 62, "Hardware-device"]]}
{"id": 161185, "text": "BLEU scores , computed with sacreBLEU [ Post , 2018 ] , are reported as well , as a precision measure of n-gram overlap between hypotheses and references .", "Comments": [], "label": [[28, 37, "Software-Entity"]]}
{"id": 161186, "text": "Write a response that appropriately completes the request . * '' Each training process was performed on 1 A40 GPU with 48GB of RAM .", "Comments": [], "label": [[104, 105, "Device-Count"], [106, 113, "Hardware-device"], [119, 130, "Device-Memory"]]}
{"id": 161187, "text": "B.2 Task Sequence Orders We report 4 different task orders used for our experiments in Table [ 7 . ] C Implementation Details Our experiments are implemented with PyTorch [ Paszke et al. , , 2019 ] and Transformer library [ Wolf et al. , 2020 ] .", "Comments": [], "label": [[163, 170, "Software-Entity"], [202, 221, "Software-Entity"]]}
{"id": 161188, "text": "The T5-Large is trained on a single NVIDIA Tesla A800 GPU and the larger backbones T5-XL , T5-XXL , LLaMA-2-7B and LLaMA-2-13B are performed on 4 NVIDIA Tesla A800 using DeepSpeed repository .", "Comments": [], "label": [[29, 35, "Device-Count"], [36, 57, "Hardware-device"], [144, 145, "Device-Count"], [146, 163, "Hardware-device"], [170, 179, "Software-Entity"]]}
{"id": 161189, "text": "All experiments are conducted on a single NVIDIA A800 GPU ( 80G ) .", "Comments": [], "label": [[35, 41, "Device-Count"], [42, 57, "Hardware-device"], [60, 63, "Device-Memory"]]}
{"id": 161190, "text": "The pricing for the models used in the paper is shown in Table [ 8 . ] We fine-tune SimCSEBASE and SimCSELARGE with different encoding configurations on a single Titan Xp GPU .", "Comments": [], "label": [[155, 161, "Device-Count"], [162, 174, "Hardware-device"]]}
{"id": 161191, "text": "We run Flan-T5SMALL and Flan-T5BASE on CPU machines , and run Flan-T5LARGE and Flan-T5XL on a Titan Xp GPU .", "Comments": [], "label": [[94, 106, "Hardware-device"]]}
{"id": 161192, "text": "All the models have 7B parameters and thus fit on a single NVIDIA A40 ( 48G VRAM ) in 16-bit precision .", "Comments": [], "label": [[52, 58, "Device-Count"], [59, 69, "Hardware-device"], [72, 80, "Device-Memory"]]}
{"id": 161193, "text": "The models are available through HuggingFace [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[33, 44, "Software-Entity"]]}
{"id": 161194, "text": "It is powered by a Mediatek MT6582 chipset with a quad-core 1.3 GHz Cortex-A7 CPU and a Mali-400MP2 GPU .", "Comments": [], "label": [[50, 67, "Device-Memory"], [68, 81, "Hardware-device"], [88, 103, "Hardware-device"]]}
{"id": 161195, "text": "The device has 8GB of internal storage and 1GB of RAM , with the option to expand storage via a microSDHC card .", "Comments": [], "label": [[15, 18, "Device-Memory"], [43, 53, "Device-Memory"]]}
{"id": 161196, "text": "It runs on Android 4.4.2 ( KitKat ) and is powered by a quad-core 1.3 GHz Cortex-A7 processor with a Mali-400MP2 GPU .", "Comments": [], "label": [[56, 73, "Device-Memory"], [74, 83, "Hardware-device"], [101, 117, "Hardware-device"]]}
{"id": 161197, "text": "We also provide the inference throughput ( TP ) on a single A100-80G GPU , including compression and generation .", "Comments": [], "label": [[53, 59, "Device-Count"], [60, 64, "Hardware-device"], [65, 72, "Device-Memory"]]}
{"id": 161198, "text": "All experiments are conducted on 8 NVIDIA A100 GPUs .", "Comments": [], "label": [[33, 34, "Device-Count"], [35, 51, "Hardware-device"]]}
{"id": 161199, "text": "Detailed Throughput Evaluation To evaluate the throughput of various methods or models , encompassing both compression and generation , we perform testing on a single A100-80G GPU .", "Comments": [], "label": [[160, 166, "Device-Count"], [167, 171, "Hardware-device"], [172, 179, "Device-Memory"]]}
{"id": 161200, "text": "The specific distinctions are as follows : Model training details For different backbones , we utilized the following hyperparameters : Experiments are carried out using 2 NVIDIA A100 with 80GB memory .", "Comments": [], "label": [[170, 171, "Device-Count"], [172, 183, "Hardware-device"], [189, 193, "Device-Memory"]]}
{"id": 161201, "text": "Each LoRA module is trained using 8 A100 80G GPUs , where each mini-batch contains 128 training examples .", "Comments": [], "label": [[34, 35, "Device-Count"], [36, 40, "Hardware-device"], [41, 49, "Device-Memory"]]}
{"id": 161202, "text": "For tokenization , we utilize the HuggingFace Tokenizer [ 1 ] tools to train a byte pair encoding ( BPE ) [ Sennrich et al. , 2016 ] tokenizer with an 8K vocabulary size on a subset of the training corpus .", "Comments": [], "label": [[34, 45, "Software-Entity"]]}
{"id": 161203, "text": "For research purposes , we will release the model checkpoint of DeepSeekMoE 16B to the public , which can be deployed on a single GPU with 40GB of memory .", "Comments": [], "label": [[123, 133, "Device-Count"], [139, 143, "Device-Memory"]]}
{"id": 161204, "text": "Given the relatively small model size , all parameters , including expert parameters , are deployed on a single GPU device to avoid unbalanced computation .", "Comments": [], "label": [[105, 115, "Device-Count"]]}
{"id": 161205, "text": "In order to optimize performance , we develop GPU kernels with CUDA and Triton [ Tillet et al. , , 2019 ] for gating algorithms and fusing computations across linear layers in different experts .", "Comments": [], "label": [[63, 67, "Software-Entity"], [72, 78, "Software-Entity"]]}
{"id": 161206, "text": "All experiments are carried out on clusters equipped with NVIDIA A100 or H800 GPUs .", "Comments": [], "label": [[58, 69, "Hardware-device"], [73, 82, "Hardware-device"]]}
{"id": 161207, "text": "Each node in the A100 cluster contains 8 GPUs connected pairwise via the NVLink bridge .", "Comments": [], "label": [[17, 21, "Hardware-device"], [39, 45, "Device-Count"]]}
{"id": 161208, "text": "The H800 cluster also features 8 GPUs per node , interconnected using NVLink and NVSwitch within nodes .", "Comments": [], "label": [[4, 8, "Hardware-device"], [31, 37, "Device-Count"]]}
{"id": 161209, "text": "For both A100 and H800 clusters , InfiniBand interconnects are utilized to facilitate communication across nodes .", "Comments": [], "label": [[9, 13, "Hardware-device"], [18, 22, "Hardware-device"]]}
{"id": 161210, "text": "The Open LLM Leaderboard is a public leaderboard supported by HuggingFace , it consists of six tasks : ARC [ Clark et al. , , 2018 ] , HellaSwag [ Zellers et al. , 2019 ] , MMLU [ Hendrycks et al. , , 2020 ] , TruthfulQA [ Lin et al. , , 2022 ] , Winogrande [ Sak ] [ aguchi et al. , , 2019 ] , and GSM8K [ Cobbe et al. , , 2021 ] .", "Comments": [], "label": [[62, 73, "Software-Entity"]]}
{"id": 161211, "text": "Robotic Planning For robotic planning tasks , we transform the natural language description into PDDL and use fastdownward [ Helmert , 2006 ] as the symbolic solver .", "Comments": [], "label": [[110, 122, "Software-Entity"]]}
{"id": 161212, "text": "F Experimental Settings In the implementation , this work fully finetunes the models with 8 * A100 ( 80GB ) with maximum sequence length set to 4,096 .", "Comments": [], "label": [[90, 91, "Device-Count"], [94, 98, "Hardware-device"], [101, 105, "Device-Memory"]]}
{"id": 161213, "text": "The tuning process is optimized and accelerated by deepspeed zero3 and FlashAttention2 .", "Comments": [], "label": [[51, 66, "Software-Entity"], [71, 86, "Software-Entity"]]}
{"id": 161214, "text": "The inference process is conducted under a single GPU of A100 ( 80GB ) with greedy search ( beam size=1 ) .", "Comments": [], "label": [[43, 53, "Device-Count"], [57, 61, "Hardware-device"], [64, 68, "Device-Memory"]]}
{"id": 161215, "text": "All experiments are conducted on two NVIDIA A800 ( 2 * 80GB ) .", "Comments": [], "label": [[33, 36, "Device-Count"], [37, 48, "Hardware-device"], [51, 59, "Device-Memory"]]}
{"id": 161216, "text": "When using tools like Echarts or Plotly , our recommended attributes allow users to easily create chart images while giving them the freedom to adjust or add attributes to suit their specific analysis needs or presentation preferences if needed .", "Comments": [], "label": [[22, 29, "Software-Entity"], [33, 39, "Software-Entity"]]}
{"id": 161217, "text": "We run all experiments on NVIDIA RTX A100 GPUs , and use the Adam [ Kingma and Ba , 2015 ] optimizer with β = 0.9 and β = 0.999 to optimize models .", "Comments": [], "label": [[26, 46, "Software-Entity"]]}
{"id": 161218, "text": "E Table Features for Clustring We use table features that introduced in VizML [ Hu et al. , 2019 ] for clustering .", "Comments": [], "label": [[72, 77, "Software-Entity"]]}
{"id": 161219, "text": "MobileSpeech achieves RTF of 0.09 on a single A100 GPU and we have successfully deployed MobileSpeech on mobile devices .", "Comments": [], "label": [[39, 45, "Device-Count"], [46, 54, "Hardware-device"]]}
{"id": 161220, "text": "For the Chinese segment , we compare our Training and Inference Settings MobileSpeech was trained on 40,000 hours of Mandarin Chinese data using 32 NVIDIA A100 40G GPUs .", "Comments": [], "label": [[145, 147, "Device-Count"], [148, 159, "Hardware-device"], [160, 168, "Device-Memory"]]}
{"id": 161221, "text": "MobileSpeech was trained for 12 epochs on 4 NVIDIA A100 40G GPUs , with each batch accommodating 3500 frames of the discrete codec .", "Comments": [], "label": [[42, 43, "Device-Count"], [44, 55, "Hardware-device"], [56, 64, "Device-Memory"]]}
{"id": 161222, "text": "The calculation is performed on an empty A100 GPU .", "Comments": [], "label": [[41, 49, "Hardware-device"]]}
{"id": 161223, "text": "For Llama-2 and Mistral , we utilize the 7B-Chat , and 7B-Instruct-v0.2 versions , respectively , from the huggingface model repository [ 5 ] .", "Comments": [], "label": [[107, 118, "Software-Entity"]]}
{"id": 161224, "text": "All experiments are performed using NVIDIA RTX A6000 GPUs .", "Comments": [], "label": [[36, 57, "Hardware-device"]]}
{"id": 161225, "text": "In our experiments , we tested the performance of our model on one A100 ( 80G ) GPU .", "Comments": [], "label": [[63, 66, "Device-Count"], [67, 71, "Hardware-device"], [74, 77, "Device-Memory"]]}
{"id": 161226, "text": "This process entails fine-tuning the LLM ( FlanT5XL ) on the PhotoChat dataset for 2 epochs , which typically requires approximately 4 A100 ( 80G ) GPU hours .", "Comments": [], "label": [[133, 134, "Device-Count"], [135, 139, "Hardware-device"], [142, 145, "Device-Memory"]]}
{"id": 161227, "text": "We present examples of style annotation data and their lexicons in Table [ 15 . ] D Implementation Details We use PyTorch [ Paszke et al. , , 2019 ] and Huggingface Transformers [ Wolf et al. , , 2020 ] in the experiments .", "Comments": [], "label": [[114, 121, "Software-Entity"], [153, 164, "Software-Entity"]]}
{"id": 161228, "text": "We fine-tuned the LLaMA-2 ( 7B ) model on 4 A40 GPUs using DeepSpeed .", "Comments": [], "label": [[42, 43, "Device-Count"], [44, 52, "Hardware-device"], [59, 68, "Software-Entity"]]}
{"id": 161229, "text": "All the other models were fine-tuned on one single A40 GPU .", "Comments": [], "label": [[44, 50, "Device-Count"], [51, 58, "Hardware-device"]]}
{"id": 161230, "text": "Using Amazon Mechanical Turk , we recruited a diverse pool of annotators to evaluate a representative subset of 3,000 validation set summaries .", "Comments": [], "label": [[6, 28, "Cloud-Platform"]]}
{"id": 161231, "text": "4.2 Visual Metrics We employ CLIPScore [ Hessel et al. , , 2021 ] for assessing the text-image compatibility , indicating how well the summary captures information from the image .", "Comments": [], "label": [[29, 38, "Software-Entity"]]}
{"id": 161232, "text": "We compute CLIPScore sentence-wise and average the scores for a final measure of compatibility score between the summary and the image .", "Comments": [], "label": [[11, 20, "Software-Entity"]]}
{"id": 161233, "text": "Finally , we employ CLIPBERTScore [ Wan and Bansal , 2022 ] to assess multimodal faithfulness . 4.3 Human Evaluation We conduct human evaluations using Amazon Mechanical Turk to determine how informative and faithful to the source text and image ( correctness ) the generated summaries are .", "Comments": [], "label": [[152, 174, "Cloud-Platform"]]}
{"id": 161234, "text": "We also observe a net increase of 0.3 % in CLIPScore , which indicates that the approach encourages summaries that effectively fuse information from both modalities without ignoring the visual modality .", "Comments": [], "label": [[43, 52, "Software-Entity"]]}
{"id": 161235, "text": "We hire workers from Amazon Mechanical Turk to obtain annotations to train the critic model and compare model performance .", "Comments": [], "label": [[21, 43, "Cloud-Platform"]]}
{"id": 161236, "text": "It takes 15 hours to call the Claude 3 Sonnet model on AWS Bedrock , and 8 GPU hours to compute Inv-TIFA .", "Comments": [], "label": [[55, 67, "Cloud-Platform"]]}
{"id": 161237, "text": "We set a frequency cutoff between 50 and 100 per month to obtain uncommon words and remove misspellings and named entities using SpaCy [ Montani et al. , 2023 ] , resulting in 74,542 candidates .", "Comments": [], "label": [[129, 134, "Software-Entity"]]}
{"id": 161238, "text": "Using SpaCy , we identify 60,671 noun and verb phrases with a Part-of-Speech tagger and remove duplicates and named entities .", "Comments": [], "label": [[6, 11, "Software-Entity"]]}
{"id": 161239, "text": "We use a named-entity recognition model via SpaCy to identify named entities that are potentially PII and largely remove this information automatically when filtering for neologism candidates .", "Comments": [], "label": [[44, 49, "Software-Entity"]]}
{"id": 161240, "text": "We tokenize the utterances with the NLTK package [ Bird et al. , , 2009 ] to get individual word counts and update a generic word counter .", "Comments": [], "label": [[36, 40, "Software-Entity"]]}
{"id": 161241, "text": "We further filter out named entities by utilizing a SpaCy named entity recognition ( NER ) model [ Honnibal and Montani , 2017 ] ( en_core_web_sm ) to detect proper nouns and update a named entity counter .", "Comments": [], "label": [[52, 57, "Software-Entity"]]}
{"id": 161242, "text": "In total , we get 60,671 noun and verb phrases with a Part-of-Speech Tagger via SpaCy ( en_core_web_sm ) that we treat as neologism candidates .", "Comments": [], "label": [[80, 85, "Software-Entity"]]}
{"id": 161243, "text": "C Experimental Details All models are evaluated on two NVIDIA A40 GPUs for a single run since models are not finetuned for NEO-BENCH tasks .", "Comments": [], "label": [[51, 54, "Device-Count"], [55, 70, "Hardware-device"]]}
{"id": 161244, "text": "We use the Thresh [ Heineman et al. , 2023 ] interface for annotating translation sentences , and Figure [ 13 ] provides a screenshot of the interface .", "Comments": [], "label": [[11, 17, "Cloud-Platform"]]}
{"id": 161245, "text": "Figure 13 : Thresh Interface used to crowdsource human annotations of Machine Translation Output . 20 13904 Table 12 : Example model outputs for all possible translation categories .", "Comments": [], "label": [[12, 18, "Cloud-Platform"]]}
{"id": 161246, "text": "We used a single H100 with 80GB VRAM to conduct our experiments with local LLMs for less than 10 hours .", "Comments": [], "label": [[10, 16, "Device-Count"], [17, 21, "Hardware-device"], [27, 36, "Device-Memory"]]}
{"id": 161247, "text": "Each example is chunked into sequences of nearly 300 tokens using the NLTK sentence splitter .", "Comments": [], "label": [[70, 74, "Software-Entity"]]}
{"id": 161248, "text": "In particular , we are able to generate 3M tokens per hour on a single A100 when using the Mistral-7B .", "Comments": [], "label": [[64, 70, "Device-Count"], [71, 75, "Hardware-device"]]}
{"id": 161249, "text": "In comparison , on 64 A100s , we achieve a throughput of 0.5M tokens per second .", "Comments": [], "label": [[19, 21, "Device-Count"], [22, 27, "Hardware-device"]]}
{"id": 161250, "text": "In particular , we are able to generate 3M tokens per hour on a single A100 when using the Mistral-7B .", "Comments": [], "label": [[64, 70, "Device-Count"], [71, 75, "Hardware-device"]]}
{"id": 161251, "text": "In comparison , on 64 A100s , we achieve a throughput of 0.5M tokens per second .", "Comments": [], "label": [[19, 21, "Device-Count"], [22, 27, "Hardware-device"]]}
{"id": 161252, "text": "The experiments are conducted using 4 NVIDIA RTX A6000 GPUs with 48GBs .", "Comments": [], "label": [[36, 37, "Device-Count"], [38, 59, "Hardware-device"], [65, 70, "Device-Memory"]]}
{"id": 161253, "text": "Implementation Details All experiments are conducted on a single NVIDIA Tesla A100 80GB card .", "Comments": [], "label": [[58, 64, "Device-Count"], [65, 82, "Hardware-device"], [83, 87, "Device-Memory"]]}
{"id": 161254, "text": "Fine-tuning of the LLaMA-2-13B model was conducted across 8 V100 GPUs .", "Comments": [], "label": [[58, 59, "Device-Count"], [60, 69, "Hardware-device"]]}
{"id": 161255, "text": "We ran both EWEK-QA and WebGLM retriever models on V100 Nvidia GPUs .", "Comments": [], "label": [[51, 67, "Hardware-device"]]}
{"id": 161256, "text": "A.2 Computing Infrastructure All experiments were done on a Ubuntu 20.4 server with 72 Intel ( R ) Xeon ( R ) Gold 6140 CPU @ 2.30GHz cores and a RAM of size 755 Gb .", "Comments": [], "label": [[84, 86, "Device-Count"], [88, 139, "Hardware-device"], [146, 164, "Device-Memory"]]}
{"id": 161257, "text": "We use a NVIDIA Tesla V100-PCIE-32GB GPU .", "Comments": [], "label": [[9, 31, "Hardware-device"], [32, 40, "Device-Memory"]]}
{"id": 161258, "text": "We train the model for one epoch on four V100 GPUs using a learning rate of ×10− and training batch size of 32 .", "Comments": [], "label": [[36, 40, "Device-Count"], [41, 50, "Hardware-device"]]}
{"id": 161259, "text": "We gather the answer directly from Gemini answering the question and use the same hyperparameters in SteerLM and Rejection Sampling . 4 Experiments 4.1 Experimental Setup For training , we utilize Azure Cloud ( NC-A100 series ) with 4 A100-80G GPUs .", "Comments": [], "label": [[197, 208, "Cloud-Platform"], [211, 225, "Cloud-Platform"], [233, 234, "Device-Count"], [235, 239, "Hardware-device"], [240, 248, "Device-Memory"]]}
{"id": 161260, "text": "The training was conducted using Deepspeed Stage-3 on a 4x A100 80GB GPU machine with LoRA for parameter-efficient fine-tuning .", "Comments": [], "label": [[33, 50, "Software-Entity"], [56, 57, "Device-Count"], [59, 63, "Hardware-device"], [64, 72, "Device-Memory"]]}
{"id": 161261, "text": "4.1 General Experimental Setup We use the Huggingface Transformers library [ Wolf et al. , , 2020 ] in all experiments , except for machine translation , where we use the PIXEL-MT and BPE-MT models .", "Comments": [], "label": [[42, 53, "Software-Entity"]]}
{"id": 161262, "text": "[ 9 ] We modified code and model checkpoints provided by [ Salesky et al. , 2023 ] based on fairseq [ Ott et al. , , 2019 ] for the two exceptions .", "Comments": [], "label": [[92, 99, "Software-Entity"]]}
{"id": 161263, "text": "All experiments are conducted on an NVIDIA-RTX A6000 GPU , and the training time ranges from 2 minutes to 50 hours , depending on the nature of the task and the size of the datasets .", "Comments": [], "label": [[36, 56, "Hardware-device"]]}
{"id": 161264, "text": "The dataset introductions , statistics , and prompt-response templates for the above tasks are detailed in Appendix [ C. ] The above tasks ' evaluation metrics or protocols are in Appendix [ C.7 . ] 4.3 Experiment Settings Computing infrastures We run all our experiments on NVIDIA A40 ( 48GB ) GPUs .", "Comments": [], "label": [[275, 285, "Hardware-device"], [288, 292, "Device-Memory"]]}
{"id": 161265, "text": "We rely on the HuggingFace Evaluate package [ 5 ] for computing this metric .", "Comments": [], "label": [[15, 26, "Software-Entity"]]}
{"id": 161266, "text": "We generate model responses from a fine-tuned model with beam size 5 with the generation function in Huggingface Transformers [ Wolf et al. , , 2020a ] .", "Comments": [], "label": [[101, 112, "Software-Entity"]]}
{"id": 161267, "text": "We further use a robust tool ImageMagisk [ ImageMagick Studio LLC ] to convert images into JPEG format for easy processing .", "Comments": [], "label": [[29, 40, "Software-Entity"]]}
{"id": 161268, "text": "* Caption Cleaning * : ( i ) Chunks with captions shorter than 5 words are removed ; ( ii ) For captions with LaTeX expressions such as math formulas and references , we apply the pylatexenc [ 2 ] to transform the LaTeX to text with math formulas retained , citations to a special symbol < cit. > , references to < ref > .", "Comments": [], "label": [[180, 190, "Software-Entity"]]}
{"id": 161269, "text": "The training process takes 70 hours with 8 NVIDIA A100s .", "Comments": [], "label": [[41, 42, "Device-Count"], [43, 55, "Hardware-device"]]}
{"id": 161270, "text": "ImageMagick Studio LLC .", "Comments": [], "label": [[0, 11, "Software-Entity"]]}
{"id": 161271, "text": "These functions are from Numpy and a string-processing codebase built by us .", "Comments": [], "label": [[25, 30, "Software-Entity"]]}
{"id": 161272, "text": "A detailed description of baseline models can be found in [ §A.1 . ] The prompt templates for each task are available in [ §B . ] We run all the experiments using FlashAttention [ Dao et al. , , 2022 ] on a single NVIDIA A800 GPU .", "Comments": [], "label": [[163, 177, "Software-Entity"], [207, 213, "Device-Count"], [214, 229, "Hardware-device"]]}
{"id": 161273, "text": "We mainly use source code from Numpy [ 15 ] .", "Comments": [], "label": [[31, 36, "Software-Entity"]]}
{"id": 161274, "text": "To prevent LLMs from answering the question based on their parametric knowledge , we replace the original function name defined in Numpy with Op1 , Op2 ... , OpN .", "Comments": [], "label": [[131, 136, "Software-Entity"]]}
{"id": 161275, "text": "Note that the cognitive plausibility of RNNGs has [ Neural LMs are trained with the fairseq toolkit ( Ott ] [ et al. , ] [ 2019 ) .", "Comments": [], "label": [[84, 91, "Software-Entity"]]}
{"id": 161276, "text": "N-gram LMs are trained with the KenLM ] toolkit [ Heafield , ] [ 2011 ) with Kneser-Ney smoothing . ] TD and LC are also denoted as pre-order and in-order traversals , respectively .", "Comments": [], "label": [[32, 37, "Software-Entity"]]}
{"id": 161277, "text": "All the models were trained/tested with a single NVIDIA A100 GPU ( 40GB ) .", "Comments": [], "label": [[42, 48, "Device-Count"], [49, 64, "Hardware-device"], [67, 71, "Device-Memory"]]}
{"id": 161278, "text": "The LLaMA-2 ( 7B ) model was used via the hugging face toolkit [ Wolf et al. , , 2020 ] .", "Comments": [], "label": [[42, 54, "Software-Entity"]]}
{"id": 161279, "text": "To create a dataset for an impossible language , we first pre-process the BabyLM dataset using Stanza [ Qi et al. , , 2020 ] .", "Comments": [], "label": [[95, 101, "Software-Entity"]]}
{"id": 161280, "text": "We train on NVIDIA RTX 3090 ( 24GB ) GPUs and NVIDIA RTX A6000 ( 48GB ) GPUs .", "Comments": [], "label": [[12, 27, "Hardware-device"], [30, 34, "Device-Memory"], [46, 62, "Hardware-device"], [65, 69, "Device-Memory"]]}
{"id": 161281, "text": "To compare with established semisupervised machine learning techniques , we implement two more strategies : Bootstrapping and Π-models , representing We use [ Chang et al. , 2022 ] 's PyTorch reimplementation obtain from [ https : //github.com/cmu-llab/ ] ( https : //github.com/cmu-llab/middle-chinese-reconstruction ) [ middle-chinese-reconstruction ] ( https : //github.com/cmu-llab/middle-chinese-reconstruction ) , with minor modifications to support multi-layer .", "Comments": [], "label": [[184, 191, "Software-Entity"]]}
{"id": 161282, "text": "( 2021 ) , we probe the model 's learned embeddings for a hierarchical organization of phonemes using sklearn 's Ward variance minimization algorithm ( Ward , 1963 ) .", "Comments": [], "label": [[102, 112, "Software-Entity"]]}
{"id": 161283, "text": "To generate a semisupervised dataset , we initialize PyTorch 's pseudo-random number generator with the dataset seed , assign a uniformly distributed number between 0 and 1 to each training example with torch.rand in the same order as they appear in the dataset , and keep the protoform label on cognate sets whose assigned number is above a threshold such that the desired percentage of labels remain .", "Comments": [], "label": [[53, 63, "Software-Entity"]]}
{"id": 161284, "text": "Hyperparameter tuning and experiments are performed on a mix of NVIDIA GeForce GTX 1080 Ti , NVIDIA GeForce RTX 2080 Ti , NVIDIA RTX 6000 Ada Generation , NVIDIA RTX A6000 , Quadro RTX 8000 , and Tesla V100-SXM2-32GB GPUs for a total of 411 GPU days .", "Comments": [], "label": [[64, 90, "Hardware-device"], [93, 120, "Hardware-device"], [122, 137, "Hardware-device"], [155, 171, "Hardware-device"], [174, 189, "Hardware-device"], [196, 211, "Hardware-device"], [212, 221, "Device-Memory"]]}
{"id": 161285, "text": "* * E Package Usage * * Our model is implemented in PyTorch ( See the code for details at [ https : //github.com/ ] ( https : //github.com/cmu-llab/dpd ) [ cmu-llab/dpd ] ( https : //github.com/cmu-llab/dpd ) ) .", "Comments": [], "label": [[52, 59, "Software-Entity"]]}
{"id": 161286, "text": "Sequence alignment is done using lingpy [ List and Forkel , 2021 ] with default parameters .", "Comments": [], "label": [[33, 39, "Software-Entity"]]}
{"id": 161287, "text": "Hierarchical clustering is visualized using scipy .", "Comments": [], "label": [[44, 49, "Software-Entity"]]}
{"id": 161288, "text": "Plots are created using Matplotlib .", "Comments": [], "label": [[24, 34, "Software-Entity"]]}
{"id": 161289, "text": "B Implementation Details Our D2LLM model is built upon PyTorch and DeepSpeed , using Qwen-7B-Chat as the teacher and the base LLM for the student due to its effectiveness with Chinese data .", "Comments": [], "label": [[55, 62, "Software-Entity"], [67, 76, "Software-Entity"]]}
{"id": 161290, "text": "Training runs on 8 NVIDIA A100 GPUs with 80GB each .", "Comments": [], "label": [[17, 18, "Device-Count"], [19, 35, "Hardware-device"], [41, 45, "Device-Memory"]]}
{"id": 161291, "text": "We utilized a computational infrastructure consisting of 4× A100 40 GPUs for the other generators and detectors .", "Comments": [], "label": [[57, 58, "Device-Count"], [60, 72, "Hardware-device"]]}
{"id": 161292, "text": "Results are presented in Table [ 8 . ] To analyze the count of words and sentences , we utilized the NLTK library using sentence and word tokenizer , removing punctuation from the word count .", "Comments": [], "label": [[101, 105, "Software-Entity"]]}
{"id": 161293, "text": "Figure 6 : Overview of the T-SNE reduced embeddings of the * CLS * token for FT and * MASK * P+FT for every second layer where instance labels are colorized .", "Comments": [], "label": [[27, 32, "Software-Entity"]]}
{"id": 161294, "text": "A Additional Details of the Experiments A.1 Training Setup For all our experiments , we use NVIDIA RTX A6000 GPUs with CUDA ( 11.7 ) , python ( 3.8.10 ) , transformers ( 4.28.0 ) , PyTorch ( 1.13.1 ) , and openprompt ( 1.0.1 ) .", "Comments": [], "label": [[92, 113, "Hardware-device"], [119, 123, "Software-Entity"], [155, 167, "Software-Entity"], [181, 188, "Software-Entity"], [206, 216, "Software-Entity"]]}
{"id": 161295, "text": "Moreover , we use the following tags from the huggingface model hub : A.3 Fold Composition With our evaluation , we want to cover a given dataset fully .", "Comments": [], "label": [[46, 57, "Software-Entity"]]}
{"id": 161296, "text": "4.3 Computational Resources For the instruction tuning , we utilize a 4x40GB A100 GPU server for all models except for the fully fine-tuned LLaMA2-7B model where we use an 8x80GB A100 GPU server .", "Comments": [], "label": [[70, 71, "Device-Count"], [72, 76, "Device-Memory"], [77, 85, "Hardware-device"], [172, 173, "Device-Count"], [174, 178, "Device-Memory"], [179, 187, "Hardware-device"]]}
{"id": 161297, "text": "We run the instruction-tuning using DeepSpeed ZeRO-3 [ Ra ] [ jbhandari et al. , , 2020 ] to optimize the computation time .", "Comments": [], "label": [[36, 52, "Software-Entity"]]}
{"id": 161298, "text": "For evaluation , we run the evaluation on a single 40GB A100 GPU server .", "Comments": [], "label": [[44, 50, "Device-Count"], [51, 55, "Device-Memory"], [56, 64, "Hardware-device"]]}
{"id": 161299, "text": "Similarly for language generation , we observe huge improvements in MT , QA , and paraphrasing tasks with at least ∼20 increase in chrF++ [ Popovic´ , 2015 ] and SacreBLEU [ Post , 2018 ] , respectively .", "Comments": [], "label": [[162, 171, "Software-Entity"]]}
{"id": 161300, "text": "As infrastructure , we have leveraged the CINECA HPC Leonardo computing cluster located in Italy , which is powered by 3,456 nodes each containing 4x custom A100 64GB GPUs .", "Comments": [], "label": [[147, 148, "Device-Count"], [157, 161, "Hardware-device"], [162, 171, "Device-Memory"]]}
{"id": 161301, "text": "We use PyTorch [ Paszke et al. , , 2019 ] , Huggingface Transformers [ Wolf et al. , , 2019 ] and Accelerate [ Gugger et al. , , 2022 ] to perform model training and inference .", "Comments": [], "label": [[7, 14, "Software-Entity"], [44, 55, "Software-Entity"], [98, 108, "Software-Entity"]]}
{"id": 161302, "text": "All model training is conducted with 1x or 2x Nvidia RTX 3090 , or 1x Nvidia RTX A6000 .", "Comments": [], "label": [[37, 38, "Device-Count"], [43, 44, "Device-Count"], [46, 61, "Hardware-device"], [67, 68, "Device-Count"], [70, 86, "Hardware-device"]]}
{"id": 161303, "text": "BLOOMZ-176b inference is conducted with 4x Nvidia A100 SMX 40G .", "Comments": [], "label": [[40, 41, "Device-Count"], [43, 58, "Hardware-device"], [59, 62, "Device-Memory"]]}
{"id": 161304, "text": "The Llama2-70bchat inference is conducted with 2x Nvidia RTX 3090 or Nvidia A40 .", "Comments": [], "label": [[47, 48, "Device-Count"], [50, 65, "Hardware-device"], [69, 79, "Hardware-device"]]}
{"id": 161305, "text": "The inference was run on A40 GPU core , we report an average per-sentence deltas . ρ are Spearman 's correlation coefficients . kenization in the standard T5 model , making it a suitable base model for our setting .", "Comments": [], "label": [[25, 32, "Hardware-device"]]}
{"id": 161306, "text": "We use Jax implementation , i.e. , t5x repository [ Roberts et al. , , 2022 ] , and the same hyperparameters as in ByT5 [ Xue et al. , 2022 ] .", "Comments": [], "label": [[35, 38, "Software-Entity"]]}
{"id": 161307, "text": "The time is an average across evaluation examples , the inference was run on an A40 GPU core .", "Comments": [], "label": [[80, 87, "Hardware-device"]]}
{"id": 161308, "text": "C Technical Details C.1 Compuatational Infrastracture The MyT5 and reimplemented ByT5 models were trained on TPUs available through Google Cloud ( c ) Sequence compression Figure 9 : Average byte sequence lengths of parallel sentences for languages unseen in the morphological analysis used in the construction of MYTE .", "Comments": [], "label": [[109, 113, "Hardware-device"], [132, 144, "Cloud-Platform"]]}
{"id": 161309, "text": "C Training Details For finetuning the pretrained models on the evaluation benchmarks we used the following NVIDIA GPUs : 24GB RTX3090 , 32GB V100 and 80GB A100 .", "Comments": [], "label": [[121, 125, "Device-Memory"], [126, 133, "Hardware-device"], [136, 140, "Device-Memory"], [141, 145, "Hardware-device"], [150, 154, "Device-Memory"], [155, 159, "Hardware-device"]]}
{"id": 161310, "text": "We used v3-8 TPUs for pretraining .", "Comments": [], "label": [[8, 17, "Hardware-device"]]}
{"id": 161311, "text": "Our approach is trained on one NVIDIA A800 GPU .", "Comments": [], "label": [[27, 30, "Device-Count"], [31, 46, "Hardware-device"]]}
{"id": 161312, "text": "We prefer to use ChatGPT for data synthesis primarily because open-source LLMs require extensive GPU resources ( 8 * A100 ) .", "Comments": [], "label": [[113, 114, "Device-Count"], [117, 121, "Hardware-device"]]}
{"id": 161313, "text": "D Implementation Details All experiments are conducted on 8 NVIDIA GeForce RTX 3090 GPUs and 2 NVIDIA A100 80GB PCIe GPUs .", "Comments": [], "label": [[58, 59, "Device-Count"], [60, 88, "Hardware-device"], [93, 94, "Device-Count"], [95, 106, "Hardware-device"], [107, 121, "Device-Memory"]]}
{"id": 161314, "text": "The DeepSpeed library [ 5 ] is utilized to facilitate the training , with a learning rate of 5e-5 over 5 epochs , resulting in approximately 8 hours of training .", "Comments": [], "label": [[4, 13, "Software-Entity"]]}
{"id": 161315, "text": "To achieve higher throughput during inference , we leverage the vLLM library [ 6 ] .", "Comments": [], "label": [[64, 68, "Software-Entity"]]}
{"id": 161316, "text": "For our implementation and evaluation , we use Huggingface library [ 7 ] and vLLM library .", "Comments": [], "label": [[47, 58, "Software-Entity"], [77, 81, "Software-Entity"]]}
{"id": 161317, "text": "Consequently , we focus on these four criteria : E.2 Implementations of Human Evaluation We employ human evaluation , outsourcing the task to assess response quality on Amazon Mechanical Turk ( AMT ) .", "Comments": [], "label": [[169, 199, "Cloud-Platform"]]}
{"id": 161318, "text": "< https : //ai.meta.com/blog/meta-llama-3/ > Renting GPUs on cloud providers such as Amazon is now relatively cheap ( approx 15 $ per hour for 8x40Gb A100 ) and would require as little as 100 $ to replicate one of our LLM training sessions and data generation .", "Comments": [], "label": [[143, 144, "Device-Count"], [145, 149, "Device-Memory"], [150, 154, "Hardware-device"]]}
{"id": 161319, "text": "We also add name of the models in Huggingface-Hub , e.g .", "Comments": [], "label": [[34, 45, "Software-Entity"]]}
{"id": 161320, "text": "The training was performed on 8 nodes , each with 4 V100 GPUs with 16GB VRAM .", "Comments": [], "label": [[50, 51, "Device-Count"], [52, 61, "Hardware-device"], [67, 76, "Device-Memory"]]}
{"id": 161321, "text": "The rest of the parameters are the daults parameters used by Huggingface Trainer .", "Comments": [], "label": [[61, 72, "Software-Entity"]]}
{"id": 161322, "text": "The fine-tuning of * llama-2-7b_it * , * llama-2-13b_it * and * mistral_it * on CHANGE-it took 64 GPU hours each on A100 64Gb GPUs .", "Comments": [], "label": [[116, 120, "Hardware-device"], [121, 130, "Device-Memory"]]}
{"id": 161323, "text": "A cumulative of 2000 hours of computation was performed on hardware similar to A100 PCIe 40/80GB ( TDP of 250W ) .", "Comments": [], "label": [[79, 88, "Hardware-device"], [89, 96, "Device-Memory"]]}
{"id": 161324, "text": "Promising autonomous agents , such as ReAct [ Yao et al. , , 2022 ] , AutoGPT [ Significant Gravitas , 2023 ] , and LangChain [ Langchain-AI , 2023 ] , require the underlying LLM to recall information from previous dialogues , resembling the * understanding * and * state tracking * stage .", "Comments": [], "label": [[70, 77, "Software-Entity"], [116, 125, "Software-Entity"]]}
{"id": 161325, "text": "Other approaches ( * e.g . * , AutoGPT ) adopt an * interactive * planning strategy , where the generation of each action is conditioned on the outcome of the previous planning steps .", "Comments": [], "label": [[31, 38, "Software-Entity"]]}
{"id": 161326, "text": "I Experimental settings for Bahavior Cloning We used 4x A100 Nvidia GPU for training .", "Comments": [], "label": [[53, 54, "Device-Count"], [56, 71, "Hardware-device"]]}
{"id": 161327, "text": "K Experimental settings for Reinforcement Learning from Game-Play ( RLGP ) We adhered to the default settings of the TRLX library for our experiments , but made modifications to certain hyperparameters .", "Comments": [], "label": [[117, 129, "Software-Entity"]]}
{"id": 161328, "text": "The RL models were trained using four Nvidia A100 cards , with a total of 32 rollouts for each training entity .", "Comments": [], "label": [[33, 37, "Device-Count"], [38, 49, "Hardware-device"]]}
{"id": 161329, "text": "The training and inference of 7B size models are conducted on 16 NVIDIA A100 80GB GPUs with DeepSpeed ZeRO-3 Offload .", "Comments": [], "label": [[62, 64, "Device-Count"], [65, 76, "Hardware-device"], [77, 86, "Device-Memory"], [92, 108, "Software-Entity"]]}
{"id": 161330, "text": "The loss weights are based on scikit-learn 's implementation using the `` balanced '' mode , see [ https : //scikit-learn.org/stable/modules/ ] ( https : //scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html ) [ generated/sklearn.utils.class_weight .", "Comments": [], "label": [[30, 45, "Software-Entity"]]}
{"id": 161331, "text": "Instead , we evaluate this through different means : We first use the all-mpnet-base-v2 model from the sentence transformers package [ Reimers and Gurevych , 2019 ] and HDBSCAN with a minimum cluster size of 3 to cluster questions .", "Comments": [], "label": [[112, 132, "Software-Entity"]]}
{"id": 161332, "text": "For the search , we opt for Bayesian hyperparameter search [ Snoek et al. , 2012 ] as implemented by Weights & Biases [ Biewald , 2020 ] .", "Comments": [], "label": [[101, 117, "Software-Entity"]]}
{"id": 161333, "text": "A.6 Environmental Impact All experiments are run on a single V100 NVIDIA GPU .", "Comments": [], "label": [[54, 60, "Device-Count"], [61, 76, "Hardware-device"]]}
{"id": 161334, "text": "A Implementation Details A.1 Deployment Details In our evaluating and detecting experiments , we utilize the widely-used Pytorch and transformers library to load all the models .", "Comments": [], "label": [[121, 128, "Software-Entity"], [133, 153, "Software-Entity"]]}
{"id": 161335, "text": "All the experiments are conducted on Ubuntu 20.04.4 server equipped with 112 Intel Xeon ( R ) Platinum 8336C CPU cores , and graphic cards that contained 8 NVIDIA A100 SXM 80GB GPUs .", "Comments": [], "label": [[73, 76, "Device-Count"], [77, 112, "Hardware-device"], [154, 155, "Device-Count"], [156, 171, "Hardware-device"], [172, 181, "Device-Memory"]]}
{"id": 161336, "text": "Besides , the CUDA version is 11.4 , the Python version is 3.10.11 , the PyTorch version is 2.0.1 and the transformers version is 4.31.0 .", "Comments": [], "label": [[14, 18, "Software-Entity"], [73, 80, "Software-Entity"], [106, 117, "Software-Entity"]]}
{"id": 161337, "text": "We also release hundreds of intermediate checkpoints available as revisions on HuggingFace .", "Comments": [], "label": [[79, 90, "Software-Entity"]]}
{"id": 161338, "text": "Finally , all code and weights are released under the Apache 2.0 License .", "Comments": [], "label": [[54, 60, "Software-Entity"]]}
{"id": 161339, "text": "3.1 Distributed Training Framework We train our models using the * ZeRO * optimizer strategy [ Rajbhandari et al. , , 2019 ] via PyTorch 's FSDP framework [ Zhao et al. , , 2023 ] , which reduces memory consumption by sharding the model weights and their corresponding optimizer state across GPUs .", "Comments": [], "label": [[129, 139, "Software-Entity"], [140, 154, "Software-Entity"]]}
{"id": 161340, "text": "To improve throughput , we employ mixedprecision training [ Micikevicius et al. , , 2017 ] through FSDP 's built-in settings and PyTorch 's amp module .", "Comments": [], "label": [[99, 106, "Software-Entity"], [129, 139, "Software-Entity"]]}
{"id": 161341, "text": "This is the standard way to clip gradients in PyTorch .", "Comments": [], "label": [[46, 53, "Software-Entity"]]}
{"id": 161342, "text": "< https : //www.lumi-supercomputer.eu > The MI250X is a dual-chip module , meaning in practice that each physical device consists of two logical devices , so each node has 8 logical GPU devices with 64GB of memory each .", "Comments": [], "label": [[172, 185, "Device-Count"], [199, 203, "Device-Memory"]]}
{"id": 161343, "text": "The experimental components of this work were made possible through a partnership with AMD and CSC , enabling use of the LUMI supercomputer , and Kempner Institute at Harvard University .", "Comments": [], "label": [[121, 139, "Hardware-device"]]}
{"id": 161344, "text": "The model trained on A100-40GB GPUs was trained in Australia , so we assume a carbon intensity factor of 0.610 , the national average for Australia in 2022 .", "Comments": [], "label": [[21, 25, "Hardware-device"], [26, 35, "Device-Memory"]]}
{"id": 161345, "text": "[ 12 ] The model trained on MI250X GPUs was trained in the LUMI supercomputer , which runs on 100 % renewable , carbon-neutral energy , so we assume a carbon intensity factor of 0 .", "Comments": [], "label": [[28, 39, "Hardware-device"], [59, 77, "Hardware-device"]]}
{"id": 161346, "text": "We use FlashAttention-2 [ Dao , 2023 ] to accelerate language model inference .", "Comments": [], "label": [[7, 23, "Software-Entity"]]}
{"id": 161347, "text": "All model inferences are conducted on A100 GPUs .", "Comments": [], "label": [[38, 47, "Hardware-device"]]}
{"id": 161348, "text": "We use the default prompt templates for each safetyaligned model , as provided in their respective papers or usage examples on Hugging Face ( Table [ 3 ] .", "Comments": [], "label": [[127, 139, "Software-Entity"]]}
{"id": 161349, "text": "Direct Disalignment B.1 Experiment Details We train all models on 8 A100 GPUs with a cosine learning rate scheduler , a learning rate of 1e-4 , weight decay of 0.05 , a global batch size of 128 , and DeepSpeed ZeRO-2 [ Rajbhandari et al. , , 2020 ] for three epochs .", "Comments": [], "label": [[66, 67, "Device-Count"], [68, 77, "Hardware-device"], [200, 216, "Software-Entity"]]}
{"id": 161350, "text": "For * * COLM * * , we use the Hugging Face implementation [ 1 ] of LLaMA-2 , and train it for 20 epochs using the AdamW optimizer [ Loshchilov and Hutter , 2019 ] and LoRA [ Hu et al. , , 2021 ] with learning rate 3e-4 and batch size 32 in Five Tesla V100 GPUs .", "Comments": [], "label": [[30, 42, "Software-Entity"], [240, 244, "Device-Count"], [245, 260, "Hardware-device"]]}
{"id": 161351, "text": "The speedup is measured using a GeForce RTX 3090 GPU with a batch size of 1 .", "Comments": [], "label": [[32, 52, "Hardware-device"]]}
{"id": 161352, "text": "We trained both models for 5 epochs , which took about 20 GPU hours on a single A100 GPU .", "Comments": [], "label": [[73, 79, "Device-Count"], [80, 88, "Hardware-device"]]}
{"id": 161353, "text": "We then finetuned the Video-LLaMA 2 ( 13B ) model on our training data on 2 A100 GPUs .", "Comments": [], "label": [[74, 75, "Device-Count"], [76, 85, "Hardware-device"]]}
{"id": 161354, "text": "We use the released model checkpoints on Huggingface for all of the open-source models tested in this paper .", "Comments": [], "label": [[41, 52, "Software-Entity"]]}
{"id": 161355, "text": "All experiments were performed on an A6000 GPU .", "Comments": [], "label": [[37, 46, "Hardware-device"]]}
{"id": 161356, "text": "We used 8-bit quantization using the BitsAndBytes library [ Dettmers et al. , 2022 ] on all models .", "Comments": [], "label": [[37, 57, "Software-Entity"]]}
{"id": 161357, "text": "We use Google 's langdetect [ Nakatani , 2010 ] and Facebook 's FastText Lang Detect [ Joulin et al. , , 2016 ] .", "Comments": [], "label": [[7, 27, "Software-Entity"], [64, 84, "Software-Entity"]]}
{"id": 161358, "text": "All experiments are conducted with 2 NVIDIA A100 ( 40GB ) GPUs .", "Comments": [], "label": [[35, 36, "Device-Count"], [37, 48, "Hardware-device"], [49, 62, "Device-Memory"]]}
{"id": 161359, "text": "We train 4-gram LMs using OpenGRM [ Roark et al. , , 2012 ] without smoothing on each corpus , leveraging their frequency countbased nature to directly compare prevalence of cultural contexts and entities across corpora .", "Comments": [], "label": [[26, 33, "Software-Entity"]]}
{"id": 161360, "text": "We used the HuggingFace inference API [ 11 ] to prompt BLOOM which returns token log probabilities when using the details : true parameter . * * JAIS * * [ Sengupta et al. , , 2023 ] : a 13 billion parameter bilingual LM trained on English and Arabic .", "Comments": [], "label": [[12, 23, "Software-Entity"]]}
{"id": 161361, "text": "Fine-tuning was performed on 1 NVIDIA A100 GPU .", "Comments": [], "label": [[29, 30, "Device-Count"], [31, 46, "Hardware-device"]]}
{"id": 161362, "text": "In specific , we use spaCy [ Honnibal and Montani , 2017 ] to extract the entities in each key point , and then collect those sharing at least one common entity .", "Comments": [], "label": [[21, 26, "Software-Entity"]]}
{"id": 161363, "text": "Our experiments run on four A5000 GPUs with 25G memory space .", "Comments": [], "label": [[23, 27, "Device-Count"], [28, 38, "Hardware-device"], [44, 54, "Device-Memory"]]}
{"id": 161364, "text": "A.2 Pseudocode Algorithm 1 MIPO `` ` 6 : for t = 0 to T do `` ` `` ` 19 : end for `` ` A.3 Experiment details For TextWorld experiments , our method and all baselines are implemented using PyTorch based on the code of GATA [ Adhikari et al. , , 2020 ] [ 1 ] .", "Comments": [], "label": [[189, 196, "Software-Entity"]]}
{"id": 161365, "text": "Spacy with en_core_web_sm model is used in all methods , which is consistent with GATA .", "Comments": [], "label": [[0, 5, "Software-Entity"]]}
{"id": 161366, "text": "All machines are equipped with 12 CPU cores and 1 RTX 3090 GPU .", "Comments": [], "label": [[31, 43, "Device-Count"], [48, 49, "Device-Count"], [50, 62, "Hardware-device"]]}
{"id": 161367, "text": "For VirtualHome experiments , all methods are trained in a local machine with 4 Nvidia GPUs ( A100 ) and 224 Intel CPU Cores .", "Comments": [], "label": [[78, 79, "Device-Count"], [80, 100, "Hardware-device"], [105, 125, "Device-Count"]]}
{"id": 161368, "text": "Implementation details In this paper , we use GPT-3.5-Turbo-0613 [ 6 ] and LongChat-13B-16k as the target LLMs , both accessible via OpenAI [ 7 ] and HuggingFace [ 8 ] .", "Comments": [], "label": [[150, 161, "Software-Entity"]]}
{"id": 161369, "text": "We implement our approach with PyTorch 1.13.1 and Hugging-Face Transformers .", "Comments": [], "label": [[31, 45, "Software-Entity"], [50, 62, "Software-Entity"]]}
{"id": 161370, "text": "Reorder : we reorder the documents with relevance metrics of different baselines as our document reordering strategy described in Sec . [ 4.2 . ] In the case of OpenAI , it corresponds to LongContextReorder in the LangChain framework [ Chase , 2022 ] .", "Comments": [], "label": [[214, 233, "Software-Entity"]]}
{"id": 161371, "text": "Latency evaluation We conducte end-to-end latency testing on a V100-32G , using the prompts from Multi-document QA , LongBench , and Zero-SCROLLS in the API call , and results are shown in Table [ 1 , ] [ 2 ] and [ 6 . ]", "Comments": [], "label": [[63, 67, "Hardware-device"], [68, 71, "Device-Memory"]]}
{"id": 161372, "text": "B.2 Other Implementation Details All experiments were conducted using a Tesla V100 ( 32GB ) .", "Comments": [], "label": [[72, 82, "Hardware-device"], [85, 89, "Device-Memory"]]}
{"id": 161373, "text": "We use tiktoken [ 15 ] and GPT-3.5- Turbo model to count all the tokens .", "Comments": [], "label": [[7, 15, "Software-Entity"]]}
{"id": 161374, "text": "All model training processes are executed on 4 Nvidia GeForce RTX 3090 GPUs , each equipped with 24GB of memory . 6 Experiments This chapter evaluates the therapy capabilities of our model and baselines . 6.1 Experimental Settings Baselines .", "Comments": [], "label": [[45, 46, "Device-Count"], [47, 75, "Hardware-device"], [97, 111, "Device-Memory"]]}
{"id": 161375, "text": "For each model , we employ default parameter settings , utilizing official models for open-source LLMs obtained from Hugging Face .", "Comments": [], "label": [[117, 129, "Software-Entity"]]}
{"id": 161376, "text": "These testing procedures take place on a computational infrastructure consisting of three Nvidia GeForce RTX 3090 GPUs , each equipped with 24GB of memory . 6.2 Testing Therapy Models with an AI Client Evaluation Metrics .", "Comments": [], "label": [[84, 89, "Device-Count"], [90, 118, "Hardware-device"], [140, 154, "Device-Memory"]]}
{"id": 161377, "text": "In all our experiments , we use only one 10 GB GPU ( RTX 3080 ) with a batch size of 64 .", "Comments": [], "label": [[37, 40, "Device-Count"], [41, 50, "Device-Memory"], [53, 61, "Hardware-device"]]}
{"id": 161378, "text": "However , in scenarios with limited computational resources , the Parameter-Efficient Fine-Tuning ( PEFT ) [ Fu et al. , , 2023 ] is ineffective for DeCoGLM due to the significant disparities between the sequence labeling task of the error detection module and the mask-infilling pretraining task of GLM .", "Comments": [], "label": [[66, 97, "Software-Entity"], [100, 104, "Software-Entity"]]}
{"id": 161379, "text": "The configurations for training the separate models , DeGLM and CoGLM , are similar to those in Table [ 8 . ] The pre-trained models include glm-roberta-large , glm-large-chinese , glm-10b , and glm-10b-chinese , accessible through HuggingFace [ 2 ] .", "Comments": [], "label": [[232, 243, "Software-Entity"]]}
{"id": 161380, "text": "We implement all the designed models using PyTorch , including DeCoGLM , DeGLM , and CoGLM .", "Comments": [], "label": [[43, 50, "Software-Entity"]]}
{"id": 161381, "text": "All models are trained by the Trainer from the transformers [ 3 ] package in Python , on NVIDIA RTX 4090 GPUs .", "Comments": [], "label": [[47, 59, "Software-Entity"], [89, 109, "Hardware-device"]]}
{"id": 161382, "text": "The experiments are conducted on an NVIDIA RTX 4090 GPU , with the same constrained batch size of 1 during inference .", "Comments": [], "label": [[36, 55, "Hardware-device"]]}
{"id": 161383, "text": "The models are trained on LibriLight using 16 NVIDIA TESLA V100 32GB GPUs with a batch size of 64 for 1M steps , which takes about 1 week .", "Comments": [], "label": [[43, 45, "Device-Count"], [46, 63, "Hardware-device"], [64, 73, "Device-Memory"]]}
{"id": 161384, "text": "All experiments are conducted using the FAIRSEQ toolkit [ Ott et al. , , 2019 ] .", "Comments": [], "label": [[40, 47, "Software-Entity"]]}
{"id": 161385, "text": "We showcase and provide three implementations of FFF forward pass based on advanced PyTorch compilation , the OpenAI Triton framework , and the Intel MKL routines .", "Comments": [], "label": [[84, 91, "Software-Entity"], [110, 116, "Software-Entity"], [144, 153, "Software-Entity"]]}
{"id": 161386, "text": "By default , we train every model for 1 day on a single A6000 GPU , except for the final UltraSparseBERT-1x11-long model , which we train 2 times longer using the same regime for slightly better downstream performance .", "Comments": [], "label": [[49, 55, "Device-Count"], [56, 65, "Hardware-device"]]}
{"id": 161387, "text": "We see that UltraSparseBERT variants trained for 1 day on a single A6000 GPU all retain at least 96.0 % of the GLUE downstream predictive performance of the original BERT-base model [ Devlin et al. , 2018 ] .", "Comments": [], "label": [[60, 66, "Device-Count"], [67, 76, "Hardware-device"]]}
{"id": 161388, "text": "For CPU inference , we use the Math Kernel Library available as a part of the Intel oneAPI .", "Comments": [], "label": [[31, 50, "Software-Entity"]]}
{"id": 161389, "text": "The support for this implementation without copying is currently only available on PyTorch nightly builds .", "Comments": [], "label": [[83, 98, "Software-Entity"]]}
{"id": 161390, "text": "For CPU inference , we perform 250 forward passes per entry on Intel ( R ) Core ( TM ) i7-6700HQ CPUs under Intel MKL v2023.2.0 , using 64-bit variants of all routines .", "Comments": [], "label": [[63, 102, "Hardware-device"], [108, 127, "Hardware-device"]]}
{"id": 161391, "text": "For GPU inference , we perform 1000 forward passes per entry on NVIDIA RTX A6000 GPUs under CUDA v12.1 and Py-Torch 2.1.1-nightly .", "Comments": [], "label": [[64, 85, "Hardware-device"], [92, 96, "Software-Entity"], [107, 115, "Software-Entity"]]}
{"id": 161392, "text": "Commercial Models < https : //platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo > Open-Source Models All models were sampled using greedy decoding , and local models were loaded using FP16 precision on 3 NVIDIA RTX A6000s .", "Comments": [], "label": [[206, 207, "Device-Count"], [208, 225, "Hardware-device"]]}
{"id": 161393, "text": "All the experiments can run via a single NVIDIA GeForce RTX 4070 GPU within a reasonable time .", "Comments": [], "label": [[34, 40, "Device-Count"], [41, 68, "Hardware-device"]]}
{"id": 161394, "text": "Finetuning was conducted with an effective batch size of 12 , using a single NVIDIA A6000 Ada GPU with 48GB VRAM .", "Comments": [], "label": [[70, 76, "Device-Count"], [77, 89, "Hardware-device"], [103, 112, "Device-Memory"]]}
{"id": 161395, "text": "We utilize the calculate-flops.pytorch [ 2 ] library to compute total FLOPs across all function calls .", "Comments": [], "label": [[15, 38, "Software-Entity"]]}
{"id": 161396, "text": "The whole process takes one day on two A100 80GB cards .", "Comments": [], "label": [[35, 38, "Device-Count"], [39, 43, "Hardware-device"], [44, 54, "Device-Memory"]]}
{"id": 161397, "text": "For example , one run of ru-en-RoSBERTa of the complete evaluation experiment on a single A100 GPU 80GB takes approximately 19 hours .", "Comments": [], "label": [[83, 89, "Device-Count"], [90, 98, "Hardware-device"], [99, 103, "Device-Memory"]]}
{"id": 161398, "text": "Training is conducted on a single H100 node .", "Comments": [], "label": [[27, 33, "Device-Count"], [34, 38, "Hardware-device"]]}
{"id": 161399, "text": "We utilize the BGE [ 31 ] codebase and adapt it to our experiments .", "Comments": [], "label": [[15, 18, "Software-Entity"]]}
{"id": 161400, "text": "PyTorch 's expandable_segments helps us to mitigate fragmentation issues due to variable sequence length .", "Comments": [], "label": [[0, 10, "Software-Entity"]]}
{"id": 161401, "text": "We run evaluation on NVIDIA A100 80GB with torch 2.2.1+cu118 and transformers 4.40.2 .", "Comments": [], "label": [[21, 32, "Hardware-device"], [33, 37, "Device-Memory"], [43, 54, "Software-Entity"], [65, 84, "Software-Entity"]]}
{"id": 161402, "text": "All experiments are trained on a single NVIDIA Tesla A100 .", "Comments": [], "label": [[33, 39, "Device-Count"], [40, 57, "Hardware-device"]]}
{"id": 161403, "text": "6.3 Visualisation Analysis To further verify that our method can effectively mitigate the heterogeneity between factor embeddings , we utilize t-SNE [ Van der Maaten and Hin ] [ ton , 2008 ] to visualize the learned entity , relation , and timestamp embeddings .", "Comments": [], "label": [[143, 148, "Software-Entity"]]}
{"id": 161404, "text": "* As shown in Table [ 3 ] ( all experiments are conducted on a single Tesla V100 GPU ) , SAPIENT-e takes similar training time with baselines because it collects all the trajectories from MCTS and do not incur additional search cost .", "Comments": [], "label": [[63, 69, "Device-Count"], [70, 84, "Hardware-device"]]}
{"id": 161405, "text": "Influence of MCTS rollouts To study the influence of MCTS rollouts , we set N from 1 ( equivalent to disabling MCTS , as there is no selection and reward back-propagation when N = 1 ) from 50 and plot the success rate and training time ( on a single Tesla V100 GPU ) in Figure [ A3 . ]", "Comments": [], "label": [[243, 249, "Device-Count"], [250, 264, "Hardware-device"]]}
{"id": 161406, "text": "Under the same training pipeline with a single Tesla V100 GPU , SAPIENT with 20 rollouts per user takes 698 seconds per 100 gradient descent steps on the LastFM dataset and 1049 seconds per 100 gradient descent steps on the Amazon-Book dataset on average , which is about twice as slow as the two competitive baselines ( HutCRS : 305 seconds/100 steps on LastFM , 465 seconds/100 steps on Amazon-Book ; MCMIPL : 429 seconds/100 steps on LastFM , 548 seconds/100 steps on Amazon-Book ) .", "Comments": [], "label": [[40, 46, "Device-Count"], [47, 61, "Hardware-device"]]}
{"id": 161407, "text": "A Implementation Details All experiments in this paper were conducted on GPU clusters with 4 NVIDIA A100 GPUs .", "Comments": [], "label": [[91, 92, "Device-Count"], [93, 109, "Hardware-device"]]}
{"id": 161408, "text": "For the embedding model for column filter in Section [ 4.3.2 , ] we utilize bge-large-en model [ Xiao et al. , , 2023 ] and employ FAISS [ Johnson et al. , , 2019 ] for efficient similarity search .", "Comments": [], "label": [[131, 136, "Software-Entity"]]}
{"id": 157358, "text": "For the image modality , we use the RandAugment technique .", "Comments": [], "label": []}
{"id": 160284, "text": "Position embeddings are crucial for modeling sequence order in transformers .", "Comments": [], "label": []}
{"id": 154469, "text": "We used the following three configurations to define nearduplicates ( there are many other ways to define near-duplicates , which we leave for future work ) .", "Comments": [], "label": []}
{"id": 152583, "text": "Pre-trained language models based on the transformer architecture [ Vaswani et al. , , 2017 ] have improved the state-of-the-art results on many NLP applications .", "Comments": [], "label": []}
{"id": 155385, "text": "In the second training stage , we utilize the distilled router to determine the tokento-expert assignment .", "Comments": [], "label": []}
{"id": 158764, "text": "The reason for this may be that CRNN is more specialized in OCR than ResNet .", "Comments": [], "label": []}
{"id": 156265, "text": "We use the contextualized representations from the pretrained language models as the token embeddings for the downstream reading comprehension tasks . 2 ) coreference resolution component .", "Comments": [], "label": []}
{"id": 152041, "text": "However , when document-level Transformer is compared , we use document-level BLEU score ( d-BLEU ) since the sentence-to-sentence alignment is not available . s-BLEU .", "Comments": [], "label": []}
{"id": 156021, "text": "The state-of-the-art Transformer models for these tasks utilize a single embedding from the top encoder layer , such as the CLS token , for prediction .", "Comments": [], "label": []}
{"id": 152307, "text": "Compared with early approaches , especially for BERTSUMEXT , we observe that BERT outperforms all previous non-BERT-based summarization systems , and Trigram-Blocking leads to a great improvement on all ROUGE metrics .", "Comments": [], "label": []}
{"id": 158511, "text": "E3 is thus defined as the task of automatically identifying Y for a given s ∈ S. 5.2 Model As our baseline E3 model , often referred to as the * evidence extractor ( EE ) * , we employ the extractor module in the DYLE [ Mao et al. , , 2022 ] model , but using a given summary sentence as an additional input for the encoder .", "Comments": [], "label": []}
{"id": 156252, "text": "6 Related Work Open-Domain Dialogue Generation Recent work focused on fine-tuning large pre-trained transformer models [ Radford et al. , , 2019 ] [ Zhang et al. , , 2020a ] [ Roller et al. , , 2021 ] on massive dialogue data .", "Comments": [], "label": []}
{"id": 157827, "text": "The pseudocode of our proposed * Graph Traversal Algorithm * is described in Appendix [ A.2 . ] https : //spacy.io/ The nodes are ordered according to their sentences ' indexes in the original context .", "Comments": [], "label": []}
{"id": 155859, "text": "However , it is challenging to encode it efficiently into the modern Transformer architecture .", "Comments": [], "label": []}
{"id": 153768, "text": "MM-Deacon and MLM-CLS both used 6 layers of Transformer blocks to process SMILES .", "Comments": [], "label": []}
{"id": 153023, "text": "4.2.1 IMT with a Single Iteration Since IMT with a single iteration can be seen as machine translation with lexical constraints where human interactions are considered as constraints , we first conduct an experiment to evaluate the performance of BiTIIMT by following [ Hokamp and ] Table 2 : BLEU / Relative decoding time cost w.r.t Transformer baseline for five settings with 1 to 5 constraint segments on WMT14 En-De , WMT14 En-Fr , and Zh-En datasets .", "Comments": [], "label": []}
{"id": 156557, "text": "In Appendix , we also provide additional results : ( i ) SINGULARITY-temporal for retrieval and QA ; ( ii ) zero-shot retrieval ; ( iii ) image-text retrieval ; ( iv ) image QA , etc .", "Comments": [], "label": []}
{"id": 160271, "text": "Table [ 3 ] shows that increasing the search space for possible insertion tokens leads to a notable drop in the retrieval Varying Lambda vs Mean MRR ( a ) Ablation on the noun-object similarity threshold t for the text-to-image retrieval task ( MSCOCO ) .", "Comments": [], "label": []}
{"id": 158619, "text": "We follow the general transformer-based encoder-decoder framework , where the encoder summarizes the source sentences and the decoder will generate the target sentences based on source representations in an autoregressive manner .", "Comments": [], "label": []}
{"id": 153671, "text": "To identify headlines related to climate change , we use keyword filtering .", "Comments": [], "label": []}
{"id": 156742, "text": "Given an input x consisting of a pair ( xtext , xmeaning ) , we use existing algorithms to align tokens in individual training examples .", "Comments": [], "label": []}
{"id": 159575, "text": "We use mBART-50 [ Tang et al. , , 2021 ] and mT5 [ Xue et al. , , 2021 ] as baselines , which have achieved state-of-the-art performances on many CLS/MLS datasets [ Perez-Beltrachini and Lapata , 2021 ] [ Hasan et al. , , 2021a ] [ Feng et al. , , 2022 ] .", "Comments": [], "label": []}
{"id": 158791, "text": "A.2 Dataset details We use the INTERGLAD V7.0 ( Interglad ) database [ NGF , 2019 ] for annotating our training set as described in Section [ 7 . ]", "Comments": [], "label": []}
{"id": 152912, "text": "For English-Ukrainian , we use the TED Talk transcripts from July 2020 in the OPUS repository [ Tiedemann , 2012 ] for finetuning and testing .", "Comments": [], "label": []}
{"id": 154800, "text": "We use N = 3 in the following experiments .", "Comments": [], "label": []}
{"id": 152466, "text": "Therefore , to better model rhymes , we use a reverse-order language model to generate sentences from right to left , as shown in Figure [ 3 . ] Doing so we can easily identify the last few words of a sentence ( now become the first few words of the reverse sentence ) to control their rhymes .", "Comments": [], "label": []}
{"id": 155155, "text": "Even after filtering by heuristic , we found that many of the remaining headline changes still do not reflect a substantive update to the article .", "Comments": [], "label": []}
{"id": 154721, "text": "HellaSWAG is a dataset built using adversarial filtering to generate challenging out-ofdomain samples .", "Comments": [], "label": []}
{"id": 159285, "text": "Some researchers blazed a brand new path that omits the whole evidence retrieval process , including corpus indexing and evidence searching , by leveraging generative language models ( such as T5 , BART , GPT ) to tackle ODQA tasks [ Roberts et al. , , 2020 ] [ Brown et al. , 2020 ] [ Lewis et al. , , 2020a ] .", "Comments": [], "label": []}
{"id": 156945, "text": "Experimental Settings during Pre-training We use the first 960 hours of speech and textual transcripts of Spotify100K dataset for pre-training .", "Comments": [], "label": []}
{"id": 156854, "text": "During the backpropagation , we use the straight-through gradient estimator [ Bengio et al. , , 2013 ] to approximate the gradient of the scores , where function h ( · ) is treated as the identity function .", "Comments": [], "label": []}
{"id": 156841, "text": "We use the model 's precision-recall curve on the dev set to determine a threshold that maximizes the token match micro-F1 score , and use this threshold for evaluation on the test set .", "Comments": [], "label": []}
{"id": 157568, "text": "Research Questions We also report the empirical results of our student 's t-tests comparing different OpenIE systems , which we use to answer the research questions we raise in section [ 6 . ]", "Comments": [], "label": []}
{"id": 155080, "text": "Then the triplet will be fed into the same Transformer encoder and get the embedding of each sequence with [ CLS ] tokens for contrastive learning . 4.1 Input Representation Source Code .", "Comments": [], "label": []}
{"id": 156595, "text": "Table 5 : SINGULARITY-temporal results on video question answering . trieval performance , but not ActivityNet QA .", "Comments": [], "label": []}
{"id": 156737, "text": "BLEU , COMET and word f-measures statistically significantly higher than no-context ( p < 0.05 ) are underlined .", "Comments": [], "label": []}
{"id": 154458, "text": "We used two models : Code2Vec from [ Alon et al. , 2019b ] and Code2Seq from [ Alon et al. , 2019a ] .", "Comments": [], "label": []}
{"id": 154913, "text": "Finally , we use the information on how mentions cluster into character entities to pool the maximum values of the Mention Logits by entity .", "Comments": [], "label": []}
{"id": 160238, "text": "OneIE uses BERTlarge as its default and we use T5-large for our proposed DICE-MI module . < https : //pytorch.org/ > < https : //github.com/huggingface/transformers > ED and EAE .", "Comments": [], "label": []}
{"id": 158612, "text": "Table 18 : Case study of commonsense inference generation ( COMET ) .", "Comments": [], "label": []}
{"id": 157507, "text": "The T5 models are trained on 1-5 digits of up to 2,000 examples and each training example consists of random numbers in the format of 2 4 1 .", "Comments": [], "label": []}
{"id": 152810, "text": "We used ALBERT-xlarge ( L=24 , H=2048 , A=32 ) and RoBERTa-base ( L=12 , H=768 , A=12 ) as the students .", "Comments": [], "label": []}
{"id": 156847, "text": "Our contributions are summarized as follows : work pruning and sharing , where we use binary masks to decide when to discard or share the parameters .", "Comments": [], "label": []}
{"id": 158410, "text": "For SupCon , we use a joint objective with Cross Entropy , with weight α = 2 to the SupCon loss .", "Comments": [], "label": []}
{"id": 157429, "text": "T5 [ Raffel et al. , , 2020 ] is pre-trained to fill in randomly corrupted text spans .", "Comments": [], "label": []}
{"id": 153284, "text": "As shown in Table [ 3 , ] the vanilla BMI-based approach requires additional 12 CPU hours to obtain the BMI values during the pre-processing stage , and about 2.0 GB of disk space to store these BMI values .", "Comments": [], "label": []}
{"id": 153545, "text": "In this work , we use a combination of all three for spatial grounding to balance between flexibility and consistency : we choose from a predefined set of questions to determine the country ( U.S. vs non-U.S. ) and state , use free text for the name of the city , and span selection for more granular locale information ( e.g. , * '' a pork plant '' * ) .", "Comments": [], "label": []}
{"id": 157426, "text": "Besides , SPARTA leverage conversation prompting to reformulate conversational QG into a masked question-filling task similar to T5 to alleviate the domain gap between the objective of pre-trained LM and conversational QG .", "Comments": [], "label": []}
{"id": 158071, "text": "Experiments conducted on three standard benchmarks with both small and largescale knowledge bases demonstrate that our retriever performs knowledge retrieval more effectively than existing methods .", "Comments": [], "label": []}
{"id": 154093, "text": "More recently , neural approaches have begun to close the gap between automatic and human judgements of semantic text similarity using Transformer-based language models such as BERT [ Zhang et al. , , 2019a ] [ Sellam et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 158569, "text": "While these works focus on the conceptualization of entities , [ He et al. , 2022 ] constructed an event conceptualization benchmark based on ATOMIC [ Sap et al. , , 2019a ] by combining syntactic parsing , semantically heuristic matching , and human annotation .", "Comments": [], "label": []}
{"id": 155543, "text": "During inference , we saw a 15 % speed-up on GPU and 57 % on CPU .", "Comments": [], "label": []}
{"id": 153090, "text": "For this reason , we use it to create a dense latent representation of each document , called document em- bedding , which is a vector of continuous numbers that indicates a point in a latent semantic space .", "Comments": [], "label": []}
{"id": 159193, "text": "Hence , we adopt a postediting method to investigate the potential of using omissions .", "Comments": [], "label": []}
{"id": 157232, "text": "Specifically , we utilize Integrated Gradients ( IG ) [ Sundararajan et al. , 2017 ] , a well-established gradient-based attribution method , to calculate the importance score of each input token .", "Comments": [], "label": []}
{"id": 157551, "text": "This problem also occurs on DeBERTa and T5 .", "Comments": [], "label": []}
{"id": 160429, "text": "] ( https : //huggingface.co/T-Systems-onsite/cross-en-de-roberta-sentence-transformer ) CATS [ Štajner et al. , ] [ 2018 ) is an alignment ] ( https : //huggingface.co/T-Systems-onsite/cross-en-de-roberta-sentence-transformer ) [ method that can also align paragraphs and sen ] ( https : //huggingface.co/T-Systems-onsite/cross-en-de-roberta-sentence-transformer ) [ tences .", "Comments": [], "label": []}
{"id": 158059, "text": "As demonstrated in Table [ 5 , ] the fast and slow learning ( FSL ) can be beneficial to deep MP , and therefore we use twospeed learning rate for pre-training the routers and the prompts of deep MP .", "Comments": [], "label": []}
{"id": 154809, "text": "Flooding Flooding [ Ishida et al. , , 2020 ] has been introduced in detail in section [ 2.1 . ] We implemented Flooding and search for the flooding level at the step of 0.01 according to the tradition .", "Comments": [], "label": []}
{"id": 158191, "text": "Specifically , we use a question intent detection dataset , TREC-10 [ Li and Roth , 2002 ] and construct 5 splits .", "Comments": [], "label": []}
{"id": 156037, "text": "We use * Coreset-select-opt * ( CS-opt ) to represent the best value from the * Coreset-select-k-1 * and * Coreset-select-x * .", "Comments": [], "label": []}
{"id": 159680, "text": "Results From Table [ 5 , ] we see that the GPT3 model is much better at repairing speech disfluencies and ASR errors than the T5 model , achieving 70 % EM .", "Comments": [], "label": []}
{"id": 158265, "text": "Following prior works , we report for each method the speedup relative to the autoregressive transformer base baseline from their original paper [ Xiao et al. , , 2022 ] .", "Comments": [], "label": []}
{"id": 159812, "text": "Next we evaluate the impact of CoT explanations in two settings : As additional context for prompting GPT-3 , and then as additional supervision signal with which to train Flan-T5 .", "Comments": [], "label": []}
{"id": 154345, "text": "To mitigate such issue , we use average perfor- mance of 10 runs on 10 randomly sampled corpus .", "Comments": [], "label": []}
{"id": 154561, "text": "Other baselines are from [ Zeng et al. , , 2021a ] . with SCL Figure [ 4 ( b ) , ] our method further push apart samples from different classes , especially for the classes of whisper-mode ( green ) and cancel ( blue ) .", "Comments": [], "label": []}
{"id": 158573, "text": "As both tasks are inherently text/triple classification , we adopt KG-BERT [ Yao et al. , , 2019 ] as the skeleton of our models .", "Comments": [], "label": []}
{"id": 155800, "text": "We parameterize them with the multi-head attention-based encoder or decoder , similar to Transformer [ Vaswani et al. , 2017 ] .", "Comments": [], "label": []}
{"id": 158484, "text": "Then , we enter these queries into the paper database to fetch the most relevant papers between 2000 and 2021 with Elasticsearch , a modern text retrieval engine that stores and retrieves papers .", "Comments": [], "label": []}
{"id": 159447, "text": "LSLs are composed of one `` regular '' Transformer encoder layer * per language * .", "Comments": [], "label": []}
{"id": 154143, "text": "Our implementation is based on open-source project * transformers * [ 6 ] .", "Comments": [], "label": []}
{"id": 155148, "text": "To verify that our pre-trained model is more suitable for NAT , we use a recently pretrained model mRASP [ Lin et al. , , 2020 ] to finetune on downstream language pairs .", "Comments": [], "label": []}
{"id": 155465, "text": "For the l-th transformer layer , the output of the multi-headed self-attention is computed via : where previous layer 's output Hl− ∈ R ×d is linearly mapped to a triplet of queries , keys and values respectively .", "Comments": [], "label": []}
{"id": 159399, "text": "Two such explanations can be both correct even though the BLEU or ROUGE similarity may be low .", "Comments": [], "label": []}
{"id": 154934, "text": "[ Shen et al. , 2021 ] also create a handcrafted linguistic probe test set , where incoherence is manually inserted based on a range of linguistic phenomena ; we use this test set for analysis ( [ §4 ] .", "Comments": [], "label": []}
{"id": 158784, "text": "We use the same index embeddings for rows and columns so that our model stays transpose-invariant .", "Comments": [], "label": []}
{"id": 160290, "text": "For example , k = 2 achieved superior performance in STS and SICK than the default value k = 3 The one-million sentences sampled from Wikipedia is taken from datasets for SimCSE [ Gao et al. , , 2021 ] , which can be downloaded from [ https : //huggingface.co/datasets/ ] ( https : //huggingface.co/datasets/princeton-nlp/datasets-for-simcse/tree/main ) [ princeton-nlp/datasets-for-simcse/tree/main ] ( https : //huggingface.co/datasets/princeton-nlp/datasets-for-simcse/tree/main ) .", "Comments": [], "label": []}
{"id": 152771, "text": "For regular Wikipedia text , we use the same index containing 21M passages as Table 2 : End-to-end open-domain QA evaluation of UDT-QA in comparison to recent state-of-the-art models on the test sets of NQ and WebQ .", "Comments": [], "label": []}
{"id": 157141, "text": "To solve Problem [ 1 ] efficiently , we adopt an SVD-free algorithm called GreBsmo [ Zhou and Tao , 2013 ] ( refer to Section [ A.2 ] .", "Comments": [], "label": []}
{"id": 151917, "text": "To calculate the attention weights eiE in R-GCNs , We use the non-linearly transformed h + s as the query , the non-linearly transformed h as the value and key .", "Comments": [], "label": []}
{"id": 160591, "text": "In nowadays GPU era , to make full use of parallel computation is an important issue to enhance the processing speed .", "Comments": [], "label": []}
{"id": 151466, "text": "Liu . 2020 . [ Exploring ] ( http : //jmlr.org/papers/v21/20-074.html ) [ the limits of transfer learning with a unified text-to ] ( http : //jmlr.org/papers/v21/20-074.html ) [ text transformer . ] ( http : //jmlr.org/papers/v21/20-074.html ) * Journal of Machine Learning Research * , 21 ( 140 ) :1–67 .", "Comments": [], "label": []}
{"id": 153016, "text": "Our BiTiIMT model is based on the base Transformer architecture and trained on the synthetic datasets mentioned in Section [ 3.3 . ]", "Comments": [], "label": []}
{"id": 159892, "text": "Linear Transformers achieve excellent performance on MNLI and SNLI , but perform poorly on QNLI and QQP .", "Comments": [], "label": []}
{"id": 154803, "text": "We employ existing datasets ( with all their limitations ) and use them only for illustration purposes and preliminary evaluation of the proposed methodology .", "Comments": [], "label": []}
{"id": 153077, "text": "It provides metaphoricity scores including 0 as no , 2 as conventional , and 3 as clear metaphor . [ 2 ] We use the examples with score 0 as literal , and others as metaphor .", "Comments": [], "label": []}
{"id": 151718, "text": "To preserve interpretability , the booster number of xgboost is set to 1 , which means it only learns one decision tree .", "Comments": [], "label": []}
{"id": 154828, "text": "To show that our framework can be applied to high-dimensional settings , we additionally train a Lorentz Transformer of the same size as Transformer base , and compare their performance on WMT'14 .", "Comments": [], "label": []}
{"id": 160239, "text": "We use authors ' codebases to produce baseline results .", "Comments": [], "label": []}
{"id": 152827, "text": "As described in Section [ 3.4 , ] we use edit-distance based filtering to choose * one * generated ( q 0 , c0 , a ) triple to augment for every original example , ( q , c , a ) .", "Comments": [], "label": []}
{"id": 155820, "text": "Following the most common practices in NAT models [ Wei et al. , , 2019 ] [ Li et al. , , 2019 ] , we use * Softcopy * mechanism for initializing the decoder inputs H = ( h1 , h2 , · · · , hm ) : where E = ( e1 , e2 , · · · , en ) is the encoded representation of X = ( x1 , x2 , · · · , xn ) , n and m are the length of source and target sentences , respectively .", "Comments": [], "label": []}
{"id": 155321, "text": "All are less effective in the multi-class setting than in the one-vs-rest settings and the transformer-based classifiers are clearly more effec- Table 3 : Effectiveness of spoiler type classification in the one-vs-one ( phrase-vs-passage ) setting on 826 test posts ( training : 2,641 ; validation : 657 ) .", "Comments": [], "label": []}
{"id": 153932, "text": "The hyperparameters except Transformer architecture for GLM410M and GLM515M are the same as those of GLMLarge .", "Comments": [], "label": []}
{"id": 155065, "text": "Other than the learning rate , we use the same approach as for zero-shot finetuning .", "Comments": [], "label": []}
{"id": 158464, "text": "We use the original first half to encode the input text as well as the verbalized condition , and we use the copied first half to encode the verbalized relation .", "Comments": [], "label": []}
{"id": 157648, "text": ". . , Rn } , the expected output of this subtask is T . 4.1 Aligning Conversations to Recipe Steps For this subtask , we adopt a simple unsupervised approach to track the instruction state .", "Comments": [], "label": []}
{"id": 159290, "text": "Specifically , we use * Q/s * to measure the processing speed , use * total memory overhead * to evaluate the memory cost , and use * EM * score to estimate the end-to-end answer prediction quality as shown in Table [ 1 . ]", "Comments": [], "label": []}
{"id": 153068, "text": "For the text classification task AG-News , our implementation is based on an open-source NLP benchmark [ 5 ] .", "Comments": [], "label": []}
{"id": 152099, "text": "Experiments on a large-scale dataset [ Zheng et al. , , 2019 ] show GIT outperforms the existing best methods by 2.8 F1 .", "Comments": [], "label": []}
{"id": 154618, "text": "7 Ethical Considerations The dialogue models we use in this work utilize large language models , and therefore have similar concerns as in other work , in particular concerns about toxic language , bias and other issues during language generation [ Bender et al. , , 2021 ] .", "Comments": [], "label": []}
{"id": 153019, "text": "We adopt a warm-up of 10,000 steps and set the initial learning rate to 0.0007 .", "Comments": [], "label": []}
{"id": 157066, "text": "Multi-Row , Multi-Span Distant Supervision For Table+Text Question Answering ( Appendix ) A Further details of MITQA modules A.1 TableRetriever and its training In the open domain QA setting ( like in OTT-QA ) where a designated table t and linked passages t [ ⋆ , ⋆ ] .psg are not provided , we employ the module TableRetriever ( q ) to retrieve the most promising tables T ⊆ T , where T is the corpus of tables .", "Comments": [], "label": []}
{"id": 153937, "text": "Table [ 8 ] shows the cloze questions and verbalizers we used in our experiments .", "Comments": [], "label": []}
{"id": 155419, "text": "Under the base setting of language modeling , we attempt to allocate 6K , 15K , and 30K steps to training stage 1 and show the results in Table [ 6 . ] We find that if we use word embeddings as the distilled router , allocating 6K steps ( 10 % of the total steps ) to training stage 1 is a good balance point .", "Comments": [], "label": []}
{"id": 158676, "text": "We adopt the cross-entropy loss to optimize the model via the Adam optimizer , which is given by where N denotes the total number of facts in the training set .", "Comments": [], "label": []}
{"id": 160402, "text": "We show that using DEPLAIN to train a transformer-based seq2seq text simplification model can achieve promising results .", "Comments": [], "label": []}
{"id": 160355, "text": "We apply GShard architecture on the decoder of XM Transformer , and expert weights are all initialized with the pretrained unit mBART . 6.1 Slavic-to-English Translation The six Slavic languages include Czech ( cs ) , Croatian ( hr ) , Lituanian ( lt ) , Polish ( pl ) , Slovak ( sk ) , and Slovenian ( sl ) .", "Comments": [], "label": []}
{"id": 154275, "text": "ity domains , we compare our work with [ He et al. , 2020 ] , a generative style transfer framework which uses a variational autoencoder ( VAE ) built using a sequence-to-sequence LSTM-based model to do unsupervised style transfer .", "Comments": [], "label": []}
{"id": 157646, "text": "For all models which utilize T5 , we use the T5- XL version .", "Comments": [], "label": []}
{"id": 156360, "text": "Here we implement another two user simulators using GPT [ Radford et al. , ] [ 2018 , 2019 ] .", "Comments": [], "label": []}
{"id": 155633, "text": "For example , in Transformer and GPT-2 , the penultimate layer seems to naturally learn alignments [ Garg et al. , , 2019 ] , so we use its average attention weights over all the attentions heads as the dependency attention distribution .", "Comments": [], "label": []}
{"id": 158268, "text": "For this reason , we experimented with several variations of these factors ( models Transformer Base vs .", "Comments": [], "label": []}
{"id": 158822, "text": "We also propose a classification method using Transformers augmented by TF-IDF-based features and report the results of several models for classifying the main intention categories .", "Comments": [], "label": []}
{"id": 152618, "text": "To that end , we use Item Response Theory ( IRT ; [ Baker and ] [ Kim , 1993 ] , a statistical framework from psychometrics that is widely used for the evaluation of test items in educational assessment .", "Comments": [], "label": []}
{"id": 160090, "text": "We use SimCSE as the base encoder of RankEncoder .", "Comments": [], "label": []}
{"id": 159866, "text": "Therefore , many current state-of-the-art models are fine-tuned extensions of large pretrained Transformers [ Bommasani et al. , , 2021 ] .", "Comments": [], "label": []}
{"id": 158081, "text": "Table 3 : Results of ablation study on MWOZ with T5 base , where `` w/o '' means without , `` distillation '' denotes distillation from response generation , `` attr_selector '' denotes the attribute selector , and `` ent_selector '' denotes the entity selector .", "Comments": [], "label": []}
{"id": 159504, "text": "1 Introduction Recent years have witnessed the success of pretrained language models ( PLMs ) [ Qiu et al. , , 2020 ] [ Zhao et al. , , 2023 ] , such as GPT-3 [ Brown et al. , , 2020 ] and T5 [ Raffel et al. , , 2020 ] , in a variety of natural language process ( NLP ) tasks .", "Comments": [], "label": []}
{"id": 158320, "text": "For the modeling of this subtask , we used the GUM corpus [ Zeldes , 2017 ] , introducing different guidelines about the following aspects : * i ) * only the mentions of the entity-target of the biography must be annotated ; * ii ) * mentions of the target entity must be selected only when they have a role in the event ( Example [ 1 , ] where the possessives `` his '' is not annotated ) ; and * iii ) * indirect mentions of the target entity must be annotated only if they are related to biographical events ( Examples [ 2 ] and [ 3 ] .", "Comments": [], "label": []}
{"id": 155585, "text": "We use ARI to compute the input grade level at the inference time .", "Comments": [], "label": []}
{"id": 157719, "text": "As a generator , we finetune T5-base [ Raffel et al. , , 2020a ] for 10 epochs with a learning rate of 3e-5 and a batch size of 8 .", "Comments": [], "label": []}
{"id": 158734, "text": "5.3 Evaluation Metrics For intent prediction , we adopt the F1 score as the evaluation metric to measure the effectiveness of our model , similar to previous work [ Zang et al. , , 2021 ] .", "Comments": [], "label": []}
{"id": 159937, "text": "Apart from Transformers on SNLI and MNLI , which take about 4 hours on slower GPUs , all experiments finished within 3 hours .", "Comments": [], "label": []}
{"id": 151811, "text": "This is why we use 2-layer as the basic unit for our model .", "Comments": [], "label": []}
{"id": 156999, "text": "We finetune BART and T5 on SPIDER and evaluate for both Exact Match and Execution accuracy . indicates a significant performance drop ( P < 0.05 ) compared to SAE performance by a bootstrap test . average cross-dialectal performance by 2.7 points .", "Comments": [], "label": []}
{"id": 158261, "text": "We tested the models on CPU since this is the default environment for MT models in production , except for the model MBart50 which runs on GPU .", "Comments": [], "label": []}
{"id": 153700, "text": "* A.2 Post-processing of Annotations To remove duplicate free-text annotations , we check if annotations along the same MRF dimension have a ROUGE-2 [ Lin , 2004 ] overlap of less than .8 .", "Comments": [], "label": []}
{"id": 152936, "text": "In fact , in spite of achieving the lowest generic translation quality , SMALL-CHAR prove on par ( for en-es ) or even better ( for en-it and en-fr ) than LARGE-BPE models at handling feminine gender translation .", "Comments": [], "label": []}
{"id": 152783, "text": "Regarding the number of parameters in the model , our verbalizer is based on T5-large , which has 770M parameters .", "Comments": [], "label": []}
{"id": 157901, "text": "For PersonaChat , we adopt the GPT2-based ( Radford et al. , 2019 ) baselines in order to compare with the recent work , Cao et al . ( 2022 ) . 5.2.1 WoW Dataset For WoW , we conduct experiments under two settings , * i.e. , * grounded knowledge available ( KA ) and grounded knowledge unavailable ( KU ) settings .", "Comments": [], "label": []}
{"id": 154096, "text": "The grey box indicates the Transformer encoder we wish to pretrain , which we refer to as the 'edit encoder ' .", "Comments": [], "label": []}
{"id": 152399, "text": "For Adapters† and our method , we use the adapter and the task embedding respectively trained on the most similar GLUE task for initialization , i.e .", "Comments": [], "label": []}
{"id": 154724, "text": "We use loss weight hyper-parameters , β , γ , δ , values as 0.8/0.1/0.1 respectively .", "Comments": [], "label": []}
{"id": 152833, "text": "We use the domain-targeted retrieval model from [ Ma et al. , 2021 ] , where synthetic questionpassage relevance pairs generated over the PubMed corpus are used to train domain-specific retrieval without any gold supervised data .", "Comments": [], "label": []}
{"id": 152008, "text": "The variances of the scores are much lower than Transformer , indicating stable training of G-Transformer .", "Comments": [], "label": []}
{"id": 152762, "text": "Following recent work [ Nan et al. , , 2021 ] , we use the pretrained T5-Large [ Raffel et al. , , 2020 ] model as our generator .", "Comments": [], "label": []}
{"id": 157530, "text": "copy : A 1 B 2 C 3 result : A 1 B 2 C 3 T5 + random marker : Same as above , but the augmented positional markers are in random order .", "Comments": [], "label": []}
{"id": 155407, "text": "TRM is a shorthand for Transformer .", "Comments": [], "label": []}
{"id": 158060, "text": "For shallow MP , we use a single learning rate of 1e-3 for the router and the modular prompts .", "Comments": [], "label": []}
{"id": 158661, "text": "Heuristic Rule on human evaluation of sampled multi-hop paths .", "Comments": [], "label": []}
{"id": 153256, "text": "We do not consider ISM ( Box ) because many new objects we used are unfamiliar to object detection models .", "Comments": [], "label": []}
{"id": 154735, "text": "of computation they need to perform grows with the length of the context , and , consequently , transformers have computational limitations about how much information can fit into memory .", "Comments": [], "label": []}
{"id": 155549, "text": "This is a comparatively low CO2 consumption for over 1500 GPU hours , largely due to the low CO2/kWh output of Quebec electricity when compared with the 2019 USA average of 400g CO2eq/kWh [ EPA , 2019 ] .", "Comments": [], "label": []}
{"id": 151373, "text": "In these works , the structural components of Chinese characters have been proven to be able to enrich the semantics of the characters , resulting in better NER performance . 3 Background The proposed method is based on the Flat-Lattice Transformer ( FLAT ) model .", "Comments": [], "label": []}
{"id": 152088, "text": "Then , in the test stage , we use the typical neural multiple classifier to predict the in-domain slot labels .", "Comments": [], "label": []}
{"id": 155731, "text": "To improve their methods on pretrained Transformer-based LM and make their results more comparable to MFS , we change some of their implementation details .", "Comments": [], "label": []}
{"id": 156784, "text": "Other observations include : 1 ) both * from pixels * and * from description * models mostly outperform the Caption Only baseline ( even for smaller model sizes ) , suggesting that the models are truly using feature interactions between cartoons/captions to improve their predictive accuracy ; 2 ) fine-tuning CLIP tends to do best for matching in the * from pixels * setting , but OFA+T5-11B is competitive for quality ranking ( and supports generation , see [ §3.2 ] ; and 3 ) the performance difference between T5 vs .", "Comments": [], "label": []}
{"id": 156531, "text": "Setup We use the ARR-22 and ACL-17 datasets , as they are of sufficient size and offer line numbers in the paper drafts .", "Comments": [], "label": []}
{"id": 153774, "text": "In Fig . [ 7 ] a ) , the representations of 6 Transformer layers and the final projection layer were compared between MM-Deacon SMILES and IUPAC branches , where the representations differ in shallow layers , while reach a high level of alignment in deeper layers .", "Comments": [], "label": []}
{"id": 153368, "text": "This heuristic does not effect Stage-2 model training . * * 6 Confidence Scoring * * The word log probabilities assigned by the Stage-2 decoder can be summed up to be used as confidence score for the extractions generated by GEN2OIE .", "Comments": [], "label": []}
{"id": 156171, "text": "The spreadsheet formula is powerful and easy-to-use for tabular data calculation , so we use the formula to record the calculations process of composite quantities in text , e.g. , '10 points higher ' ( * =G23-G24 * ) .", "Comments": [], "label": []}
{"id": 151333, "text": "We create triples { Xsem , Xsyn , Y } , where Xsem has the same meaning but different form to Y ( i.e. , it is a paraphrase , as in Table [ 1 ] and Xsyn is a question with the same form but different meaning The number and dimensionality of the quantizer heads need not be the same as the number of transformer heads .", "Comments": [], "label": []}
{"id": 156726, "text": "The corpus-level metrics BLEU and COMET are calculated over the entire corpus , rather than just the sentences tagged by MuDA .", "Comments": [], "label": []}
{"id": 156243, "text": "For the generative model , we use COMET [ Bosselut et al. , , 2019 ] as a commonsense knowledge generator ( KG-COMET ) .", "Comments": [], "label": []}
{"id": 156431, "text": "[ fbaipublicfiles ] ( https : //dl.fbaipublicfiles.com/fairseq/models/roberta.base.tar.gz ) .com/ [ fairseq/models/roberta ] ( https : //dl.fbaipublicfiles.com/fairseq/models/roberta.base.tar.gz ) .base.tar.gz https : //github .", "Comments": [], "label": []}
{"id": 157492, "text": "We use p start , p end P|+|X|+3 to denote the vectors storing the probabilities of all tokens to be the start position and end position , which are calculated as where B ∈ R ( |P|+|X|+3 ) ×2 and W ∈ R d ′×2 and are both trainable parameters .", "Comments": [], "label": []}
{"id": 158209, "text": "For the experiment on different averaging strategies , we use instead a sequence of 2-layer MLPs with 128 hidden nodes and ReLU activation function , one at each time step , as the discriminators .", "Comments": [], "label": []}
{"id": 156030, "text": "A Transformer model , e.g .", "Comments": [], "label": []}
{"id": 157496, "text": "The detail about LDA and ERNIE is shown in Appendix [ C.4 . ]", "Comments": [], "label": []}
{"id": 155363, "text": "] ( https : //dl.fbaipublicfiles.com/fairseq/wav2vec/wav2vec_small.pt ) [ fbaipublicfiles.com/fairseq/wav2vec/ ] ( https : //dl.fbaipublicfiles.com/fairseq/wav2vec/wav2vec_small.pt ) [ wav2vec_small.pt ] ( https : //dl.fbaipublicfiles.com/fairseq/wav2vec/wav2vec_small.pt ) < https : //github.com/mjpost/sacrebleu > sacreBLEU signature : nrefs:1 | bs:1000 | seed:12345 | case : mixed | eff : no | tok:13a | smooth : exp | version:2.0.0 < https : //github.com/pytorch/fairseq > Fairseq ST [ Wang et al. , , 2020a ] , AFS [ Zhang et al. , , 2020 ] , DDT [ Le et al. , , 2020 ] , MTL [ Tang et al. , , 2021b ] , Self-training [ Pino et al. , , 2020 ] , BiKD [ In ] [ aguma et al. , , 2021a ] , FAT-ST [ Zheng et al. , , 2021a ] , JT-S-MT [ Tang et al. , , 2021a ] , SATE [ Xu et al. , , 2021 ] , Chimera [ Han et al. , , 2021 ] and XSTNet [ Ye et al. , 2021 ] .", "Comments": [], "label": []}
{"id": 153403, "text": "We compare with four existing methods that represent different methods of vocabulary creation and allocation of budget across languages : Methods compared In Table [ 4 ] we observe that across all four tasks , zero-shot LRL accuracy improves compared to BPE .", "Comments": [], "label": []}
{"id": 158722, "text": "We use Vision Transformer [ Dosovitskiy et al. , 2020 ] to learn visual representations of images .", "Comments": [], "label": []}
{"id": 159730, "text": "Dataset for DFER : For the auxiliary DFER task , we use the Aff-Wild2 dataset [ Kollias and ] [ Zafeiriou , 2019 ] [ Kollias , 2022 ] , which contains 548 video clips collected from YouTube in realworld environments .", "Comments": [], "label": []}
{"id": 155356, "text": "Each of these transformer layers comprises 512 hidden units , 8 attention heads , and 2048 feed-forward hidden units .", "Comments": [], "label": []}
{"id": 154640, "text": "We use the EfficientNet-B3 versions of MDETR .", "Comments": [], "label": []}
{"id": 154554, "text": "To reduce the deviation , we use two basic distances , Euclidean and Cosine ( more discussed in Appendix [ A.2 ] , to calculate the LOF score .", "Comments": [], "label": []}
{"id": 155275, "text": "Contrary to our initial For clarity we include standard deviations and Contiguous results in Appendix [ D ] When evaluating out-of-domain , we use the average rationale length of the dataset we evaluate on .", "Comments": [], "label": []}
{"id": 158735, "text": "For multi-modal dialog retrieval , we use ranking-based evaluation metrics such as recall * n * at * k * including * R @ 1 * , * R @ 5 * and * R @ 10 * in accordance with prior studies [ Zang et al. , , 2021 ] [ Shuster et al. , , 2018 ] .", "Comments": [], "label": []}
{"id": 159627, "text": "We use a special operator H ( · ) to represent vector representations , for example , H ( m1 ) is the representation of the mention m1 .", "Comments": [], "label": []}
{"id": 152689, "text": "This paper aims to distill these large Transformer summarization models into smaller ones with minimal loss in performance .", "Comments": [], "label": []}
{"id": 153967, "text": "We use the MediaWiki API [ 2 ] to find a set of relevant Wikipedia pages P for nk .", "Comments": [], "label": []}
{"id": 159613, "text": "One reason may be that in a T5 model , some important information is distributed across encoder and decoder .", "Comments": [], "label": []}
{"id": 154239, "text": "We use a Metropolis-Hastings sampling scheme to sample from this energy-based model using bidirectional context and global attribute features .", "Comments": [], "label": []}
{"id": 152255, "text": "The heuristic functions focus on detecting disengagement from user responses , as it directly indicates poor user experience .", "Comments": [], "label": []}
{"id": 159427, "text": "Same as [ Sun et al. , 2022 ] , we employ variational inference to compute a variational upper bound for I ( z , G ) as follow : where r ( z ) is the variational approximation to the prior distribution p ( z ) of z , which is treated as a fixed d1-dimensional spherical Gaussian as in [ Alemi et al. , 2017 ] , i.e. , r ( z ) = N ( z|0 , I ) .", "Comments": [], "label": []}
{"id": 153575, "text": "As the visual information has a spatial prior and is organized in a 2-D grid , we adopt an axial positional embedding [ Ho et al. , , 2019 ] for visual tokens .", "Comments": [], "label": []}
{"id": 155360, "text": "We use sacreBLEU [ 5 ] [ Post , 2018 ] to compute case-sensitive detokenized BLEU [ Papineni et al. , , 2002 ] scores and the statistical significance of translation results with paired bootstrap resampling [ Koehn , 2004 ] for a fair comparison [ 6 ] .", "Comments": [], "label": []}
{"id": 154693, "text": "The morphology encoder is made of a small transformer encoder that is applied to each analyzed token separately in order to extract its morphological features .", "Comments": [], "label": []}
{"id": 157078, "text": "In the second stage , we feed both images and the observation plan into a Transformer model to generate the report .", "Comments": [], "label": []}
{"id": 157970, "text": "Figure [ 3 ] shows the binary F1 score of the flan-t5-xl model by relationship , relative to the percent of training instances the model saw that were inappropriate ; Appendix Table [ 11 ] shows full results Table 3 : Performance ( Binary F1 ) at recognizing whether a message was * in * appropriate in a relationship context .", "Comments": [], "label": []}
{"id": 160425, "text": "It is capable of ] ( https : //huggingface.co/T-Systems-onsite/cross-en-de-roberta-sentence-transformer ) 1:1 , 1 : m , and n:1 [ alignments .", "Comments": [], "label": []}
{"id": 153755, "text": "For example , DMP [ Zhu et al. , , 2021 ] was built on the consistency of SMILES and 2D molecule graphs , with SMILES encoded by Transformers and 2D molecule graphs encoded by GNNs .", "Comments": [], "label": []}
{"id": 156722, "text": "] ( deepl.com ) Translations were obtain from version of engines available in April 2021 Figure 2 : Impact of context on BLEU , COMET , and Word f-measure per tag for base context-aware models .", "Comments": [], "label": []}
{"id": 157872, "text": "We use the average linkage to compute D as At training stage , ground-truth relation y ( i , k ) and y ( j , k ) are identical if m and m belong to the same entity , for all m ∈ M .", "Comments": [], "label": []}
{"id": 158338, "text": "Given the current proof state , a causal language model ( usually a decoder-only transformer like GPT [ Radford et al . ] ) is used to predict possible proof steps that can be applied .", "Comments": [], "label": []}
{"id": 154242, "text": "Hence , we use BERT-base as a black-box to model the form and fluency of sentences .", "Comments": [], "label": []}
{"id": 156929, "text": "For the task of metaphor detection , we used the Adam optimizer during the training with batch size 16 .", "Comments": [], "label": []}
{"id": 156263, "text": "DialoGPT-medium : [ https : //huggingface.co/ ] ( https : //huggingface.co/microsoft/DialoGPT-medium ) [ microsoft/DialoGPT-medium ] ( https : //huggingface.co/microsoft/DialoGPT-medium ) [ https : //github.com/huggingface/ ] ( https : //github.com/huggingface/transfer-learning-conv-ai ) < https : //github.com/Maluuba/nlg-eval > < https : //github.com/li3cmz/GRADE > Figure 7 : Human evaluation interface for response quality on dimensions : grammar , coherence , and engagingness .", "Comments": [], "label": []}
{"id": 157449, "text": "SPARTA ( BART ) : are there more drunk drivers on the highways in new york SPARTA ( T5 ) : would traffic be better or worse on new years eve ?", "Comments": [], "label": []}
{"id": 154012, "text": "Next , we employ the error analysis framework in a comparison of two state-of-the-art documentlevel neural template-filling approaches , DyGIE++ [ Wadden et al. , , 2019 ] and GTT [ Du et al. , , 2021b ] , across three template-filling datasets ( SciREX , ProMED [ Patwardhan and Riloff , 2009 ] [ 3 ] , and MUC-4 ) .", "Comments": [], "label": []}
{"id": 152284, "text": "Inspired by the differential amplifier of analog electronics [ 1 ] , we propose a heuristic model , DifferSum , as shorthand for Differential Amplifier for Extractive Summarization to enhance the representation of the summary sentences .", "Comments": [], "label": []}
{"id": 156584, "text": "For all downstream tasks , we use the same input image size 224×224 and image augmentations as in pre-training .", "Comments": [], "label": []}
{"id": 154451, "text": "It shows that the entity type prediction error from the NER model may be propagated to the RE model if we adopt the type markers as input features .", "Comments": [], "label": []}
{"id": 151533, "text": "Vision-Trace Encoder The visual embeddings V and traces embeddings T are encoded separately and then concatenated together as a single input sequence feeding into a transformer encoder .", "Comments": [], "label": []}
{"id": 156560, "text": "Specifically , we add a twolayer temporal transformer encoder following the vision encoder , and use its outputs as inputs to the multi-modal encoder ( see details in Appendix ) .", "Comments": [], "label": []}
{"id": 158762, "text": "We observe that our model based on ResNet still significantly outperforms It-Net .", "Comments": [], "label": []}
{"id": 153011, "text": "In fact , thanks to the powerful Transformer architecture and sufficient training data in our scenario , our model is able to implicitly learn the constraint b ( Y b ) = b ( Y¯ ) with about 99.39 % accuracy in our preliminary experiments .", "Comments": [], "label": []}
{"id": 152448, "text": "This may be because we use a batch size of 4 ( due to compute limits ) while they reported a batch size of 50 .", "Comments": [], "label": []}
{"id": 152646, "text": "To fine-tune the backbone model , we used same hyperparameters over all tasks ( see Section [ D ] for details ) .", "Comments": [], "label": []}
{"id": 158162, "text": "Overall , to the best of our knowledge , this paper makes the first attempt to design a fine-tuning method that leverages graph-induced signals during the internal encoding of Transformer-based PLMs for improving MPC understanding .", "Comments": [], "label": []}
{"id": 157515, "text": "Following GPT3 , [ Chowdhery et al. , 2022 ] further scale the Transformer-based LMs to a 540-billion parameter model , called Pathways Language Model ( PaLM ) .", "Comments": [], "label": []}
{"id": 156788, "text": "Q2 : Is computer vision a bottleneck for topquality explanation generation ? * Test : T5-11B ( in the FD setting ) vs .", "Comments": [], "label": []}
{"id": 155455, "text": "In contrast , other works [ Kanade et al. , , 2019 ] [ Feng et al. , , 2020 ] pre-train a bidirectional Transformer encoder on source code , which significantly improves the performance of code-related understanding tasks .", "Comments": [], "label": []}
{"id": 154359, "text": "We adopt the multi-task learning idea to jointly minimize the loss in accuracy and total sequence length over all layers .", "Comments": [], "label": []}
{"id": 157622, "text": "For example , to encode row r in Figure [ 1 ] a ) we use : < HEADER > * rank * < HEADER_SEP > * mountain peak * < HEADER_SEP > * mountain range * < HEADER_SEP > * elevation * < HEADER_END > < ROW > * 2 * < ROW_SEP > * red slate mountain * < ROW_SEP > * sierra nevada * < ROW_SEP > * 13 , 149 ft * < ROW_END > .", "Comments": [], "label": []}
{"id": 154760, "text": "We use our implementation of the model .", "Comments": [], "label": []}
{"id": 154887, "text": "[ De ] [ vlin et al. , 2019 ] followed along the same lines and implemented Bidirectional Encoder Representations from Transformers , or BERT in short .", "Comments": [], "label": []}
{"id": 154748, "text": "Because of this , before applying multivariate ridge regression to convert the discrete sequence X into a continuous signal , we use a simple convolutional layer ( with stride = 1 and width = 3 ) as a gate , to smooth the sequence : To train the model we use the cross entropy loss .", "Comments": [], "label": []}
{"id": 159451, "text": "As there is no constraint on the mixing weights , other than that they sum to 1 and are non-negative [ 2 ] , the model is incentivized to use all the sub-layers , We implement this constraint by applying the softmax function to the 3 scalar parameters .", "Comments": [], "label": []}
{"id": 154951, "text": "While when the document and the summary are translated into German , the ROUGE between the sentence and the summary is significantly lower Work done during the first author 's internship at Microsoft Research Asia . ( fewer n-gram overlap ) .", "Comments": [], "label": []}
{"id": 157402, "text": "( d ) The end-to-end model is only validated for the en_US locale since the Transformer models utilized in the work are most performant in English and many conversation domains are country-specific .", "Comments": [], "label": []}
{"id": 159535, "text": "In < https : //github.com/jasivan/FERMAT > We use the terms type , aspect and view interchangeably . addition , it contains a tool to automatically generate new instances for each of its aspects .", "Comments": [], "label": []}
{"id": 160252, "text": "We use CLIP as the encoder for this step as it is a strong representative of the state-of-the-art vision-language models .", "Comments": [], "label": []}
{"id": 157248, "text": "We use whole-model fine-tuning [ Radford et al. , , 2018 ] for all models , which works better than the feature-based contextual word embedding approach of the original ELMo .", "Comments": [], "label": []}
{"id": 158065, "text": "In stage I , we use Bayesian optimization ( BO ) with the acquisition function of upper confidence bound ( UCB ) with κ = 2 to tune the parameters of the router ( s ) .", "Comments": [], "label": []}
{"id": 152842, "text": "Total compute time is approximately 96 TPU-hours and 192 GPUhours , which we estimate as 43 kg CO2e using the method of [ Luccioni et al. , 2019 ] [ 10 ] .", "Comments": [], "label": []}
{"id": 156162, "text": "Results for ProofWriter ( `` All '' ) [ T5-11B ] are copied from the paper .", "Comments": [], "label": []}
{"id": 156913, "text": "With the same idioms used in different ways as natural positive and negative examples whose representations could be better by using contrastive learning , we utilize contrastive learning to address the first challenge to produce a better representation of non-compositional expressions for recognizing their usage .", "Comments": [], "label": []}
{"id": 155492, "text": "We use PY150 [ Raychev et al. , , 2016 ] and Github Java Corpus [ Allamanis and Sutton , 2013 ] provided by CodeXGLUE [ Lu et al. , , 2021 ] .", "Comments": [], "label": []}
{"id": 155735, "text": "Instead , we use the text near the end of the corpus as the validation and test set to reduce information leakage .", "Comments": [], "label": []}
{"id": 151870, "text": "Instead , it relies on https : //github.com/dmmiller612/bert-extractivesummarizer https : //github.com/huggingface/transformers https : //github.com/reinaldncku/ESCOFILT https : //github.com/ShomyLiu/Neu-Review-Rec Table 3 : Performance comparison of the recommender models .", "Comments": [], "label": []}
{"id": 151770, "text": "We trained the TRANSFORMER-BIG model for experiments .", "Comments": [], "label": []}
{"id": 153687, "text": "We randomly sample model-generated `` writer 's intent '' implications from T5 models and GPT-2 large over 196 headlines where generated implications were unique for each model type . [ 17 ] We elicit 3 unique judgements per headline .", "Comments": [], "label": []}
{"id": 156226, "text": "For the identification of the most appropriate glosses we employ the pre-trained model in [ Huang et al. , , 2019 ] which has achieved the state-of-the-art results in the Word Sense Disambiguation ( WSD ) task .", "Comments": [], "label": []}
{"id": 154339, "text": "BERT vs. fastText ; [ Dufter et al. , 2021 ] .", "Comments": [], "label": []}
{"id": 154655, "text": "E.7 Dataset Information All datasets that we use are focused on English .", "Comments": [], "label": []}
{"id": 158092, "text": "Settings We adopt a 12-layer Transformer-based language model with 768 dimensions and 12 attention heads .", "Comments": [], "label": []}
{"id": 158879, "text": "For fair comparison , we use frozen models for all baselines in the in-context learning experiments , i.e. , a pre-trained language/NER model is used for entity extraction without finetuning .", "Comments": [], "label": []}
{"id": 159109, "text": "Additionally , we show that the Indic-COMET can outperform COMET on some unseen Indian languages .", "Comments": [], "label": []}
{"id": 157467, "text": "Although we use the standard IIT- CDIP dataset for pre-training in all experiments , the proposed method is not limited to using specific datasets for pre-training .", "Comments": [], "label": []}
{"id": 151994, "text": "Different from the baseline of document-level Transformer , G-Transformer can be successfully trained on small TED and News .", "Comments": [], "label": []}
{"id": 157843, "text": "Furthermore , our algorithm to select the relevant turns in the conversational history to generate the conversational questions is a heuristic of selecting a maximum of three previous turns .", "Comments": [], "label": []}
{"id": 158715, "text": "Second , transformer-based models perform better on claim targets than noun-phrase targets .", "Comments": [], "label": []}
{"id": 152661, "text": "For this purpose we use a web-scraped dataset from the Urban Dictionary , previously collected by [ Wilson et al. , 2020 ] .", "Comments": [], "label": []}
{"id": 156327, "text": "In the cases where the web pages of the source news articles was removed , we used the Wayback Machine [ 8 ] .", "Comments": [], "label": []}
{"id": 151398, "text": "For the two small datasets , Weibo and Resume , we used the SMAC algorithm to search for the best hyper-parameters .", "Comments": [], "label": []}
{"id": 154032, "text": "Our error analysis tool can be run completely on a CPU and takes a couple of minutes to run , depending on the size of the dataset and the predicted outputs .", "Comments": [], "label": []}
{"id": 157327, "text": "We use micro-averaged F as the evaluation metric .", "Comments": [], "label": []}
{"id": 158584, "text": "The performance gain on the two test set splits between the best conceptualization-aided COMET and the COMET trained on the ATOMIC subset only is reported in Figure [ 4 . ]", "Comments": [], "label": []}
{"id": 151996, "text": "G-Transformer outperforms previous documentlevel MT models on News and Europarl with a significant margin .", "Comments": [], "label": []}
{"id": 155493, "text": "We have two considerations for the module design : 1 ) the module should recognize the semantic connection between the assumption and the context , and 2 ) the module should uniformly support various discrete operations to Note that we adopt the do-expression [ Pearl , 2009 ] of counterfactual .", "Comments": [], "label": []}
{"id": 152737, "text": "We use u [ m ] to denote the mth prefix presented to the online system , and t to denote the time at which it is presented . t denotes the time at which the complete utterance u is presented .", "Comments": [], "label": []}
{"id": 159852, "text": "We show the descriptions in Appendix [ A.1 . ] ChatGPT with descriptions In the case of Chat-GPT [ OpenAI , 2022 ] , we employ the same descriptions as those used for GPT-3 .", "Comments": [], "label": []}
{"id": 155430, "text": "4.1 Data We used a series of commonsense reasoning tasks and evaluated on the publicly available development sets .", "Comments": [], "label": []}
{"id": 157782, "text": "Proposed : high-prec . c2f ( CLS , MD , MLM ) We use the same model architecture and pre-trained encoder as the baseline , but also incorporate the joint training objective CL + MD .", "Comments": [], "label": []}
{"id": 155333, "text": "For TAKD , we use KD to train a teacher assistant model with 10 Transformer layers .", "Comments": [], "label": []}
{"id": 156491, "text": "We use cross-entropy loss as our objective function : where y is the ground truth sentiment polarity , D contains all sentence-aspect pairs and C contains all sentiment polarities .", "Comments": [], "label": []}
{"id": 154205, "text": "We use the finbert-finvocab-uncased version from https : // [ github.com ] ( https : //github.com/yya518/FinBERT ) /yya518/FinBERT .", "Comments": [], "label": []}
{"id": 152732, "text": "We use Transformer-Big model as the teacher and Transformer-Base as the student .", "Comments": [], "label": []}
{"id": 153521, "text": "Evaluation Datasets For the hypothesis-only bias , we use the challenge sets SNLI-hard [ Gu ] [ rurangan et al. , , 2018 ] and MNLI-hard [ Williams et al. , 2018 ] , which were produced by filtering the test set with a hypothesis-only model ( Section [ 5.2 ] .", "Comments": [], "label": []}
{"id": 159259, "text": "Specifically , we adopt the most representative financial sentiment lexicon—the * Loughran–McDonald Master Dictionary * [ Loughran and Mcdonald , 2011 ] —and assume that in addition to the revised words in the heuristic approach , the 3,872 sentiment words in the dictionary also reveal important financial signals ( i.e. , are labeled as positive ) .", "Comments": [], "label": []}
{"id": 158455, "text": "[ Deepspeed .", "Comments": [], "label": []}
{"id": 156448, "text": "A comparison of this model with ours is given in Section 3.1 . 2.2 Span-Based Parsing Clark ( 2021 ) applied Transformers to span-scorebased PCFGs ( Stern et al. , 2017 ; Kitaev and Klein , 2018 ) for CCG parsing , achieving significant performance gains .", "Comments": [], "label": []}
{"id": 154574, "text": "For pair examples , see Appendix [ B . ] In the code summarization task , we add a 6-layer Transformer-based decoder to generate summarization as in CodeBERT .", "Comments": [], "label": []}
{"id": 154389, "text": "For this task , we use a large-scale dialogue dataset , Persona-Chat [ Zhang et al. , , 2018 ] .", "Comments": [], "label": []}
{"id": 159317, "text": "Moreover , our method achieves significant performance gains on three well-established tasks while considerably compressing parameters . 1 Introduction Transformers have shown promising performance across various tasks .", "Comments": [], "label": []}
{"id": 154824, "text": "We use two popular knowledge graph completion benchmarks , FB15k-237 [ Toutanova and Chen , 2015 ] and WN18RR [ Dettmers et al. , , 2018 ] in our experiments .", "Comments": [], "label": []}
{"id": 160193, "text": "We use max ensembling as it is found to work best .", "Comments": [], "label": []}
{"id": 156818, "text": "Thus we use the raw texts from their datasets as inputs .", "Comments": [], "label": []}
{"id": 156135, "text": "Given the architectural similarity of transformer language models , we may be able to extrapolate the results to other models , but further work is needed to confirm our findings to other languages or model architectures .", "Comments": [], "label": []}
{"id": 157653, "text": "The total GPU hours for training a GPT-J model in the Response Generation task is about 5.3 hours , and the total GPU hours for training the T5 models in the User Intent Detection task is about 4 hours .", "Comments": [], "label": []}
{"id": 158648, "text": "Although we use this setting for baseline comparison purposes and a testament to the feasibility of our fMRI2text task , additional tests under more practical conditions could be an essential step in future work , further elucidating the applicability and robustness of the methods .", "Comments": [], "label": []}
{"id": 159843, "text": "We use Φ ( y ) to denote all keywords in class y , and the final prediction is : 5 Experiment We conduct extensive zero-shot learning experiments to demonstrate the effectiveness of our method .", "Comments": [], "label": []}
{"id": 159479, "text": "G COMET results We show COMET , CHRF , and SPBLEU scores , averaged over all language pairs in Table [ 12 . ] We show the scores for the baseline ( i.e. , non-LSL ) , our LSL model , and the best Adapter model for both the separate decoder and the shared decoder architectures .", "Comments": [], "label": []}
{"id": 158636, "text": "We use mean average error ( MAE ) as the loss function for both phases of cognitive signal reconstruction .", "Comments": [], "label": []}
{"id": 154221, "text": "Before predicting the next explanation token xt+1 , the Transformer decoder computes a hidden state h based on the problem context vectors w and previously generated explanation tokens x1 , · · · , x .", "Comments": [], "label": []}
{"id": 155914, "text": "For SUBJ , COLA , TREC , Yelp , AG News and IMDB we use the same hypotheses as [ Wang et al. , 2021 ] .", "Comments": [], "label": []}
{"id": 156836, "text": "Figure [ 1 ] shows our implementation of the EffectiveCAN model with the attention supervision mechanism for evidence extraction . 5.2.1 Unsupervised Attention EffectiveCAN uses text encoding from multiple layers of Res-SE block to generate the key for the attention module .", "Comments": [], "label": []}
{"id": 159085, "text": "Inspired by Chen et al . ( 2022 ) , we use the whole words in BERT pre-trained embedding for model training , and infer reasonable embeddings for the words which were tokenized into pieces .", "Comments": [], "label": []}
{"id": 156107, "text": "https : //pytorch.org/docs/stable/checkpoint.html Table 11 : BRIDGE-type question example , where PATHFID predicts the correct answer while FID fails to do so .", "Comments": [], "label": []}
{"id": 151374, "text": "Thus , we first briefly introduce FLAT that improves the encoder structure of Transformer by adding word lattice information , including semantic and position boundary information .", "Comments": [], "label": []}
{"id": 158894, "text": "For stock prediction , we use three metrics to measure performance : Accuracy , F1 score , and Matthews correlation coefficient ( MCC ) .", "Comments": [], "label": []}
{"id": 154163, "text": "Then , we use several advanced technologies to align En↔Zh and En↔De subtitles .", "Comments": [], "label": []}
{"id": 156554, "text": "In Table [ 1 , ] we compare SINGULARITY with existing methods on Table 1 : Comparison to existing methods on text-to-video retrieval .", "Comments": [], "label": []}
{"id": 155120, "text": "We use RMSE , mean absolute error ( MAE ) and symmetric mean absolute percentage error ( SMAPE ) as metrics .", "Comments": [], "label": []}
{"id": 155596, "text": "On WMT16 En-De task , our model achieves 1.80 SacreBLEU improvement over vanilla transformer . 1 Introduction Neural machine translation models have achieved great success in recent years [ Sutskever et al. , , 2014 ] [ Bahdanau et al. , , 2015 ] [ Gehring et al. , , 2017 ] [ Vaswani et al. , , 2017 ] .", "Comments": [], "label": []}
{"id": 156469, "text": "The average F1 on all tasks after training on the final task is reported following the common protocol [ Lopez- ] [ Paz and Ranzato , 2017 ] [ Madotto et al. , , 2021 ] : To measure the forgetting during lifelong learning , we use the BWT , which assesses the impact that learning on subsequent tasks has on a previous task .", "Comments": [], "label": []}
{"id": 155821, "text": "With the decoder input H = h1 : and the discretized latent variable sequence z = z1 : m , we adopt the glancing sampling technique for training the latent predictor in the following steps : where τ is the sampling ratio decreasing in the training steps , and we use Hamming distance [ Hamming , 1950 ] for measuring the prediction quality .", "Comments": [], "label": []}
{"id": 153359, "text": "Therefore , we use a new generative model , GEN2OIE , that contains two stages : the first stage produces all the relations in the sentence and the second stage generates the extractions containing the given relation .", "Comments": [], "label": []}
{"id": 156462, "text": "Furthermore , we use a simple yet effective method to perform mask selection and show the powerful transferability of Lottery Prompts to novel tasks .", "Comments": [], "label": []}
{"id": 155784, "text": "The most successful method is the glancing transformer ( GLAT , [ Qian et al. , , 2021a ] , which trains the NAT model by sampling partial target words as inputs to predict the remaining target words , explicitly building dependencies between the observed and unobserved words .", "Comments": [], "label": []}
{"id": 152885, "text": "Note that for both cases , we use auxiliary languages which are not in the Indo-Aryan group to remove the impact of language similarity .", "Comments": [], "label": []}
{"id": 153880, "text": "We use E2ENLG [ Novikova et al. , , 2017 ] and four different domains ( restaurant , hotel , tv , laptop ) from RNNLG [ Wen et al. , , 2015 ] to form five * similar * tasks .", "Comments": [], "label": []}
{"id": 152200, "text": "[ Mikolov et al. , 2013b ] showed that word embeddings , in particular Word2vec embeddings , were able to solve analogy problems by simple vector operations ( e.g .", "Comments": [], "label": []}
{"id": 155235, "text": "For τ , we already < https : //huggingface.co/docs/datasets/ > described how to choose the best pair of τ during training and testing in Sec .", "Comments": [], "label": []}
{"id": 154545, "text": "ROUGE-L + RELAX ( 56.76/25.06/30.76/37.10 ) Archaeologists have made `` one of the most important discoveries '' in the history of the Burgas Regional Museum of History in Bulgaria , says the museum 's director : a lead reliquary that contains ashes from the alleged grave of one of Jesus ' Twelve Apostles .", "Comments": [], "label": []}
{"id": 159226, "text": "We use mBart-50-large [ Tang et al. , , 2020 ] with a condition generation head to fine-tune ACLM .", "Comments": [], "label": []}
{"id": 160421, "text": "Our adaptation of this method was only ] ( https : //huggingface.co/T-Systems-onsite/cross-en-de-roberta-sentence-transformer ) [ by using a dedicated German sentence transformer ] ( https : //huggingface.co/T-Systems-onsite/cross-en-de-roberta-sentence-transformer ) [ model in the algorithm procedure .", "Comments": [], "label": []}
{"id": 153298, "text": "We tune the number of BPE merges as recommended by [ Ding et al. , 2019 ] ; the resulting subword vocabulary sizes for each dataset are tabulated in Table [ 1 . ]", "Comments": [], "label": []}
{"id": 156337, "text": "Specifically , we used our article , Twitter profile , Twitter Follower , and YouTube embeddings .", "Comments": [], "label": []}
{"id": 154720, "text": "To test the benefits of our proposed method for pre-trained language model calibration , we use in-domain trained models to predict out-of-distribution test samples .", "Comments": [], "label": []}
{"id": 160135, "text": "To do this , we use a one-tailed Fisher 's Exact Test [ Fisher , 1922 ] , which has been shown to be effective in rare event detection applications in the Table 8 : Approved COVID-19 treatments used in evaluation based on lists from the New York Times [ Zimmer et al. , 2020 ] and the Centers for Disease Control [ CDC , 2021 ] .", "Comments": [], "label": []}
{"id": 155400, "text": "The rest of the hyper-parameters are summarized in Appendix [ A . ] Multilingual Machine Translation Following [ Ma et al. , , 2020 ] , we use the Sentence-Piece [ Kudo and Richardson , 2018 ] model to tokenize sentences .", "Comments": [], "label": []}
{"id": 153254, "text": "We use VQGAN with codebook size Z = 16384 and downsampling factor f = 16 , and CLIP with ViT-B/32 [ Dosovitskiy et al. , , 2020 ] architecture .", "Comments": [], "label": []}
{"id": 160288, "text": "The transformer parameters were initialized from uncased BERT base model [ Devlin et al. , , 2019 ] , and parameters for graph adapters were initialized randomly .", "Comments": [], "label": []}
{"id": 156569, "text": "References A Appendix In Section [ A.1 , ] we show details of our open-ended QA model and SINGULARITY-temporal model , as well as pre-training objectives .", "Comments": [], "label": []}
{"id": 152802, "text": "Since the results of the RoBERTa-large model with a small batch size were unstable , we also performed a distributed training with three GPUs , resulting in a batch size of 108 .", "Comments": [], "label": []}
{"id": 152349, "text": "We used the hours of each day of the week as seven fields and used all metadata contained in attributes as each field .", "Comments": [], "label": []}
{"id": 158023, "text": "The maximum estimation of costed computing resources in this study is ∼ 500 x 8 GPU hours .", "Comments": [], "label": []}
{"id": 159245, "text": "[ 5 ] Note that the difference is calculated between the two consecutive ROUGE-2 scores in S¯ ( t ) .", "Comments": [], "label": []}
{"id": 155721, "text": "The methods are similar to MoS and our approach , but they add the multiple embeddings inside each layer of the transformer encoder while the proposed MFS is an alternative to the output softmax layer .", "Comments": [], "label": []}
{"id": 160432, "text": "Officially the code was published in Java [ 33 ] , for better integrity with the other alignment methods , we used the existing Python version of it [ 34 ] .", "Comments": [], "label": []}
{"id": 154189, "text": "By contrast , finer-139 uses a specialized set of 139 highly technical economic tags derived from the real-world need of xbrl tagging , and we employ no handcrafted features .", "Comments": [], "label": []}
{"id": 154026, "text": "We used the tool to analyze the errors of two state-of-theart models on three datasets from varying domains and compared the error profiles of these models to four of the earliest systems in the field on a dataset from that era .", "Comments": [], "label": []}
{"id": 160029, "text": "Following previous work [ Lin et al. , , 2009 ] , we use the gold standard arguments and focus on relation prediction .", "Comments": [], "label": []}
{"id": 156308, "text": "We use a batch size of 32 and a learning rate of 2e-5 that linearly decreases to zero .", "Comments": [], "label": []}
{"id": 152784, "text": "F Data-to-text Examples In the top half of [ Table 11 ] we show examples from DART that are filtered out by our method , i.e . low ROUGE scores between input and target .", "Comments": [], "label": []}
{"id": 156852, "text": "Assuming the Transformer has L layers , we can split the parameters θ into [ θt,1 , ... , θt , L ] according to their layer indexes .", "Comments": [], "label": []}
{"id": 157840, "text": "In particular , we train a T5 model [ Raffel et al. , , 2020 ] in the answer-aware setting .", "Comments": [], "label": []}
{"id": 151441, "text": "With an output layer , e.g . linear layers , upon the R2 , we can get the generated response Rˆ . 3.4 Training Objectives We employ negative log-likelihood ( NLL ) loss and unlikelihood loss for dialogue generation and consistency understanding .", "Comments": [], "label": []}
{"id": 158140, "text": "A.4 Data Augmentation Following prior work [ Shi et al. , , 2022b ] , we use many noise categories for data augmentation .", "Comments": [], "label": []}
{"id": 155364, "text": "Besides , we implement a strong baseline W2V2-Transformer based on Wav2vec2.0 .", "Comments": [], "label": []}
{"id": 159121, "text": "We find that the recent MT models ( NLLB , IndicTrans ) have fewer errors compared to the relatively older models ( CVIT ) .", "Comments": [], "label": []}
{"id": 151843, "text": "We use the standard training/validation/testing splits ( 204 , 045/11 , 332/11 , 334 ) and follow the pre-processing in [ Narayan et al. , 2018 ] .", "Comments": [], "label": []}
{"id": 154060, "text": "We chose TAB-T5 and OCR-T5 for Statista and OCR-BART and OCR-T5 models for Pew .", "Comments": [], "label": []}
{"id": 157486, "text": "Alternatively , we adopt the frontdoor adjustment [ Peters et al. , , 2017 ] and design a mediator to mitigate the concept bias .", "Comments": [], "label": []}
{"id": 158075, "text": "Finally , we calculate the KL-divergence between the selection scores s of retrieved entities and cross-attention distribution c as the training loss : 3.4 Response Generator Inspired by Fusion-in-Decoder [ Izacard and Grave , 2020 ] in open-domain question answering , we employ a modified sequence-to-sequence structure for the response generator to facilitate direct interaction between dialog context and retrieved entities .", "Comments": [], "label": []}
{"id": 160068, "text": "In our experiments , we implemented ENS on the top of RSOTA setting .", "Comments": [], "label": []}
{"id": 152522, "text": "3.3 Continued MLM pretraining We use our objective to extend the pretraining of a transformer-based language model [ Vaswani et al. , , 2017 ] , as this represents the state-of-the-art encoder in NLP .", "Comments": [], "label": []}
{"id": 156260, "text": "For automatic metrics , we use the nlg-eval package [ 7 ] and the GRADE repo [ 8 ] .", "Comments": [], "label": []}
{"id": 155185, "text": "For the fraud dataset we use the simlar sentence `` * A fraud occured . * '' After inspecting the matches , Table 12 : Comparison of our AOSUMM model trained on data using ROUGE ( RS ) or BERTScore ( BS ) as the scoring metric for oracle extraction .", "Comments": [], "label": []}
{"id": 154588, "text": "When training initial models with little in-domain supervised data [ Section 5 ; ] [ Section 6 ] , we use a learning rate of 3e-5 with a linear schedule , batch size 10 , and 10 epochs .", "Comments": [], "label": []}
{"id": 153966, "text": "We use a high temperature ( 1.5 ) and nucleus sampling [ Holtzman et al. , , 2020 ] with p = 0.9 during decoding to encourage the model to generate unexpected and non-contextual entities in the responses .", "Comments": [], "label": []}
{"id": 152039, "text": "We thank Westlake University High-Performance Computing Center for supporting on GPU resources .", "Comments": [], "label": []}
{"id": 157540, "text": "DeBERTa / T5 + fine-grained steps : The training data used in this setting follow the format as the exemplar in * GPT3 + fine-grained steps * .", "Comments": [], "label": []}
{"id": 156078, "text": "We employ precision , recall and F1-score to evaluate the performance .", "Comments": [], "label": []}
{"id": 151684, "text": "Therefore we use the accuracy of similarity search tasks as a quantitative indicator of cross-lingual representation alignment .", "Comments": [], "label": []}
{"id": 153924, "text": "Table 6 : Ablation study on the SuperGLUE dev set . ( T5 ≈ GLM – shuffle spans + sentinel tokens . ) Summary .", "Comments": [], "label": []}
{"id": 156586, "text": "For retrieval tasks , we use the average recall , which is the average score of R @ { 1,5,10 } ) to more holistically compare the model performance .", "Comments": [], "label": []}
{"id": 153446, "text": "While for BERTAbsbased methods , we use the base version of Chinese BERT-wwm [ 9 ] .", "Comments": [], "label": []}
{"id": 152395, "text": "We compare our models to the state-of-the-art T5 model , in which we fine-tune all parameters of the model on all tasks .", "Comments": [], "label": []}
{"id": 154659, "text": "Well-tested architectures , such as BERT-style transformer models [ Vaswani et al. , 2017 ] , are thus flexibly extended to either speech or audio data .", "Comments": [], "label": []}
{"id": 158622, "text": "We use the official train/valid/test splits for experiments on all three datasets .", "Comments": [], "label": []}
{"id": 153865, "text": "In language domain , prior work often utilizes adapter [ Houlsby et al. , , 2019 ] [ Madotto et al. , , 2021 ] [ Ermis et al. , , 2022 ] , which could be considered as task-specific MLPs inserted into frozen transformer layers .", "Comments": [], "label": []}
{"id": 156100, "text": "As presented in Table [ 7 , ] although the performance difference on the supporting fact prediction is relatively small ( 1 % ) , answer prediction performance drops significantly ( by 3.2 % ) when we switch from T5-large to T5-base .", "Comments": [], "label": []}
{"id": 158127, "text": "Following prior work [ Shi et al. , , 2022b ] , we employ data augmentation and noisy test set based on MUSAN Function log ( x ) + log ( 1 − x ) reaches maximum at x = 0.5 , and the minimum is obtained around x = 0 and x = 1 .", "Comments": [], "label": []}
{"id": 158624, "text": "We use Adam [ Kingma ] [ and Ba , 2015 ] optimizer followed by the inverse square root learning rate scheduler for model training .", "Comments": [], "label": []}
{"id": 157597, "text": "We use int8bit [ Zeng et al. , , 2022 ] quantization with OPT 13 billion and GPT-Neo 20 billion models to make them fit our GPUs .", "Comments": [], "label": []}
{"id": 154955, "text": "3.2 Neural Extractive Summarizer Our sentence encoder builds upon the recently proposed XLMR [ Conneau et al. , , 2020 ] architecture , which is based on the deep bidirectional Transformer [ Vaswani et al. , , 2017 ] and has achieved state-of-the-art performance in many multilingual zero-shot understanding tasks .", "Comments": [], "label": []}
{"id": 152090, "text": "While , for novel slot detection , we use the multiple classifier , or the binary classifier , or both of them .", "Comments": [], "label": []}
{"id": 156208, "text": "We adopt SpreadsheetCoder [ Chen ] * et [ al . ] * , [ 2021a ] , TaPEx ( Liu * [ et al . ] * , [ 2021 ] , and TUTA as our baselines .", "Comments": [], "label": []}
{"id": 155990, "text": "Reasons could be that T5-based augmentation produces samples with fewer grammatical errors . ( will further discuss in Sec [ 4.7 ] .", "Comments": [], "label": []}
{"id": 153723, "text": "We use the same implementation setup as our knowledge generation method .", "Comments": [], "label": []}
{"id": 156249, "text": "Compared with COMET that also generates knowledge , we find TBS models generate more knowledge that follows common sense and is relevant to the dialogue .", "Comments": [], "label": []}
{"id": 155565, "text": "Re-ranking time per query and relevance for top 1,000 passages in milliseconds on a GPU using the Dev query set .", "Comments": [], "label": []}
{"id": 152159, "text": "Results with are ♠ results we implement on our own .", "Comments": [], "label": []}
{"id": 157641, "text": "To address this challenge , we utilize natural language descriptions of the intents , following [ Zhao et al. , 2022 ] .", "Comments": [], "label": []}
{"id": 158383, "text": "In comparison , performing SOCRATIC pretraining on the same corpus improves performance by +0.41 Rouge-1 .", "Comments": [], "label": []}
{"id": 155193, "text": "For all the models , we adopt the experimental setup used in [ Devlin et al. , , 2019 ] , i.e .", "Comments": [], "label": []}
{"id": 157581, "text": "Since accuracy is a non-distributional statistic , we use the most frequent label for inference ( though not during training ; we use the same trained models as in Table [ 2 ] .", "Comments": [], "label": []}
{"id": 152332, "text": "For evaluation , we used the data used in previous research [ Chu and Liu , 2019 ] [ Brazinskas and Titov ] ˇ , [ 2020 ] .", "Comments": [], "label": []}
{"id": 157910, "text": "Using T5-Base also brings noticeable improvements , as shown in Appendix E .", "Comments": [], "label": []}
{"id": 155460, "text": "Different from current unified models , UniXcoder is based on a multi-layer Transformer and utilizes mask attention matrices with prefix adapters All the codes and data are available at [ https : // ] ( https : //github.com/microsoft/CodeBERT ) [ github.com/microsoft/CodeBERT ] ( https : //github.com/microsoft/CodeBERT ) .", "Comments": [], "label": []}
{"id": 152045, "text": "We use the default setting proposed by Transformer [ Vaswani et al. , , 2017 ] , which uses Adam optimizer with β = 0.9 and β = 0.98 , a learning rate of 5e−4 , and an inversesquare schedule with warmup steps of 4000 .", "Comments": [], "label": []}
{"id": 153396, "text": "3.2 Our Proposal : OBPE The key idea in OBPE is to maximize the overlap between an LRL and a closely related HRL while simultaneously encoding the input corpora compactly as in BPE .", "Comments": [], "label": []}
{"id": 154109, "text": "We use an identical setup to [ Sellam et al. , , 2020 ] and [ Zhang et al. , , 2019a ] , where we use the subset of data for which the candidate and reference are in English , which we will refer to as the to-English subset .", "Comments": [], "label": []}
{"id": 151573, "text": "To ensure high trustworthiness of posts ( and thus our true claims ) and the linked sources , we employ several filtering steps .", "Comments": [], "label": []}
{"id": 160283, "text": "Transformers Transformer based neural networks have demonstrated exemplary performance in the fields of natural language processing but they only accept inputs as sequence data .", "Comments": [], "label": []}
{"id": 152761, "text": "We then find high quality verbalized outputs based on the ROUGE-1 score between the model inputs and model outputs , and sample instances with score higher than a threshold for the next-round training .", "Comments": [], "label": []}
{"id": 152947, "text": "B.1 Architecture The architecture of our ST models is composed of two strided 2D convolutional layers with 64 3x3 kernels , followed by a Transformer [ Vaswani et al. , , 2017 ] with 11 encoder layers and 4 decoder layers .", "Comments": [], "label": []}
{"id": 153673, "text": "For unreliable news ( misinformation ) , we use The CoronaVirusFacts/DatosCoronaVirus Alliance Database , a dataset of over 10,000 mostly false claims related to Covid-19 and the ESOC Covid-19 Misinformation Dataset , which consists of over 200 additional URLs for ( mis/dis ) information examples .", "Comments": [], "label": []}
{"id": 153504, "text": "More radically , [ Lee-Thorp et al. , 2021 ] reform Transformer by replacing the entire self-attention sub-layer with discrete Fourier transforms along sequence dimension and hidden dimension respectively .", "Comments": [], "label": []}
{"id": 157197, "text": "For efficiency reasons , in the very compute demanding experiment of the number of contributors ( [ §5.4 ] we randomly sampled 5 datasets to act as a consistent test set . 4.2 Models and Baselines For experiments in the main text , we use RoBERTabase [ Liu et al. , , 2019 ] as our initial model θ0 .", "Comments": [], "label": []}
{"id": 159922, "text": "The HyperMixer encoder arguably possesses similar inductive biases as Transformers .", "Comments": [], "label": []}
{"id": 153814, "text": "We compare the following position encodings : abs : sinusoidal absolute position encodings ( as used in the original Transformer ) [ 2 ] .", "Comments": [], "label": []}
{"id": 157900, "text": "5.2 Baselines For WoW , we use baselines based on BlenderBot ( Roller et al. , 2021 ) , since it is commonly used in recent work ( Lin et al. , 2022 ; Cui et al. , 2021 ) .", "Comments": [], "label": []}
{"id": 153928, "text": "Instead , T5 [ Raffel et al. , , 2020 ] formulates most language tasks in the text-to-text framework .", "Comments": [], "label": []}
{"id": 158976, "text": "References A Appendix A.1 Speaker Relation Graph We use a sliding window around each utterance , and count the frequency of occurrence for each speaker in this sliding window .", "Comments": [], "label": []}
{"id": 158886, "text": "We employed the identical fine-tuning approach as previous works [ Huang et al. , 2021 ] [ Lu et al. , , 2022b ] [ Chen et al. , , 2022a ] .", "Comments": [], "label": []}
{"id": 152494, "text": "For short-text EL , some prior works [ Sakor et al. , , 2019 ] [ Ferragina and Scaiella , 2012 ] [ Mendes et al. , , 2011 ] address the joint problem of mention detection and linking , with primary focus on identifying mention spans , while linking is done via heuristic methods without learning .", "Comments": [], "label": []}
{"id": 153566, "text": "Specifically , we propose the * Multimodally-Enhanced Transformer for Explainable Recommendation * ( METER ) approach for improved text explanations based on conditional image generation and text–image matching .", "Comments": [], "label": []}
{"id": 153397, "text": "The first term in the objective compactly represents the total corpus , as in BPE 's ( Eq [ 1 ] ) .", "Comments": [], "label": []}
{"id": 159768, "text": "This phenomenon suggests that the shortcut connections can also be learned in transformer-based models between some decisive words or tokens , which provides the design intuition of NOTABLE .", "Comments": [], "label": []}
{"id": 155520, "text": "We use the English EWT dataset from Universal Dependencies ( UD ) v2.8 [ Nivre et al. , , 2020 ] [ 3 ] .", "Comments": [], "label": []}
{"id": 152089, "text": "Meanwhile , we use the detection algorithm , MSP or GDA to figure out novel slot tokens .", "Comments": [], "label": []}
{"id": 157191, "text": "For both SoTA models ( 1 and 2 ) , we use the implementation released by the authors and also apply the default training parameters disclosed in the referring papers .", "Comments": [], "label": []}
{"id": 159323, "text": "[ Michel et al. , 2019 ] and [ Voita et al. , 2019 ] found that only a subset of the attention heads have significant utilities in transformer , where the important heads could be identified by Expected Sensitivity and Layer-wise Relevance Propagation ( LRP ) [ Ding et al. , , 2017 ] .", "Comments": [], "label": []}
{"id": 154749, "text": "< SEP > 1 3 2 0 To understand if the long-term memory is being effectively used and the transformer is not only performing sorting by modeling the most recent tokens , we design the token probability distribution to * change over time * : namely , we set it as a mixture of two distributions , p = αp + ( 1 − α ) p1 , where the mixture coefficient α ∈ [ 0 , 1 ] is progressively increased from 0 to 1 as the sequence is generated .", "Comments": [], "label": []}
{"id": 156359, "text": "Secondly , we adopt a sampling schema based on a well-designed distribution ( see Eq . [ 4 ] instead of taking the arm with the highest expectation .", "Comments": [], "label": []}
{"id": 156366, "text": "We use them to build three new user simulators denoted as GPTAT , GPTAR , and GPTAG based on the GPT model respectively .", "Comments": [], "label": []}
{"id": 152535, "text": "Finally , we evaluate the pretrained transformer model 's performance before it is subjected to training with our contrastive objective , denoted `` Transformer- * '' .", "Comments": [], "label": []}
{"id": 160228, "text": "To account for this issue , we use trigger mentions for the ED task and argument mentions for the EAE task as candidate pools as opposed to using words of a certain part-of-speech or named entities type .", "Comments": [], "label": []}
{"id": 156368, "text": "For the NLU results N , we use five categories : `` inform '' , `` request '' , `` book inform '' , `` select '' , `` recommend '' same as [ Shi et al. , 2019 ] to represent them .", "Comments": [], "label": []}
{"id": 151470, "text": "Textworld Probe For Textworld , the proposition embedder converts propositions φ to natural language descriptions ( `` the is `` for properties and `` the o is r o2 '' for relations ) and encodes them with the BART or T5 LM encoder .", "Comments": [], "label": []}
{"id": 159108, "text": "We further leverage our MQM annotations to develop an Indic-COMET metric and show that it outperforms COMET counterparts in both human scores correlations and robustness scores in Indian languages .", "Comments": [], "label": []}
{"id": 158311, "text": "Since all datasets in this paper are labeled , we use the supervised CL method to strengthen the representation of hard samples .", "Comments": [], "label": []}
{"id": 158070, "text": "For topic classification and sentence-pair classification tasks , we use the original input text as the context and construct a query containing all valid labels .", "Comments": [], "label": []}
{"id": 152921, "text": "We use 2,500 warm-up steps to reach a maximum learning rate of 3e-5 , and use 0.3 dropout and 0.2 label smoothing .", "Comments": [], "label": []}
{"id": 160159, "text": "We use CRESS to denote our method with token-level adaptive training .", "Comments": [], "label": []}
{"id": 154043, "text": "Hence , we employ the recently proposed self-supervised strategy called * Barlow Twins * [ Zbontar et al. , , 2021 ] which tries to make the embedding vectors of distorted versions of an image sample to be similar , while minimizing the redundancy between the components of these vectors .", "Comments": [], "label": []}
{"id": 152943, "text": "6.2 Manual analysis Our manual inspection recovers a total of ∼1,200 OOC agreement chains from CHAR and BPE output .", "Comments": [], "label": []}
{"id": 157243, "text": "Nevertheless , BERT differs from its prior approaches on many * other * factors ( * e.g. , * the pre-training data corpus and size , model size , using Transformers vs LSTMs , length of the training cycle , tokenizer , etc . ) .", "Comments": [], "label": []}
{"id": 151968, "text": "We use the Base model and fixed dataset for this comparison .", "Comments": [], "label": []}
{"id": 152879, "text": "For decoding , we use beam-search with beam size 5 for all translation directions and do not tune length penalty .", "Comments": [], "label": []}
{"id": 160208, "text": "Another limitation of PROMPTRANK is imposed by the transformer context window length .", "Comments": [], "label": []}
{"id": 160282, "text": "We propose to incorporate graph neural networks into transformers to adapt to AMR graph structures and derive more descriptive contextual embeddings .", "Comments": [], "label": []}
{"id": 153310, "text": "Table 6 : Results on IWSLT14 De→En with baseline Transformer and CipherDAug using different vocabulary sizes and embedding dimensions .", "Comments": [], "label": []}
{"id": 158873, "text": "Architecture Given the above task formulation , we employ an encoder-decoder network like T5 [ Raffel et al. , , 2020 ] , where the encoder encodes < instruction , demonstrations , text > and the decoder generates all extractions as a tokenized text sequence Y = [ y1 , . . . , yn ] .", "Comments": [], "label": []}
{"id": 157424, "text": "all these variants achieve higher BLEU-4 scores compared to the variant SPARTA ( T5 ) ( -w/o CS ) shown in Table [ 4 . ]", "Comments": [], "label": []}
{"id": 154395, "text": "The `` decoder-last '' means the contrastive loss is computed on the hidden states from the last Transformer layer of the decoder after a linear transform .", "Comments": [], "label": []}
{"id": 153788, "text": "Our results include approximately 60 model permutations for a total of 240-360 GPU hours . roberta-base has 125M parameters , but we did not pre-train any models from scratch .", "Comments": [], "label": []}
{"id": 155986, "text": "After generating the augmented samples , we use the classifier f for scoring .", "Comments": [], "label": []}
{"id": 153056, "text": "For WMT17 En-Tr and IWSLT15 En-Vi , we use case-sensitive Sacre-BLEU [ Post , 2018 ] to report reproducible BLEU scores .", "Comments": [], "label": []}
{"id": 154888, "text": "We make our PLM [ https : //huggingface.co/ ] ( https : //huggingface.co/onlplab/alephbert-base ) [ onlplab/alephbert-base ] ( https : //huggingface.co/onlplab/alephbert-base ) and demo [ https : //nlp .", "Comments": [], "label": []}
{"id": 154996, "text": "Each vector p ( 1 ≤ i ≤ L ) is a concatenation of key-value pairs in all N Transformer layers , which directly affect the computation of every attention layer .", "Comments": [], "label": []}
{"id": 160144, "text": "If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE , etc. ) ? * We provide footnotes with links to all source codes utilized in this work ; Sections 3-5 *", "Comments": [], "label": []}
{"id": 152708, "text": "The warmup step we use is 4000 .", "Comments": [], "label": []}
{"id": 153884, "text": "( vi ) Adapter+LAMOL : We only inserted adapter modules into every transformer layer for the first task , then used those adapter modules to learn the whole the task sequence with pseudo experience replay .", "Comments": [], "label": []}
{"id": 152863, "text": "We use the Adam optimizer and set the extractor learning rate to 5e-5 and the generator learning rate to 5e-6 .", "Comments": [], "label": []}
{"id": 156758, "text": "C.2 VQA Transformer Details The Transformer takes tokenized images x and the question x and outputs answers as follows : We follow the hyper-paramters provided in [ Popel ] [ and Bojar , 2018 ] .", "Comments": [], "label": []}
{"id": 158667, "text": "Rel-CSKGC differs from them in that we utilize pretrained language models to predict the relation given the * head event * and the * tail event * .", "Comments": [], "label": []}
{"id": 157351, "text": "With the CLS fusion , each transformer has access to the resume of the input from the other transformer with only a token .", "Comments": [], "label": []}
{"id": 158519, "text": "When M = 1 , in the `` ROG-Oracle '' subcolumn , an interesting result is that Multi-DYLE ( XROG CES o ) achieves the best F1 result and ROUGE score , meaning This DYLE model is made publicly available by the authors of DYLE .", "Comments": [], "label": []}
{"id": 158547, "text": "To address this issue , we investigate a semantic graph-based transformer .", "Comments": [], "label": []}
{"id": 157769, "text": "D.2 Faithfulness For BARTScore , we use a PEGASUS [ Zhang et al. , 2020a ] model pretrained on the PubMed 'NP ' using the annotation scheme from the Penn Treebank [ Marcus et al. , , 1993 ] .", "Comments": [], "label": []}
{"id": 155280, "text": "We use the Adam optimizer [ Kingma and ] [ Ba , 2014 ] for all models with a learning rate between 1e − 5 and 1e − 4 and a weight decay of 1e − 5 .", "Comments": [], "label": []}
{"id": 153872, "text": "Adapter The * module * used in our framework refers to adapter [ Houlsby et al. , , 2019 ] , which is a task-specific module inserted into each frozen pretrained transformer layers [ Vaswani et al. , , 2017 ] .", "Comments": [], "label": []}
{"id": 153756, "text": "SMILES and IUPAC are encoded by separate Transformers .", "Comments": [], "label": []}
{"id": 156779, "text": "In addition to a Random-guess baseline , we fine-tune T5-11B given just the caption , i.e. , without knowledge of the cartoon [ Trichelair et al. , , 2019 ] [ Poliak et al. , , 2018 ] .", "Comments": [], "label": []}
{"id": 159191, "text": "Table [ 4 ] demonstrates the Pearson correlations between omission rate and other reference-based metrics , including n-gram based metrics ROUGE [ Lin , 2004 ] and BLEU [ Papineni et al. , 2002 ] , embedding-based metric BERTScore [ Zhang et al. , , 2019 ] , and learning-based metric BLEURT [ Sellam et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 157614, "text": "We use a recent version of TaPas proposed by [ Eisen ] [ schlos et al. , 2020 ] which leverages intermediate pretraining and table pruning , e.g. , cell truncation to the first token and dropping rows that exceed the limit , improving significantly the initially released model ( TaPasv0 ) .", "Comments": [], "label": []}
{"id": 156748, "text": "For experiments on CLEVR , we use a simplified model in which both questions and images are mapped to answers by a transformer model , similarly to [ Ramesh et al. , 2021 ] .", "Comments": [], "label": []}
{"id": 157768, "text": "We use test set fine-tuning ( FT ) scores to compute mean and standard deviation so that RelAgg is 0 after FT and > 0 values are standard deviation improvements from calibration .", "Comments": [], "label": []}
{"id": 152305, "text": "4.3 Metric ROUGE [ Lin , 2004 ] is the standard metric for evaluating the quality of summaries .", "Comments": [], "label": []}
{"id": 155701, "text": "We use papers published before Feb. 2020 as in PubmedBERT . use the Pubmed Parser [ 5 ] to extract citation links between articles .", "Comments": [], "label": []}
{"id": 159854, "text": "In our approach , we utilize T5 base/GPT2-base to generate the missing spans at the end of the prompt text .", "Comments": [], "label": []}
{"id": 157892, "text": "Although this heuristic sampling-based dialogue flow construction method is not universally applicable , it can be migrated to other grounded dialogue datasets with minor modifications .", "Comments": [], "label": []}
{"id": 153877, "text": "To ensure k+1 =1 λt , l = 1 , we use softmax function to produce λ1 , l , . . . , λk+1 , l from c1 , l , . . . , ck+1 , l : Using this mixing approach in every transformer layer , we optimize our model using Ltrain ( see Sec [ 4.2 ] for the new task and find the most suitable modules for each layer .", "Comments": [], "label": []}
{"id": 153965, "text": "For generating response claims which belong to RE-FUTED or NEI categories , we use the following types of evidence sentences to condition the infilling : a ) empty evidence , b ) evidence sentences selected randomly from the knowledge article set K belonging to the original response , and c ) evidence sentences from a Wikipedia article of an entity retrieved using sense2vec based on its similarity with the entities in the original response .", "Comments": [], "label": []}
{"id": 152463, "text": "We use Tw and Tb to represent the timestamps of w and b respectively .", "Comments": [], "label": []}
{"id": 159556, "text": "Table 5 presents the full results of T5 models .", "Comments": [], "label": []}
{"id": 155767, "text": "We use heuristics to derive four IS types : * old * , * mediated/bridging * , * mediated/comparative * and * other * .", "Comments": [], "label": []}
{"id": 158905, "text": "We use * italics * to indicate the performance before consistency transformation , use bold to express the performance that has been reduced after consistency transformation , and do not deal with other parts that have not decreased , for the convenience of readers .", "Comments": [], "label": []}
{"id": 156433, "text": "Evaluation Metrics We utilize BLEU ( B-n ) [ Pa ] [ pineni et al. , , 2002 ] , ROUGE ( R-n ) [ Lin , 2004 ] , Lexical Repetition ( LR-n , 4-gram repetition for n-times ) [ Shao et al. , , 2019 ] , Semantic Repetition ( SR-n , average top-n semantic similarity between any two sentences ) [ Guan et al. , , 2021b ] [ ¶ ] , average semantic overlap ( S-m , average semantic similarity of all the sentences ) , Distinct ( D-n ) [ Li et al. , , 2016 ] and BERTScore [ Zhang et al. , , 2019 ] for the storytelling task .", "Comments": [], "label": []}
{"id": 157612, "text": "Since WikiTQ does not provide SQL annotations , which are needed to obtain gold answer cell coordinates for supervising the ITR retriever ( see [ §3.2 ] , we use the model trained on WikiSQL in a zero-shot fashion to retrieve the relevant table items for the WikiTQ TableQA task .", "Comments": [], "label": []}
{"id": 152290, "text": "The Transformer aims at reducing the fundamental constraint of sequential computation which underlies most architecture [ Liu et al. , , 2019 ] .", "Comments": [], "label": []}
{"id": 155432, "text": "For MCTACO , we used a reduced subset as provided by [ Shwartz et al. , 2020 ] where each question Table 1 : * Standard evaluation results on unsupervised commonsense question answering tasks : * ( Top ) Dev set accuracies ( unless specified otherwise ) with gpt2-xl are presented for baselines and ALC along with an unscaled version of ALC where the bias term is not scaled .", "Comments": [], "label": []}
{"id": 158998, "text": "4 Experimental Setup In this section , we will detail the datasets we use for our experiments , the baseline defense mechanisms , and the adversarial attacks they will be pitted against , and the three metrics we will use to compare the defense methods .", "Comments": [], "label": []}
{"id": 157329, "text": "We use News20 and Social21 from SemEval shared tasks [ Da San Martino et al. , , 2020 ] [ Dimitrov et al. , , 2021 ] .", "Comments": [], "label": []}
{"id": 160347, "text": "Since we adopt the same model as the previous work [ Lee et al. , 2022a ] and the only difference lies in the train set , it is straightforward to compare with existing results .", "Comments": [], "label": []}
{"id": 154459, "text": "We used four automatic metrics that are frequently reported in prior work : Precision , Recall , F1 , and EM ( exact match accuracy ) .", "Comments": [], "label": []}
{"id": 157594, "text": "Based on those findings , we devise simple-yet-effective Adaptive Retrieval , which only retrieves when necessary , using a heuristic based on entity popularity and relationship types .", "Comments": [], "label": []}
{"id": 158498, "text": "5.2 Text Generation Pretrained language models , including T5 [ Raf ] [ fel et al. , , 2020 ] , BART [ Lewis et al. , , 2020 ] , and GPT [ Radford et al. , , 2018 ] have become the mainstream modules of text generation since they contain billions of parameters and use a large number of corpus for training to achieve good performance .", "Comments": [], "label": []}
{"id": 155132, "text": "We use to denote the new sentence pair after * aligned codeswitching & masking * , which will be further dynamically dual-masked at random . 2.3 Dynamic Dual-Masking Limited by the dictionary , the ratio of aligned word pairs is usually small .", "Comments": [], "label": []}
{"id": 153866, "text": "We adopt this pseudo experience replay along to alleviate the forgetting in the shared modules of our approach .", "Comments": [], "label": []}
{"id": 153176, "text": "Finally , an utterance u can be represented as a vector of features X = ( x1 , x2 , ... , x ) , where N represents the number of features we used to describe a clause .", "Comments": [], "label": []}
{"id": 154740, "text": "In the following section ( [ §3.1 ] , we will explain how it can be used for transformer multi-head attention .", "Comments": [], "label": []}
{"id": 156304, "text": "We use 4 different multidocument summarization datasets from the Text Analysis Conference ( TAC ) [ 6 ] : TAC-2008 , TAC-2009 , TAC-2010 and TAC-2011 .", "Comments": [], "label": []}
{"id": 153440, "text": "The process is the same with the agent decoder , and two decoders decode interactively . 3.2.3 Training and Inference In the training phase , we use the teacher-forcing method to jointly train two role decoders and use the Negative Log-Likelihood loss to optimize .", "Comments": [], "label": []}
{"id": 156877, "text": "Finally , the decoder Dec attends to the representations of all passages and generates an answer a : 2.3 Prompt-tuning Prompt-tuning adapts pre-trained transformer models to downstream tasks by optimizing continuous prompting vectors [ Li and Liang , 2021 ] [ Liu et al. , 2022 ] .", "Comments": [], "label": []}
{"id": 154657, "text": "For example , the recent Megatron-Turing NLG 530B model was trained on the Pile , which includes 800GB+ of text [ Gao et al. , , 2021 ] , and other large language models utilize large portions of the 200TB+ common crawl data .", "Comments": [], "label": []}
{"id": 153497, "text": "[ Tay et al. , 2020c ] categorize the researches of efficient Transformers : ( a ) Fixed patterns or combination of patterns [ Beltagy et al. , , 2020 ] [ Zaheer et al. , , 2020 ] , in which the field to be attended is pre-defined by fixed pattern .", "Comments": [], "label": []}
{"id": 156423, "text": "For Open-LTG , we utilize the nucleus sampling [ Holtzman et al. , , 2019 ] decoding strategy instead of beam search .", "Comments": [], "label": []}
{"id": 153561, "text": "We use this meta-framework to build datasets on three separate sociopolitical events : the COVID-19 pandemic , BLM protests , and California fires .", "Comments": [], "label": []}
{"id": 155240, "text": "Driven by this motivation , we use an encoder-decoder architecture , initialized with BART [ Lewis et al. , , 2020 ] , where the encoder directly takes the targeted word and definition as inputs .", "Comments": [], "label": []}
{"id": 156962, "text": "However , it does not provide humanannotated sentences . 3.2 Automatic Evaluation We adopt transfer accuracy and content preservation to evaluate our method which are currently the new sequence xˆsˆ .", "Comments": [], "label": []}
{"id": 158971, "text": "Although the baseline methods use the heuristic graph construction method ( * e.g. , * using discourse parsing result ) or use the pre-trained language model GPT-2 to explore the deep semantic information , their performance is still worse than SDDS which combines the human prior knowledge of dialogue structure and the deep semantic relationship using the static-dynamic graph .", "Comments": [], "label": []}
{"id": 153914, "text": "Moreover , T5 always predicts spans in a fixed left-to-right order .", "Comments": [], "label": []}
{"id": 156161, "text": "Table 12 : Comparison of FAIRR with ProofWriter ( `` All '' ) [ T5-11B ] when trained on D3+ParaRules and tested on ParaRules .", "Comments": [], "label": []}
{"id": 152102, "text": "Though [ Zheng et al. , 2019 ] takes a first step to fuse the sentences and entities information via Transformer , they neglect the interdependency among events .", "Comments": [], "label": []}
{"id": 156524, "text": "And even with using expensive GPUs , the latency cost is unavoidably much higher than most simple logical rules .", "Comments": [], "label": []}
{"id": 158325, "text": "Datasets we use are IMDb movie reviews [ Maas et al. , , 2011 ] for Sentiment , AGNews [ Zhang et al. , , 2015 ] for Topic , and Jigsaw Toxic Comment Classification Challenge Dataset for Detoxification .", "Comments": [], "label": []}
{"id": 157183, "text": "We use micro-averaged metrics , meaning the scores are computed across all documents .", "Comments": [], "label": []}
{"id": 151939, "text": "We pretrain a Transformer with 12 layers and the hidden size of 768 , where the parameters are initialized with XLM-R [ Conneau et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 158535, "text": "To clearly demonstrate the difference between these types of extractive oracles , Tables [ 10 ] and [ 11 ] show the turn-level and sentence-level ROUGE-based extractive oracles obtained for the same query in Table [ 8 , ] where bold-faced index refers to humanannotated aligned CESs .", "Comments": [], "label": []}
{"id": 159158, "text": "We apply gradual unfreezing and discriminative learning rates , meaning that the encoder model is frozen Table 7 : Hyper-parameters for training the various Indic-COMET model .", "Comments": [], "label": []}
{"id": 154797, "text": "We used a smaller version of the * Wiki * dataset as provided in Nejadgholi and Kiritchenko ( 2020 ) .", "Comments": [], "label": []}
{"id": 159608, "text": "To predict a lemma during inference , we use beam search with a width of 20 .", "Comments": [], "label": []}
{"id": 151630, "text": "For both fine-tuning and adapter-based tuning , we use batch size 32 , and tune the learning rates in { 1e-5 , 2e-5 , 3e-5 , 4e-5 , 5e-5 } .", "Comments": [], "label": []}
{"id": 158792, "text": "Since the Interglad database is not publicly available , we use SciGlass [ Epam ] database ( released under Open Database License ) as a proxy for Interglad in the shared code .", "Comments": [], "label": []}
{"id": 159187, "text": "Following Nallapati et al . [ 2017 ] , we use a greedy algorithm to select utterances from the dialogue that maximizes the Rouge score [ Lin , 2004 ] with respect to the summary .", "Comments": [], "label": []}
{"id": 156899, "text": "For answer generation , we use the token-level F1 score introduced in SQUAD [ Ra ] [ jpurkar et al. , , 2016 ] .", "Comments": [], "label": []}
{"id": 152429, "text": "When rendering utterances , we use a template-based language generator as in [ He et al. , , 2018 ] , and insert population-specific tokens in utterances by sampling according to different opponent types .", "Comments": [], "label": []}
{"id": 152268, "text": "We use BERT [ Devlin et al. , , 2019 ] as the text encoder φ of our classification model * * θ * * ( φ , * * ) * * ( φ ( * x * ) ) .", "Comments": [], "label": []}
{"id": 157866, "text": "For COREF , we use 1/0 to denote whether a mention pair belongs to the same entity .", "Comments": [], "label": []}
{"id": 153945, "text": "During decoding , we use beam search with beam size 5 and tweak the value of length penalty on the development set .", "Comments": [], "label": []}
{"id": 155749, "text": "[ Hou et al. , 2018 ] propose a pipeline system built on top of complex manually designed features .", "Comments": [], "label": []}
{"id": 155809, "text": "< https : //github.com/pytorch/fairseq > [ https : //www.kaggle.com/c/ ] ( https : //www.kaggle.com/c/quora-question-pairs/data ) [ quora-question-pairs/data ] ( https : //www.kaggle.com/c/quora-question-pairs/data ) [ https : //github.com/gmftbyGMFTBY/ ] ( https : //github.com/gmftbyGMFTBY/MultiTurnDialogZoo ) [ MultiTurnDialogZoo ] ( https : //github.com/gmftbyGMFTBY/MultiTurnDialogZoo ) We evaluate BLEU using fairseq_score script .", "Comments": [], "label": []}
{"id": 153283, "text": "5.2 Costs of Computing and Storage In this section , we compare our CBMI-based approach with the BMI-based adaptive training in terms of the number of trainable parameters , the CPU computational costs of pre-processing , the GPU computational costs of training , and disk cost for storing intermediate variables .", "Comments": [], "label": []}
{"id": 153524, "text": "We use the development sets of MNLI-hard reconstructed by [ Karimi Mahabadi et al. , 2020 ] to develop our methods .", "Comments": [], "label": []}
{"id": 153495, "text": "For instance , more than 68GB GPU memory and 1.6T multiply-accumulation operations are required for a 64 × 64 × 32 3D feature volume .", "Comments": [], "label": []}
{"id": 157076, "text": "Did you report the number of parameters in the models used , the total computational budget ( e.g. , GPU hours ) , and computing infrastructure used ?", "Comments": [], "label": []}
{"id": 152005, "text": "Figure 7 : G-Transformer compared with Transformer . ( a ) Cross-Attention ( b ) Encoder Self-Attention Figure 8 : Comparison on the development of crossattention and encoder self-attention .", "Comments": [], "label": []}
{"id": 159545, "text": "During fine-tuning , we used 100 epochs with an early stopping patience of 10 and threshold of 1.0 .", "Comments": [], "label": []}
{"id": 159005, "text": "For the SST-2 dataset , we train four variants of ATINTER with different sizes : t5-small , t5-base , t5-large , t5-3b .", "Comments": [], "label": []}
{"id": 156716, "text": "We use our benchmark of the TED talk dataset enhanced with MuDA tags to perform an exploration of context usage across languages with 4 models , including commercial systems .", "Comments": [], "label": []}
{"id": 158856, "text": "We report BLEU , ROUGE , BERTScore ( BScore ) , Distinct-1 ( Dist-1 ) , and Distinct-2 ( Dist-2 ) metrics . †We calculate metrics over 100 random generations because of our limited access to the GPT3.5 API ( text-davinci-002 ) .", "Comments": [], "label": []}
{"id": 156719, "text": "To evaluate stronger models , we additionally train a large transformer model ( large ) that was pretrained on a large , sentence-level corpora , for German , French , Japanese and Chinese .", "Comments": [], "label": []}
{"id": 154620, "text": "Quality Control To optimize the quality of collected data , we implement a list of quality controls in both the conversation summarization task and the multi-session chat task .", "Comments": [], "label": []}
{"id": 157350, "text": "After that , the last token produced for each subsequent layer is replaced by the corresponding CLS token of the other modality built in the corresponding transformer block .", "Comments": [], "label": []}
{"id": 152100, "text": "Further analysis reveals GIT is effective in extracting multiple correlated events and event arguments that scatter across the document .", "Comments": [], "label": []}
{"id": 152808, "text": "For the RoBERTa model as a student , we use the same teacher with ALBERT .", "Comments": [], "label": []}
{"id": 151818, "text": "In particular , [ Srivastava and Sutton , 2017 ] propose the autoencoding variational inference ( AEVB ) [ Kingma ] [ and Welling , 2013 ] for LDA by using Laplace approximation [ Hennig et al. , , 2012 ] for the Dirichlet prior , and building logistic-normal ( LN ) encoding posterior .", "Comments": [], "label": []}
{"id": 155499, "text": "We implement TAGOP-L2I based on TAGOP [ 4 ] .", "Comments": [], "label": []}
{"id": 157274, "text": "†The number is much higher than 81.4 , the GLUE score reported by [ Aroca-Ouellette and Rudzicz , 2020 ] because we continue training from the pretrained BERT and we use better fine-tuning hyperparameters .", "Comments": [], "label": []}
{"id": 154401, "text": "In the experiments of Appendix [ C.1 , ] we adopt BART-large with 12 encoder layers , 12 decoder layers and hidden state dimension 1024 , to evaluate the quantization ability on larger models .", "Comments": [], "label": []}
{"id": 151746, "text": "The framework of the proposed method is shown in Figure [ 2 . ] In the M3T model , we adopt the CMA module to fuse acoustic information into the textual representations .", "Comments": [], "label": []}
{"id": 159823, "text": "Using this new * GPT-generated training data * , we again fine-tune Flan-T5 ( Large ) as described above ( Section [ 4.2.2 ] , and evaluate it on the validation set .", "Comments": [], "label": []}
{"id": 154241, "text": "[ 1 , ] we use the following black-box experts in our experiments as modules that we can add or remove to produce desired behavior : Emlm ( X ) : Recent work has shown that large masked language models ( MLM ) like BERT can discriminate between well-formed and ill-formed sentences [ Zhang et al. , , 2020 ] and induce an implicit energy function over the sequences [ Goyal et al. , , 2021 ] .", "Comments": [], "label": []}
{"id": 153548, "text": "Within those reported in Table [ 3 , ] 500 quantities in each domain are fully labeled with both typing and spatiotemporal extent , and we use these as our test sets .", "Comments": [], "label": []}
{"id": 159620, "text": "Finally , we acknowledge and thank for crucial support from the Google TPU Research Cloud program , for granting us access to their TPUs .", "Comments": [], "label": []}
{"id": 159360, "text": "Subsequently , we use the silver-standard training data generated from these two steps to train a detector .", "Comments": [], "label": []}
{"id": 152475, "text": "Finally , we utilize knowledge distillation to train a targetlanguage NER model on this selected dataset .", "Comments": [], "label": []}
{"id": 157188, "text": "In case of an entity being expanded differently in two or more instances , we use the shorter of all variants . ( 3 ) * VanillaNER * [ 4 ] : Our developed NER model is intended to be used as a baseline for comparison .", "Comments": [], "label": []}
{"id": 153740, "text": "However , its usage of COMeT [ Bosselut et al. , 2019 ] as the generator confines its applicability to the social commonsense domain .", "Comments": [], "label": []}
{"id": 152561, "text": "We use the predictions from the same ASR systems as references and filter the candidate segments by a maximum threshold of 20 % character error rate ( CER ) .", "Comments": [], "label": []}
{"id": 156749, "text": "Both LEXSYM augmentation and this VQA-Transformer model operate over sequences of discrete visual codes produced by a vector-quantized variational autoencoder .", "Comments": [], "label": []}
{"id": 154585, "text": "[ 9 ] 4 Experimental Setup Data We use six English QA datasets that provide substantial amount of annotated training data taken from the MRQA training portion [ Fisch et al. , 2019 ] : SQUAD [ Rajpurkar et al. , , 2016 ] , NewsQA [ Trischler et al. , , 2017 ] , SearchQA [ Dunn et al. , 2017 ] , TriviaQA [ Joshi et al. , , 2017 ] , HotpotQA [ Yang et al. , , 2018 ] , and NaturalQuestions ( NQ ; [ Kwiatkowski et al. , , 2019 ] .", "Comments": [], "label": []}
{"id": 153138, "text": "For real-time inference , depending on one 's CPU , either of the 3 variants can be used with the trained models , since all the 3 BlazePose models are trained on the same dataset to return same number of keypoints .", "Comments": [], "label": []}
{"id": 154320, "text": "Specifically , we adopt the nine data perturbation methods introduced in Section [ 3.3 ] and include the * no-perturbation * variant that indicates directly using an unlabeled sentence and its pseudo target to train the unsupervised loss .", "Comments": [], "label": []}
{"id": 153793, "text": "In this paper we focus on Transformers [ Vaswani et al. , , 2017 ] , which have been shown in the literature to exhibit poor compositional generalization ( see Section [ 2 ] .", "Comments": [], "label": []}
{"id": 159551, "text": "For datasets in GLUE ( Wang et al. , 2019 ) ( SST2 , RTE , MNLI ) and SNLI , we use the original validation sets as test sets following Zhang et al . ( 2021 ) .", "Comments": [], "label": []}
{"id": 159374, "text": "Recent approaches for detecting these propaganda techniques rely on pre-trained transformers [ Morishita et al. , , 2020 ] [ Feng et al. , , 2021 ] .", "Comments": [], "label": []}
{"id": 153003, "text": "Previous observations on GPU throughput only hold for inference with the same batch size .", "Comments": [], "label": []}
{"id": 156106, "text": "Further research on making generative approaches with large pretrained models more efficient without losing on the performance side holds a great potential impact to accelerate the progress of fully generative models for question answering .", "Comments": [], "label": []}
{"id": 152121, "text": "Besides , Doc2EDAG [ Zheng et al. , , 2019 ] uses transformer encoder to obtain sentence and entity embeddings , followed by another transformer to fuse cross-sentence context .", "Comments": [], "label": []}
{"id": 151420, "text": "Recent works utilise powerful transformers such as GPT-2 [ Peng et al. , 2020 ] [ Hosseini-Asl et al. , , 2020 ] or T5 [ Lin et al. , 2020b ] for dialogue modeling and reach stateof-the-art performance ; however , the area of having a user simulator involved during training is unexplored .", "Comments": [], "label": []}
{"id": 155486, "text": "We use the same hyper-parameters as CSN dataset but fine-tune the model for 2 epochs .", "Comments": [], "label": []}
{"id": 157332, "text": "The |GPU| column denotes the GPU memory footprint of each variant under the same experimental setup .", "Comments": [], "label": []}
{"id": 153071, "text": "We employ both edge and MDL probing in this work .", "Comments": [], "label": []}
{"id": 154312, "text": "Specifically , we adopt * style accuracy * , * source * -BLEU , and * perplexity * as three metrics to filter out low-quality pseudo-parallel data based on a threshold .", "Comments": [], "label": []}
{"id": 154857, "text": "The initial hidden state of decoder RNN is defined as : < https : //spacy.io > Figure 2 : Overview of our proposed method . and then the decoder will reconstruct the phrase p based on s .", "Comments": [], "label": []}
{"id": 156555, "text": "On DiDeMo and ActivityNet Captions , it outperforms all previous work , including many that pretrain on significantly larger amounts of data , e.g. , 400M image-text pairs in CLIP4Clip , or 136M video-text pairs in VideoCLIP compared to 5M image-text and video-text pairs in SINGULARITY .", "Comments": [], "label": []}
{"id": 159966, "text": "We use a beam width of 10 for the first-pass decoder in S2SpecT2 . 3.8 Vocoder We use a HiFi-GAN vocoder [ Kong et al. , , 2020 ] to convert spectrograms to the waveform for TTS and direct speech-to-spectrogram models .", "Comments": [], "label": []}
{"id": 154579, "text": "The term * context * here refers to the input to the learner policy , and is different from the term * context * as we use it later in extractive QA , where the term * context * refers to the evidence document given as input to the model .", "Comments": [], "label": []}
{"id": 157262, "text": "E Dataset and Artifacts License The Wikipedia dataset is available under the Creative Common license , cc-by-sa-3.0 , which we use solely for research purposes in accordance with its terms .", "Comments": [], "label": []}
{"id": 152660, "text": "We start by fine-tuning a bi-directional language model on a slang-dense corpus ( Section [ 5.6.1 ] , after which we survey the literature and propose metrics ( Section [ 5.6.2 ] that we use to perform an extensive experimentation study to find the most suitable one ( Section [ 5.6.3 ] .", "Comments": [], "label": []}
{"id": 159448, "text": "Simply replacing all layers in the Transformer with LSLs would significantly increase the number of parameters , and reduce the sharing between languages .", "Comments": [], "label": []}
{"id": 152130, "text": "Table 4 : The decrease of F1 scores on ablation study for GIT 's heterogeneous graph interaction network .", "Comments": [], "label": []}
{"id": 159070, "text": "Ethics Statement For the present work , we used an existing anonymised dataset without any data protection issues .", "Comments": [], "label": []}
{"id": 156889, "text": "Due to ( academic ) limitations of computational resources , this paper employs a low-capacity T5 model for experiments .", "Comments": [], "label": []}
{"id": 160372, "text": "It has two unit decoders with 6 and 2 Transformer decoder layers for target and source unit prediction respectively .", "Comments": [], "label": []}
{"id": 154219, "text": "To utilize world knowledge in the computation process , we used ELECTRA [ Clark et al. , , 2020 ] , a pre-trained language model .", "Comments": [], "label": []}
{"id": 153810, "text": "For this systematic experimentation , we used small Transformer models , without pre-training ( all models are trained from scratch ) .", "Comments": [], "label": []}
{"id": 160070, "text": "To compute 95 % confidence intervals for these estimates , we use 5 random seeds of initialization , resulting in 5 ensemble models for each ensemble size .", "Comments": [], "label": []}
{"id": 157075, "text": "We use the following range of values for selecting the best hyper-parameter • Batch Size : 8 , 16 , 32 • Learning Rate : 1e-3 , 1e-4 , 1e-5 , 1e-6 , 3e-3 , 3e-4 , 3e-5 , 3e-6 , 5e-3 , 5e-4 , 5e-5 , 5e-6 C Anecdotes of Gains C.1 Answer in Table Cell We present in Figure [ 5 ] an example where MITQA is able to predict the answer correctly even when the correct answer is in a table cell and not a span in the passages .", "Comments": [], "label": []}
{"id": 157281, "text": "We use the first 5k validation samples to select the best fine-tuned model checkpoints for the evaluation .", "Comments": [], "label": []}
{"id": 157950, "text": "Here , we use Mahalanobis distance .", "Comments": [], "label": []}
{"id": 158911, "text": "These include settings where ( i ) perturbations are induced either in the source Our code and data to replicate our experiments are available in [ https : //github.com/deep-spin/ ] ( https : //github.com/deep-spin/ot-hallucination-detection ) [ ot-hallucination-detection ] ( https : //github.com/deep-spin/ot-hallucination-detection ) .", "Comments": [], "label": []}
{"id": 156790, "text": "Compared to the same model trained with access to human written descriptions available at test < https : //huggingface.co/docs/accelerate > Also , crowd selectors greatly outnumber New Yorker editors , so crowd rankings may be a more dependable target , statistically speaking .", "Comments": [], "label": []}
{"id": 157068, "text": "We use Sentence-BERT [ Reimers and Gurevych , 2019 ] to get question and passage embeddings and we perform asymmetric semantic search to rank the passages .", "Comments": [], "label": []}
{"id": 157050, "text": "Specifically , we use the testing examples provided by previous work [ 8 ] [ Huang and Chang , 2021 ] and calculate the BLEU score between the ground-truth and the generated output as the evaluation metric .", "Comments": [], "label": []}
{"id": 154940, "text": "We use a queue size of l = 1000 and set our momentum coefficient µ = 0.9999999 , with loss weighting parameter λ = 0.85 .", "Comments": [], "label": []}
{"id": 155291, "text": "We use the CARB benchmark introduced in [ Bhardwaj et al. , , 2019 ] for evaluating English OpenIE n-ary extraction .", "Comments": [], "label": []}
{"id": 158874, "text": "Specifically , to sample in-context NER task x , we use traditional NER corpus DNER where each NER instance is a ( text , entities ) pair as follows : Pre-training Loss Based on the in-context pretraining corpus Din-context , we pre-train our incontext NER model by optimizing the loss : where Lmeta-function is the meta-function loss which ensures PLMs can implicitly generate accurate entity extractors ( Section [ 4.2 ] , Lextraction is the extraction loss which ensures PLMs have good extraction ability ( Section [ 4.3 ] , α is the coefficient of metafunction loss .", "Comments": [], "label": []}
{"id": 154085, "text": "We use the following two of them : Figure 3 : Handcrafted Features vs .", "Comments": [], "label": []}
{"id": 157148, "text": "We have conducted a set of experiment to compare our methods Table 8 : The statistics of datasets we used for experiments .", "Comments": [], "label": []}
{"id": 151379, "text": "Last , we use the max-pooling and fully connected layers to get the feature embedding of Chinese characters at the radical-level .", "Comments": [], "label": []}
{"id": 151541, "text": "3.4 Loss Finally , the model is trained with three losses Lgen , Latt , and Lcts , where Lgen is the caption generation loss , Latt is the spatial attention guidance loss , and Lcts is the temporal contrastive loss.We jointly optimize our model by minimizing all losses added together : 4 Experiments 4.1 Dataset We use the annotated COCO subset of Localized Narratives to evaluate our method .", "Comments": [], "label": []}
{"id": 155013, "text": "For the inference , we use the beam search algorithm to obtain translation from the mGPT model , and the beam size is set to 4 .", "Comments": [], "label": []}
{"id": 151837, "text": "Following [ Liu and Lapata , 2019 ] , in the test stage , we use beam search with size 5 , select the top-3 checkpoints based on their evaluation loss on the validation set , and report the averaged results on the test set .", "Comments": [], "label": []}
{"id": 154985, "text": "Assume using the * transformer-base * setting with a vocabulary size of 32K .", "Comments": [], "label": []}
{"id": 157282, "text": "When running large datasets in GLUE Full and SuperGLUE Full ( MNLI , QQP , QNLI , SST-2 , BoolQ , MultiRC , and WiC ) using BERTLarge , we use learning rates c = 2 , 4 , 6 , 8 , 10 , 14 to accelerate the training .", "Comments": [], "label": []}
{"id": 159540, "text": "5.3 Training dependency of performance Figure 1 : Training and test data overlap separated between correct and incorrect predictions made by FLANlarge ( left bars ) and T5-base ( right bars ) .", "Comments": [], "label": []}
{"id": 153139, "text": "Based on our experience , we prefer only * lite * or * full * variants depending on the CPU-type , and we find the * heavy * model only suitable if we employ frame-skipping and use decoder models that also work at a lower FPS ( below 8fps ) .", "Comments": [], "label": []}
{"id": 155971, "text": "As it comes to the PoWER-based models , we finetune Vanilla transformers with a progressive elimination of word vectors on the encoder side , following the approach of [ Goyal et al. , 2020 ] .", "Comments": [], "label": []}
{"id": 155149, "text": "Configuration We use almost the same configuration as the pre-training and AT except the following differences .", "Comments": [], "label": []}
{"id": 155504, "text": "As the concurrent work , [ Chiang and yi Lee , 2022 ] trained Transformer encoders on artificial data with token dependencies in the sequences and showed that they perform reasonably well on the GLUE benchmark [ Wang et al. , , 2019 ] .", "Comments": [], "label": []}
{"id": 153570, "text": "Explanation sentences can either be generated by filling predefined templates [ Zhang et al. , , 2014 ] [ Wang et al. , , 2018 ] or through flexible natural language approaches such as Attn2Seq [ Dong et al. , , 2017 ] , based on recurrent neural networks , and PETER [ Li et al. , , 2021a ] , which is powered by a personalized Transformer .", "Comments": [], "label": []}
{"id": 158436, "text": "Transition-based parsing with stack-transformers .", "Comments": [], "label": []}
{"id": 154875, "text": "We consider < https : //github.com/mjpost/sacrebleu > This is because the official pre-processing script of Multi30K dataset lowercases the corpus , see [ https : //github.com/multi30k/dataset/ ] ( https : //github.com/multi30k/dataset/blob/master/scripts/task1-tokenize.sh ) [ blob/master/scripts/task1-tokenize.sh ] ( https : //github.com/multi30k/dataset/blob/master/scripts/task1-tokenize.sh ) < https : //github.com/pytorch/fairseq > Figure 3 : Visualization of different visual representations .", "Comments": [], "label": []}
{"id": 156570, "text": "In Section [ A.2 , ] we show more experimental details , such as SIN-GULARITY-temporal results on existing datasets , SINGULARITY zero-shot results , impact of image size , and results on image-text tasks such as textto-image retrieval tasks Flickr30K [ Young et al. , , 2014 ] , COCO [ Chen et al. , , 2015 ] and image question answering task VQA [ Antol et al. , , 2015 ] , model size and inference cost of our approach w.r.t .", "Comments": [], "label": []}
{"id": 159853, "text": "Additionally , NPPrompt can be adapted for text generation models such as T5 [ Raffel et al. , , 2020 ] and GPT-2 [ Radford et al. , , 2019 ] ) with minor modifications .", "Comments": [], "label": []}
{"id": 154386, "text": "< http : //huggingface.co/models > Table 1 : Results of language modeling on the test set of WikiText2 , PTB and WikiText103 datasets , and next utterance prediction on the validation set of Persona-Chat dataset , with quantized GPT-2 .", "Comments": [], "label": []}
{"id": 158759, "text": "ResNet is used as the image encoder to encode the latent semantic representations of images .", "Comments": [], "label": []}
{"id": 156179, "text": "We also present an annotated table https : //huggingface.co/ Figure 3 : Level-wise QA accuracy and proportion of samples with MAPO and hierarchy-aware logical form .", "Comments": [], "label": []}
{"id": 151554, "text": "To make our task as accessible and easy-to-study as possible , however , we use the provided labels in place of a separate model however .", "Comments": [], "label": []}
{"id": 152832, "text": "We use QED to categorize our data , as described in [ §3.5 . ]", "Comments": [], "label": []}
{"id": 156036, "text": "For dataset statistics such as the number of classes and input sequence length , and the details of the backbone Transformers , see Appendix [ C. ] For sequence-length configurations , we generate a set F of 30 sequence-length configurations using Equation [ 5 , ] the details of which are listed in Table [ 9 ] Appendix [ C. ] The high level idea is to cover multiple tradeoffs between efficiency and accuracy .", "Comments": [], "label": []}
{"id": 152636, "text": "We use different colors for different models ( e.g. , dark blue for ALBERT-XXL-v2 ) , and different shapes to mark different checkpoints .", "Comments": [], "label": []}
{"id": 153241, "text": "For all attack methods , we use the default parameters of thirdparty libraries .", "Comments": [], "label": []}
{"id": 159073, "text": "The experiments were performed on T5-effective-base .", "Comments": [], "label": []}
{"id": 152876, "text": "2.1 Data : AUX6 corpus We utilize De , Es , Fi , Hi , Ru , and Zh as the auxiliary source languages , which are high-resource languages from different language families .", "Comments": [], "label": []}
{"id": 155508, "text": "We study two encoder architectures : LSTM [ Hochreiter and Schmidhuber , 1997 ] and Transformer [ Vaswani et al. , , 2017 ] .", "Comments": [], "label": []}
{"id": 157225, "text": "We use the standard [ Zettlemoyer and Collins , 2007 ] split of 600 train questions and 280 test questions and the logical form representation of [ Dong and La ] [ pata , 2016 ] [ Kwiatkowski et al. , , 2011 ] , which is a variant of typed lambda calculus [ Carpenter , 1997 ] .", "Comments": [], "label": []}
{"id": 152511, "text": "We use the same hyperparameter configuration as mentioned in the original papers .", "Comments": [], "label": []}
{"id": 155096, "text": "We consider three pre-trained models , one graph-based model [ Ye et al. , , 2020 ] and one 12-layer Transformer model trained from scratch as baselines .", "Comments": [], "label": []}
{"id": 156615, "text": "At a high level , it decomposes each transformer block into a sum of functions of individual tokens and views an output representation as a summation of transformed input vectors .", "Comments": [], "label": []}
{"id": 156811, "text": "We use FinBERT [ Araci , 2019 ] as the shared encoder .", "Comments": [], "label": []}
{"id": 154921, "text": "Note that we use d com for complex definitions and d sim for simple ones . 3.2 Multitasking Framework We design the three sub-tasks in the SimpDefiner to learn different abilities .", "Comments": [], "label": []}
{"id": 159122, "text": "NLLB is the best performing model on Marathi and Bing API for Gujarati .", "Comments": [], "label": []}
{"id": 158318, "text": "In the general domain , we employ TA-CRED [ Zhang et al. , , 2017 ] , TACREV [ Alt et al. , , 2020 ] , and Re-TACRED [ Stoica et al. , , 2021 ] .", "Comments": [], "label": []}
{"id": 156224, "text": "For measuring the performance of the model we use the GAP score [ Kishida , 2005 ] [ 5 ] which is a variant of the MAP ( Mean Average Precision ) .", "Comments": [], "label": []}
{"id": 154950, "text": "For the example in Table [ 1 , ] the English sentence is quite likely to be selected as a summary sentence , since it greatly overlaps with the English reference ( high ROUGE ) .", "Comments": [], "label": []}
{"id": 157501, "text": "We employ four annotators in total to ensure the quality of the assessment .", "Comments": [], "label": []}
{"id": 159065, "text": "The proposed evaluation framework better reflects human judgement : We note several important differences between our proposed metrics and ROUGE-based evaluation .", "Comments": [], "label": []}
{"id": 152167, "text": "Different from [ Shibuya and Hovy , 2020 ] , we use only one CRF instead of employing different CRFs for different entity types .", "Comments": [], "label": []}
{"id": 159111, "text": "We use the aforementioned dataset to establish correlations between the annotator scores and existing automatic metrics scores belonging to the following classes : ( i ) n-gram and character based such as BLEU , METEOR , chrF++ , ( ii ) embeddings based such as Vector Extrema , BERTScore , ( iii ) pre-trained metrics like BLEURT-20 , COMET .", "Comments": [], "label": []}
{"id": 151810, "text": "This indicates that the 2-layer transformer encoder is an ideal structure for our proposed training tasks on the document classification task .", "Comments": [], "label": []}
{"id": 156891, "text": "In NQ-Open , we use Recall @ k ( R @ k for short ) as the metric , which considers retrieval to be successful if at least one answer is included in the top-k ranked passages .", "Comments": [], "label": []}
{"id": 153225, "text": "The parsing algorithms of [ Corro , 2020 ] and [ Xin et al. , 2021 ] can be viewed as adapting the hook trick to accelerate discontinuous and continuous constituency parsing , respectively .", "Comments": [], "label": []}
{"id": 151364, "text": "C Evaluation C.1 Automatic Metrics Links to the automatic metrics : [ ROUGE , ] ( https : //pypi.org/project/pyrouge/ ) [ BLEU , ] ( https : //www.nltk.org/_modules/nltk/translate/bleu_score.html ) [ METEOR , ] ( https : //www.cs.cmu.edu/~alavie/METEOR/README.html ) [ TER ] ( https : //github.com/snover/terp ) , [ SARI , ] ( https : //github.com/cocoxu/simplification/blob/master/SARI.py ) [ BERTScore , ] ( https : //github.com/Tiiiger/bert_score ) [ BLEURT .", "Comments": [], "label": []}
{"id": 151409, "text": "4 Experimental setup 4.1 Datasets For the intrinsic evaluation , we used the GYAFC dataset .", "Comments": [], "label": []}
{"id": 153533, "text": "We feed it into the Transformer encoder to obtain contextualized representations , h i 1 , h i 2 , · · · , h i si where h i d . * Positive pairs * : Given each token x , we randomly sample a positive sample c from nearby tokens in the same context ( sequence ) within a window span where W is the window size .", "Comments": [], "label": []}
{"id": 156710, "text": "3.1 Data & Model To compare linguistic phenomena that arise during document-level translation across language pairs , we use a dataset consisting of TED talks ' transcripts and translations [ Qi et al. , , 2018 ] .", "Comments": [], "label": []}
{"id": 158986, "text": "We employ the BERTScore [ Zhang et al. , , 2020b ] , BARTScore [ Yuan et al. , , 2021 ] and Mover-Score [ Zhao et al. , , 2019 ] as the embedding based evaluation .", "Comments": [], "label": []}
{"id": 158761, "text": "In order to fairly compare with ItNet , we also used ResNet as the visual encoder .", "Comments": [], "label": []}
{"id": 159242, "text": "Specifically , for a particular company , we use the document published in the previous year as an information anchor ( i.e. , the reference ) to construct a year-to-year structure and locate important financial signals in the report of the subsequent year ( i.e. , the target ) .", "Comments": [], "label": []}
{"id": 158534, "text": "A Dataset Construction A.1 Annotation Process ExplainMeetSum was built on top of the QMSum dataset .", "Comments": [], "label": []}
{"id": 154175, "text": "Gate-Trans . [ Zhang et al. , , 2018 ] and NCT [ Ma et al. , 2020 ] : Both are document-level NMT Transformer models where they introduce the dialogue history by a gate and by sharing the first encoder layer , respectively .", "Comments": [], "label": []}
{"id": 156095, "text": "Pre-trained transformer models like BERT [ Devlin et al. , , 2019 ] have accelerated the development of neural text retrievers [ Lee et al. , , 2019 ] [ Karpukhin et al. , , 2020 ] [ Asai et al. , , 2020 ] [ Xiong et al. , , 2021 ] [ Liu et al. , , 2021 ] in the retrieverreader framework [ Chen et al. , , 2017 ] .", "Comments": [], "label": []}
{"id": 155589, "text": "[ https : //github.com/cocoxu/ ] ( https : //github.com/cocoxu/simplification ) [ simplification ] ( https : //github.com/cocoxu/simplification ) < https : //github.com/pltrdy/rouge > Models We compare our proposed approaches against the following models trained from scratch in controlled conditions : 1 ) * * AR * * is a auto-regressive ( AR ) transformer model [ Scarton and Specia , 2018 ] .", "Comments": [], "label": []}
{"id": 153114, "text": "We believe such standardization can help accelerate dataset collection and model benchmarking .", "Comments": [], "label": []}
{"id": 154826, "text": "We omit layer normalization for the difficulty of defining hyperbolic mean and variance , but it is still kept in our Euclidean Transformer baseline .", "Comments": [], "label": []}
{"id": 154956, "text": "Our extractive model is composed of a sentence-level Transformer T ( initialized with XLMR ) and a document-level Transformer T ( a two-layer Transformer ) .", "Comments": [], "label": []}
{"id": 153416, "text": "TDIL data used for Indic languages uses Research license type and Xtreme dataset uses Apache License 2.0 .", "Comments": [], "label": []}
{"id": 157944, "text": "* * C List of data sources * * The datasets and repositories used in this project involve : AI4Bharat , [ 5 ] AIFORTHAI-LotusCorpus , [ 6 ] Add [ El-Haj et al. , , 2018 ] , AfriBERTa [ Ogueji et al. , 2021b ] , AfroMAFT [ Adelani et al. , , 2022 ] [ Xue et al. , , 2021 ] , Anuvaad , [ 7 ] AraBench [ Sajjad et al. , 2020 ] , AUTSHUMATO , [ 8 ] Bloom [ Leong et al. , 2022 ] , CC100 [ Conneau et al. , , 2020 ] [ Wenzek et al. , , 2020a ] , CCNet [ Wenzek et al. , , 2020b ] , CMU_Haitian_Creole , [ 9 ] CORP.NCHLT , [ 10 ] Clarin , [ 11 ] DART [ Alsarsour et al. , , 2018 ] , Earthlings [ Dunn , 2020 ] , FFR , [ 12 ] Flores200 [ Costa-jussà et al. , 2022 ] , GiossaMedia [ Góngora et al. , ] [ 2022 , 2021 ] , Glosses [ Camacho-Collados et al. , , 2016 ] , Habibi [ El-Haj , 2020 ] , HinDialect [ Bafna , 2022 ] , HornMT , [ 13 ] IITB [ Kunchukuttan et al. , , 2018 ] , IndicNLP [ Nakazawa et al. , , 2021 ] , Indiccorp [ Kak ] [ wani et al. , , 2020 ] , isiZulu , [ 14 ] JParaCrawl [ Morishita et al. , 2020 ] , KinyaSMT , [ 15 ] LeipzigData [ Goldhahn et al. , 2012 ] , Lindat , [ 16 ] Lingala_Song_Lyrics , [ 17 ] Lyrics , [ 18 ] MC4 [ Raffel et al. , , 2020 ] , MTData [ Gowda et al. , , 2021 ] , MaCoCu [ Bañón et al. , , 2022 ] , Makerere MT Corpus , [ 19 ] Masakhane community , [ 20 ] Mburisano_Covid , [ 21 ] Menyo20K [ Ade ] [ lani et al. , , 2021 ] , Minangkabau corpora [ Koto ] [ and Koto , 2020 ] , MoT [ Palen-Michel et al. , , 2022 ] , NLLB_seed [ Costa-jussà et al. , , 2022 ] , Nart/abkhaz , [ 22 ] OPUS [ Tiedemann , 2012 ] , OS-CAR [ Suárez et al. , , 2019 ] , ParaCrawl [ Bañón et al. , 2020 ] , Parallel Corpora for Ethiopian Lan- `` ` 5https : //ai4bharat.org/ 6https : //github.com/korakot/corpus/releases/ download/v1.0/AIFORTHAI-LotusCorpus.zip 7https : //github.com/project-anuvaad/ anuvaad-parallel-corpus 8https : //autshumato.sourceforge.net/ 9http : //www.speech.cs.cmu.edu/haitian/text/ 10https : //repo.sadilar.org/handle/20.500.12185/ 7 11https : //www.clarin.si/ 12https : //github.com/bonaventuredossou/ffr-v1/ tree/master/FFR-Dataset 13https : //github.com/asmelashteka/HornMT 14https : //zenodo.org/record/5035171 15https : //github.com/pniyongabo/kinyarwandaSMT 16https : //lindat.cz/faq-repository 17https : //github.com/espoirMur/songs_lyrics_ webscrap 18https : //lyricstranslate.com/ 19https : //zenodo.org/record/5089560 20https : //github.com/masakhane-io/ masakhane-community 21https : //repo.sadilar.org/handle/20.500.12185/ 536 22https : //huggingface.co/datasets/Nart/abkhaz_ text `` ` Table 11 : List of languages used to train Glot500-m ( Part I ) .", "Comments": [], "label": []}
{"id": 152733, "text": "C Experiments of Changing the Softmax Temperature in the Final Decoder Layer It 's a more direct idea to change the softmax temperature in the final decoder layer rather than attention temperatures , namely changing the T in Table 9 : ROUGE scores of BART 12-6 student models with more values of λ on CNNDM dataset .", "Comments": [], "label": []}
{"id": 159055, "text": "5 Results and Analysis Proposed approach improves performance on KPA task : Our proposed two-step method outperforms the reference implementations on the full KPA task , with improvements of up to 12 % and 14 % in ROUGE and Soft-F1 , respectively , as shown in Table [ 1 . ]", "Comments": [], "label": []}
{"id": 153477, "text": "Generally , we use the settings described in previous work [ Ive et al. , , 2019 ] [ Yao and Wan , 2020 ] [ Liang et al. , , 2021c ] to conduct experiments on our MSCTD .", "Comments": [], "label": []}
{"id": 155123, "text": "We use model forecasts as optimization inputs to find W that maximizes risk-adjusted returns in a future horizon .", "Comments": [], "label": []}
{"id": 157504, "text": "1 Introduction Transformer-based large pretrained Language Models , such as GPT3 and T5 [ Vaswani et al. , , 2017 ] [ Brown et al. , , 2020 ] [ Raffel et al. , , 2020 ] , have been widely used as few-shot learners in many NLP tasks .", "Comments": [], "label": []}
{"id": 158790, "text": "Since the baselines do not have the benefit of regular expressions , features , and constraints , we implement a version of our model without these , which we call V-DISCOMAT .", "Comments": [], "label": []}
{"id": 157002, "text": "If it were , we might expect a smaller performance gap for Gujurati , a low-resource Indo-European language , since it has low SAE translation performance ( 21.7 SacreBLEU on 615M ) , but in fact , English7→Gujurati has the second * largest * dialectal translation performance gap ( -17.2 % on 615M ; -15.7 % on 1.3B ) .", "Comments": [], "label": []}
{"id": 160369, "text": "For example , we use a threshold of 1.09 for es-en and of 1.06 for hr-en .", "Comments": [], "label": []}
{"id": 157736, "text": "In this paper , we adopt the score function proposed in TransE [ Bordes et al. , , 2013 ] as : In order to increase the score of the correct triple ( h , r , t ) while decrease the score of the false triple ( h , r , t0 ) , we minimize the following margin-based ranking loss : where T is the triple set consisting of all triples from the given N KGs , λ > 0 is a margin hyperparameter and [ x ] = max ( x , 0 ) .", "Comments": [], "label": []}
{"id": 158798, "text": "We use TABERT 's source code from their GitHub repository .", "Comments": [], "label": []}
{"id": 151613, "text": "For fine-tuning experiments and evaluation of models in the pretraining experiments , we use tasks from GLUE benchmark [ Wang et al. , , 2018 ] and a questionanswering dataset SQuAD v1.1 [ Rajpurkar et al. , , 2016 ] .", "Comments": [], "label": []}
{"id": 152801, "text": "We followed the hyperparameters used in TAPT except for batch size and the maximum sequence length because we used the same computing resource as DoKTra for a fair comparison .", "Comments": [], "label": []}
{"id": 155422, "text": "4.4.3 Variants of Distilled Routers In Table [ 6 , ] in addition to word embedding , we also investigate four variants of the distilled router including CNN and three Transformers with different numbers of layers .", "Comments": [], "label": []}
{"id": 152770, "text": "To retain as much input information as possible , a re-ranking stage is carried out over these predictions based on the ROUGE-1 score between the model inputs and model outputs .", "Comments": [], "label": []}
{"id": 159915, "text": "Versatility One of the most impressive qualities of Transformers is their versatility : Not only are they now the standard architecture for all NLP tasks , but over the years they have also become ubiquitous in a wide range of applications domains outside of NLP .", "Comments": [], "label": []}
{"id": 152326, "text": "Each review text consists of l tokens and htext ∈ R ( N−1 ) ×lD×e . 4.2 Image Encoder We use a convolutional neural network specialized in analyzing visual imagery .", "Comments": [], "label": []}
{"id": 155951, "text": "The evaluation of the data presented in Table [ 3 ] leads to the unexpected conclusion that our Blockwise Transformer baseline , despite its simplicity , is sufficient to outperform deeper , denser , and additionally pretrained models that were recently reported as stateof-the-art .", "Comments": [], "label": []}
{"id": 156642, "text": "COMET-QE , however , has less errors .", "Comments": [], "label": []}
{"id": 157126, "text": "For simplicity , all the label words we use consist of a single token , so we can easily get the probability of each label .", "Comments": [], "label": []}
{"id": 155554, "text": "Specifically , we adopt codebook-based quantization to compress embeddings while explicitly decoupling the ranking contributions of document specific and document-independent information in contextual embeddings .", "Comments": [], "label": []}
{"id": 154058, "text": "This is likely because , without the data table as input , OCR-T5 model often fails to generate factually correct statements from the OCR text .", "Comments": [], "label": []}
{"id": 152487, "text": "https : //www.clips.uantwerpen.be/conll2002/ner/ https : //www.clips.uantwerpen.be/conll2003/ner/ https : //github.com/huggingface Table 2 : Results of our approach and prior state-of-the-art methods for zero-shot cross-lingual NER .", "Comments": [], "label": []}
{"id": 157502, "text": "To justify it , we compared KPCE with another variant , namely KPCE LDA , where the topics are the keywords obtained by running Latent Dirichlet Allocation ( LDA ) [ Blei et al. , , 2001 ] over all entities ' abstracts .", "Comments": [], "label": []}
{"id": 157419, "text": "Similarly , SPARTA ( T5 ) also outperforms SPARTA ( BART ) and SPARTA ( PEGASUS ) .", "Comments": [], "label": []}
{"id": 160359, "text": "Overall the best Slavic-to-English translation is achieved by XM Transformer with GShard trained in multilingual setting .", "Comments": [], "label": []}
{"id": 152887, "text": "Table 7 : ROUGE results for zero-shot cross-lingual abstractive summarization .", "Comments": [], "label": []}
{"id": 152222, "text": "These high results and the fact that * WVs * work better in general than * Sent * may be indicators that Transformers are properly incorporating contextual knowledge .", "Comments": [], "label": []}
{"id": 151979, "text": "In G-Transformer , we use the group-tag sequence G and G for representing the alignment between X and Y , and for generating the * localized * contextual representation of X and Y .", "Comments": [], "label": []}
{"id": 154081, "text": "Among all claims under the top-5 matched grants , we then find the most similar one to the focal claim using sentence-transformer [ Reimers and ] [ Gurevych , 2019 ] with stsb-roberta-large pre-trained bi-encoder model .", "Comments": [], "label": []}
{"id": 156645, "text": "Note that since COMET-QE is the state-of-theart quality estimator , it is a very strong baseline for the reranking stage where the goal is to find a better translation .", "Comments": [], "label": []}
{"id": 158020, "text": "Limitations GPU resources .", "Comments": [], "label": []}
{"id": 158814, "text": "For BB2 , there are no direct knowledge responses as this is implicit in FiD , so in that case we use a similar method to [ Han ] [ cock et al. , 2019 ] whereby we train in a supervised fashion with the knowledge response as a target , but add special tokens to both input and target to indicate this is not a standard dialogue response task .", "Comments": [], "label": []}
{"id": 151943, "text": "Fine-tuning Following [ Hu et al. , 2020b ] , we adopt the zero-shot transfer setting for evaluation , where the models are only fine-tuned on English training data but evaluated on all target languages .", "Comments": [], "label": []}
{"id": 155119, "text": "We use a pre-trained Wikipedia2Vec [ Yamada et al. , , 2020 ] model to pre-encode textual news to capture the rich knowledge present within the Wikipedia knowledge base ( see Table [ 1 ] and Appendix [ A.1 ] for more details on datasets ) .", "Comments": [], "label": []}
{"id": 153271, "text": "Following previous work , we share the vocabulary for the En-De task and segment words into subwords using byte pair encoding ( BPE ) [ Sennrich et al. , , 2016 ] with 32k merge operations for both datasets . 4.2 Implementation Details Training .", "Comments": [], "label": []}
{"id": 158269, "text": "This is particularly relevant in limited-resource scenarios where NATs are not a viable option and to speed up any transformer model , especially fine-grained or character-level models [ Edman et al. , , 2023 ] .", "Comments": [], "label": []}
{"id": 158957, "text": "And U = { u1 , . . . , uL } are the representations for all utterances . 4.2 Static Graph Construction In this section , we first propose 4 heuristic dialogue structure modeling methods to build the relationships between utterances using a graph network . * 1 .", "Comments": [], "label": []}
{"id": 158536, "text": "Although Multi-DYLE includes its evidence extractor , we used a separate EE model to address the E3 task .", "Comments": [], "label": []}
{"id": 155215, "text": "When training on CLUES-Real , we use 256 tokens as limit for RoBERTa w/ Exp . using explanations as the real-world tasks have roughly two times more explanations on average than synthetic tasks .", "Comments": [], "label": []}
{"id": 154696, "text": "The choice of this transformer-based architecture for morphology encoding is motivated by two factors .", "Comments": [], "label": []}
{"id": 151925, "text": "The guidelines are included in the appendix . 8 https : //appen.com/ to ensure annotation quality , following [ Gretz et al. , 2020 ] and [ Bar-Haim et al. , 2020b ] .", "Comments": [], "label": []}
{"id": 152592, "text": "We use classifier cascades : we first use a less accurate classifier for prediction , abstain on examples with low confidence , then send them to more accurate but more costly classifiers .", "Comments": [], "label": []}
{"id": 155509, "text": "For both LSTM and Transformer encoders , the number of layers is set to 3 , and the number of parameters is configured to be the same ( 6.9M parameters ) to enable a fair comparison between architectures ( for further details , see Appendix [ B ] .", "Comments": [], "label": []}
{"id": 158473, "text": "In our experiments , we use 3 as the threshold , which is tuned based on the tolerance for CACC drop .", "Comments": [], "label": []}
{"id": 152401, "text": "4.3 Impact of the Framework Components We investigate the impact of the components of our framework including : ( 1 ) task conditional adapter blocks ; ( 2 ) task conditional layer normalization ; ( 3 ) task projection network ; ( 4 ) fine-tuning of layer normalizations in the T5 model ; ( 5 ) task conditional layer normalization in adapter modules and fine-tuning of layer normalizations inside the T5 model .", "Comments": [], "label": []}
{"id": 152729, "text": "With a proper T ( i.e. , T = 0.5 ) during pseudo label generation , the resulting student model slightly outperforms the baseline student model with regular pseudo labeling method on ROUGE-2/L ( see Table [ 5 ] , but worse than PLATEλ=2.0 .", "Comments": [], "label": []}
{"id": 159955, "text": "Cascaded ( ASR→MT→TTS ) We combine a Conformer ASR , a Transformer MT , and a Transformer TTS model .", "Comments": [], "label": []}
{"id": 158728, "text": "The loss function of image-context matching is defined as : In addition , we use the CAPTION expert learned in Stage I as a teacher to facilitate the learning of CONTEXT expert .", "Comments": [], "label": []}
{"id": 153364, "text": "For each parallel sentence pair * * e * * and * * f * * , we use the sentence * * f * * itself as the reference * * r * * .", "Comments": [], "label": []}
{"id": 159789, "text": "Under this refined evaluation , we find that : ( 1 ) * Few-shot * prompting with GPT-3 achieves near SOTA performance , i.e. , roughly equivalent to existing * fully supervised * models ; ( 2 ) Flan-T5 is not as capable in the fewshot setting , but supervising and fine-tuning it with Chain-of-Thought ( CoT ) style explanations ( generated via GPT-3 ) yields SOTA results .", "Comments": [], "label": []}
{"id": 156290, "text": "2.1 Unsupervised metrics To address the limitations of ROUGE and BLEU , variants based on static word embeddings [ Mikolov et al. , 2013 ] were developed , e.g. , ROUGE-WE [ Ng and Abrecht , 2015 ] , BLEU2VEC [ Tättar and ] [ Fishel , 2017 ] , and MEANT 2.0 [ Lo , 2017 ] .", "Comments": [], "label": []}
{"id": 154739, "text": "To sum up , our contributions are the following : 2 Background 2.1 Transformer A transformer [ Vaswani et al. , , 2017 ] is composed of several layers , which encompass a multi-head self-attention layer followed by a feed-forward layer , along with residual connections [ He et al. , , 2016 ] and layer normalization [ Ba et al. , , 2016 ] .", "Comments": [], "label": []}
{"id": 155019, "text": "While there is still a noticeable performance gap between Transformer and mGPT with MSP , using mGPT as a translator with MSP is much more parameter-efficient than training a separate NMT model .", "Comments": [], "label": []}
{"id": 157808, "text": "We use the terms discriminative and extractive interchangeably when referring to these methods .", "Comments": [], "label": []}
{"id": 154190, "text": "[ Zhang et al. , 2020 ] released numbert , a Transformer-based model that handles numerical reasoning tasks by representing numbers by their scientific notation and applying subword tokenization .", "Comments": [], "label": []}
{"id": 153374, "text": "We list the BLEU scores of the various systems in Appendix [ C. ] We use the OIE4 training corpus from [ Kolluru et al. , 2020b ] and transfer it to other languages for training OpenIE systems .", "Comments": [], "label": []}
{"id": 158037, "text": "We use T5-Large for RL training on top of mined intent ( RL-ToM-Mined ) and generated intent ( RL-ToM-Gen. ) models .", "Comments": [], "label": []}
{"id": 158885, "text": "meta-function pre-training ; ( 2 ) MetaNER w/o LM : remove pseudo extraction LM pre-training ; ( 3 ) MetaNER w/o anonymization : we use the original entity type names in both pre-training and incontext NER , without using type anonymization .", "Comments": [], "label": []}
{"id": 151591, "text": "[ https : //github.com/pytorch/fairseq/ ] ( https : //github.com/pytorch/fairseq/tree/master/examples/roberta ) [ tree/master/examples/roberta ] ( https : //github.com/pytorch/fairseq/tree/master/examples/roberta ) [ https : //github.com/pytorch/fairseq/ ] ( https : //github.com/pytorch/fairseq/blob/master/examples/roberta/README.glue.md ) [ blob/master/examples/roberta/README.glue .", "Comments": [], "label": []}
{"id": 153244, "text": "To overcome this difficulty , we adopt negative sampling techniques [ Mikolov et al. , , 2013 ] .", "Comments": [], "label": []}
{"id": 160274, "text": "Ablations on independent effects of lambda values , where the default lambdas are : λ = 1 , λ = 5 , and λ = 5 .", "Comments": [], "label": []}
{"id": 159320, "text": "GHT and GHT-PS achieve significant improvements over the strong baselines in Machine Translation ( MT ) BLEU scores ( +3.8 % and +4.4 % averaged on 7 datasets ) , Language Modeling ( LM ) perplexity ( -2.8 % and -2.9 % ) , and Abstractive summarization ( AS ) F1-Rouge ( +6.7 % and +7.0 % on average ) .", "Comments": [], "label": []}
{"id": 160190, "text": "Next , we look at the performance of the zero-shot PROMPTRANK ( T5-XL ) which uses no instructions , i.e. , the prompt consists of only the document path .", "Comments": [], "label": []}
{"id": 155578, "text": "Prior work on curriculum learning ( CL ) does not agree on standard measures of sample difficulty for seq2seq tasks [ Kumar et al. , , 2019 ] [ Yao et al. , , 2021 ] [ Zhang et al. , , 2018 ] [ Zhou et al. , , 2020 ] or apply CL for the different problem of shifting the training of a Transformer model from AR to NAR regimes [ Guo et al. , , 2020 ] [ Liu et al .", "Comments": [], "label": []}
{"id": 154303, "text": "Inference Hyper-parameters : We used nucleus sampling with p ∈ [ 0.6 , 0.9 ] .", "Comments": [], "label": []}
{"id": 153231, "text": "As more transformer layers are stacked with larger self-attention blocks , the complexity of PLMs increases rapidly .", "Comments": [], "label": []}
{"id": 151828, "text": "Assuming that GT domain labels are unavailable , we use mATM to infer the clustering assignment to guide the learning of EnsLM , which obtains the SOTA performance on all three basic models , even better than the models using GT domain label .", "Comments": [], "label": []}
{"id": 152518, "text": "We demonstrate our objective 's effectiveness by using it to extend the pretraining of a transformer-based language model and obtain state-of-the-art results on SentEval [ Conneau and Kiela , 2018 ] – a benchmark of 28 tasks designed to evaluate universal sentence embeddings .", "Comments": [], "label": []}
{"id": 157545, "text": "On T5 , incorporating finegrained computation steps does not improve the OOD performance as significantly as on GPT3 ( Figure [ 9 ] f ) ) .", "Comments": [], "label": []}
{"id": 159958, "text": "Second , we replace LSTM decoders with Transformer decoders .", "Comments": [], "label": []}
{"id": 155095, "text": "Following CodeXGLUE 's design , we use MAP @ R as the metric for POJ-104 and precision/recall/F1 as the metric for BigCloneBench .", "Comments": [], "label": []}
{"id": 154614, "text": "Comparing different context lengths As shown in [ Table 3 ] changing the context length of a Transformer can impact the performance in our task .", "Comments": [], "label": []}
{"id": 152825, "text": "Fluency is very high from RGF , as the generation step leverages a high-quality pretrained langauge model ( T5 ) .", "Comments": [], "label": []}
{"id": 153596, "text": "For the diversity in feature word level , we adopt Feature Diversity ( DIV ) proposed in [ Li et al. , 2020 ] , which measures the intersection of features between any two generated explanations .", "Comments": [], "label": []}
{"id": 154646, "text": "C Semantic Formalism C.1 Relation Semantics We use deterministic heuristics to compute the semantics of the following six relations : * left * , * right * , * above * , * below * , * bigger * , and * smaller * .", "Comments": [], "label": []}
{"id": 156587, "text": "For QA tasks , we use accuracy .", "Comments": [], "label": []}
{"id": 152523, "text": "We implement the MLM objective as described in [ Devlin et al. , , 2019 ] on each anchor span in a minibatch and sum the losses from the MLM and contrastive objectives before backpropagating This is similar to existing pretraining strategies , where an MLM loss is paired with a sentence-level loss such as NSP [ Devlin et al. , , 2019 ] or SOP [ Lan et al. , 2020 ] .", "Comments": [], "label": []}
{"id": 159348, "text": "We take vanilla transformer as the backbone .", "Comments": [], "label": []}
{"id": 156425, "text": "We choose RoBERTa base [ Liu et al. , 2019 ] as our backbone model and use BLEU , ROUGE , Distinct , and Lexical Repetition metrics for evaluation .", "Comments": [], "label": []}
{"id": 158751, "text": "To transfer knowledge from the pre-trained MT decoder to the shared decoder , we employ the knowledge distillation ( KD ) method presented in ( Liu et al. , 2019 ) and define the KD loss LKD as follows : mathbf { p } ( mathbf { y } _n = k | mathbf { y } _ { |C| is the vocabulary size of the output target text .", "Comments": [], "label": []}
{"id": 155612, "text": "Word-Drop We implement word-dropout [ Gal ] [ and Ghahramani , 2016 ] by randomly replace word embeddings with zero vectors with α = 0.05 in Eq . [ 3 ] .", "Comments": [], "label": []}
{"id": 157891, "text": "The heuristic constraints are defined based on our observation and summary of the dialogue flow patterns from the training set of PersonaChat/WoW .", "Comments": [], "label": []}
{"id": 157212, "text": "In order to capture the structure of G with the transformer , our model makes use of two types of information within the attention modules of each layer of the decoder .", "Comments": [], "label": []}
{"id": 152148, "text": "To tackle the challenges , we introduce Heterogeneous Graph-based Interaction Model with a Tracker ( GIT ) .", "Comments": [], "label": []}
{"id": 154382, "text": "ℓ Besides , we use an additional distillation loss Ldist over the logits .", "Comments": [], "label": []}
{"id": 157922, "text": "If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE , etc. ) ? * 5.4 * * Our annotations are few and simple .", "Comments": [], "label": []}
{"id": 158099, "text": "Specifically , we use a * Transformer-big * size fully NAT [ Gu and Kong , 2021 ] as the base model .", "Comments": [], "label": []}
{"id": 153751, "text": "Transformers are used as base encoders in MM-Deacon to encode SMILES and IUPAC , and embeddings from encoders are projected to a joint embedding space .", "Comments": [], "label": []}
{"id": 153471, "text": "Since using coarse-grained image features may be insufficient to model fine-grained visual elements in images including the specific locations , objects , and facial expressions , we use a bag-ofobjects representation where the objects are obtained using an off-shelf Faster R-CNNs [ Ren et al. , , 2015 ] pre-trained on Visual Genome [ Krishna et al. , , 2017 ] .", "Comments": [], "label": []}
{"id": 154136, "text": "We use a batch size of 1024 for all experiments , and change the number of negatives with a binary mask over the softmax logits .", "Comments": [], "label": []}
{"id": 156609, "text": "Overall , we show that : 2 Background and Setting In this section , we describe the framework and data we use for evaluation of hallucination detection and mitigation methods .", "Comments": [], "label": []}
{"id": 157938, "text": "We use the WikiANN dataset [ Pan et al. , , 2017 ] for NER and version v2.11 of Universal Dependencies ( UD ) [ de Marneffe et al. , , 2021 ] for POS .", "Comments": [], "label": []}
{"id": 151899, "text": "We [ https : //github.com/huggingface/ ] ( https : //github.com/huggingface/transformers ) [ transformers ] ( https : //github.com/huggingface/transformers ) < https : //github.com/thunlp/DocRED > [ https : //competitions.codalab.org/ ] ( https : //competitions.codalab.org/competitions/20717 ) [ competitions/20717 ] ( https : //competitions.codalab.org/competitions/20717 ) [ https : //github.com/thunlp/ ] ( https : //github.com/thunlp/RE-Context-or-Names ) [ RE-Context-or-Names ] ( https : //github.com/thunlp/RE-Context-or-Names ) < https : //github.com/thunlp/ERNIE > Table 9 : Results on dev sets of GLUE Benchmark .", "Comments": [], "label": []}
{"id": 160325, "text": "Based on the c , we further propose the definition of toxic embedding t of x : Since the element-wise addition of multiple linear vector representations is an efficient way to fully incorporate various information [ Mikolov et al. , 2013 ] , we utilize this method to integrate toxic and word embedding .", "Comments": [], "label": []}
{"id": 153976, "text": "We use their model which uses kernel graph attention network ( KGAT ) [ Liu et al. , , 2020 ] and fine-tune it on Colloquial claims .", "Comments": [], "label": []}
{"id": 158717, "text": "Particularly , we fine-tune a multilingual transformer model BERT ( mBERT ) [ Devlin et al. , , 2019 ] .", "Comments": [], "label": []}
{"id": 156129, "text": "We use the English portion of the parallel universal dependencies treebank ( ud-pud , [ Nivre et al. , , 2016 ] .", "Comments": [], "label": []}
{"id": 156935, "text": "We implemented the first seven convolutional layers to be the same as WavLM , and added another convolutional layer with 512 channels , 5 strides and 5 kernels size , in order to shorten the length of the output speech features .", "Comments": [], "label": []}
{"id": 155290, "text": "We use the English RE-OIE2016 [ Zhan and ] [ Zhao , 2020 ] training dataset used in [ Ro et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 152995, "text": "Sparsity [ % ] Sparsity [ % ] Sparsity [ % ] Sparsity [ % ] Figure 4 : Accuracy on XNLI with translate-train-all setting and dynamic sparsification , the number of parameters ( Params ) , CPU and GPU throughput ( the number of sentences per second ) vs. the sparsity .", "Comments": [], "label": []}
{"id": 153237, "text": "Therefore , we speculate that the sparsity pattern of robust tickets is task-dependent . 5.3 Speedup Training Process An important property of winning tickets is to accelerate the convergence of the training process [ Chen et al. , , 2021 ] [ You et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 154129, "text": "For textual descriptions , we use the data provided by KG-BERT [ Yao et al. , , 2019 ] for WN18RR and FB15k-237 datasets .", "Comments": [], "label": []}
{"id": 152379, "text": "We release our code to facilitate future work . 2 HYPERFORMER In this section , we present our HYPERFORMER model , which integrates hypernetwork-based adapter layers into a multi-task transformer model .", "Comments": [], "label": []}
{"id": 153362, "text": "To generate a translation * * f * * ( language * F * ) of text * * e * * ( language * E * ) , consistent with a reference * * r * * ( language * F * ) , we use the following procedure .", "Comments": [], "label": []}
{"id": 156421, "text": "Model Inference We use an iterative refinement strategy to generate text like CMLM [ Ghazvinine ] [ jad et al. , , 2019 ] .", "Comments": [], "label": []}
{"id": 155232, "text": "Especially with the recent adoption of * transformer * architecture in both NLP and computer vision [ Carion et al. , , 2020 ] [ Chen et al. , , 2020 ] , potential future work includes extending SHIELD to patch other complex NN models ( e.g. , T5 [ Raf ] [ fel et al. , , 2020 ] ) or other tasks and domains such as Q & A and language generation .", "Comments": [], "label": []}
{"id": 151563, "text": "Acknowledgements The authors would like to thank Ryan Qiu for help with analysis , and the Amazon Mechanical Turk community for help with annotation .", "Comments": [], "label": []}
{"id": 156634, "text": "5.3.1 Automatic Evaluation Figure [ 5 ] shows that , regardless of the generation method , LaBSE is the best reranker and it performs notably better than the strong COMET-QE baseline .", "Comments": [], "label": []}
{"id": 157930, "text": "We employ filters CF1 and CF2 . * * CF1 * * In case of * * mismatch between language and script * * , the corpus is removed ; e.g. , Chinese written in Arabic is unlikely to be Chinese . * * CF2 * * Perplexity mismatch .", "Comments": [], "label": []}
{"id": 153105, "text": "First , we leveraged the checkpoint `` * sentencetransformers/allenai-specter * '' [ Cohan et al. , , 2020 ] , which is a scientific BERT-based model trained to create document embeddings by using paper citations .", "Comments": [], "label": []}
{"id": 153826, "text": "Our results show that , compared to a baseline Transformer , significant gains in compositional generalization can be achieved .", "Comments": [], "label": []}
{"id": 153974, "text": "At test time , we use the evidence sentences in the top k rank with a score of more than 0 .", "Comments": [], "label": []}
{"id": 155379, "text": "MoE-based Transformers have a set of expert modules , and only a few experts will be activated for each input token .", "Comments": [], "label": []}
{"id": 159708, "text": "We use the English training data for training and evaluate on the test sets of the target languages .", "Comments": [], "label": []}
{"id": 154424, "text": "The dimension P of hidden states z is set to 64 and we will analyze the effect of P in § [ 3.4.1 . ] We use Adam optimizer [ Kingma and Ba , 2014 ] with a learning rate of 3 × 10− for pre-training .", "Comments": [], "label": []}
{"id": 155416, "text": "By default , the MoE layer stacks 3 MoE sublayers and is inserted after the 2 -th Transformer block ( middle ) .", "Comments": [], "label": []}
{"id": 156552, "text": "We use the train split with 10K videos for training , and we report results on the widely used val1 split , with 4.9K videos .", "Comments": [], "label": []}
{"id": 160153, "text": "The maximum learning rate < http : //opus.nlpl.eu/opus-100.php > < https : //github.com/google/sentencepiece > [ https : //dl.fbaipublicfiles.com/hubert/hubert_ ] ( https : //dl.fbaipublicfiles.com/hubert/hubert_base_ls960.pt ) [ base_ls960.pt ] ( https : //dl.fbaipublicfiles.com/hubert/hubert_base_ls960.pt ) Table 1 : BLEU scores on MuST-C tst-COMMON set .", "Comments": [], "label": []}
{"id": 152180, "text": "We use * transformed nearest neighbor ( NN ) * to denote the the NN of a source KG entity after it is transformed to the target embedding space .", "Comments": [], "label": []}
{"id": 156024, "text": "Concluding , our paper provides a novel , theoretically justified technique for sequence length reduction , achieving a speedup and memory reduction for both training and inference of transformers , while suffering significantly less in terms of predictive performance when compared to other existing techniques .", "Comments": [], "label": []}
{"id": 159862, "text": "Both the label name and related words are phrases Since the search space of a related phrase is exponentially large in its length , we use another prompt to filter candidate words .", "Comments": [], "label": []}
{"id": 156041, "text": "Table 4 : LRA * test * set performances at 70 % space complexity reduction for * Big Bird * ( top ) and * Performers * ( bottom ) as the backbone Transformer .", "Comments": [], "label": []}
{"id": 157956, "text": "Dash line indicates data is not available or not meaningful .", "Comments": [], "label": []}
{"id": 152613, "text": "Table 4 : Model Performance on OOD evaluation sets HANS and PAWS for MNLI and QQP respectively We use the same model checkpoint as the one presented in Table [ 1 ] and compare against DistilRoBERTa .", "Comments": [], "label": []}
{"id": 154301, "text": "Since the distribution of the labels may be different , we report the F1 score on our constraints . 4.1 Model Details For the encoders , we use a one-layer LSTM network with 300 hidden dimensions for all the datasets .", "Comments": [], "label": []}
{"id": 157437, "text": "SPARTA ( T5 ) : what did the bus driver say ?", "Comments": [], "label": []}
{"id": 157057, "text": "1 Introduction Transformer-based question answering ( QA ) methods have evolved rapidly in recent years to handle open-domain , multi-hop reasoning over retrieved context paragraphs .", "Comments": [], "label": []}
{"id": 151856, "text": "BERT is a multi-layer Transformer [ Vaswani et al. , , 2017 ] network , which is pre-trained in a self-supervised manner on a large corpus .", "Comments": [], "label": []}
{"id": 152262, "text": "For heuristic group 4 ( `` End with non-positive responses '' ) , we assign disengaged labels only if the disengaged intents are detected in the last segment .", "Comments": [], "label": []}
{"id": 154521, "text": "The best results have been achieved with the inclusion of the coverage term , with an improvement of +0.36 ROUGE-2 pp over the NLL baseline and a marked improvement of +0.92 ME-TEOR pp .", "Comments": [], "label": []}
{"id": 155829, "text": "P denotes soft prompt . hXi means the mask of typical encoder-decoder model like T5 and CPM-2 . prompts .", "Comments": [], "label": []}
{"id": 157855, "text": "Central Events Prediction and EIG Enhancement Once obtained the centrality-aware event embeddings , we use them to predict whether an event is a central event : pe = f ( ceiWc ) , where f denotes the sigmoid function , W ∈ R d×1 is the parameter weight matrix .", "Comments": [], "label": []}
{"id": 156451, "text": "We adopted either of two methods of norm constraint .", "Comments": [], "label": []}
{"id": 153959, "text": "4 Dataset Construction and Annotation We use two approaches to create claim responses for DIALFACT : 1 ) Automatically generated claims , and 2 ) Human written claims to emulates claims created by dialogue systems and humans respectively .", "Comments": [], "label": []}
{"id": 158802, "text": "We use the same λ for every constraint penalty term .", "Comments": [], "label": []}
{"id": 158956, "text": "An overview is shown in Figure [ 2 . ] 4.1 Utterance Encoder We employ the pre-trained BART [ Lewis et al. , , 2020 ] to encode each utterance independently : where Enc ( · ) is the encoder module in BART which outputs the vector representation hi , j of j-th input word wi , j in i-th utterance .", "Comments": [], "label": []}
{"id": 153143, "text": "For both pretraining and finetuning , we used Adam optimizer with an initial LR of 1e-3 .", "Comments": [], "label": []}
{"id": 159947, "text": "In this paper , we use a more general definition that the entire architecture is optimized jointly and the translation is conducted in a more direct way .", "Comments": [], "label": []}
{"id": 154223, "text": "We use unique initial input values for all numbers and variables .", "Comments": [], "label": []}
{"id": 157823, "text": "AG-DST does not condition generation on the schema and slot semantics are learned implic- [ https : //huggingface.co/datasets/kilt_ ] ( https : //huggingface.co/datasets/kilt_wikipedia ) [ wikipedia ] ( https : //huggingface.co/datasets/kilt_wikipedia ) Table 1 : Results on the SGD test set .", "Comments": [], "label": []}
{"id": 156563, "text": "On the other hand , after adding the 2-layer temporal encoder , the 4 frame SINGULARITY-temporal model gets a significant performance boost from the single-frame model , surpassing the baseline methods .", "Comments": [], "label": []}
{"id": 156741, "text": "Finally , we use this relational representation to define compositionality of interpretation functions : Definition 4 .", "Comments": [], "label": []}
{"id": 154155, "text": "The hyper-parameter sensitivity for the AGG are given in Appendix [ D. ] We use three quantitative metrics to evaluate our method : Perplexity , Uniq , and I ( W ) .", "Comments": [], "label": []}
{"id": 154830, "text": "The results of TRANSFORMER and HATT are taken from their original paper respectively . in hyperbolic space , leaving the remaining computational blocks in the Euclidean space .", "Comments": [], "label": []}
{"id": 154342, "text": "[ 2020 ] , T5-small and T5-base ( Raff [ el et al. , , 2020 ] that are general domain generation PLMs , and SciFive-base & SciFive-large [ Phan et al. , , 2021 ] that are pre-trained on large biomedical corpora .", "Comments": [], "label": []}
{"id": 153578, "text": "FFN is a feedforward module consisting of two fully-connected layers with ReLU in between for the l-th Transformer layer .", "Comments": [], "label": []}
{"id": 154142, "text": "For re-ranking , we use 5-hop neighbors for WN18RR and 2-hop neighbors for other datasets .", "Comments": [], "label": []}
{"id": 155745, "text": "G.4 ProtoQA Evaluation In our experiments , we use the scraped development set as our validation set and the crowdsourced development set as our test set .", "Comments": [], "label": []}
{"id": 153067, "text": "We use stochastic gradient descent with Nesterov momentum for optimization and set the initial learning rate to 0.1 , momentum to 0.9 .", "Comments": [], "label": []}
{"id": 155219, "text": "Weights link : [ https : //huggingface.co/ ] ( https : //huggingface.co/textattack/roberta-base-MNLI ) [ textattack/roberta-base-MNLI ] ( https : //huggingface.co/textattack/roberta-base-MNLI ) < https : //pytorch.org/ > Table 7 : List of datasets and URLs that make up CLUES-Real .", "Comments": [], "label": []}
{"id": 159711, "text": "Specifically , to reduce the impact from the prompt , we use calibration [ Zhao et al. , , 2021 ] to rescale the scores before making the final prediction .", "Comments": [], "label": []}
{"id": 153805, "text": "In this paper , we use parameters much smaller than that , as we are evaluating the architectural decisions on relatively small datasets . 3 Evaluation Datasets We use a collection of 12 datasets that require different types of compositional generalization .", "Comments": [], "label": []}
{"id": 159930, "text": "They achieved paralleled performance to the state-ofthe-art at the time , whilst being more parameter efficient . [ Karimi Mahabadi et al. , 2021 ] applied hypernetworks to Transformers to allow for parameter sharing in multitask learning .", "Comments": [], "label": []}
{"id": 153103, "text": "Better ROUGE scores are bolded .", "Comments": [], "label": []}
{"id": 153156, "text": "To be consistent with the text representation , we adopt a linear transformation layer to project visual features to d-dimensional vectors , denoted by V ∈ R d×36 .", "Comments": [], "label": []}
{"id": 151448, "text": "For persona consistency , we employ two metrics .", "Comments": [], "label": []}
{"id": 155686, "text": "For each document X ( i ) , we use the common TF-IDF cosine similarity metric [ Chen et al. , , 2017 ] [ Yasunaga et al. , , 2017 ] to obtain top-k documents X ( j ) 's and make edges ( X ( i ) , X ( j ) ) .", "Comments": [], "label": []}
{"id": 153125, "text": "For ST-GCN and SL-GCN , we use a batch size of 32 and LR of 0.001 .", "Comments": [], "label": []}
{"id": 155876, "text": "We use equal weights for prediction loss and summarization loss .", "Comments": [], "label": []}
{"id": 156698, "text": "Given a model , we use the question pair ( q , q ′ ) to obtain a pair of answer distributions P ( R|t , n ) and P ( R|t ′ , n ′ ) , which we use to measure the causal effect of the intervention .", "Comments": [], "label": []}
{"id": 159754, "text": "6.3 Analysis on Relation Embeddings As for TeAST , we employ the Archimedean spiral to map relations into the polar coordinate system .", "Comments": [], "label": []}
{"id": 158741, "text": "For MMConv , we adopt the strong baseline SimpleTOD [ Hosseini- ] [ Asl et al. , , 2020 ] implemented by [ Liao et al. , , 2021 ] .", "Comments": [], "label": []}
{"id": 155901, "text": "Word embeddings For the English experiments , we use Word2Vec [ Mikolov et al. , , 2013 ] embeddings [ 3 ] .", "Comments": [], "label": []}
{"id": 151502, "text": "Since no public implementation is available , we implement our own .", "Comments": [], "label": []}
{"id": 156512, "text": "These are two Bert-like bidirectional transformer models [ Devlin et al. , , 2018 ] each responsible for learning embedding representations of their respective inputs .", "Comments": [], "label": []}
{"id": 158526, "text": "Under this conversion process ψ , we compute ROUGE scores by matching an output sequence resulting from the application of Multi-DYLE and the baseline EE model with its corresponding groundtruth sequence .", "Comments": [], "label": []}
{"id": 158719, "text": "A recent popular trend on textual tasks is to build unified pre-trained foundation models by multi-task learning , e.g. , T5 [ Raffel et al. , 2020 ] .", "Comments": [], "label": []}
{"id": 159492, "text": "( 2 ) Content Preservation : We use BLEU-n ( n=1,2 ) [ Pa ] [ pineni et al. , , 2002 ] and BERTScore ( BS ) [ Zhang * et al. , 2020 ] between generated and input texts to measure their lexical and semantic similarity , respectively .", "Comments": [], "label": []}
{"id": 153107, "text": "Second , we used two different checkpoints with a specific DPR training , such as `` * facebook/dpr-question_encodersingle-nq-base * '' for encoding the background and `` * facebook/dpr-ctx_encoder-single-nq-base * '' for encoding each document in the cluster .", "Comments": [], "label": []}
{"id": 157292, "text": "We refer to these Our code implementation is publicly available at < https : //github.com/maastrichtlawtech/VendorLink.git > Figure 1 : ( i ) Closed-Set Vendor Verification Task : A supervised pre-training task that performs classification using a BERT-cased classifier in a closed-set environment to verify unique vendor migrants across existing markets , ( ii ) Open-Set Vendor Identification Task : A text-similarity task in an open-set environment that utilizes style representations from the established BERT-cased classifier to verify known vendors and identify potential-aliases , ( iii ) Low-Resource Market Adaptation Task : A knowledge-transfer task in a closed-set environment to adapt new market knowledge and verify migrants across Low-Resource ( LR ) emerging markets .", "Comments": [], "label": []}
{"id": 157510, "text": "We further do a preliminary experiment where a T5 model is fine-tuned using the data containing repeating numbers of up to 80 digits , T5 still can not achieve 100 % in-distribution accuracy on long repeating digits .", "Comments": [], "label": []}
{"id": 156886, "text": "Then , a sophisticated verifier based on T5-3B FiD verifies each answer with an aggregation module .", "Comments": [], "label": []}
{"id": 153313, "text": "Learning BPE vocabularies jointly vs. separately From Table [ 7 , ] we see that there is no significant impact on BLEU if we learn BPE vocabu- Figure 2 : Ablation over number of distinct keys ( left ) and weight β of agreement loss ( right ) .", "Comments": [], "label": []}
{"id": 154746, "text": "It corresponds to O ( L × ( L + LSTM ) + L × N ) when also using a short-term memory and O ( L + L × N ) when only using the LTM , both O ( L × ( L + LLTM ) ) , which would be the complexity of a vanilla transformer attending to the same context .", "Comments": [], "label": []}
{"id": 151425, "text": "We use S as the reference and T as the hypothesis . [ ♠ ⊕ † ] Embedding-based similarity .", "Comments": [], "label": []}
{"id": 160285, "text": "Graph Neural Networks Adapter To generalize transformers to capture the structural information of AMR graphs , we incorporate graph neural networks as adapter into transformer layers .", "Comments": [], "label": []}
{"id": 153713, "text": "Specifically , we use each knowledge statement to prompt the model , forming M knowledgeaugmented questions : where [ ·||· ] denotes text concatenation .", "Comments": [], "label": []}
{"id": 155620, "text": "We used the test samples as the unlabeled set for each dataset , which is referred to as transductive setting in few-shot classification [ Liu et al. , , 2019 ] . 4.2 Methods for Comparison In this section , we describe the various stop-criteria for comparison with our method .", "Comments": [], "label": []}
{"id": 152779, "text": "Specifically , we use the Adam optimizer and a pergpu batch size of 32 for NQ and 24 for WebQ , respectively .", "Comments": [], "label": []}
{"id": 156230, "text": "Table 1 : Results of mean ± standard deviation of five runs from our implementation of LexSubCon and Bertp , s [ Zhou et al. , , 2019 ] .", "Comments": [], "label": []}
{"id": 154422, "text": "Other word-overlap-based metrics , METEOR , ROUGE-L , and CIDEr , which are also reported for the DSTC7-AVSD dataset , same as DSTC7 reviews [ Alamri et al. , , 2019b ] .", "Comments": [], "label": []}
{"id": 155175, "text": "On the SPACE dataset , the gold summaries are abstractive , so we only calculate ROUGE scores . 5.2 Baselines & Competitor Models On the SPACE corpus , we primarily focus on comparisons to quantized transformer ( QT ) [ Angelidis et al. , 2021 ] and CTRLSUM [ He et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 160221, "text": "In other words , the path scores output by T5-XL does not appear to be sensitive to the document order prompt and can still .", "Comments": [], "label": []}
{"id": 151782, "text": "Our proposed generative task performs effectively for the lightweight dual-transformer framework while other generative tasks should be implemented via a large-capacity model .", "Comments": [], "label": []}
{"id": 152097, "text": "For the first challenge , GIT constructs a heterogeneous graph interaction network to capture global interactions among different sentences and entity mentions .", "Comments": [], "label": []}
{"id": 153177, "text": "An example of a re-annotation was removing `` I * would * kill you ! '' from the hedge classes . 4.2 Features Label from rule-based classifier ( Label RB ) : We use the class label predicted by the rule-based classifier described in Section [ 4.3 ] as a feature .", "Comments": [], "label": []}
{"id": 155858, "text": "The reason is that during optimization , PT only needs to update the prompt parameters , which means the momentum and gradients of other parameters are not required to be stored and transmitted to between different GPU devices .", "Comments": [], "label": []}
{"id": 154909, "text": "We used the sentiment analysis dataset of [ Am ] [ ram et al. , 2018 ] for training and evaluating AlephBERT on a sentence level task , and we follow their terms of use .", "Comments": [], "label": []}
{"id": 154602, "text": "We use the 1,155 personas crowdsourced from [ Zhang et al. , 2018 ] , validation and test use separate personas from the ones used in the training set .", "Comments": [], "label": []}
{"id": 159462, "text": "Architecture In our experiments , we use a deep encoder , shallow decoder architecture [ Kasai et al. , , 2021 ] with 16 encoder layers and 3 decoder layers .", "Comments": [], "label": []}
{"id": 155045, "text": "In Appendix , we also present the ablation study with OntoNote 5.0 . 5.4 Visualization of Token Embeddings Figure [ 4 ] shows the t-sne plots of hidden states of tokens from 10-shot * LOC * → * PER * ( explained in the caption ) .", "Comments": [], "label": []}
{"id": 159143, "text": "Indic-COMET-MQM outperforms both the COMET baselines on 3 out of the 5 languages and shows higher correlations than COMET-MQM across all languages .", "Comments": [], "label": []}
{"id": 158831, "text": "The transformer is then trained on our intention categories , and its outputs are fed into global max pooling .", "Comments": [], "label": []}
{"id": 158133, "text": "We implement them in our framework as comparison to our proposed MIR-GAN , where we employ their designed similarity cost functions on frame-level representations .", "Comments": [], "label": []}
{"id": 160405, "text": "[ 23 ] As baseline we use a src2src-baseline or identitybaseline , which As baseline we report the results of a src2src-baseline or identity-baseline in which the complex source sentences are just copied and , hence , the complex sentences are used as original and potential simplification data .", "Comments": [], "label": []}
{"id": 158928, "text": "CometKiwi was trained on human annotations from the MLQE-PE [ https : //huggingface.co/ ] ( https : //huggingface.co/sentence-transformers/LaBSE ) [ sentence-transformers/LaBSE ] ( https : //huggingface.co/sentence-transformers/LaBSE ) dataset [ Fomicheva et al. , , 2022 ] , which contains a high percentage of hallucinations for some language pairs [ Specia et al. , , 2021 ] [ Tang et al. , , 2022 ] .", "Comments": [], "label": []}
{"id": 151435, "text": "The company works with a large number of tutors and students ; we use data that represents 108 tutors and 1821 students . 70 % of tutors in the data are male , complementing the other datasets where the majority of teachers are female .", "Comments": [], "label": []}
{"id": 153942, "text": "During decoding , we use beam search with beam size of 5 and remove repeated trigrams .", "Comments": [], "label": []}
{"id": 154753, "text": "Both the transformer-XL and the compressive transformer require relative positional encodings .", "Comments": [], "label": []}
{"id": 152178, "text": "We use the full DBpedia ( Lehmann et al. , 2015 ) to build a new dataset and the key challenge lies in that we need to guarantee the selected dangling entities actually do not have counterparts .", "Comments": [], "label": []}
{"id": 155033, "text": "For learning prompts , we do not apply additional preprocessing on translation tasks except Romanian-English translation task , where we use a script [ 12 ] to remove diacritics in the Romanian side .", "Comments": [], "label": []}
{"id": 155223, "text": "To do this , we utilize the DARTS algorithm [ Liu et al. , , 2019a ] as follows .", "Comments": [], "label": []}
{"id": 157028, "text": "We use Adam [ Kingma and Ba , 2014 ] optimizer , in which β = 0.9 , β = 0.98 .", "Comments": [], "label": []}
{"id": 159498, "text": "As for content preservation , Style Transformer and Re- Table 6 : Human evaluation results on Chinese for transfer direction in LX and JY . κ denotes Fleiss ' kappa [ Fleiss , 1971 ] to measure the inter annotator agreement ( all are moderate or substantial ) .", "Comments": [], "label": []}
{"id": 154713, "text": "Even though our POS tagger uses heuristic methods and was evaluated mainly through qualitative exploration , we can still see its positive impact on the pre-trained language model .", "Comments": [], "label": []}
{"id": 152706, "text": "We use a batch size of around 80 documents ( we limit the max number of tokens on each GPU to 2048 ) and train our models for 20,000/15,000/6,000 steps with 500 warmup steps for CNNDM , XSum , and NYT , respectively .", "Comments": [], "label": []}
{"id": 157410, "text": "Then , we employ a pre-trained document-level co-reference resolution model , SpanBERT [ Joshi et al. , , 2020 ] , to cluster mentions in the long text which refer to the same real-world entities .", "Comments": [], "label": []}
{"id": 159129, "text": "COMET shows the highest correlations , followed by BLEURT-20 .", "Comments": [], "label": []}
{"id": 155376, "text": "In the second training stage , we utilize the distilled router to determine the token-to-expert assignment and freeze it for a stable routing strategy .", "Comments": [], "label": []}
{"id": 152751, "text": "They are trained for 50 epochs and we use the last checkpoint for all evaluations without model averaging and ensemble decoding .", "Comments": [], "label": []}
{"id": 159307, "text": "We use an image resolution of 384 × 384 for Flickr30K and 576×576 for VQAv2 for a fair comparison with BridgeTower .", "Comments": [], "label": []}
{"id": 157244, "text": "For all three models , we use the original BERT pre-training data [ Devlin et al. , , 2019 ] , containing a combination of Wikipedia [ 4 ] and BookCorpus [ Zhu et al. , , 2015 ] .", "Comments": [], "label": []}
{"id": 160024, "text": "Did you report the number of parameters in the models used , the total computational budget ( e.g. , GPU hours ) , and computing infrastructure used ?", "Comments": [], "label": []}
{"id": 152122, "text": "Due to limited space , we leave the results of * entity extraction * and * event types detection * in Appendix [ B , ] which shows GIT only slightly outperform Doc2EDAG , because we mainly focus on event record extraction and the methods are similar to Doc2EDAG for these two sub-tasks .", "Comments": [], "label": []}
{"id": 153875, "text": "Inspired by interpolation-based data augmentation [ Chen ] [ et al. , ] [ 2020 , 2021 ] and neural architecture search [ Liu et al. , , 2018 ] , we utilize Hidden State Mixing for module selection .", "Comments": [], "label": []}
{"id": 155689, "text": "To predict r , we use the representation of [ CLS ] token , as in NSP .", "Comments": [], "label": []}
{"id": 159194, "text": "Instead of inputting raw dialogue , we use the concatenation of candidate summary , omission utterances , and non-omission utterances as the input : `` * Candidate * < sep > * Omission * < sep > * Non-Omission * '' .", "Comments": [], "label": []}
{"id": 154648, "text": "ALBEF is pretrained with three losses : ( 1 ) an image-text contrastive ( ITC ) loss that works just like CLIP 's and uses the outputs of the image-only and text-only transformers , ( 2 ) an image-text matching ( ITM ) Table 6 : Accuracy on CLEVR image-text matching task .", "Comments": [], "label": []}
{"id": 153705, "text": "We use a single final training/evaluation run and hyperparameters are manually tuned in the range of 1e-2 to 6e-6 .", "Comments": [], "label": []}
{"id": 158778, "text": "For CRNN , we use a pre-trained text recognition model from easyocr to initialize parameters .", "Comments": [], "label": []}
{"id": 156059, "text": "We use a character-based tokenizer because ( 1 ) Chinese characters as themselves stand for individual semantic units [ Li et al. , , 2019b ] and ( 2 ) char-based models are much more robust under noisy and adversarial scenarios [ El Boukkouri et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 158194, "text": "To measure whether plausibility of examples impacts their usefulness for CCL , we use an entirely different dataset , Wikitext-103 , as our novel set .", "Comments": [], "label": []}
{"id": 158888, "text": "Lambda calculi with types .", "Comments": [], "label": []}
{"id": 153954, "text": "LAMBDA is a cloze-style dataset to test the ability of long-range dependency modeling .", "Comments": [], "label": []}
{"id": 151632, "text": "We use BERTbase for all experiments .", "Comments": [], "label": []}
{"id": 154202, "text": "We use a linear-chain crf layer with log-likelihood optimization and Viterbi decoding . e.g. , labeling two consecutive subwords as beginning and inside of different entity types , especially given the large set of 279 labels ( Table [ 1 ] .", "Comments": [], "label": []}
{"id": 159949, "text": "UnitY has five major architecture modifications from Translatotron2 [ Jia et al. , , 2022b ] , ( 1 ) generating subwords instead of phonemes in the first pass , ( 2 ) generating discrete units instead of spectrograms in the second pass to bypass duration modeling , ( 3 ) replacing Long Short-Term Memory ( LSTM ) [ Hochreiter and Schmidhuber , 1997 ] layers with Transformer layers in both decoders , ( 4 ) introducing a T2U encoder between the two decoders , and ( 5 ) assigning more model capacities to the first pass .", "Comments": [], "label": []}
{"id": 158129, "text": "In addition , the Conformer backbone consistently outperforms Transformer ( 2.1 % vs. 2.8 % , 8.5 % Table 2 : WER ( % ) of our MIR-GAN and prior works on the LRS2 benchmark .", "Comments": [], "label": []}
{"id": 152423, "text": "We implement a straightforward approach to select rationales by setting a threshold t and make z = 1 ( * i.e. , * unmasked ) if the importance score s > t and z = 0 ( * i.e. , * masked ) otherwise .", "Comments": [], "label": []}
{"id": 151779, "text": "To improve the computational efficiency , we utilize a lightweight dual-transformer architecture with just 2 layers , significantly decreasing the memory consumption and accelerating the training to further improve the efficiency .", "Comments": [], "label": []}
{"id": 156936, "text": "The Transformer encoder module is equipped with a convolution-based relative position embedding layer and 12 WavLM Transformer layers .", "Comments": [], "label": []}
{"id": 153531, "text": "In this paper , we adopt a straightforward probing method to get global information hidden in contextualized representations , where Given contextualized representations of an token x and its nearby tokens c in the same context , we use g and g to represent global semantics hidden in these representations .", "Comments": [], "label": []}
{"id": 159752, "text": "We use Adagrad [ Duchi et al. , , 2011 ] optimizer and employ grid search to find the best hyperparameters based on the performance on the validation datasets .", "Comments": [], "label": []}
{"id": 153799, "text": "The example in the introduction of knowing the meaning of `` jump '' , `` jump twice '' and `` jax '' and from those inferring the meaning of `` jax twice '' is an example of systematicity . * Productivity * , on the other hand , is the ability to extrapolate to longer sequences than those seen during training .", "Comments": [], "label": []}
{"id": 158412, "text": "We adopt the following three metrics as Figure 2 : Effect of fine-tuning on ID accuracy and OOD detection performance , across different objectives and detection methods .", "Comments": [], "label": []}
{"id": 155225, "text": "To report prediction performance on clean examples , we use the * weighted F1 * score to take the distribution of prediction labels into consideration .", "Comments": [], "label": []}
{"id": 153088, "text": "Such models are often datasetspecific because of the custom architecture , so they struggle to adapt to other datasets and effectively leverage pre-trained state-of-the-art Transformers .", "Comments": [], "label": []}
{"id": 158215, "text": "We use two parts of links , namely party affiliation and co-sponsorship in voting .", "Comments": [], "label": []}
{"id": 152714, "text": "All ROUGE scores are computed using the ROUGE-1.5.5.pl script [ 4 ] .", "Comments": [], "label": []}
{"id": 159599, "text": "The total loss is calculated as : We use this approach for the Perseus XPoS dataset .", "Comments": [], "label": []}
{"id": 152953, "text": "The SMALL- * models are initialized with the weights of an ASR trained only on the * ( audio , transcript ) * pairs of MuST-C , while the LARGE-BPE is initialized with an ASR trained on all the available data .", "Comments": [], "label": []}
{"id": 155692, "text": "We use -tiny mainly for ablation studies .", "Comments": [], "label": []}
{"id": 159759, "text": "We use the difference batches to refine and differentiate these feature representations .", "Comments": [], "label": []}
{"id": 158688, "text": "Code , outputs and offline ST models used for our experiments are released under Apache License 2.0 at : [ https : ] ( https : //github.com/hlt-mt/fbk-fairseq ) [ //github.com/hlt-mt/fbk-fairseq ] ( https : //github.com/hlt-mt/fbk-fairseq ) .", "Comments": [], "label": []}
{"id": 151453, "text": "Finally , we describe an experiment investigating the causal role of semantic representations by directly manipulating E ( x ) ( [ §4.4 ] . [ 5 ] 4.1 Preliminaries Model In all experiments , the encoder E comes from a BART [ Lewis et al. , , 2020 ] or T5 [ Raffel et al. , 2020 ] model .", "Comments": [], "label": []}
{"id": 152696, "text": "We adopt the Seq2Seq Transformer [ Vaswani et al. , , 2017 ] model .", "Comments": [], "label": []}
{"id": 152431, "text": "For the Generator , we use a retrieval-based model similar to [ He et al. , , 2018 ] which samples an utterance from the top 10 matched templates .", "Comments": [], "label": []}
{"id": 159616, "text": "We showcase the versatility of encoderdecoder models , ( i ) by offering a novel end-to-end contextualized lemmatization model for AG and Latin , with a greatly simplified architecture that clearly outperforms prior work ; ( ii ) while MLM in encoder-only models is restricted to single-token predictions , our T5-based models exhibit great flexibility for formulating probing tasks , which help exploring what models learn from pre-training data .", "Comments": [], "label": []}
{"id": 160095, "text": "4 Data In our experiments , we use ClaimRev [ Skitalin ] [ skaya et al. , , 2021 ] : a corpus of over 124,312 claim revision histories , collected from the online debating platform Kialo , [ 2 ] which uses the structure of dialectical reasoning .", "Comments": [], "label": []}
{"id": 151628, "text": "Since the weights of the PrLM are the same before and after adapter-based tuning , to verify this , we use Representational Similarity Analysis ( RSA ) [ Laakso and Cottrell , 2000 ] to assess the similarity of tuned representations to those without tuning at each transformer layer .", "Comments": [], "label": []}
{"id": 156833, "text": "We used Krippendorf 's α [ Krippendorff , 2004 ] as an agreement measure , as it allows for assigning multiple labels to a span , which is the case in medical coding .", "Comments": [], "label": []}
{"id": 158537, "text": "The performances in Table [ 13 ] are * per-query * scores of the meeting summarization and E3 task , which is computed specifically for the query in Table [ 8 ; ] 'Summarization performance ' indicates the per-query ROUGE score under the standard summarization metric , and 'E3 performance ' refers to the per-query extraction ( `` summary-level '' ) and Joint ROUGE scores as the joint evaluation metric , as in Section [ 6.3 ] and Table [ 5 . ]", "Comments": [], "label": []}
{"id": 158921, "text": "That is because we used the NMT models that produced the translations in the datasets we analyze , i.e , the models that actually * hallucinate * for the source sequences in the dataset .", "Comments": [], "label": []}
{"id": 159637, "text": "Before the annotation , we adopt a character list containing the 20 characters as the 20 entities , [ 6 ] and first extract the their mentions with string matching .", "Comments": [], "label": []}
{"id": 154623, "text": "The typical fine-tuning time for standard transformer encoder-decoder is 8 hrs before it early stops , and for retrieval-based model is 16 hrs .", "Comments": [], "label": []}
{"id": 160048, "text": "Furthermore , we use a similar setup to PDTBs for PCC , considering only relations whose frequency is not too low ( over 10 in our setting ) .", "Comments": [], "label": []}
{"id": 157008, "text": "Following [ Xie et al. , 2022 ] , for T5-base we adopted the AdamW optimizer , while Adafactor was used for T5-3B and the two BART models .", "Comments": [], "label": []}
{"id": 154156, "text": "We adopt cosine distance to compute the similarity between embeddings .", "Comments": [], "label": []}
{"id": 159315, "text": "E Detailed Comparison with Previous Arts Due to the space limitations , we omit some baselines and details in Table [ 3 . ] Here we provide more details on the comparison with previous arts in Table [ 7 . ] We use Facebook Research 's [ fvcore ] ( https : //github.com/facebookresearch/fvcore ) to calculate FLOPs .", "Comments": [], "label": []}
{"id": 155921, "text": "We used an in-house list of 23 phrases for German and Spanish and 126 for English .", "Comments": [], "label": []}
{"id": 158425, "text": "Over the scale of our experiments , we have used about 200 hours of GPU training time .", "Comments": [], "label": []}
{"id": 156415, "text": "Table 3 : Generation time for each continuation of maximum length 20 in the toxicity experiments using the same GPU resource .", "Comments": [], "label": []}
{"id": 157447, "text": "SPARTA ( T5 ) : would the traffic be better before or after midnight ?", "Comments": [], "label": []}
{"id": 158138, "text": "In addition , we also employ Conformer as backbone , where the depth-wise convolution kernel size is set to 31 .", "Comments": [], "label": []}
{"id": 152602, "text": "Both models have a vocabulary size of 50,265 extracted using the Byte Pair Encoding ( BPE ) [ Sen ] [ nrich et al. , , 2016 ] tokenization method .", "Comments": [], "label": []}
{"id": 156942, "text": "We use the hidden state of < s > token in H , denoted as h , and pass it through a prediction head with two fully-connected layers and a GELU activation [ Hendrycks and Gimpel , 2016 ] between them to get the prediction : where σ denotes the GELU activate function , ( 1 ) h×d , ( 2 ) dh×d , b ( 1 ) dh , b ( 2 ) are new learnable parameters in the fine-tuning stage .", "Comments": [], "label": []}
{"id": 159361, "text": "We achieve the second sub-goal using self-critical sequence training [ Rennie et al. , , 2017 ] with an NLI component , which we use as a reward function for generation .", "Comments": [], "label": []}
{"id": 156553, "text": "We use standard train/val/test splits for the three tasks , and report accuracy . 4.2 Comparison on Existing Datasets Text-to-Video Retrieval Results .", "Comments": [], "label": []}
{"id": 154106, "text": "REAP [ Goyal ] [ and Durrett , 2020 ] uses a Transformer to generate syntactically diverse paraphrases by including an additional position embedding representing the syntactic tree .", "Comments": [], "label": []}
{"id": 160218, "text": "First , we note that PROMPTRANK uses the most number of parameters since it is based on T5-XLwhile PathRetriever and MDR both rely on much smaller LMs such as BERT and RoBERTa .", "Comments": [], "label": []}
{"id": 159318, "text": "However , they have some issues , e.g. , redundancy and over-parameterization , which is mainly caused by Multi-Head Attention ( MHA ) [ Michel et al. , , 2019 ] [ Voita et al. , , 2019 ] and Feed-Forward Network ( FFN ) [ Sukhbaatar et al. , 2019 ] [ Wu et al. , ] [ 2019 , 2020 ] of transformer .", "Comments": [], "label": []}
{"id": 155352, "text": "* and * * mean the improvements over W2V2-Transformer baseline is statistically significant ( p < 0.05 and p < 0.01 , respectively ) .", "Comments": [], "label": []}
{"id": 152677, "text": "It is a variant of the masked prediction loss [ Baevski et al. , , 2020a ] and no target labels are required in our implementation .", "Comments": [], "label": []}
{"id": 152402, "text": "In contrast , we use a single compact hypernetwork allowing us to efficiently condition on multiple tasks and layers of a transformer model . 6 Conclusion We propose a parameter-efficient method for multi-task fine-tuning .", "Comments": [], "label": []}
{"id": 158304, "text": "For Embedding-based alignment ( CTC-E ) , we use BERTAligner/BERT embedding ( default ) .", "Comments": [], "label": []}
{"id": 157466, "text": "The selected < https : //github.com/JaidedAI/EasyOCR > < spacy.io > Ablation Studies We experiment with 32x32 image patch size , resulting in additional 256 image tokens to the model .", "Comments": [], "label": []}
{"id": 157164, "text": "During training , we use the ground truth response Y to guide the learning of knowledge discernment mechanisms .", "Comments": [], "label": []}
{"id": 153484, "text": "The batch size for each GPU is set to 4096 tokens .", "Comments": [], "label": []}
{"id": 151359, "text": "Training time/energy was likely significantly smaller than the original release ; existing code and hyperparameters were available , and we use a smaller dataset .", "Comments": [], "label": []}
{"id": 158108, "text": "BEIT-3 [ Wang et al. , , 2022c ] utilizes a novel shared Multiway Transformer network with a shared self-attention module to align different modalities and provide deep fusion .", "Comments": [], "label": []}
{"id": 153830, "text": "Additionally , we would like to highlight some implementation details , which surprisingly had large effects on our experimental results . * Layer normalization * operations in our Transformer implementation were done * after * each sublayer ( attention and feed forward ) .", "Comments": [], "label": []}
{"id": 159796, "text": "We adopt forms of instructional in-context few-shot prompting to GPT-3 . [ 2 ] Motivated by the preceding discussion regarding evaluation challenges , we collect human annotations judging the model 's generations against the gold references .", "Comments": [], "label": []}
{"id": 154179, "text": "B Implementation Details For all experiments , we follow the settings of [ Vaswani et al. , 2017 ] , namely * Transformer-Base * and * Transformer-Big * .", "Comments": [], "label": []}
{"id": 158125, "text": "We also use Conformer [ Gulati et al. , , 2020 ] as our backbone .", "Comments": [], "label": []}
{"id": 158803, "text": "Did you report the number of parameters in the models used , the total computational budget ( e.g. , GPU hours ) , and computing infrastructure used ?", "Comments": [], "label": []}
{"id": 155174, "text": "One downside of F is that the model may be penalized even when the predicted sentence is very similar to the annotation , for this reason we also calculate ROUGE-1 , -2 , and -L scores [ Lin , 2004 ] .", "Comments": [], "label": []}
{"id": 157166, "text": "Specifically , we adopt the binary cross-entropy ( BCE ) loss Lcoarse as the mutual information estimator that maximizes the mutual information between rcog and remo : where eemo and ecog are the encoded negative samples .", "Comments": [], "label": []}
{"id": 153515, "text": "We also remove the sparse attention module , and test the architecture of only integrating the pooled hidden state cross into the attention mechanism , corresponding to Formula [ 5 . ] We call this architecture Fourier Attention for Transformer ( FAT ) .", "Comments": [], "label": []}
{"id": 152213, "text": "We use this resource to perform a systematic evaluation of contextualized word meaning representations .", "Comments": [], "label": []}
{"id": 160335, "text": "We used various publicly available ASR data sets which cover our languages to train the speech encoders , including CoVoST 2 [ Wang et al. , ] [ 2020 , 2021b ] , Common Voice [ Ardila et al. , , 2020 ] , Europarl [ Ardila et al. , , 2020 ] , mTedx [ Salesky et al. , 2021 ] , Must-C [ Di Gangi et al. , , 2019 ] and VoxPopuli [ Wang et al. , , 2021a ] , as well as speech translation data from the foreign languages into English and from English into German .", "Comments": [], "label": []}
{"id": 151637, "text": "We use the Adam optimizer [ Kingma ] [ and Ba , 2015 ] with a linear learning rate scheduler .", "Comments": [], "label": []}
{"id": 153518, "text": "Big bird : Transformers for longer sequences .", "Comments": [], "label": []}
{"id": 160345, "text": "A Textless model consists of a speech encoder , Transformer encoder and decoder .", "Comments": [], "label": []}
{"id": 159037, "text": "However , in practice this is not feasible as the computation of ROUGE scores requires the availability of gold standard key points .", "Comments": [], "label": []}
{"id": 153864, "text": "Another line of work continually inserts new task-specific modules ( adapters proposed by [ Houlsby et al. , , 2019 ] into every transformer layer for every new task while freezing pretrained models and modules used by old tasks ( Fig [ 1 ] b , [ Madotto et al. , 2021 ] , which might prevent knowledge transfer between tasks and introduce possible parameter redundancy .", "Comments": [], "label": []}
{"id": 156963, "text": "[ https : //github.com/lijuncen/Sentiment-and-Style- ] ( https : //github.com/lijuncen/Sentiment-and-Style-Transfer ) [ Transfer ] ( https : //github.com/lijuncen/Sentiment-and-Style-Transfer ) < https : //github.com/fastnlp/style-transformer > Same process as Lintra .", "Comments": [], "label": []}
{"id": 158614, "text": "Table 19 : Full experiment results ( % ) by GPT2 ( XL ) on commonsense inference generation ( COMET ) task .", "Comments": [], "label": []}
{"id": 157390, "text": "It is important to note that we use the BIO encoding for the NER tasks .", "Comments": [], "label": []}
{"id": 152426, "text": "We adopted a computational approach to investigate our research question , and this method inherently shares common limitations with observational studies , * e.g. , * prone to bias and confounding [ Benson and ] [ Hartz , 2000 ] .", "Comments": [], "label": []}
{"id": 157877, "text": "( 2 ) Transformer-based methods , which exploits pretrained language model to learn cross-sentence relations either implicitly or explicitly .", "Comments": [], "label": []}
{"id": 159133, "text": "Additionally , we also test the zero-shot evaluation ability of the Indic-COMET metric in [ §6.3 . ] 6.1 Training We build our metric with the architecture of COMET [ Rei et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 157349, "text": "The inputs for the i + 1 block of each model are : The cross-modal CLS token concatenation only happens in the output of the first transformer block .", "Comments": [], "label": []}
{"id": 153128, "text": "Making the trade-off between accuracy and latency , we use the ST-GCN model for the pretrained model we discuss later .", "Comments": [], "label": []}
{"id": 151472, "text": "We probe BART-base , a 12-layer encoderdecoder Transformer model with 139M parameters , and T5-base , a 24-layer encoder-decoder Transformer model which has 220M parameters .", "Comments": [], "label": []}
{"id": 158024, "text": "[ 1 ] The success of the teacher 's guidance depends on whether the student 's subsequent action matches Here we use * actions * to indicate any linguistic behavior with intention [ Allwood , 1976 ] .", "Comments": [], "label": []}
{"id": 153246, "text": "To predict whether two sentences form a claim-evidence pair , we adopt a table-filling approach by pairing each sentence in the claim candidates with each sentence in the evidence candidates to form a table .", "Comments": [], "label": []}
{"id": 159247, "text": "Specifically , we use two functions , ROUGE-2 and Sentence-BERT [ Reimers ] [ and Gurevych , 2019 ] cosine similarity , [ 7 ] to assess the syntactic and semantic similarity between each reference-to-target pair ( r , t ) ∈ T . [ 8 ] The scores for the syntactic and semantic similarity are denoted as ϕsyn ( r , t ) and ϕsem ( r , t ) , respectively .", "Comments": [], "label": []}
{"id": 152177, "text": "Specifically , current methods are all built upon the assumption that any source entity has a counterpart in the target KG ( Sun et al. , 2020c ) , and are accordingly developed with learning resources that enforce the same assumption .", "Comments": [], "label": []}
{"id": 151390, "text": "We used the SMAC [ Hutter et al. , , 2011 ] algorithm to search for the optimal hyper-parameters .", "Comments": [], "label": []}
{"id": 158139, "text": "We use a dropout of p = 0.1 after the self-attention block within each Transformer layer , and each Transformer layer is dropped [ Fan et al. , , 2019 ] at a rate of 0.1 .", "Comments": [], "label": []}
{"id": 154597, "text": "Perhaps for that reason , the current state-of-the-art models such as Meena [ Adiwar ] [ dana et al. , , 2020 ] and BlenderBot [ Roller et al. , , 2020 ] employ Transformers with token truncation lengths of only the 128 most recent tokens , and We use this term colloquially , see [ Agranoff et al. , 1965 ] for evidence of goldfish long-term memory .", "Comments": [], "label": []}
{"id": 160087, "text": "We use RankEncoder with base encoder SimCSE .", "Comments": [], "label": []}
{"id": 156513, "text": "After obtaining the representation for both the exemplars x and the text x , we use the cosine function to measure the similarity between them : We employ a contrastive loss [ Hadsell et al. , , 2006 ] to learn the embedding representations for our rule and text encoder .", "Comments": [], "label": []}
{"id": 159801, "text": "So we instead ask annotators to evaluate outputs from Flan-T5 fine-tuned on the NYT train set .", "Comments": [], "label": []}
{"id": 151891, "text": "Extractive QA For extractive QA , we adopt three widely-used datasets : SQuAD [ Rajpurkar et al. , 2016 ] , TriviaQA [ Joshi et al. , , 2017 ] and NaturalQA [ Kwiatkowski et al. , , 2019 ] in MRQA [ Fisch et al. , 2019 ] to evaluate ERICA in various domains .", "Comments": [], "label": []}
{"id": 155838, "text": "The second is the hybrid strategy in Section [ 2 . ] We also consider LM Adaption used in [ Lester et al. , 2021 ] in which the T5 model is further pre-trained for 10K steps with language modeling to reduce the gap between the pre-training and PT .", "Comments": [], "label": []}
{"id": 156580, "text": "We use an initial learning rate of 1e-5 , and warm up the learning rate in the first half epoch , followed by cosine decay to 1e-6 .", "Comments": [], "label": []}
{"id": 160368, "text": "It has two unit decoders with 6 and 2 Transformer decoder layers for target and source unit prediction respectively .", "Comments": [], "label": []}
{"id": 151738, "text": "Then , we use a convolution front end to down-sample the long acoustic features .", "Comments": [], "label": []}
{"id": 154472, "text": "The number of parameters in each model are : DeepComHybrid 15.6M ; Seq2Seq 31.3M ; Transformer 68.2M ; Code2Seq 5.7M ; Code2Vec 33.1M .", "Comments": [], "label": []}
{"id": 154512, "text": "Appendix [ A.2 ] reports the datasets ' main statistics as presented in the original papers [ Fabbri et al. , , 2019 ] [ Gholipour Ghalandari et al. , 2020 ] [ 2 ] . 4.2 Evaluation Metrics Like most previous works , we use the F1 variants of the ROUGE- * N * scores [ 3 ] [ Lin , 2004 ] for performance evaluation .", "Comments": [], "label": []}
{"id": 154757, "text": "This is because the transformer-XL is able to keep almost the entire sequence in memory .", "Comments": [], "label": []}
{"id": 151386, "text": "We use the state of the art method , FLAT , as the baseline model .", "Comments": [], "label": []}
{"id": 154953, "text": "Experimental results demonstrate the effectiveness of NLSSum , which significantly outperforms original XLMR by 2.25 ROUGE-L score on ML-SUM [ Scialom et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 157946, "text": "Here , we adopt the parameter-efficient tuning framework which is able Figure 1 : The process of our method .", "Comments": [], "label": []}
{"id": 151594, "text": "Abstract Heavily overparameterized language models such as BERT , XLNet and T5 have achieved impressive success in many NLP tasks .", "Comments": [], "label": []}
{"id": 159030, "text": "First , we introduce multiple knowledge sources to construct HKG , and encoding this knowledge through LM consumes more GPU resources .", "Comments": [], "label": []}
{"id": 153626, "text": "Leveraging the rich and effective training signal produced by MLM , Condenser learns to utilize the powerful Transformer architecture to generate dense CLS representations .", "Comments": [], "label": []}
{"id": 155238, "text": "During training , we use a * batch size * of 32 , * learning rate * of 0.005 , * gradient clipping * of 10.0 .", "Comments": [], "label": []}
{"id": 154672, "text": "We used GNU Parallel for much of the dataset processing [ Tange , 2011 ] .", "Comments": [], "label": []}
{"id": 158255, "text": "GPU times are calculated after calling torch.cuda.synchronize ( ) .", "Comments": [], "label": []}
{"id": 158333, "text": "Did you report the number of parameters in the models used , the total computational budget ( e.g. , GPU hours ) , and computing infrastructure used ?", "Comments": [], "label": []}
{"id": 152760, "text": "In particular , we compute ROUGE-1 [ Lin , 2004 ] scores between the input and target of training instances and filter out those whose scores are below a certain threshold .", "Comments": [], "label": []}
{"id": 160059, "text": "Our implementation are denoted as replicated-SOTA ( RSOTA ) .", "Comments": [], "label": []}
{"id": 155753, "text": "Specifically , we employ the set of corpus-specific rules designed by [ Rösiger et al. , 2018 ] .", "Comments": [], "label": []}
{"id": 155057, "text": "We use the Base version of XLM-R for our experiments .", "Comments": [], "label": []}
{"id": 151723, "text": "Dash ( - ) indicates classifier fails to converge . also do ablation analysis for HIF-ALONE as follows : HIF-WBOW replaces outputs of HIF with d-dimensional WBOW vectors using PCA .", "Comments": [], "label": []}
{"id": 156331, "text": "For articles , we used the SBERT RoBERTa model to generate an embedding for each article , which was a 768 dimensional vector representing the article text , up to the first 512 tokens .", "Comments": [], "label": []}
{"id": 159529, "text": "Besides , the visual prompts of our framework can also be used for parameter-effcient tuning , which can boost the performance of large language models , such as T5-3b .", "Comments": [], "label": []}
{"id": 152366, "text": "The transformer models also exhibited strong biases in target coverage .", "Comments": [], "label": []}
{"id": 158217, "text": "We adopt a masked language model objective to pre-train the language model .", "Comments": [], "label": []}
{"id": 153508, "text": "We test our models on the Long Range Arena ( LRA ) benchmark [ Tay et al. , , 2020b ] , since it is specifically designed for evaluating the performance of efficient Transformers on various long sequence tasks , and there are quite a number of baseline models evaluated on this benchmark .", "Comments": [], "label": []}
{"id": 158290, "text": "For Topical-Chat ( USR-TC and UniEval-TC ) , the systems are : Human annotations – Human Generated Old , Human Generated New , and four systems that origin from Transformers with different decoding systems , such as Nucleus Decoding p = 0.3 , Nucleus Decoding p = 0.5 , Nucleus Decoding p = 0.7 , Argmax Decoding – greedy decoding .", "Comments": [], "label": []}
{"id": 154883, "text": "We adopt a latent-variable model to filter out irrelevant information and obtain a better representation . 7 Conclusion In this paper , we propose a retrieval-based MMT method , which learns a phrase-level universal visual representation to improve NMT .", "Comments": [], "label": []}
{"id": 158294, "text": "For each system output and the corresponding references , the score based on BLEU , ROUGE , BERTScore , and human ratings ( Coherence , Consistency , Fluency , Relevance ) are already included in dataset .", "Comments": [], "label": []}
{"id": 160203, "text": "However , it is worth noting that the best instruction on a relatively small held-out set will not necessarily generalize during test time : The search-based instruction produces AR @ 2 and R @ 2 that are almost the same or worse than the median instruction , respectively with T5- Large .", "Comments": [], "label": []}
{"id": 156764, "text": "C.5 Alignment Model Details In our experiments , we use the best alignment method reported in [ Akyurek and Andreas , 2021 ] , which is IBM Model 2 for all datasets except the SCAN dataset that uses their proposed algorithm , to obtain our initial alignments A = { ( x , x ) : set of tuples contains aligned tokens .", "Comments": [], "label": []}
{"id": 155603, "text": "We use news-dev 2016 and news-test 2016 as the validation set and test set respectively .", "Comments": [], "label": []}
{"id": 155861, "text": "Nonetheless , state-of-the-art abstractive summarization systems , all built on the Transformer architecture [ Zhang et al. , , 2020 ] [ Lewis et al. , , 2020 ] , use attentions to estimate relations between pairwise tokens and largely ignore document structures .", "Comments": [], "label": []}
{"id": 156753, "text": "We used a code-book with n = 32 tokens associated with d = 64 dimensional Figure 3 : Overview of our approach in VQA .", "Comments": [], "label": []}
{"id": 156924, "text": "For the transfer study , we use the random split of MAGPIE dataset and VUA18 .", "Comments": [], "label": []}
{"id": 159881, "text": "We employ FNet [ Lee-Thorp et al. , 2021 ] and Linear Transformers [ Katharopou ] [ los et al. , , 2020 ] as representatives of these as a baseline . 4 Experiments Our experiments are designed to test the following three hypotheses .", "Comments": [], "label": []}
{"id": 158970, "text": "Transformer [ Vaswani et al. , , 2017 ] is a self-attention-based text generation framework .", "Comments": [], "label": []}
{"id": 157216, "text": "To answer these questions , we evaluated our approach on several datasets : 1 ) CFQ [ Keysers et al. , , 2019 ] , 2 ) the SQL versions of Geoquery [ Zelle and Mooney , 1996 ] , ATIS [ Dahl et al. , 1994 ] , and Scholar [ Iyer et al. , , 2017 ] , each of which were curated by [ Finegan-Dollak et al. , , 2018 ] , and 3 ) the version of Geoquery [ Zelle and ] [ Mooney , 1996 ] used by [ Dong and Lapata , 2016 ] [ Kwiatkowski et al. , , 2011 ] which maps questions to a variant of typed lambda calculus [ Carpenter , 1997 ] .", "Comments": [], "label": []}
{"id": 152816, "text": "For each ( c 0 i , a i ) , we use beam decoding to generate 15 different questions q 0 .", "Comments": [], "label": []}
{"id": 156344, "text": "Later in this section , we will also evaluate a version of our model where we use the same initial representations as [ Nguyen et al. , , 2020 ] , instead of the RoBERTa SBERT ones .", "Comments": [], "label": []}
{"id": 159161, "text": "For Malayalam Bing API is a close-second and NLLB for Tamil .", "Comments": [], "label": []}
{"id": 152210, "text": "References Eric Sigler , Mateusz Litwin , Scott Gray , Benjamin Chess , Jack Clark , Christopher Berner , Sam Mc-Candlish , Alec Radford , Ilya Sutskever , and Dario Amodei . 2020 .", "Comments": [], "label": []}
{"id": 157771, "text": "As such , we implement FactScore , which is based on the state of the art model ( MultiVERS [ Wadden et al. , , 2022 ] ) trained on the SciFact scientific claims dataset [ Wadden et al. , 2020 ] .", "Comments": [], "label": []}
{"id": 154369, "text": "1 Introduction Transformer-based generative pre-trained language models ( PLMs ) show strong abilities of multitask and few-shot learning , and achieve remarkable performances on various tasks [ Radford and ] [ Narasimhan , 2018 ] [ Brown et al. , , 2020 ] [ Lewis et al. , , 2020 ] [ Raffel et al. , , 2020 ] [ Chen et al. , , 2021 ] .", "Comments": [], "label": []}
{"id": 155305, "text": "We adopt a fully-connected layer to calculate the correlation score between hidden vector h and superordinate type/role p . where uij represents the correlation score and [ ; ] denotes the concatenation of two vectors .", "Comments": [], "label": []}
{"id": 152860, "text": "We implement the extractive oracle using greedy search .", "Comments": [], "label": []}
{"id": 159256, "text": "Table 3 : Highlighting performance Note that for R-Prec , we use majority voting to derive single ground-truth labels from the three annotators , whereas for PCC , we take the mean agreement of the three annotations as the ground truth .", "Comments": [], "label": []}
{"id": 152187, "text": "3.2 Data Since existing datasets do not allow us to conduct experiments following the described setup , we create new datasets in a weakly-supervised fashion that is conceptually similar to the method proposed by [ Mintz et al. , 2009 ] : we employ large datasets annotated for sentiment or topicality , extract derivationally complex words , and use the dataset labels to establish their semantic classes .", "Comments": [], "label": []}
{"id": 152060, "text": "Following the same setting in [ Vaswani et al. , , 2017 ] , we use * newstest2013 * as validation set and * newstest2014 * as test set , which contain 3000 and 3003 sentences , respectively .", "Comments": [], "label": []}
{"id": 159849, "text": "Semantic Retrieval We utilize sentence embedding models [ Reimers and Gurevych , 2019 ] to obtain the embedding for each sentence and descriptions for each class .", "Comments": [], "label": []}
{"id": 153783, "text": "Also , while similar studies of bias have been successfully applied to vision transformers ( Steed and Caliskan , 2021 ; Srinivasan and Uchino , 2021 ) , our results may vary for substrates other than English language .", "Comments": [], "label": []}
{"id": 153393, "text": "This motivates the following efficient greedy algorithm to implement the above optimization [ Sennrich et al. , ] Algorithm 1 Overlap based BPE ( OBPE ) for i ∈ { 1 , 2 , ... , n } do Split words in D into characters C with a special marker after every word end for V = ∪ n =1C while |V| < V do Update token and pair frequency on { Di } , V Add to V token k formed by merging pairs u , v ∈ V with the largest value of ( 1 − α ) X j fkj + α X i∈LLRL max ∈LHRL f p ki + f p kh 2 1 p end while [ 2016 ] .", "Comments": [], "label": []}
{"id": 154089, "text": "We utilize the merged data and inferred date to extract classification labels .", "Comments": [], "label": []}
{"id": 157261, "text": "First , we revisit and augment the baseline ELMo model — which incorporates a degree of bidirectionality at fine-tuning ( albeit not at pre-training ) — with Transformer architectures and whole model finetuning , hence facilitating a fair comparison with the BERT model .", "Comments": [], "label": []}
{"id": 160331, "text": "The complementary experiment is conducted to further https : //huggingface.co/bert-base-chinese https : //huggingface.co/hfl/chinese-roberta-wwm-ext https : //ai.baidu.com/tech/textcensoring evaluate the performance of models to identify the toxic content with different expressions .", "Comments": [], "label": []}
{"id": 156666, "text": "We use models trained on base sets alone as baselines and evaluate whether augmenting the base sets using DISCO data would improve the baselines ' performances following Z-aug [ Wu et al. , , 2022 ] and WANLI [ Liu et al. , , 2022 ] .", "Comments": [], "label": []}
{"id": 154299, "text": "Among many self-supervised metric losses such as Triplet Loss [ Hoffer and Ailon , 2015 ] and NT- We tried with separate encoders and decoders , but encoders with tied weights work best Xent loss [ Chen et al. , , 2020 ] , we use one that is amenable to multiple positive instances [ Khosla et al. , 2020 ] .", "Comments": [], "label": []}
{"id": 152547, "text": "Each image is represented by a pretrained , 2048-dimensional embedding produced by a ResNet-101 [ He et al. , , 2016 ] .", "Comments": [], "label": []}
{"id": 158765, "text": "The reason for worse performance in the pre-training stage is that we used the United Nations Parallel Corpus to pre-train PEIT in pre-training stage , the domain of which is far different from the e-commerce domain of ECOIT .", "Comments": [], "label": []}
{"id": 155571, "text": "For ColBERT , we use the default version that the authors selected for fair comparison .", "Comments": [], "label": []}
{"id": 155714, "text": "As a remedy , our theorems not only provide geometrical intuitions of why and when the multiple embedding representation such as MoS would do better but also suggest that the * softmax bottleneck * might not be completely solved even if we adopt a very large hidden state size .", "Comments": [], "label": []}
{"id": 152141, "text": "Finally , we remove the whole * Tracker * module ( GIT-No Tracker ) .", "Comments": [], "label": []}
{"id": 154889, "text": "Concretely , we employ the following datasets for pre-training : ( i ) Oscar : Deduplicated Hebrew portion extracted from Common Crawl via language classification , filtering and cleaning [ Ortiz Suárez et al. , 2020 ] .", "Comments": [], "label": []}
{"id": 151873, "text": "4.1 Data Sets We use two datasets to evaluate our proposed method .", "Comments": [], "label": []}
{"id": 156648, "text": "Being an internal method is an advantage of ALTI , but it is also a limitation : this method is suitable only for transformer-based translation models .", "Comments": [], "label": []}
{"id": 153614, "text": "Two annotator-agnostic baselines ( i.e. , ALL and MV ) and the silver-corpus trained model Silver are all implemented in the same baseline structure and hyper-parameters .", "Comments": [], "label": []}
{"id": 152815, "text": "We use the trained model to generate questions ( q 0 1 , q0 2 , . . . q k ) for each of the the retrieved set of alternate contexts and answers , ( ( c 0 1 , a 1 ) , ( c 0 2 , a 2 ) , . . . ( c 0 k , a k ) ) .", "Comments": [], "label": []}
{"id": 151826, "text": "For our proposed EnsLM , we use TextCNN , BiLSTM and DocBERT as the backbone of EnsLM .", "Comments": [], "label": []}
{"id": 159819, "text": "In sum , fine-tuning Flan-T5 ( large ) with both train labels and CoT explanations produced by GPT-3 yields SOTA performance across RE datasets by a considerable ( 5-10 points micro-F1 ) margin ( Figure [ 1 ] .", "Comments": [], "label": []}
{"id": 154504, "text": "Reinforcement learning approaches have used metrics such as ROUGE-1 , ROUGE-2 and ROUGE-L F1 [ Paulus et al. , 2018 ] , and also more contemporary scoring functions such as BERTScore [ Zhang et al. , , 2020b ] as rewards , often mixed with maximum-likelihood objectives .", "Comments": [], "label": []}
{"id": 152866, "text": "For the ArXiv dataset , we used λ * * = 0.5 , λ * * = 1 , λ * * = 5 .", "Comments": [], "label": []}
{"id": 154416, "text": "2.7 Position Embeddings Absolute Position Embeddings Besides tokenlevel learned position embeddings used in original Transformer , we also consider turn level and speaker-level position embeddings like PLATO [ Bao et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 151529, "text": "Besides , each generated text token is supervised to attend to the correct visual objects under heuristic spatial attention guidance .", "Comments": [], "label": []}
{"id": 152469, "text": "5 Experimental Setup 5.1 Model , Data , and Training Configuration Our DeepRapper model is built on the autoregressive Transformer decoder [ Vaswani et al. , , 2017 ] [ Radford et al. , ] [ 2018 , 2019 ] , where the hidden size , the number of attention heads and the number of Figure 5 : The distribution of beat frequencies in D-RAP .", "Comments": [], "label": []}
{"id": 153798, "text": "2 Background This section briefly provides background on compositional generalization and Transformer models .", "Comments": [], "label": []}
{"id": 154332, "text": "During the annotation , we randomly shuffle the sentences The pseudo target can also be obtained by sampling methods . https : //github.com/makcedward/nlpaug Table 9 : Examples of data perturbation methods .", "Comments": [], "label": []}
{"id": 158426, "text": "We adopt an AMR paser and aligner to parse the input document for AMR information , and it is combined with spans generated by a span proposal module to form the Tailored AMR Graph .", "Comments": [], "label": []}
{"id": 158406, "text": "They derive OOD scores from the confidence or logits from the classification head of the model . 4 Experimental Setup Datasets We adopt the benchmark in [ Hendrycks et al. , 2020 ] and [ Zhou et al. , 2021 ] , examining 9 diverse ID-OOD dataset pairs .", "Comments": [], "label": []}
{"id": 151683, "text": "Second we adopt the sentence representation of mRASP2 to retrieve similar sentences across languages .", "Comments": [], "label": []}
{"id": 151414, "text": "During inference , we use beam search with a beam size of 4 and beam https : //github.com/raosudha89/GYAFC-corpus https : //www.grammarbot.io/ https : //www.grammarly.com/ Figure 2 : The performance ( BLEU ) of different rule injection methods with different training size .", "Comments": [], "label": []}
{"id": 160415, "text": "We got the best precision score ] ( https : //huggingface.co/T-Systems-onsite/cross-en-de-roberta-sentence-transformer ) ( * precision=0.96 * [ ) when we used LaBSE at a thresh ] ( https : //huggingface.co/T-Systems-onsite/cross-en-de-roberta-sentence-transformer ) [ old value of 0.9 .", "Comments": [], "label": []}
{"id": 154600, "text": "Long-context Transformers consider ways to speed up the self-attention mechanism [ Child et al. , , 2019 ] [ Kitaev et al. , , 2019 ] [ Beltagy et al. , , 2020 ] and retrieval-augmented methods consider ways to select the pertinent parts of the context to consider [ Dinan et al. , , 2019 ] [ Lewis et al. , , 2020b ] [ Shuster et al. , 2021 ] which can also be related to earlier neural QA methods [ Chen et al. , , 2017 ] .", "Comments": [], "label": []}
{"id": 153331, "text": "The oracle is { q → pi , yi } i=1 , ... , m , where y is the span label and we use l = min ( q , pi ) and r = max ( q , pi ) to denote the left and right boundary of the i-th span , respectively .", "Comments": [], "label": []}
{"id": 152942, "text": "INL relies on generic expressions rather than genderspecific ones ( e.g . * service * vs. * waiter/tress * ) See Section [ 8 . ] We noticed that CHAR 's lower translation quality may have to do more with fluency rather than lexical issues . conveys both lexical meaning and gender realization better than BPE .", "Comments": [], "label": []}
{"id": 157297, "text": "In this research , we utilize a similar approach to extract the style representations from the advertisements of darknet vendors by passing it through a Transformer-based classifier pre-trained for a closed-set vendor verification task .", "Comments": [], "label": []}
{"id": 159047, "text": "Key Point Generation : We choose Flan-T5 [ Chung et al. , , 2022 ] as our KPG model , which is fine-tuned on more than 1000 different tasks , and it has received a lot of attention as a potential alternative of GPT-3 [ Brown et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 159205, "text": "A.3 Candidate Generation We use 6 abstractive models to generate candidates for the dialogues in OLDS , including BARTlarge/base , T5base/small , vanilla Transformer , and Pegasuslarge .", "Comments": [], "label": []}
{"id": 151769, "text": "One possible reason is that the TRANSFORMER-BIG model to construct the pseudo-sentences was trained on the whole WMT19 En-De data that contains the heldout data , which serves as self-training to decently improve the supervised baseline [ He et al. , , 2019 ] .", "Comments": [], "label": []}
{"id": 151897, "text": "Extractive QA For extractive question answering , we adopt MRQA [ Fisch et al. , , 2019 ] as the testbed and choose three datasets : SQuAD [ Ra ] [ jpurkar et al. , , 2016 ] , TriviaQA [ Joshi et al. , , 2017 ] and NaturalQA [ Kwiatkowski et al. , , 2019 ] .", "Comments": [], "label": []}
{"id": 160237, "text": "For results in Table [ 4 , ] we use BART-large for [ Yan et al . ] because [ Yan et al. , 2021 ] only supports a generative model with absolute position embedding .", "Comments": [], "label": []}
{"id": 157121, "text": "We use SST-2 and BoolQ as the source tasks , and IMDb [ Maas et al. , , 2011 ] and BoolQ Contrast Set [ Gard ] [ ner et al. , , 2020 ] as our target tasks , respectively .", "Comments": [], "label": []}
{"id": 154331, "text": "We omit the case when we adopt style strength filtering because it does not use the dynamic threshold and is more straightforward to understand .", "Comments": [], "label": []}
{"id": 156110, "text": "Therefore , instead of using the vocabulary V , we use the 5,000 highest frequency words in Wikipedia as the search space .", "Comments": [], "label": []}
{"id": 159948, "text": "We build the speech encoder based on Conformer [ Gulati et al. , , 2020 ] , which augments Transformer [ Vaswani et al. , 2017 ] with a convolution module , while implementing the rest three modules based on Transformer .", "Comments": [], "label": []}
{"id": 155188, "text": "We hypothesize this is because ROUGE score maximization essentially limits what the model learns to lexical matching , while BERTScore can score based on more abstract , semantic criteria .", "Comments": [], "label": []}
{"id": 158417, "text": "For CLINC150 , we use the 'out of scope ' class as the test set .", "Comments": [], "label": []}
{"id": 152059, "text": "For En→De , we use 4.5M training data .", "Comments": [], "label": []}
{"id": 158228, "text": "If we use the same LM as both amateur and expert and restrict the context window of the amateur LM [ §3.4 ] , our method is equivalant to the MMI decoding objective [ Li et al. , 2016 ] sometimes used in dialog systems , which explicitly maximizes the pointwise mutual information between the xpre and xcont .", "Comments": [], "label": []}
{"id": 154509, "text": "This allows the model to learn to score the soft predictions and their targets in a way that mimics the ROUGE prediction-reference score .", "Comments": [], "label": []}
{"id": 155785, "text": "Although existing NAT models achieve competitive results compared to autoregressive models in translation tasks , it is not negligible that they still need the help of an autoregressive Transformer ( AT , [ Vaswani et al. , , 2017 ] as a teacher for training , i.e. , sequence-level knowledge distillation [ Kim and ] [ Rush , 2016 ] .", "Comments": [], "label": []}
{"id": 158010, "text": "We use the standard ApteMode version of the dataset .", "Comments": [], "label": []}
{"id": 151671, "text": "We also list BLEU of pivot-based model ( X→En then En→Y using m-Transformer ) as a reference , mRASP2 only lags behind Pivot by -0.25 BLEU . ( * ) Note that Dutch ( Nl ) is not included in PC32 .", "Comments": [], "label": []}
{"id": 160307, "text": "Transformers can only handle limited sequence lengths due to the computational and memory complexity of attention calculation .", "Comments": [], "label": []}
{"id": 154984, "text": "For example , for the method described in this paper , supporting a new translation direction with a pre-trained LM occupies disk spaces below 20M , which is much smaller than training a separate neural machine translation model , where the model size is typically larger than 60M per language pair for the Transformer architecture .", "Comments": [], "label": []}
{"id": 159877, "text": "Transformers achieve systematicity through the attention mechanism which is a learnable set of functions that determines the interaction between entities by matching query representations to key representations ( as shown in Figure [ 1 ] .", "Comments": [], "label": []}
{"id": 156328, "text": "For Twitter users , we used the Twitter API [ 9 ] to scrape 5000 followers for each Twitter account we could find ( 72.5 % of the sources , identical to [ Baly et al. , 2020b ] .", "Comments": [], "label": []}
{"id": 154114, "text": "With a dataset size of only 1 % ( 40 training examples , 14 validation examples ) , ParaBLEUlarge achieves a Pearson r of 0.571 , still correlating significantly more strongly with human judgements than BLEU , TER , ROUGE , METEOR and MoverScore .", "Comments": [], "label": []}
{"id": 155328, "text": "In the experiments of this paper , we use mean squared error between the hidden states of the teacher and the student for both our method and the KD baseline since recent study [ Kim et al. , 2021 ] finds that it is more stable and slightly outperforms than KL divergence .", "Comments": [], "label": []}
{"id": 152913, "text": "Coverage ( F/T ) represent the percentage of entity types and counts in the * * F * * inetuning ( * * T * * est ) data that are covered by the pre-training data . we use the FLORES dataset in [ Guzmán et al. , 2019 ] and follow the paper 's setting to finetune on parallel data in the OPUS repository .", "Comments": [], "label": []}
{"id": 157898, "text": "At the utterance-level , we first mask each utterance in a dialogue U t in turn , then fine-tune a T5 model as our utterance-level scorer P to predict the masked utterance .", "Comments": [], "label": []}
{"id": 154656, "text": "Parts [ 11a ] and [ 11b ] show cases where the meaning of `` behind '' does not match our heuristic , which checks which proposal 's y-coordinate is smaller .", "Comments": [], "label": []}
{"id": 155244, "text": "Subfigures ( d ) and ( e ) show the Len and LC label distributions of examples in the training set . 3.3 Improve the Word Coverage Since we utilize the BART tokenizer , feeding the original form of a targeted word into the encoder may hinder the decoder from injecting it into the output .", "Comments": [], "label": []}
{"id": 159172, "text": "Refer to Appendix [ D ] for hyperparameter configurations of PEFT approaches .", "Comments": [], "label": []}
{"id": 151343, "text": "We use two LMs ( −→LM and ←− LM ) to * generate * contexts for a given input , which implicitly capture aspects of its meaning ( the contextualization step ) .", "Comments": [], "label": []}
{"id": 159225, "text": "3.3 Data Generation Post fine-tuning on the text reconstruction task , we utilize ACLM to generate synthetic data for data augmentation .", "Comments": [], "label": []}
{"id": 157434, "text": "The metrics , such as BLEU , ROUGE and METEOR , are calculated with the package released by [ Sharma et al. , , 2017 ] .", "Comments": [], "label": []}
{"id": 153693, "text": "While for most models the trend is a decrease in trust for both real news and misinformation , for the T5-base model there is a statistically significant correlation of Pearson 's r = .24 showing shifts in trust align with gold labels .", "Comments": [], "label": []}
{"id": 152938, "text": "This motivates us to continue our multifaceted evaluation by taking into account only small models – henceforth CHAR and BPE – that , being trained on the same MuST-C data , allow for sound and transparent comparison .", "Comments": [], "label": []}
{"id": 159003, "text": "We use the ATINTER trained Table 2 : Results comparing model robustness using the clean accuracy ( % ) and adversarial accuracy ( % ) on the five adversarial attacks : None indicates the BERT model without any defense and therefore acts as a baseline model .", "Comments": [], "label": []}
{"id": 154961, "text": "We train the extractive model using both the English labels y a ( created using the greedy algorithm ) as well as the label weights generated in Section [ 3.6 . ] To train Tα , we use binary labels y α , where in one document , y α = 1 when i ∈ U , otherwise y α = 0 .", "Comments": [], "label": []}
{"id": 153730, "text": "6 % ( 66.18 → 72.47 ) improvement over the previous best method based on the zero-shot T5 model .", "Comments": [], "label": []}
{"id": 158881, "text": "Notice that , for PLMs , we use original type names rather than type indicators to capture the label semantics .", "Comments": [], "label": []}
{"id": 159380, "text": "In terms of models , both BART and the NLI model we used are licensed under the MIT License .", "Comments": [], "label": []}
{"id": 156681, "text": "We use log pseudo-perplexity in masked language modeling , defined as However , the majority of the language models employ sub-word tokenization methods to segment and encode text .", "Comments": [], "label": []}
{"id": 154591, "text": "For single-pass online learning experiments [ Section 5 ; ] [ Section 7 ] , we use a constant learning rate of 1e-5 .", "Comments": [], "label": []}
{"id": 156253, "text": "For each utterance , we use a partof-speech ( POS ) tagger to find the nouns , verbs , and adjectives that are not stopwords and then construct a set of potential concepts by including the lemmatized version of these words .", "Comments": [], "label": []}
{"id": 159064, "text": "ROUGE = 23.2 , sF1 = 0.56 ) but worse than Unsup-KPM+IC ( ROUGE = 28.1 , sF1 = 0.59 ) .", "Comments": [], "label": []}
{"id": 155999, "text": "Here we implement a new version with punctuation marks since we find them important for hard tasks .", "Comments": [], "label": []}
{"id": 156220, "text": "The input embedding vectors pass through multiple attentionbased transformer layers where each layer produces a contextualized embedding of each token .", "Comments": [], "label": []}
{"id": 156644, "text": "For reranking , the goal is the opposite : finding the best translations ( as opposed to the worst ) , which is closer to the COMET training objective .", "Comments": [], "label": []}
{"id": 153351, "text": "We use different settings for single-source domain adaptation and multi-source domain adaptation .", "Comments": [], "label": []}
{"id": 152623, "text": "For log α , we use N ( 0 , σ ) where we set σ by searching [ 0.25 , 0.5 ] by increments of 0.05 and use the value yielding the highest ELBO after excluding degenerate runs .", "Comments": [], "label": []}
{"id": 157845, "text": "A.2 Graph Traversal Algorithm We present the pseudocode of our * Graph Traversal Algorithm * , which is described in Section [ 3.1 . ] Table 7 : Performance of the T5 model , training with different fixed number of previous turns on our validation set .", "Comments": [], "label": []}
{"id": 154666, "text": "Following the practice of [ Li et al. , , 2020 ] , we used the editdistance [ 10 ] library to calculate the Phone Error Rate ( PER ) .", "Comments": [], "label": []}
{"id": 155636, "text": "In the same task , we use exactly the same hyper-parameters and setups for the pairs of base models and corresponding DM-models .", "Comments": [], "label": []}
{"id": 153513, "text": "Compared with the vanilla Transformer , our FSAT significantly reduces the computational complexity with faster training speed and lower memory usage , which demonstrates that directly predicting the sparse structure of attention matrix is an effective way for building efficient Transformer architecture .", "Comments": [], "label": []}
{"id": 160407, "text": "We further thank NVIDIA for the access to GPUs .", "Comments": [], "label": []}
{"id": 154329, "text": "A.4 Details of Data Perturbation All our data perturbation methods are implemented based on the nlpaug [ 6 ] library .", "Comments": [], "label": []}
{"id": 152949, "text": "In both encoder and decoder Transformer layers , we use 8 attention heads , 512 embedding features , and 2048 features for the feed-forward inner-layers .", "Comments": [], "label": []}
{"id": 156070, "text": "3.1 Task Formulation We use ( X , Y ) to denote a training sample , where X is a sentence consisting of N words labeled by a set of triples Y = { < Y l k , Y k , Y k > } G−1 =0 .", "Comments": [], "label": []}
{"id": 157366, "text": "ViT improves over ResNet 152 by 6.4 % on micro-F1 .", "Comments": [], "label": []}
{"id": 159424, "text": "For VSG , we employ the FasterRCNN [ Ren et al. , 2015 ] as an object detector to obtain all the object nodes , and use MOTIFS [ Zellers et al. , , 2018 ] as a relation classifier to obtain the relation labels ( nodes ) as well as the relational edges , which is trained using the Visual Genome ( VG ) dataset [ Krishna et al. , , 2017 ] .", "Comments": [], "label": []}
{"id": 155680, "text": "For the optimizer , we use AdamW [ Loshchilov and Hutter , 2019 ] and choose learning rate from { 5×10− , 1× 10− , 2×10−5 } .", "Comments": [], "label": []}
{"id": 155478, "text": "We use average MAP score as the evaluation metric .", "Comments": [], "label": []}
{"id": 157173, "text": "The decoder then predicts the next token y given the previously decoded tokens y < m , as done in the standard Transformer decoder .", "Comments": [], "label": []}
{"id": 153213, "text": "Similarly , we use deep biaffine functions to score the labels of dependency arcs for a given gold or predicted tree [ 6 ] : where MLPparent and MLPchild are MLPs that map word representations into d 0 -dimensional spaces ; Wlabel ∈ R ( +1 ) × ( d +1 ) for each relation type r ∈ R in which R is the set of all relation types .", "Comments": [], "label": []}
{"id": 152890, "text": "We use the first 20M sentence pairs of the CCAligned corpus for Es-En and Ru-En language pairs as training data .", "Comments": [], "label": []}
{"id": 159417, "text": "We use the pre-trained language-vision model CLIP ( vit-base-patch32 ) to encode the visual and textual inputs .", "Comments": [], "label": []}
{"id": 155351, "text": "`` ` Montreal-Forced-Aligner 3https : //github.com/google/ `` ` `` ` sentencepiece `` ` < http : //opus.nlpl.eu/opus-100.php > [ https : //github.com/ ] ( https : //github.com/MontrealCorpusTools/Montreal-Forced-Aligner ) [ MontrealCorpusTools/ ] ( https : //github.com/MontrealCorpusTools/Montreal-Forced-Aligner ) Table 2 : BLEU scores on MuST-C tst-COMMON set .", "Comments": [], "label": []}
{"id": 155561, "text": "When M = 16 , K = 256 , D = 128 as in our experiments , if we use the product quantization with the hidden dimension h = 8 , the codebook size is 131 MB .", "Comments": [], "label": []}
{"id": 156149, "text": "Knowledge Composer ( KC ) The knowledge composer is a generative text-to-text transformer T5 [ Raffel et al. , , 2020 ] ( T5-large ) that can compose a set of facts and a rule to output a novel conclusion .", "Comments": [], "label": []}
{"id": 159732, "text": "3.3 Implementation For our FacialMMT framework , we employ either BERT [ Devlin et al. , , 2019 ] or RoBERTa [ Liu et al. , , 2019 ] as the textual encoder and use tiny version of Swin Transformer [ 1 ] .", "Comments": [], "label": []}
{"id": 156636, "text": "Compared to the COMET-QE baseline , LABSE improves the scores for hallucinations and correct translations , but drops quality for other pathologies .", "Comments": [], "label": []}
{"id": 152553, "text": "The tasks were trained with the Transformer [ Vaswani et al. , , 2017b ] , the current state-of-the-art neural model architecture for morphological tasks [ Vylomova et al. , 2020 ] [ Liu and Hulden , 2020b ] .", "Comments": [], "label": []}
{"id": 157871, "text": "Note that n = 1 for the task of COREF and n = |R| + 1 for RE , where we use a dummy class TH to learn a dynamic threshold for multi-label classification [ Zhou et al. , , 2021 ] .", "Comments": [], "label": []}
{"id": 154214, "text": "https : // [ www.tensorflow.org ] ( https : //www.tensorflow.org/ ) / We used keras tuner ( https : // [ keras-team.github.io ] ( https : //keras-team.github.io/keras-tuner/documentation/tuners/ ) / keras-tuner/ [ documentation ] ( https : //keras-team.github.io/keras-tuner/documentation/tuners/ ) /tuners/ ) Table 8 : Entity-level micro-averaged P , R , F1 ± std .", "Comments": [], "label": []}
{"id": 152296, "text": "4 Experiments 4.1 Datasets As shown in Table [ 1 , ] we employ two datasets widely-used with multiple sentences summary : CNN and Dailymail ( CNN/DM ) [ Hermann et al. , , 2015 ] and New York Times ( NYT ) [ Sandhaus , 2008 ] .", "Comments": [], "label": []}
{"id": 154862, "text": "We adopt * KL cost annealing * and * word dropout * tricks to alleviate the posterior collapse problem following [ Bowman et al. , 2016 ] .", "Comments": [], "label": []}
{"id": 157951, "text": "Given a sequence of learned prefixes P = { P1 , ... , Pt−1 } , we try several ways to utilize the knowledge acquired from preceding tasks in the hope that they could improve and accelerate the current task learning , 4 Experiments Datasets To demonstrate the generalizability of our approach , we use two kinds of datasets , differentiated according to the domain relevance between tasks .", "Comments": [], "label": []}
{"id": 153381, "text": "Transformer [ Vaswani et al. , , 2017 ] based multilingual models pre-trained on unlabeled data from multiple languages are the stateof-the-art means for cross-lingual transfer [ Ruder et al. , 2019 ] [ Devlin et al. , , 2019a ] .", "Comments": [], "label": []}
{"id": 155929, "text": "The specific contributions of this paper are the following : ( 1 ) We propose a method to sparsify Transformer architecture in a novel , previously unrecognized way , achieving sublinear time and memory complexity .", "Comments": [], "label": []}
{"id": 152865, "text": "For the GovReport dataset , we used λ * * = 0.5 , λ * * = 1 , λ * * = 1 .", "Comments": [], "label": []}
{"id": 154171, "text": "Specifically , we report character-level BLEU for En→Zh , case-insensitive BLEU score for Zh→En , and casesensitive BLEU score likewise for En↔De . 4.2 Implementation Details In this paper , we adopt the settings of standard * Transformer-Base * and * Transformer-Big * in [ Vaswani et al. , 2017 ] .", "Comments": [], "label": []}
{"id": 157162, "text": "To encode the emotional concept graph GEC , we adopt a vanilla Graph Transformer ( i.e. , omitting the relation enhancement part in the above Graph Transformer ) .", "Comments": [], "label": []}
{"id": 156470, "text": "5.3 Baselines and Training Details We adopt the following methods including recent SOTA as our baselines , which covers both * continual learning ( CL ) * and * Non-CL * methods .", "Comments": [], "label": []}
{"id": 151488, "text": "Thus , a suitable heuristic for selecting decoy entities s 0 and o 0 is to choose ones whose embeddings are dissimilar to es , eo .", "Comments": [], "label": []}
{"id": 158103, "text": "Table [ 7 ] lists detailed hyper-parameters we used in pretraining .", "Comments": [], "label": []}
{"id": 156630, "text": "To test this , we use the following methods : Unless stated otherwise , n = 10 in all experiments .", "Comments": [], "label": []}
{"id": 156695, "text": "4.1 Datasets For our analyses , we use instances of math word problems from three popular datasets : ASDiv-A [ Miao et al. , , 2020 ] , MAWPS [ Koncel-Kedziorski et al. , 2016 ] , and SVAMP [ Patel et al. , , 2021 ] .", "Comments": [], "label": []}
{"id": 158674, "text": "Hy-Transformer [ Yu and Yang , 2021 ] , GRAN [ Wang et al. , 2021 ] and QUAD [ Shomer et al. , , 2022 ] further improve it with alternative designs of encoders and via auxiliary training tasks .", "Comments": [], "label": []}
{"id": 151474, "text": "We use two publicly available benchmark datasets for link prediction - WN18RR and FB15k-237 .", "Comments": [], "label": []}
{"id": 151978, "text": "Formally , for a source document X and a target document Y , the probability model of Transformer can be written as and G-Transformer extends it by having where G and G denotes the two sequences of group tags where sent represents the k-th sentence of X or Y .", "Comments": [], "label": []}
{"id": 157702, "text": "3.2 Semantic Consistency Detection Since attention weights from the transformer could indicate the correlations and consistency between tweets , BIC adopts the attention weights to extract the semantic consistency information .", "Comments": [], "label": []}
{"id": 155512, "text": "We observe that the Transformer encoders give lower perplexity scores compared to LSTM regardless of pretraining language .", "Comments": [], "label": []}
{"id": 159662, "text": "We implement a library consisting of composable * actions * , * constraints * , and * combinators * .", "Comments": [], "label": []}
{"id": 158313, "text": "For each hard sample x i hard selected by the agent , we employ three approaches to construct positive and negative samples .", "Comments": [], "label": []}
{"id": 159594, "text": "PHILBERTA and We use the term `` pre-modern '' and `` Ancient '' interchangeably following the convention of calling every pre-modern language stage `` Ancient '' .", "Comments": [], "label": []}
{"id": 156727, "text": "First , we find that BLEU scores are highest for context-gold models for most language pairs , but context-agnostic models have higher COMET scores .", "Comments": [], "label": []}
{"id": 153190, "text": "First , and perhaps surprisingly , we notice that the use of `` Knowledge-Driven '' features based on rules built from linguistic knowledge of hedges in the LightGBM model outperforms the use of pre-trained embeddings within a fine-tuned BERT model ( 79.0 vs. 70.6 ) , and in the neural baseline from [ Goel et al. , , 2019 ] ( 79.0 vs 64.5 ) .", "Comments": [], "label": []}
{"id": 152286, "text": "These works usually instantiate their encoder-decoder architecture by choosing RNN [ Nallapati et al. , , 2017 ] [ Zhou et al. , , 2018 ] , Transformer [ Wang et al. , , 2019 ] [ Zhong et al. , , 2019b ] [ Liu and Lapata , 2019 ] [ Zhang et al. , , 2019b ] or GNN [ Wang et al. , , 2020 ] [ Jia et al. , , 2020b ] as encoder , autoregressive [ Jad ] [ hav and Rajan , 2018 ] [ Liu and Lapata , 2019 ] or RL-based [ Narayan et al. , , 2018 ] [ Arumae and Liu , 2018 ] [ Luo et al. , , 2019 ] decoders .", "Comments": [], "label": []}
{"id": 158468, "text": "Inspired by previous works on creating natural adversarial attacks [ Li et al. , ] [ 2020 , 2021a ] , we use a masked language model LM to generate possible word-level operations that can be applied to a sentence for introducing new words .", "Comments": [], "label": []}
{"id": 152289, "text": "In this paper , our sentence encoder builds on the BERT architecture [ Devlin et al. , , 2019 ] , a recently proposed highly efficient model which is based on the deep bidirectional Transformer [ Vaswani et al. , , 2017 ] and has achieved state-ofthe-art performance in many NLP tasks .", "Comments": [], "label": []}
{"id": 151484, "text": "Thus , we use a greedy approach to select the decoy entity o 0 .", "Comments": [], "label": []}
{"id": 152437, "text": "We use a sequence model with two-layer LSTM with 300 hidden units for both the encoder and the decoder .", "Comments": [], "label": []}
{"id": 155853, "text": "We use learning rate 1e-4 , batch size 16 , warm-up rate 0.01 , and train the model for 10 epochs .", "Comments": [], "label": []}
{"id": 159770, "text": "Therefore , we utilize an adaptive verbalizer to achieve this goal .", "Comments": [], "label": []}
{"id": 156058, "text": "V i ( c ) is defined as : M is a learnable matrix and we utilize Resnet18 [ He et al. , , 2016 ] to map I ( c ) into a onedimentional vector ( freezed during training ) .", "Comments": [], "label": []}
{"id": 153734, "text": "Table 4 : Performance with different knowledge integration methods ( QASC dev set , T5-11b inference model ) . our method still bridges the performance gap without referring to Google search .", "Comments": [], "label": []}
{"id": 160151, "text": "Model Setting We use the pre-trained HuBERT model [ 4 ] to encode the input audio .", "Comments": [], "label": []}
{"id": 157187, "text": "While available in multiple variants , in this study we use * LUKE ( base ) * , which has a total of 253M parameters .", "Comments": [], "label": []}
{"id": 153000, "text": "Although pruning algorithms here optimize the model size instead of inference efficiency , it is expected that the resulting sparse models still have speedup as shown in CPU and in other work [ Wang et al. , , 2020c ] .", "Comments": [], "label": []}
{"id": 153784, "text": "Pronoun ranking in particular may be more reliable for transformer models than Figure 5 : FPR gap ( downstream bias ) after scrubbing toxic mentions of identity terms from the WIKI finetuning dataset .", "Comments": [], "label": []}
{"id": 159774, "text": "If not specified , we use BERT-base-uncased [ Devlin et al. , , 2019 ] for most of our experiments .", "Comments": [], "label": []}
{"id": 155792, "text": "To alleviate the multimodality problem , [ Kaiser et al. , 2018 ] ; [ Shu et al. , 2019 ] ; [ Ma et al. , 2019 ] ; [ Bao et al. , 2021 ] propose Latent Transformer ( LT ) , introducing latent variables z for NAT predictions as : We use BPE segmentation in our experiments , and they are strictly tokens .", "Comments": [], "label": []}
{"id": 155122, "text": "Table 3 : Ablation Studies 6 Application Case Studies We use model forecasts for investment and risk management applications to evaluate the quality of forecasts .", "Comments": [], "label": []}
{"id": 156006, "text": "As a result , in some tasks whose sentence is short , we induce the T5 model to get some new information by adding extra sentences from other examples in the training data set .", "Comments": [], "label": []}
{"id": 155085, "text": "Second , CodeT5 implements the identifier tagging task as a binary classification ( 0/1 ) for each token , while our NT-MLM reconstructs the local ASTs out of hundreds of distinct types . 4.2.3 Contrastive Learning We adopt contrastive learning to focus on the functional characteristics of code .", "Comments": [], "label": []}
{"id": 152510, "text": "In this paper we follow the lead of [ Sundararajan et al. , , 2017 ] and use the nomenclature from [ Aumann and Shap ] [ ley , 2015 ] , which additionally introduces the axiom of Implementation Invariance .", "Comments": [], "label": []}
{"id": 156245, "text": "We use Fleiss ' Kappa ( κ ) [ Fleiss , 1971 ] to measure agreement among the annotators .", "Comments": [], "label": []}
{"id": 158171, "text": "We employ this test as a tool to measure the political leanings of pretrained language models .", "Comments": [], "label": []}
{"id": 153242, "text": "We adopt a pipeline approach and an end-to-end method for each integrated task separately .", "Comments": [], "label": []}
{"id": 158098, "text": "Specifically , we adopt PBD losses on the encoder and decoder when finetuning .", "Comments": [], "label": []}
{"id": 154187, "text": "The abundance of numeric tokens also leads to a very high ratio of out-of-vocabulary ( oov ) tokens , approx . 10.4 % when using a custom word2vec [ Mikolov et al. , , 2013a ] model trained on our corpus .", "Comments": [], "label": []}
{"id": 154866, "text": "We use beam search with a beam size of 4 .", "Comments": [], "label": []}
{"id": 152355, "text": "For evaluation , we used the data that were not used for training data : we left 10 % of the data for Yelp and 5 % for Amazon .", "Comments": [], "label": []}
{"id": 157389, "text": "[ Transformers : ] ( https : //doi.org/10.18653/v1/2020.emnlp-demos.6 ) [ State-of-the-Art Natural Language Processing .", "Comments": [], "label": []}
{"id": 154730, "text": "Abstract Transformers are unable to model long-term memories effectively , since the amount of computation they need to perform grows with the context length .", "Comments": [], "label": []}
{"id": 153350, "text": "To summarize , the main contributions of this work are as follows : ( 1 ) In prompt tuning , we adopt separate soft prompts to learn embeddings enriched with the domain knowledge , thus alleviating the domain discrepancy of the [ MASK ] position .", "Comments": [], "label": []}
{"id": 153238, "text": "To avoid the effect of structures and preserve the effect of initializations , we use the full BERT and re-initialize the weights that are not contained in the robust tickets .", "Comments": [], "label": []}
{"id": 155117, "text": "To forecast correlations of asset returns over the horizon period [ t , t + K′ ] , we use weights from linear layers in DW : Qcorr ( t ) = Linear-DW ( Z ′′′ ( t ) ) ; Kcorr ( t ) = Linear-DW ( Z ′′′ ( t ) ) .", "Comments": [], "label": []}
{"id": 159861, "text": "We consider two scenarios : The label name is multi-word ( i.e. , phrase ) and related words are still single-words To model the phrase , we use average contextualized embedding instead of word embedding for both label names and related single-words to compute cosine similarity .", "Comments": [], "label": []}
{"id": 159482, "text": "Borrowing these ideas , we adopted a mask-and-fill framework to enhance content preservation in text style transfer .", "Comments": [], "label": []}
{"id": 152709, "text": "We train Transformer for 100 epochs and select the best model w.r.t . their ROUGE scores on validation sets .", "Comments": [], "label": []}
{"id": 153417, "text": "Section [ 4.2 ] has further discussion.Table [ 4 ] contains the values corresponding to rows BPE , BPE-dp , CV , TokComp , OBPE ( −∞ ) averaged over LRLs and HRLs , Table [ 7 ] contains the values corresponding to rows Bsamp , Osamp averaged over LRLs and HRLs , , Figure [ 3 ] plots the rows correponding to varying p values .", "Comments": [], "label": []}
{"id": 159125, "text": "( ii ) BLEURT [ Sellam et al. , , 2020 ] and ( iii ) COMET variants [ Rei et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 154549, "text": "As mentioned in Section [ 4.4 , ] shorter summaries are involved in this dataset , and are more likely to result in higher ROUGE scores .", "Comments": [], "label": []}
{"id": 152011, "text": "Looking into the training process of the above experiments , we see that both the training and validation losses of G-Transformer converge much faster than Transformer , using almost half time to reach the same level of loss .", "Comments": [], "label": []}
{"id": 152195, "text": "In this paper , we analyze the capabilities of transformer-based language models on this unsupervised task , using benchmarks obtained from educational settings , as well as more commonly used datasets .", "Comments": [], "label": []}
{"id": 155230, "text": "We use 10 % of the training set as a separate development set during training with early-stop to prevent overfitting .", "Comments": [], "label": []}
{"id": 159048, "text": "Data Augmentation : We employ GPT2-XL [ Radford et al. , , 2019 ] as the data augmentation model with default settings , setting the maximum output length to 40 .", "Comments": [], "label": []}
{"id": 159569, "text": "Comparing mBART ( M2MS ) and mBART ( MLS ) , it is apparent to find that mBART ( M2MS ) significantly outperforms mBART ( MLS ) in cross-lingual directions ( * e.g . * , 26.9 vs. 11.7 ROUGE-1 in average ) , while achieving competitive results in monolingual directions ( * e.g . * , 33.9 vs. 34.2 ROUGE-1 in average ) .", "Comments": [], "label": []}
{"id": 160103, "text": "To do so , we use a leave-one-category-out paradigm , ensuring that all claims from the same category are confined to a single split .", "Comments": [], "label": []}
{"id": 151332, "text": "The VAE objective is the standard evidence lower bound ( ELBO ) , given by We use the usual Gaussian reparameterisation trick , and approximate the expectation in Equation [ 6 ] by sampling from the training set and updating via backpropagation [ Kingma and Welling , 2014 ] .", "Comments": [], "label": []}
{"id": 151432, "text": "We use a dataset collected by [ Cohen et al. , 2020 ] , via a mixed reality simulation platform in which novice teachers get to practice key classroom skills in a virtual classroom interface populated by student avatars .", "Comments": [], "label": []}
{"id": 151520, "text": "We utilize different types of datasets in different epochs .", "Comments": [], "label": []}
{"id": 151465, "text": "References [ deep bidirectional transformers for language under ] ( https : //doi.org/10.18653/v1/N19-1423 ) [ standing .", "Comments": [], "label": []}
{"id": 155452, "text": "In addition , we used token-based and BERT-based retrieval as comparison methods for obtaining examples relevant to EB-GEC outputs . [ Figure 4 ] shows the matching percentage of edits and error types between the GEC outputs and the k-nearest neighbors examples .", "Comments": [], "label": []}
{"id": 153315, "text": "To understand the effect of these extra parameters , we compare CipherDAug against the baseline Transformer model with different vocabulary and embedding sizes .", "Comments": [], "label": []}
{"id": 155756, "text": "We use hyperparameters αb1 , αb2 , and αb to determine their trade-offs .", "Comments": [], "label": []}
{"id": 156919, "text": "For MAGPIE and SemEval5B , we use their respective official random/typebased and train/test splits .", "Comments": [], "label": []}
{"id": 156805, "text": "[ 18 ] While the precise details of how the API works are not currently available ( e.g. , which parameters are updated , or which version of davinci is used ) , we use the same cross-validation setup as for the other models so that the results are comparable .", "Comments": [], "label": []}
{"id": 152397, "text": "Our HYPERFORMER++ obtains a better score on average compared to full fine-tuning and Adapters† , while being more parameter-efficient . ♠ : Our re-implementation of [ Raffel et al. , 2020 ] , ❣ : Applying method of [ Houlsby et al. , 2019 ] on T5 .", "Comments": [], "label": []}
{"id": 152331, "text": "To initialize the gate values at approximately 0 , we use ReLU ( tanh ( x ) ) as φ ( x ) .", "Comments": [], "label": []}
{"id": 160378, "text": "For XM Transformer model , we use a learning rate of 0.0001 , a dropout of 0.1 and a label smoothing weight of 0.2 .", "Comments": [], "label": []}
{"id": 157054, "text": "Although we use perplexity to filter out those paraphrases , there must be some imperfect paraphrases remaining .", "Comments": [], "label": []}
{"id": 151825, "text": "i ) Raw+kmeans performs K-means on raw BoW vectors , and PCA+kmeans uses PCA extract low-dimensional features and then uses K-means for clustering ; ii ) Train a topic model and then perform Kmeans for clustering on topic proportions , where we consider LDA+kmeans [ Blei et al. , , 2003 ] , AVITM+kmeans [ Srivastava and Sutton , 2017 ] , and PFA+kmeans [ Zhou et al. , , 2012 ] ; iii ) Deep neural network based clustering methods , including Deep clustering [ Xie et al. , , 2016 ] , and DCN [ Yang et al. , , 2017 ] , which jointly consider the feature extracting and clustering .", "Comments": [], "label": []}
{"id": 155897, "text": "[ https : //www.apache.org/licenses/ ] ( https : //www.apache.org/licenses/LICENSE-2.0 ) [ LICENSE-2.0 ] ( https : //www.apache.org/licenses/LICENSE-2.0 ) < https : //opensource.org/licenses/MIT > [ https : //github.com/google-research/ ] ( https : //github.com/google-research/google-research/tree/master/rouge ) [ google-research/tree/master/rouge ] ( https : //github.com/google-research/google-research/tree/master/rouge ) Example 1 Q1 : What incited the start of the FY2009 appropriation process ?", "Comments": [], "label": []}
{"id": 157542, "text": "T5 + fine-grained steps + ordered / random marker : The training data in this setting follow a similar format as the exemplar in * GPT3 + finegrained steps + ordered marker * , but the positional markers can be in random order .", "Comments": [], "label": []}
{"id": 154761, "text": "[ 8 ] For these experiments , we consider transformers with input size L = 512 , for the compressive transformer we use a compressed memory of size 512 , and for the ∞-former we consider a LTM with N = 512 Gaussian RBFs and a memory threshold of 2,048 tokens , having the same computational budget for the two models .", "Comments": [], "label": []}
{"id": 156907, "text": "Our implementation also supports an automatic notification system that ensures subjects join each time slot on time .", "Comments": [], "label": []}
{"id": 151438, "text": "< https : //huggingface.co/ > Table 9 : Mapping between uptake phenomena and tags from SWBD-DAMSL [ Jurafsky et al. , , 1997 ] .", "Comments": [], "label": []}
{"id": 158394, "text": "In all our experiments , we use AdamW optimizer with 5k warmup steps , learning rate 3e-5 , weight decay of 0.01 , max grad norm of 0.1 , and bfloat16 .", "Comments": [], "label": []}
{"id": 153894, "text": "We use the < https : //github.com/chho33/LAMOL > < https : //github.com/voidism/L2KD > architecture from [ Houlsby et al. , 2019 ] in Adapter-Hub [ Pfeiffer et al. , , 2020 ] with its default setting , in which the reduce factor for bottle-neck architecture is 16 .", "Comments": [], "label": []}
{"id": 160298, "text": "However , this is not the most optimal choice , as matrix computations can be accelerated by GPUs .", "Comments": [], "label": []}
{"id": 158043, "text": "We use two annotated tags from PBP the data : `` * name_mention * '' and `` * reply_to * '' to locate the DM posts that address the player who makes the ability check .", "Comments": [], "label": []}
{"id": 159015, "text": "For datasets where we use a subsample of the test set , we use the random seed 1 to first shuffle and then evaluate on first 1000 instances .", "Comments": [], "label": []}
{"id": 156072, "text": "We use P δ = [ P δ i0 , P i1 , · · · , P iN−1 ] , δ ∈ { l , r } to weigh all words and then concatenate them with instance queries .", "Comments": [], "label": []}
{"id": 159998, "text": "S2SpecT We used a six-layer Transformer spectrogram decoder .", "Comments": [], "label": []}
{"id": 155854, "text": "D Hard Prompts In this section , we describe the hard prompts we use in Hybrid PT and Hybrid PPT .", "Comments": [], "label": []}
{"id": 154732, "text": "In this paper , we propose the ∞ * -former * , which extends the vanilla transformer with an * unbounded * longterm memory .", "Comments": [], "label": []}
{"id": 158904, "text": "XGBoost tends to make contradictory predictions in terms of the lowest scores in three settings .", "Comments": [], "label": []}
{"id": 154421, "text": "For evaluation , we use the same metrics as used in PLATO , except for knowledge-related metrics , since this paper does not focus on utilizing knowledge .", "Comments": [], "label": []}
{"id": 155467, "text": "Unidirectional Language Modeling We use unidirectional language modeling ( ULM ) pretraining task to pre-train decoder-only mode for supporting auto-regressive tasks like code completion , as shown in Figure [ 2 ] ( b ) .", "Comments": [], "label": []}
{"id": 154586, "text": "We compute performance measures and learning curves on development sets following prior work [ Rajpurkar et al. , 2016 ] [ Ram et al. , , 2021 ] .", "Comments": [], "label": []}
{"id": 158067, "text": "For shallow MP , we use µ = 0 and σ = 0.1 for initialization of the CMA-ES .", "Comments": [], "label": []}
{"id": 155303, "text": "Next , we use the context representation of the fact statement , h¯ , to query all candidate articles in order to mine the most relevant semantics in the article texts .", "Comments": [], "label": []}
{"id": 159461, "text": "Similarly to [ NLLB Team et al. , 2022 ] , we add a tag ( e.g . < HQ > or < LQ > ) to the beginning of the source sentence , so that the model can learn to distinguish between higher quality ( WMT21 , Opus-100 , and Tatoeba ) and lower quality examples ( CCMatrix ) .", "Comments": [], "label": []}
{"id": 157721, "text": "During inference we use τ = −1.3890 and τ = 1.0030 for RnG-KBQA A and τ = −0.7682 and τ = 1.0230 for RnG-KBQA A+U .", "Comments": [], "label": []}
{"id": 152702, "text": "We use the official splits of [ Narayan et al. , 2018 ] .", "Comments": [], "label": []}
{"id": 156403, "text": "1 Introduction Trained on huge amount of text corpora , Transformer-based [ Vaswani et al. , , 2017 ] pretrained language models ( LMs ) have led to a wave of advances in natural language generation tasks [ Radford et al. , 2019 ] ; [ Lewis et al. , 2019 ] ; [ Roberts et al. , 2019 ] ) .", "Comments": [], "label": []}
{"id": 160243, "text": "Did you report the number of parameters in the models used , the total computational budget ( e.g. , GPU hours ) , and computing infrastructure used ?", "Comments": [], "label": []}
{"id": 151999, "text": "G-Transformer+BERT improves the scores by margin of 0.20 , 1.62 , and 0.47 s-BLEU points on TED , News , and Europarl , respectively .", "Comments": [], "label": []}
{"id": 159723, "text": "Next , we use a ResNet-50 model [ He et al. , , 2016 ] pre-trained on a face recognition dataset MS-Celeb-1M [ Guo et al. , , 2016 ] to extract visual features for the images in the library and in different face clusters .", "Comments": [], "label": []}
{"id": 156960, "text": "3 Experimental Settings 3.1 Datasets Following prior work on text style transfer , we use two common datasets : Yelp Review Dataset ( Yelp ) and IMDb Movie Review Dataset ( IMDb ) .", "Comments": [], "label": []}
{"id": 154881, "text": "Recent methods based on Transformer [ Vaswani et al. , , 2017 ] achieve better performance .", "Comments": [], "label": []}
{"id": 157455, "text": "Finally , FormNetV1 includes a graph convolutional network ( GCN ) contextualization step * before * serializing the text to send to the ETC transformer component .", "Comments": [], "label": []}
{"id": 153958, "text": "On January 13 , 1966 , the Rams traded smith to the Detroit Lions for Paul Hornung , and later that year he was traded to the Lions for Ray `` the Lion '' Jones in exchange for Linebacker Jim `` the Hawk '' Johnson .", "Comments": [], "label": []}
{"id": 159952, "text": "More details are described in Appendix [ E. ] 3.3 Pre-training We use the same En/Es wav2vec2.0 and En-Es umBART models as [ Popuri et al. , 2022 ] .", "Comments": [], "label": []}
{"id": 154396, "text": "Besides the difference in the modality and tasks , our method also differs from theirs in ( 1 ) * Model : * We quantize the model parameters and activations while they do not ; ( 2 ) * Representation : * For each sample , we use the output of the full-precision and the quantized networks as its two views , while they use the quantized and the contextualized representation .", "Comments": [], "label": []}
{"id": 158895, "text": "These metrics are calculated as follows : For a given confusion matrix : We use both Profit Ratio and Sharpe Ratio for the trading simulation task as performance indicators .", "Comments": [], "label": []}
{"id": 153606, "text": "3.1 BERT-BiLSTM-CRF Baseline Given a sentence x = x · · · x ( where n denotes the sentence length ) , we first convert it into contextual representations r · · · r by the pre-trained BERT with adapter tuning [ Houlsby et al. , , 2019 ] : Unlike the standard BERT exploration , AD-BERT introduces two extra adapter modules inside each transformer layer , as shown in Figure [ 2 ] for the Figure 2 : The Adapter ( right ) and Transformer integrated with Adapter inside ( left ) .", "Comments": [], "label": []}
{"id": 155868, "text": "To linearize a QS hierarchy for the Transformer sequential decoder , we concatenate its QS pairs following a depth-first traversal .", "Comments": [], "label": []}
{"id": 157897, "text": "More precisely , inspired by Ke et al . ( 2022 ) , we train two models based on T5 by a text infilling task to score the dialogue flow F s and the synthetic dialogue U s , respectively .", "Comments": [], "label": []}
{"id": 158948, "text": "We utilize SimCSE-BERTbase and SimCSE-BERTlarge as a multi-teacher for RankCSE-BERTbase and RankCSE-BERTlarge , while SimCSE-RoBERTabase and SimCSE-RoBERTalarge as a multi-teacher for RankCSE-RoBERTabase and RankCSE-RoBERTalarge .", "Comments": [], "label": []}
{"id": 159525, "text": "We frst propose three variants that incorporate powerful PLMs , * i.e. , * RoBERTa-base , T5- Large , and T5-3b respectively , to replace the text encoder of CLIP in our framework .", "Comments": [], "label": []}
{"id": 155093, "text": "We use accuracy as the metric , following the design of the benchmark .", "Comments": [], "label": []}
{"id": 159495, "text": "More ablation results shown in Appendix [ E. ] This means Style Transformer only takes the target style signals as noise , which may result from the stylistic features existing in the contents .", "Comments": [], "label": []}
{"id": 155863, "text": "Again , our system summaries obtain uniformly higher ROUGE scores than comparisons , demonstrating the generalizability of HIBRIDS . 2 Related Work Document Structure-aware Summarization .", "Comments": [], "label": []}
{"id": 151765, "text": "Specifically , we performed our approach by sampling 8M sentences from the 40M monolingual data and then combining the corresponding 8M synthetic data with the 8M bitext to train the TRANSFORMER-BASE model .", "Comments": [], "label": []}
{"id": 152230, "text": "Besides static word embeddings , we have assessed the ability of state-of-the-art monolingual and multilingual models based on the Transformers architecture to identify unambiguous cases of homonymy and synonymy .", "Comments": [], "label": []}
{"id": 160143, "text": "Did you report the number of parameters in the models used , the total computational budget ( e.g. , GPU hours ) , and computing infrastructure used ? * We utliize BERT and RoBERTa , and cite their papers where these parameters are listed .", "Comments": [], "label": []}
{"id": 159665, "text": "We experiment with two ways of implementing the two steps : we either fine-tune two separate T5-base models [ Raffel et al. , , 2022 ] that run in a pipeline for each command , or we prompt GPT3 [ Brown et al. , , 2020 ] [ 20 ] to generate both the normalized utterance [ 21 ] and the interpretation output in a single inference step .", "Comments": [], "label": []}
{"id": 154042, "text": "Following [ Xu et al. , 2015 ] , we use the ResNet50 [ He et al. , , 2016 ] as the image encoder and a unidirectional LSTM [ Hochreiter and Schmidhuber , 1997 ] as the decoder for text .", "Comments": [], "label": []}
{"id": 153639, "text": "We also report performance on Trivia QA , whose pre-processed version is released in DPR . 4.2.1 Setup Dataset We use MS-MARCO passage ranking [ Bajaj et al. , , 2018 ] , Natural Question ( NQ ; [ Kwiatkowski et al. , 2019 ] ) and Trivia QA ( TQA ; [ Joshi et al. , 2017 ] ) .", "Comments": [], "label": []}
{"id": 158233, "text": "Empirically , having a smaller summaization model ( BART-small finetuned on summarization data ) as the amateur LM yields lower ROUGE score than employing a uniform distribution as the amateur LM , which is equivalent to beam search based on log-probabilities .", "Comments": [], "label": []}
{"id": 153047, "text": "We use an uncased BERT-large whole word masking version as the encoder .", "Comments": [], "label": []}
{"id": 155086, "text": "More specifically , we have a minibatch of N programs , and for each program , we extract the sequence representation from the Transformer outputs h = h [ CLS ] .", "Comments": [], "label": []}
{"id": 157818, "text": "Following the evaluation setting from each dataset we use a fuzzy matching score for slot values in SGD and exact match in MultiWOZ .", "Comments": [], "label": []}
{"id": 154107, "text": "DNPG [ Li et al. , , 2019 ] is a paraphrase generation system that uses a cascade of Transformer encoders/decoders to control whether paraphrasing is sentential/phrasal .", "Comments": [], "label": []}
{"id": 154016, "text": "We use the tuning data as training data and reserve 10 % of the test data , i.e . 12 examples , to create a development/validation set . 19.83 % of the documents in the dataset have no templates .", "Comments": [], "label": []}
{"id": 151854, "text": "Second , previous literature [ Fan et al. , , 2020 ] [ Michel et al. , , 2019 ] [ Zhou et al. , , 2020 ] pointed out that large PLMs with dozens of stacked Transformer layers are over-parameterized and could suffer from the `` overthinking '' problem [ Kaya et al. , , 2019 ] .", "Comments": [], "label": []}
{"id": 156271, "text": "We use the coreference matrix and the word embeddings to construct a directed and labeled graph G = ( V , E , R ) , with nodes ( subwords ) v ∈ V , edges ( relations ) ( v , r , v ) ) ∈ E , where r ∈ R is one of the two relation types ( 1 indicates coreference relation and self-loop ; 2 indicates global relation ) , as shown in Figure [ 3 ] .", "Comments": [], "label": []}
{"id": 160266, "text": "XMAI 's poor BLEU scores can be largely attributed to BLEU 's tendency to penalize novel insertions severely compared to removals or replacements , as it is a precision- < https : //www.nltk.org/ > Table 1 : Results on the text-to-image retrieval task .", "Comments": [], "label": []}
{"id": 154286, "text": "As such , we did not need GPUs for training ( we finetuned two classifier for demonstration purposes , which took less than two GPU hours ) .", "Comments": [], "label": []}
{"id": 160417, "text": "It has two main advantages , it can produce ] ( https : //huggingface.co/T-Systems-onsite/cross-en-de-roberta-sentence-transformer ) n : m [ alignments , and it can work with more than ] ( https : //huggingface.co/T-Systems-onsite/cross-en-de-roberta-sentence-transformer ) [ 200 languages ( as it uses the LASER ] ( https : //huggingface.co/T-Systems-onsite/cross-en-de-roberta-sentence-transformer ) [ 32 ] [ Artetxe ] [ and Schwenk , ] [ 2019 ) sentence representation model ] ( https : //huggingface.co/T-Systems-onsite/cross-en-de-roberta-sentence-transformer ) [ in the background ; which is multilingual ) .", "Comments": [], "label": []}
{"id": 156391, "text": "We used a pre-trained Transformer big model trained from JParaCrawl v3 [ Morishita et al. , , 2022 ] and evaluated its performance on Asian Scientific Paper Excerpt Corpus ( ASPEC ) [ Nakazawa et al. , , 2016 ] and Kyoto Free Translation Task ( KFTT ; created from Wikipedia 's Kyoto articles ) [ Neubig , 2011 ] .", "Comments": [], "label": []}
{"id": 160292, "text": "As the transformer model treats each token as independent , position embedding is added to retain the order information of tokens .", "Comments": [], "label": []}
{"id": 156813, "text": "We follow [ Vu et al . ' ] s [ 2020 ] calculation setting , except that we use FinBERT instead of BERT : we first calculate task and text embeddings of FinDATA and GLUE tasks .", "Comments": [], "label": []}
{"id": 151588, "text": "The fact that we use our model to generate false claims also helps to address the concerns of biased language generation .", "Comments": [], "label": []}
{"id": 157976, "text": "Experimental Setup To estimate context sensitivity , we use our most accurate model to label a large selection of dialog turns from the PRIDE dataset [ Tigunova et al. , , 2021 ] .", "Comments": [], "label": []}
{"id": 154563, "text": "Extensive experiments conducted on four challenging datasets show our approach achieves consistent improvements without restrictions on feature distribution .", "Comments": [], "label": []}
{"id": 153223, "text": "These two tricks have been used in the parsing literature to accelerate parsing .", "Comments": [], "label": []}
{"id": 152240, "text": "Our implementation is available at [ https : // ] ( https : //github.com/Weixin-Liang/HERALD/ ) [ github.com/Weixin-Liang/HERALD/ ] ( https : //github.com/Weixin-Liang/HERALD/ ) . 1 Introduction Evaluation metrics heavily influence a field 's research direction .", "Comments": [], "label": []}
{"id": 159036, "text": "A set of sentences which have the highest scores based on ROUGE [ Lin , 2004 ] ranking , is then selected as key points .", "Comments": [], "label": []}
{"id": 155891, "text": "We use Adam [ Kingma and Ba , 2015 ] as the optimizer , with a maximum learning rate of 5 × 10− .", "Comments": [], "label": []}
{"id": 158924, "text": "We use that same held-out set in this work .", "Comments": [], "label": []}
{"id": 152196, "text": "To solve such problems , [ Turney , 2005 ] proposed Latent Relational Analysis ( LRA ) , which was essentially designed as a relational counterpart to Latent Semantic Analysis [ Landauer and Dumais , 1997 ] .", "Comments": [], "label": []}
{"id": 158844, "text": "In our work , we experiment with prompt-based and finetuning methods . 3 Identifying and Reframing Unhelpful Thoughts We use the ten categories of unhelpful thought patterns described in lay terms in a widely used CBT self-help book used for bibliotherapy [ Burns , 1980 ] .", "Comments": [], "label": []}
{"id": 154078, "text": "Given the imbalanced nature of our dataset , we adopt both the Area Under the Curve for the ROC plot [ Fawcett , 2004 ] ( AUROC ) and macro F1 score as our evaluation metrics .", "Comments": [], "label": []}
{"id": 154305, "text": "We use the stronger , entire system rather than the weaker DELETEONLY and RETRIEVEONLY baselines ; b ) ARAE : Adversarially regularized autoencoders our system is based on [ Zhao et al. , , 2018b ] ; c ) ARAE * seq2seq * : Our model without the contrastive learning or cooperative classifier ; d ) ARAE * seq2seq * + CONTRA : Our model with the contrastive learning ; e ) ARAE * seq2seq * + CLF : Our model with the cooperative classifier ; f ) ARAE * seq2seq * +CLF+CONTRA : Our model with both the cooperative losses .", "Comments": [], "label": []}
{"id": 159792, "text": "But we then propose an approach to training Flan-T5 with * Chain-of-Thought * ( CoT ) style `` explanations '' ( generated automatically by GPT-3 ) that support relation inferences ; this achieves SOTA results . 3 .", "Comments": [], "label": []}
{"id": 152662, "text": "We use both the definitions and the word usage examples for fine-tuning , producing a final dataset of 200 , 000 sequences .", "Comments": [], "label": []}
{"id": 156752, "text": "B Enumerating L-homomorphisms A simple algorithm is given below : C Implementation Details C.1 VQVAE Details We use a discrete variational auto-encoder [ van den ] [ Oord et al. , , 2017 ] to encode the images 16 × 16 grids of discrete codes .", "Comments": [], "label": []}
{"id": 154595, "text": "We use the seed 46 sets publicly available at [ https : ] ( https : //github.com/oriram/splinter ) [ //github.com/oriram/splinter ] ( https : //github.com/oriram/splinter ) .", "Comments": [], "label": []}
{"id": 152610, "text": "Although they augment the data about 20 times depending on the GLUE task , we observed poor results if we use all this data to fine-tune with KD .", "Comments": [], "label": []}
{"id": 158775, "text": "For evaluation , we use beam search with a width of 5 .", "Comments": [], "label": []}
{"id": 158203, "text": "We use this value as a baseline : generation n-gram overlap should be Figure 15 : * Outlier Exposure is sensitive to the size of the novel set on TREC-10 . * We vary the novel set size from 0 to 100K , finding that both accuracy and AUROC decrease with as few as 100 novel generations .", "Comments": [], "label": []}
{"id": 153552, "text": "Specifically , for each quantity , the input sequence to T5 is the string of the previous 3 sentences , the current sentence with a special marker token right before the quantity span , the next 3 sentences , the title , and document creation time ( DCT ) .", "Comments": [], "label": []}
{"id": 159441, "text": "Particularly relevant is the shared-encoder , separate-decoder architecture proposed by [ Dong et al. , 2015 ] which we use as the base for some of our experiments .", "Comments": [], "label": []}
{"id": 154281, "text": "We used the parameters α= 100 , β = 50 , θ= 100 , where θ is the coefficient assigned to the agency scorer , and α and β are defined in Equations [ 1 ] and [ 2 . ] B.7 Sentiment Transfer : Hyperparameters In this section we discuss the hyperparameters used for sampling and see the effects of each one .", "Comments": [], "label": []}
{"id": 152501, "text": "In a baseline implementation ( no learning ) , we use the scoring function in Figure [ 5 ] c ) , where rw denote manually assigned rule weights , while fw are manually assigned feature weights .", "Comments": [], "label": []}
{"id": 153453, "text": "PGN-both method achieves 0.90 points of ROUGE-2 and 1.29 points of MoverScore improvement , while BERT-both achieves 0.76 points of ROUGE-2 and 0.66 points of MoverScore improvement .", "Comments": [], "label": []}
{"id": 155481, "text": "The pre-training multi-modal data we use includes 2.3M functions paired with comments from CodeSearchNet dataset [ Husain et al. , , 2019 ] for six programming languages ( i.e . ruby , java , python , php , go and javascript ) .", "Comments": [], "label": []}
{"id": 159885, "text": "To enable a controlled experiment , we use the same feature mixing module in all models ; the models only differ in their token mixing module .", "Comments": [], "label": []}
{"id": 158401, "text": "A.3 Automated Evaluation Details We perform an automated evaluation using Rouge and BERTScore metrics following best practices from previous work .", "Comments": [], "label": []}
{"id": 153229, "text": "We batch sentences of similar lengths to better utilize GPUs .", "Comments": [], "label": []}
{"id": 159778, "text": "For a fair comparison , we evaluate on RoBERTalarge [ Ott et al. , , 2019 ] , the same architecture used in BadPrompt , and we use the same poisoning rate ( i.e. , 10 % ) in BadPrompt and our method .", "Comments": [], "label": []}
{"id": 156940, "text": "We utilize the cross-entropy loss to optimize the crossmodal response selection task , denoted as LCRS .", "Comments": [], "label": []}
{"id": 154775, "text": "We also experiment training the compressive transformer with and without the attention reconstruction auxiliary loss .", "Comments": [], "label": []}
{"id": 157099, "text": "The input dimension and the feed-forward network dimension of a Transformer block are set to 512 , and each block contains 8 attention heads .", "Comments": [], "label": []}
{"id": 160308, "text": "Recent models , such as BERT [ Devlin et al. , , 2018 ] or RoBERTa [ Liu et al. , , 2019 ] , are more and more taking advantage of the huge quantities of unlabeled data thanks to recent unsupervised approaches like masked language models based on the Transformers architecture [ Vaswani et al. , , 2017 ] .", "Comments": [], "label": []}
{"id": 152964, "text": "[ Min et al. , 2021 ] adopted a generative encoder-decoder reader initialized with T5-3b , and used the fusion-in-decoder method from [ Izacard and Grave , 2021b ] which efficiently aggregates evidence from multiple passages .", "Comments": [], "label": []}
{"id": 160382, "text": "Multilingual training brings further gains to XM Transformer with +7.9 and +5.1 BLEU over bilingual training in EP/VP and FLEURS test set respectively .", "Comments": [], "label": []}
{"id": 160423, "text": "] ( https : //huggingface.co/T-Systems-onsite/cross-en-de-roberta-sentence-transformer ) MASSAlign [ Paetzold et al. , ] [ 2017 ) is a Python ] ( https : //huggingface.co/T-Systems-onsite/cross-en-de-roberta-sentence-transformer ) [ package which includes an easy-to-use alignment ] ( https : //huggingface.co/T-Systems-onsite/cross-en-de-roberta-sentence-transformer ) [ method on the paragraph- and sentence-level by ] ( https : //huggingface.co/T-Systems-onsite/cross-en-de-roberta-sentence-transformer ) [ Paetzold and Specia ] [ ] ( https : //huggingface.co/T-Systems-onsite/cross-en-de-roberta-sentence-transformer ) [ 2016 ] [ ) .", "Comments": [], "label": []}
{"id": 156646, "text": "For research , it can motivate future work on model understanding ; for practitioners , it means that hallucination mitigation is not limited to language pairs where external models such as COMET-QE exist : model understanding might be enough .", "Comments": [], "label": []}
{"id": 159067, "text": "One possible explanation is that ROUGE focusses on the overlap of n-grams rather than on semantic similarity , resulting in the fact that summaries that repeat words appearing in the reference , but with a lower semantic similarity overall , may receive higher scores .", "Comments": [], "label": []}
{"id": 155616, "text": "To regularize both under-fitting and over-fitting , we use prediction difference as a regularization term ( PD-R ) and apply it on word-dropout regularization .", "Comments": [], "label": []}
{"id": 153817, "text": "Table [ 1 ] shows sequence-level classification accuracy for * small * Transformers ( l = 2 , d = 64 , f = 256 , h = 4 ) .", "Comments": [], "label": []}
{"id": 153505, "text": "Secondly , the output should be L vectors which is too large to be attended in the Transformer model .", "Comments": [], "label": []}
{"id": 155342, "text": "As shown in Table [ 4 , ] the cross-taught students underperform the static students taught by their own teachers by 0.27 and 0.12 for ResNet-32 and ResNet-20 , respectively .", "Comments": [], "label": []}
{"id": 151391, "text": "We can find that MECT further improves the performance of BERT significantly . 5.3 Effectiveness of Cross-Transformer There are two sub-modules in the proposed Cross-Transformer method : lattice and radical attentions .", "Comments": [], "label": []}
{"id": 156147, "text": "Similar to ProofWriter , we use synthetic rulebases to train FAIRR .", "Comments": [], "label": []}
{"id": 153078, "text": "The statistics of the datasets are shown in Table [ 2 . ] Example sentences with the corresponding annotations can be seen in Table [ 1 . ] Setup In implementing the edge probe , we use batch size = 32 and learning rate = 5e-5 and train for five epochs in all experiments .", "Comments": [], "label": []}
{"id": 154432, "text": "The latent variable is incorporated into the sequence-to-sequence framework based on Transformer , and obtains a robust and diverse response generation model through 4 training targets .", "Comments": [], "label": []}
{"id": 158833, "text": "The selfattention and the multi-head attention mechanism in Transformers encode each input w.r.t all other inputs , enabling the use of context and considering the relationship between words which is beyond matching sole words .", "Comments": [], "label": []}
{"id": 159703, "text": "Additionally , we use GLUE benchmark and SQuAD2.0 to evaluate the English performance of our model .", "Comments": [], "label": []}
{"id": 156293, "text": "When creating its synthetic dataset , BLEURT automatically annotates the ( original , perturbed ) sequence pairs with numerical and categorical `` signals '' : BLEU , ROUGE , BERTscore , backtranslation likelihood , textual entailment ( probability of three labels : entail , contradict , and neutral , given by BERT fine-tuned on MNLI ) , and backtranslation flag .", "Comments": [], "label": []}
{"id": 152140, "text": "Next , we remove all the tracking information except the own path for a record , to explore whether the tracking of other records makes effect indeed ( GIT-Own Path ) .", "Comments": [], "label": []}
{"id": 160098, "text": "5 Methods To study the two proposed tasks , we consider two experimental settings : ( i ) extracting claim representations by using embeddings as input to an SVM [ Joachims , 2006 ] , ( ii ) adding a classifier layer on top of pre-trained transformer models with further fine-tuning ( FT ) .", "Comments": [], "label": []}
{"id": 154035, "text": "Recently , [ Chen et al. , 2019 ] used the ResNet [ He et al. , , 2016 ] to encode the chart image and an LSTM decoder to create the caption .", "Comments": [], "label": []}
{"id": 154135, "text": "In Table [ 4 , ] we use SimKGCIB with batch size 256 as a baseline .", "Comments": [], "label": []}
{"id": 155495, "text": "Inspired by the success of cross-attention [ Kim et al. , , 2018 ] in associating different sources , * e.g. , * image-image [ Hou et al. , , 2019 ] and image-text [ Lu et al. , , 2019 ] , we adopt cross-attention between the factual representation and the assumption representation , followed by self-attention respectively .", "Comments": [], "label": []}
{"id": 152958, "text": "In the rich-data condition case , as we did not see benefits by the average of the checkpoints , we used the best checkpoint instead .", "Comments": [], "label": []}
{"id": 153688, "text": "Implications are templated in the We use news articles from 2021 and the last two months of 2020 for the test set .", "Comments": [], "label": []}
{"id": 152049, "text": "To encourage inferencing the translation from the context , we apply a word-dropout Table 7 : G-Transformer on different model size . [ Bowman et al. , , 2016 ] with a probability of 0.3 on both the source and the target inputs .", "Comments": [], "label": []}
{"id": 151381, "text": "The Cross-Transformer network is illustrated in Figure [ 2b . ] We use two Transformer encoders to cross the lattice and radical information of Chinese characters , which is different from the selfattention method in Transformer .", "Comments": [], "label": []}
{"id": 159328, "text": "Following [ Wu et al . ] [ 2019 , 2020 ] , for Machine Translation and Abstractive Summarization , we adopt the same 8-head encoderdecoder architecture with 6 layers for both encoder and decoder , where the model dimension dmodel = 512 and feed-forward dimension d = 2048 .", "Comments": [], "label": []}
{"id": 157638, "text": "In particular , we use N = 10 for our main ITR variant and ITRngram , while for the column and row only ITR variants , we set N = 5 and N = 10 .", "Comments": [], "label": []}
{"id": 152711, "text": "It takes about 45 minutes for one epoch , and we need 6 epochs in total . 4.3 Evaluations We evaluate the quality of different summarization systems using ROUGE .", "Comments": [], "label": []}
{"id": 151428, "text": "We use ( S , T ) pairs from three sources to form our training data : the NCTE dataset [ Kane et al. , , 2015 ] ( Section [ 3 ] , Switchboard [ God ] [ frey and Holliman , 1997 ] and a one-on-one online tutoring dataset ( Section [ 6 ] — we use a combination of datasets instead of one dataset in order to support the generalizability of the model .", "Comments": [], "label": []}
{"id": 153926, "text": "We note that T5 is pretrained with a similar blank infilling objective .", "Comments": [], "label": []}
{"id": 156639, "text": "[ 9 ] We compare the original translations and three reranking methods : the baseline COMET-QE used in [ Guerreiro et al. , 2022 ] , the best overall reranker LaBSE , and the only internal method ALTI .", "Comments": [], "label": []}
{"id": 158524, "text": "To jointly evaluate a generated summary and its aligned evidence sentences in a single metric , we adopt ROUGE scores by merely viewing the addressed task as a unified sequence `` generation '' task .", "Comments": [], "label": []}
{"id": 154088, "text": "[ Tietze , 2018 ] ) or implemented in practice [ Lu et al. , , 2017 ] , most work focus on determining patent content classes to save manpower or concern only with patent grants rather than applications [ Verberne et al. , 2010 ] [ D'hondt et al. , , 2013 ] [ Hu et al. , , 2016 ] [ Balsmeier et al. , , 2018 ] [ Lee and Hsiang , 2019 ] .", "Comments": [], "label": []}
{"id": 154980, "text": "We use the official split to get the training and development set .", "Comments": [], "label": []}
{"id": 158935, "text": "To test this hypothesis , we implemented the binary heuristic top n-gram count [ Raunak et al. , , 2021 ] [ Guerreiro et al. , , 2022 ] to verify whether a translation is a severe oscillation : given the entire Dheld , a translation is flagged as an oscillatory hallucination if ( i ) it is in the set of 1 % lowest-quality translations according to CometKiwi and ( ii ) the count of the top repeated 4-gram in the translation is greater than the count of the top repeated source 4-gram by at least 2 .", "Comments": [], "label": []}
{"id": 153260, "text": "A.1 Spatial Relations in Visual Dependency Grammar We use the rules defined in Visual Dependency Grammar [ Elliott and Keller , 2013 ] to determine the positional relationship between bounding boxes .", "Comments": [], "label": []}
{"id": 156027, "text": "The quadratic time and space complexity of the attention mechanism with respect to the input sequence serves the main efficiency bottleneck of Transformers , and thus is prohibitively expensive for training long-sequence data .", "Comments": [], "label": []}
{"id": 154822, "text": "In the following sections , we denote the models built with our proposed framework as HY-BONET .", "Comments": [], "label": []}
{"id": 158387, "text": "On the summarization side , we find a more significant improvement with up to +1.9 Rouge-1 with blueprint QA control .", "Comments": [], "label": []}
{"id": 154547, "text": "ROUGE-L + RELAX ( 70.58/68.75/70.58/56.68 ) Conservative politician Kyriakos Mitsotakis is sworn in as the new Prime Minister of Greece .", "Comments": [], "label": []}
{"id": 154570, "text": "In particular , we adopt Code-BERT [ Feng et al. , , 2020 ] , which is a bimodal pretrained model for both programming language and natural language .", "Comments": [], "label": []}
{"id": 152101, "text": "Our code is available at [ https : ] ( https : //github.com/RunxinXu/GIT ) [ //github.com/RunxinXu/GIT ] ( https : //github.com/RunxinXu/GIT ) .", "Comments": [], "label": []}
{"id": 159062, "text": "There is a strong positive correlation—increasing the threshold results in higher ROUGE scores ( Spearman 's r = 0.94 , p = 2.5e −9 ) .", "Comments": [], "label": []}
{"id": 158544, "text": "Specifically , we first design a cross-modal locality-constrained transformer to explore the multimodal interaction .", "Comments": [], "label": []}
{"id": 157441, "text": "T5 : when did bouvier graduate ?", "Comments": [], "label": []}
{"id": 154381, "text": "For the i-th token t , suppose its hidden states of the last Transformer layer from the quantized and full-precision network are linearly projected to ( h s i , h t i ) ∈ R d , and q s i is the smoothed representation of h s i in the memory bank .", "Comments": [], "label": []}
{"id": 152503, "text": "All the baselines are trained for 30 epochs , except for BLINK which we use as a zero-shot approach .", "Comments": [], "label": []}
{"id": 151685, "text": "We conducted comprehensive experiments to support our argument and experiment on mRASP2 and mRASP2 w/o AA .We divide the experiments into two scenarios : First we evaluate our method Figure 3 : Accuracy Improvements of m-Transformer → mRASP2 w/o AA → mRASP2 for Ted-M .", "Comments": [], "label": []}
{"id": 156510, "text": "Abstract Classic approaches to content moderation typically apply a rule-based heuristic approach to flag content .", "Comments": [], "label": []}
{"id": 159385, "text": "Then , the visual and linguistic representations are fed into the multi-modal encoder , which consists of multiple transformer layers .", "Comments": [], "label": []}
{"id": 154514, "text": "Since we use the ROUGE-L F1 score in our reward , to avoid circularity we also include METEOR [ 4 ] [ Lavie and Agarwal , 2007 ] in the performance evaluation .", "Comments": [], "label": []}
{"id": 157742, "text": "Second , the results of W/O LSA decreases slightly on Hits @ 1 and MRR , indicating that only using multi-graph attention without language-indicators can not aggregate information from different KGs effectively .", "Comments": [], "label": []}
{"id": 151853, "text": "During decoding we used beam search < https : //github.com/nlpyang/BertSUM > Figure 5 : Example topics and their segment clusters inferred by a mATM from the COCO corpus , and the generated sentences under segment cluster guidance .", "Comments": [], "label": []}
{"id": 154851, "text": "The context representation c is the distance-based attention [ López and Strube , 2020 ] result over the Transformer encoder 's output h : Table 12 : Hyper-parameters for dependency tree probing . | Optimizer | Adam | Adam | Adam |", "Comments": [], "label": []}
{"id": 160145, "text": "The translation model follows standard Transformer [ Vaswani et al. , , 2017 ] encoder-decoder architecture , where the encoder contains N Transformer encoder layers , and the decoder contains N Transformer decoder layers .", "Comments": [], "label": []}
{"id": 159387, "text": "4.2 Main Result We implement SRCL based on ALBEF [ Li et al. , , 2021 ] framework and evaluate it in four widely used downstream tasks : image-text retrieval , zeroshot image-text retrieval ( ZSR ) , visual question answering ( VQA ) , and natural language for visual reasoning ( NLVR ) .", "Comments": [], "label": []}
{"id": 158848, "text": "We use the labeling tasks ( 2nd and 4th task ) to select a pool of high-quality workers ( that is , crowdsource workers whose generative work was validated by a majority of separate annotators in a separate labeling task ) , after first seeding the set of annotators through manual inspection of a first batch of data .", "Comments": [], "label": []}
{"id": 158471, "text": "We use BERT-Base [ Devlin et al. , , 2019 ] as the victim model .", "Comments": [], "label": []}
{"id": 152712, "text": "On CNNDM and XSum datasets , we report full-length F1 based ROUGE-1 ( R1 ) , ROUGE-2 ( R2 ) , and ROUGE-L ( RL ) scores .", "Comments": [], "label": []}
{"id": 159199, "text": "For each omission detection framework , we employ BERTbase and RoBERTabase as the backbone model to extract text features .", "Comments": [], "label": []}
{"id": 152251, "text": "Instead of hand-labeling training data points , we use heuristic functions to label each training datum automatically .", "Comments": [], "label": []}
{"id": 154376, "text": "This causes difficulty in estimating the clipping factor α of the quantizer by heuristic methods , or even by PACT which learns the α through gradient descent .", "Comments": [], "label": []}
{"id": 155451, "text": "The example [ https : //huggingface.co/ ] ( https : //huggingface.co/bert-base-cased ) [ bert-base-cased ] ( https : //huggingface.co/bert-base-cased ) Table 1 : Results of the human evaluation of the usefulness of Token-based retrieval , BERT-based retrieval and EB-GEC examples .", "Comments": [], "label": []}
{"id": 159898, "text": "For these experiments , we use the best performing learning rate found in the grid search from Section [ 4.3 . ]", "Comments": [], "label": []}
{"id": 158522, "text": "Overall , the results confirm that a strong correlation exists between ROUGE and F1 scores , enabling us to reasonably predict whether the model improves , based on the F1 scores of the extractors .", "Comments": [], "label": []}
{"id": 158625, "text": "sacrebleu ( < https : //github.com/mjpost/sacrebleu > ) .", "Comments": [], "label": []}
{"id": 159787, "text": "This is because when updating encoder layers , the attention mechanism of the transformer block at the encoder layers will pay more attention to the specific trigger ( s ) if they appear .", "Comments": [], "label": []}
{"id": 156506, "text": "The strategy we use to prevent the checker from seeing the unsafe spans is setting the attention weights of multi-head attention [ Vaswani et al. , , 2017 ] corresponding to the unsafe spans as 0 [ 5 ] .", "Comments": [], "label": []}
{"id": 155051, "text": "Pretrained multilingual models represent the extension of multilingual embeddings to pretrained transformer models .", "Comments": [], "label": []}
{"id": 155982, "text": "In our implementation , we first use a cloze pattern [ Schick and Schutze , 2021 ] to combine both x and y into a single sequence , and then randomly mask a fixed percentage of the input tokens .", "Comments": [], "label": []}
{"id": 159258, "text": "5.5.2 Effect of Lexicon-based labeling Recall that in Section [ 3.4 , ] we introduced a heuristic pseudo-labeling approach that views all revised words in target segment t as important words and marks them as positive while we randomly sample other words as negative words .", "Comments": [], "label": []}
{"id": 153435, "text": "Above experimental results have demonstrated the effectiveness of the adoption of code classification in code search . 5 Conclusion To accelerate code search , we present CoSHC , a general method that incorporates deep hashing techniques and code classification .", "Comments": [], "label": []}
{"id": 155968, "text": "Efficient Transformer .", "Comments": [], "label": []}
{"id": 157691, "text": "Documentation was the most appreciated effort by students , and also the third most common complaint , suggesting that it can make or break a beginner 's experience in reproducing [ https : //aclrollingreview.org/ ] ( https : //aclrollingreview.org/responsibleNLPresearch/ ) [ responsibleNLPresearch/ ] ( https : //aclrollingreview.org/responsibleNLPresearch/ ) < https : //pypi.org/project/pip/ > < https : //docs.conda.io/en/latest/ > < https : //python-poetry.org/ > < https : //www.docker.com/ > results .", "Comments": [], "label": []}
{"id": 155827, "text": "3.1 Overview Following the approach of T5 [ Raffel et al. , , 2020 ] and PT [ Lester et al. , , 2021 ] , we solve all downstream tasks in a text-to-text format .", "Comments": [], "label": []}
{"id": 152329, "text": "We use sd to help generate a pseudo summary d during training and set it as 0 to generate a summary with average semantic of input reviews during inference .", "Comments": [], "label": []}
{"id": 158056, "text": "For datasets without development sets , we use the original test sets . 4.2 Backbones and Baselines We choose CPT-large [ Shao et al. , , 2021 ] as our backbone model , which is a competitive Chinese Table 3 : Main results on downstream tasks .", "Comments": [], "label": []}
{"id": 152588, "text": "Following the setting of the GLUE benchmark [ Wang et al. , , 2018 ] , we use the training set for training/finetuning and the development set for evaluation ( the test set 's labels are not publicly available ) ; MNLI 's development set has two parts , * matched * and * mismatched * ( m/mm ) .", "Comments": [], "label": []}
{"id": 157393, "text": "Did you report the number of parameters in the models used , the total computational budget ( e.g. , GPU hours ) , and computing infrastructure used ?", "Comments": [], "label": []}
{"id": 159882, "text": "H2 : Since HyperMixer has similar inductive biases as transformers but is considerably simpler conceptually and in terms of computational complexity , it can be seen as a low cost alternative to Transformers , reducing the cost in terms of single example processing time ( Section [ 4.4 ] , required dataset size ( Section [ 4.5 ] , and hyperparameter tuning ( Section [ 4.6 ] .", "Comments": [], "label": []}
{"id": 152085, "text": "We list the translation of the Transformer baseline ( BASE ) and our MSO method ( OURS ) .", "Comments": [], "label": []}
{"id": 159729, "text": "The descriptions are as follows : Dataset for MERMC : We use the MELD dataset [ Poria et al. , , 2019 ] , which is a publicly available dataset for MERMC .", "Comments": [], "label": []}
{"id": 158297, "text": "For CtrlEval and UniEval , we use 11 references as evaluation target for the metrics .", "Comments": [], "label": []}
{"id": 156159, "text": "We use the same splits of train/dev/test as provided in the original datasets [ Clark et al. , , 2020 ] [ Tafjord et al. , , 2021 ] .", "Comments": [], "label": []}
{"id": 155639, "text": "To reach a good trade-off between quality and diversity , we adopt nucleus sampling with p = 0.7 for all the models to generate samples . baselines .", "Comments": [], "label": []}
{"id": 152611, "text": "TinyBERT and MobileBERT are the current state-of-the-art 6 layer transformer models on the GLUE leaderboard .", "Comments": [], "label": []}
{"id": 156316, "text": "4.1 Dataset and Collection To evaluate our model 's ability to predict the factuality of news medium , we used the Media Bias/Fact Check dataset [ Baly et al. , ] [ 2018 , 2020b ] .", "Comments": [], "label": []}
{"id": 155511, "text": "We summarize the average scores and standard deviations in Figure [ 2 . ] The Transformer encoder is more flexible than LSTM .", "Comments": [], "label": []}
{"id": 155277, "text": "We use the AdamW optimizer [ Loshchilov and Hutter , 2017 ] with an initial learning rate of 1e − 5 for fine-tuning BERT and 1e − 4 for the fully-connected classification layer .", "Comments": [], "label": []}
{"id": 158954, "text": "We use the si to denote the speaker of i-th utterance and |S| to denote the number of speakers .", "Comments": [], "label": []}
{"id": 156450, "text": "We employ HolE ( Nickel et al. , 2016 ) due to its desirable properties for embedding phrase structures without additional parameters , as we describe below .", "Comments": [], "label": []}
{"id": 159987, "text": "We used a threshold of 0.7 for the multidomain En→Es corpus while using ∞ for the rest .", "Comments": [], "label": []}
{"id": 160115, "text": "] ( https : //huggingface.co/transformers/pretrained_models.html ) In all experiments , the batch size was set to 8 .", "Comments": [], "label": []}
{"id": 151836, "text": "Besides , we adopt the settings in the BertSUM .", "Comments": [], "label": []}
{"id": 158517, "text": "It is clearly shown that the ROUGE scores tend to be proportional to the F1 score of the extraction when either XROG or XCES o is the groundtruth set .", "Comments": [], "label": []}
{"id": 154610, "text": "Documents to be retrieved are stored in an approximate nearest-neighbor FAISS index [ Johnson et al. , , 2019 ] , and a DPR ( Dense Passage Retrieval ) [ Karpukhin et al. , , 2020 ] Transformer biencoder model is used to score document-context pairs in order to rank them based on their match , where the base DPR model is pre-trained on QA data pairs .", "Comments": [], "label": []}
{"id": 159632, "text": "To better illustrate the effectiveness and efficiency , we report more results on WikiCoref in Figure [ 3 . ] In this setting , we adopt a CR model trained with unbounded memory , so it does not have a preference of the cache structure .", "Comments": [], "label": []}
{"id": 155770, "text": "Prior works mainly resort to heuristic textlevel manipulations ( e.g . utterances shuffling ) to bootstrap incoherent conversations ( negative examples ) from coherent dialogues ( positive examples ) .", "Comments": [], "label": []}
{"id": 152295, "text": "The weight we employed is to rebalance the observations for each class , so the sum of observations for each class are equal .", "Comments": [], "label": []}
{"id": 153219, "text": "For UD , we use MaltParser v1.9.2 [ 9 ] to adopt the pseudo-projective transformation [ Nivre and Nilsson , 2005 ] to convert nonprojective trees into projective trees when training , and convert back when evaluating , for both our model and reimplemented baseline model .", "Comments": [], "label": []}
{"id": 152654, "text": "As for evaluation , we used the Average Precision ( AP ) and False Positive Rate ( FPR ) metrics by comparing the remaining tokens to the human rationale annotations .", "Comments": [], "label": []}
{"id": 153988, "text": "Table 4 : Mean and standard deviation of scores on the development and test sets for BETO and mBERT tuned Transformer-based embeddings pretrained on codeswitched data .", "Comments": [], "label": []}
{"id": 153808, "text": "the training and test sets come from the same distribution , and most Transformer models achieve near 100 % accuracy ( except a few hard tasks like the Cartesian product or set intersection ) .", "Comments": [], "label": []}
{"id": 156075, "text": "Please refer to Appendix [ A ] for statistical information about the datasets . 4.2 Implementation Details In our experiments , we use pretrained BERT [ Devlin et al. , , 2019 ] in our encoder .", "Comments": [], "label": []}
{"id": 156670, "text": "We used ADAM [ Kingma and Ba , 2015 ] as our optimizer .", "Comments": [], "label": []}
{"id": 156565, "text": "Besides Figure 6 : Model performance as a function of pretraining data size , for SINGULARITY ( 1-frame ) and SINGULARITY-temporal ( 4-frame ) .", "Comments": [], "label": []}
{"id": 154631, "text": "Text-pair tasks have 295 instances each ; image-pair tasks have 227 instances each . 3.2 Can we use CLIP to resolve spatial relations ?", "Comments": [], "label": []}
{"id": 159519, "text": "We use the cross-entropy loss for classifcation and the mean squared error loss for regression . 4.2 Main Experimental Results In this part , we conduct a series of experiments on NLU , commonsense reasoning , text generation , and cross-modal commonsense reasoning tasks .", "Comments": [], "label": []}
{"id": 156526, "text": "While the recent F1000RD corpus < https : //f1000research.com/ > We use the terms manuscript , paper etc .", "Comments": [], "label": []}
{"id": 158361, "text": "[ 5 ] For computing efficiency , we use intralayer model parallelism [ Shoeybi et al. , , 2019 ] and fully sharded data parallel [ Ott et al. , , 2021 ] .", "Comments": [], "label": []}
{"id": 159069, "text": "Table [ 4 ] demonstrates our finding that Soft-F1 is indeed a more truthful reflection of human judgment than ROUGE ( rsF = 0.72 , p = 0.01 vs. rROUGE = 0.61 , p = 0.03 ) and S+WMS ( rsF = 0.72 , p = 0.01 vs. rS+WMS = 0.60 , p = 0.04 ) .", "Comments": [], "label": []}
{"id": 151489, "text": "We use the soft-logical model defined in Step 2 for selecting decoy triples .", "Comments": [], "label": []}
{"id": 153640, "text": "Data Preparation We use Natural Question , Trivia QA , and Wikipedia cleaned and released with DPR toolkit [ Karpukhin et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 152576, "text": "We adopt the few-shot phoneme recognition setup on CV v3 from [ Riviere et al . ] ` [ 2020 ] , with which domain adaptation is limited during fine-tuning due to the small data volume — it has 1-hour train set , 20-minute development set and 1-hour test set for 10 languages including 5 VoxPopuli ones .", "Comments": [], "label": []}
{"id": 152226, "text": "Finally , we have observed how Transformers representations vary across the vector space .", "Comments": [], "label": []}
{"id": 159384, "text": "Following [ Dou et al. , , 2021 ] [ Shen et al. , , 2021 ] , we use a visual transformer [ Dosovitskiy et al. , , 2020 ] directly on the image patches as the visual encoder , which is more computation-friendly than using pretrained object detectors for visual feature extraction [ Anderson et al. , , 2018 ] [ Zhang et al. , , 2021 ] .", "Comments": [], "label": []}
{"id": 152455, "text": "We used early fusion for saliency classification in the end-to-end model due to strong empirical performance there .", "Comments": [], "label": []}
{"id": 159728, "text": "Similarly , Hl− is then fused with visual modality to obtain the utterance-level text-audio-visual fused representation Hl−a− below : Hl−a− = CM-Transformer ( Hl−a , Hv ) ( 9 ) Finally , Hl−a− is fed to a softmax layer for emotion classification : The standard cross-entropy loss is used to optimize the parameters for the MERMC task : where N is the number of utterance samples .", "Comments": [], "label": []}
{"id": 159415, "text": "Did you report the number of parameters in the models used , the total computational budget ( e.g. , GPU hours ) , and computing infrastructure used ?", "Comments": [], "label": []}
{"id": 151644, "text": "Based on the full-modality training set , we construct another training set that contains cross-modality pairs to simulate the possible missing-modality conditions and we define it as the missing-modality training set , which we use to train the proposed MMIN .", "Comments": [], "label": []}
{"id": 152293, "text": "Following previous work [ Nallapati et al. , , 2017 ] [ Liu et al. , , 2019 ] , we use a sigmoid function after a linear transformation to calculate the probability r of selecting s as a summary sentence : 3.6 Weighted Objective Function To rebalance the bias of minority 1-class and majority 0-class , we have built a deep differential amplifier to amplify and capture the unique information for summary sentences .", "Comments": [], "label": []}
{"id": 156429, "text": "The aforementioned process is shown in Figure [ 4 . ] 4.3 Training and Inference Given the parallel data , we use vanilla self-attention to obtain source sentence representation and sliding window mixed-attention with fixed window size to generate the target during the training stage .", "Comments": [], "label": []}
{"id": 155967, "text": "We use block attention in the encoder , and the decoder features dense attention .", "Comments": [], "label": []}
{"id": 154908, "text": "The NEMO corpus [ Bareket and Tsarfaty , 2020 ] used to train and evaluate word and morpheme level NER is an extension of the SPMRL dataset augmented with entities and follows the same license terms .", "Comments": [], "label": []}
{"id": 155329, "text": "We use MSE loss between model logits as the distillation objective .", "Comments": [], "label": []}
{"id": 152029, "text": "On Europarl7 , G-Transformer gives 33.75 , 33.87 , and 33.84 d-BLEU with top 1 , 2 , and 3 layers with combined attention , respectively , showing that K = 2 is sufficient .", "Comments": [], "label": []}
{"id": 157701, "text": "BIC adopts transformer with multi-head attention [ Vaswani et al. , , 2017 ] as the language model LM .", "Comments": [], "label": []}
{"id": 158035, "text": "* 5.1 Compared Models We use T5-3B [ Raffel et al. , , 2020 ] as our base model .", "Comments": [], "label": []}
{"id": 153949, "text": "For comparison with previous work , we use the same test set constructed by [ Shen et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 158657, "text": "For each candidate , we Table 6 : * * COMET * * vs. * * COMET * * ours .", "Comments": [], "label": []}
{"id": 151898, "text": "We adopt Adam as the optimizer , set the learning rate to 3 × 10− and train for two epochs .", "Comments": [], "label": []}
{"id": 154166, "text": "* Cross-lingual NUD ( XNUD ) , given the intuition that more dialogue-related tasks may Figure 2 : The effect of each task on validation sets in different training stages , under transformer * Base * setting , where `` All '' denotes all four auxiliary tasks .", "Comments": [], "label": []}
{"id": 157220, "text": "However , a better decoding strat- Table 4 : Performance as compared to task-specific T5-base models from [ Herzig et al. , , 2021 ] .", "Comments": [], "label": []}
{"id": 154074, "text": "For pie charts , the values are estimated by calculating the angle between each two neighbouring points . < huggingface.co/transformers > Figure 10 : The user interface for human evaluation : it presents two summaries at a time and asks the participant to compare between them based on three measures .", "Comments": [], "label": []}
{"id": 159748, "text": "In this work , we employ the Archimedean spiral to model TKGs .", "Comments": [], "label": []}
{"id": 159102, "text": "Table A3 : Grid search results of all hyperparameters in GRM on different background models , i.e. , Word2Vec and BERT . |B| is the batch size of the GRM model .", "Comments": [], "label": []}
{"id": 154427, "text": "We use the BERT-uncased dictionary , and replace some unused tokens to custom special symbols ( such as [ SOT ] , denoting the beginning of the conversation , which is suitable for conversation datasets containing knowledge , like PersonaChat and DSTC7-AVSD ) .", "Comments": [], "label": []}
{"id": 156026, "text": "In addition , various types of distillation techniques [ Sanh et al. , , 2019 ] [ Sun et al. , , 2019 ] [ Jiao et al. , , 2019 ] [ Wang et al. , , 2020b ] have been proposed to remove encoders by training a compact Transformer to reproduce the output of a larger one .", "Comments": [], "label": []}
{"id": 159198, "text": "4.2 Evaluation Metrics We use the standard Precision ( P ) , Recall ( R ) , and F1-score ( F1 ) metrics on the utterance level to evaluate omission detection models .", "Comments": [], "label": []}
{"id": 159977, "text": "S2UT The architecture of S2UT is shown in Figure [ 2c . ] In addition to the primary S2UT loss and auxiliary S2TT and ASR losses , we use a CTC loss on top of the unit decoder following [ Lee et al. , 2022a ] .", "Comments": [], "label": []}
{"id": 157438, "text": "BART : what did the bus driver say after kendra told him about T5 : what did the bus driver say he was sure of ?", "Comments": [], "label": []}
{"id": 154770, "text": "These approaches are orthogonal to the ∞-former 's LTM and in future work the two can be combined . 6 Conclusions In this paper , we proposed the ∞-former : a transformer extended with an unbounded long-term memory .", "Comments": [], "label": []}
{"id": 154372, "text": "In practice , following previous works on BERT quantization [ Zhang et al. , , 2020 ] [ Bai et al. , , 2021 ] , we use layer-wise quantization ( * i.e .", "Comments": [], "label": []}
{"id": 154728, "text": "For vanila pre-trained language models with/without label smoothing results , we use the reported results from [ Desai and ] [ Durrett , 2020 ] . https : //github.com/shreydesai/calibration Table 1 : Expected Calibration Error ( ECE ) in percentage ( % ) on BERT ( top ) and RoBERTa ( bottom ) .", "Comments": [], "label": []}
{"id": 157011, "text": "We fixed the batch size at 32 when fine-tuning T5-BASE and BARTs .", "Comments": [], "label": []}
{"id": 151500, "text": "For re-training the model on poisoned dataset , we use the same hyperparameters as the original model .", "Comments": [], "label": []}
{"id": 158207, "text": "Model architecture We use a one-layer linear generator with |X| input nodes and |Y| output nodes , with no bias .", "Comments": [], "label": []}
{"id": 153887, "text": "We use two random seeds for each task sequence .", "Comments": [], "label": []}
{"id": 159397, "text": "With the help of a unified dataset format , we evaluated the proposed metric on five datasets ( e.g. , e-SNLI ) against two model architectures ( T5 and BART ) , and the results show that our proposed metric can objectively evaluate the quality of human-annotated explanations , while Simulatability falls short .", "Comments": [], "label": []}
{"id": 157885, "text": "We use the synthetic grounded dialogue data produced by our framework as additional training data for commonly used grounded dialogue models .", "Comments": [], "label": []}
{"id": 156944, "text": "The cross-entropy loss is utilized for the rest of tasks . 4 Experiments 4.1 Pre-training Data In this paper , we adopt Spotify100K [ Clifton et al. , , 2020 ] to pre-train SPECTRA , which is a real-world scene speech-text dialog dataset .", "Comments": [], "label": []}
{"id": 159274, "text": "We use biaffine module in our model to obtain the relation score s b i , j between the word pair ( w , w ) as an enhancement , i.e. , where U1 , U and b are trainable weights and bias .", "Comments": [], "label": []}
{"id": 155818, "text": "Specifically , we employ discrete latent variables to capture the word categorical information and divide the original goal into the latent variables modeling and word prediction tasks .", "Comments": [], "label": []}
{"id": 154344, "text": "To investigate how much knowledge is stored in each Transformer layer , we chopped the last layers of PLMs and applied Contrastive-Probe to evaluate the probing performance based on the first * * ∈ { 3 , , , , , } layers on MedLAMA .", "Comments": [], "label": []}
{"id": 156157, "text": "ProofWriter [ Tafjord et al. , 2021 ] is a T5-based [ Raffel et al. , , 2020 ] model , that iteratively generates one-hop conclusions and proofs from a theory .", "Comments": [], "label": []}
{"id": 156618, "text": "We use : sentence similarity .", "Comments": [], "label": []}
{"id": 152850, "text": "First , * sparse attention * [ Child et al. , 2019 ] [ Beltagy et al. , , 2020 ] [ Tay et al. , , 2020b ] is used to reduce the memory complexity of the Transformers so that they can attend to more tokens .", "Comments": [], "label": []}
{"id": 158884, "text": "For a fair comparison , we also pre-train a 220M T5-v1.1-base [ Raffel et al. , , 2020 ] model with our meta-function pre-training algorithm ( MetaNER-base ) .", "Comments": [], "label": []}
{"id": 157948, "text": "Hence , the same storage budget for two tasks with full PLM fine-tuning can support 370 tasks with prefix tuning . 3.2 Task Identification As we use different prefixes for different tasks , when a test sample comes , it is necessary to determine which prefix should be applied .", "Comments": [], "label": []}
{"id": 153169, "text": "We employ BARTbase [ Lewis et al. , , 2020 ] as our framework .", "Comments": [], "label": []}
{"id": 157903, "text": "Besides , we also report the results using random sampling ( BB-RS and GPT2-RS ) instead of heuristic sampling when determining the dialogue flow for comparison .", "Comments": [], "label": []}
{"id": 153507, "text": "5.1 Long-context Sequence Modeling As the primary goal , we evaluate the proposed Fourier Sparse Attention for Transformer ( FSAT ) on multiple tasks requiring long-context perception .", "Comments": [], "label": []}
{"id": 151868, "text": "For DeepCoNN , MPCN , and NARRE , we employed the extensible NRRec framework [ 5 ] and retained the other hyperparameters reported in the framework [ Liu et al. , , 2019b ] .", "Comments": [], "label": []}
{"id": 154370, "text": "To investigate the difficulty of quantizing generative PLMs , we find that the learned embeddings tend to be homogeneous and hard to distinguish due to the reduced capacity caused by quantization , while the weight distributions also vary significantly across different modules and different Transformer layers .", "Comments": [], "label": []}
{"id": 159962, "text": "On the multi-domain corpus , we use ( 12 , 2 , 2 ) when pre-training the first-pass decoder with tmBART .", "Comments": [], "label": []}
{"id": 153532, "text": "[ 3 , ] we adopt the widely-used Transformer [ Vaswani et al. , , 2017 ] as the encoder and take the last hidden states as the contextualized representations h and hc .", "Comments": [], "label": []}
{"id": 153591, "text": "Note that here BLEU and ROUGE scores are percentage values , while the other metrics are absolute values . ( i.e. , encoder , decoder , and visual codebook ) of VQ-GAN are first pre-trained on the collected images of the two datasets .", "Comments": [], "label": []}
{"id": 160377, "text": "Table 15 : BLEU of Slavic-to-English multilingual XM Transformer models across FLEURS ( FL ) and EP/VP domains ( for EP/VP column , underlined scores are on EPST data , and others on VoxPopuli data ) . and a label smoothing weight of 0.2 for Slavic-to-English training .", "Comments": [], "label": []}
{"id": 154003, "text": "The two best-performing models were the BiLSTM-CRF with codeswitch and BPE embeddings ( F1 : 84.06 ) and the BiLSTM-CRF model with codeswitch , BPE and character embeddings ( F1 : 84.22 ) .", "Comments": [], "label": []}
{"id": 152165, "text": "The concatenation of them is fed into the encoding layer as the token representation x = [ w , c ] ∈ R dx . 2.5 Encoding Layer We employ a three-layered bidirectional LSTM to encode sentences and leverage contextual information , where h ∈ R is the hidden state .", "Comments": [], "label": []}
{"id": 159933, "text": "For tokenization , we use the pretrained tokenizer from BERT-Base [ De ] [ vlin et al. , , 2019 ] .", "Comments": [], "label": []}
{"id": 155410, "text": "With regard to the convergence speed , MoEbased Transformers usually exceed dense models .", "Comments": [], "label": []}
{"id": 152256, "text": "To build the heuristics functions , we first summarize the heuristic rules shared among users .", "Comments": [], "label": []}
{"id": 154308, "text": "Using Transformers : We also replace our LSTM auto-encoders with both pre-trained and randomly initialized transformer encoder–decoders [ Rothe et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 159081, "text": "But GRM performed slightly worse on the RareWord , SimLex , and simverb datasets . https : //wordnet.princeton.edu/ Table 3 : The experimental results of extending Word2Vec and BERT to NER and POS tagging tasks .", "Comments": [], "label": []}
{"id": 151404, "text": "Then , we use the [ CLS ] 's pooled embedding at the last layer as the feature representation of u .", "Comments": [], "label": []}
{"id": 160332, "text": "Specifically , we utilize the optimal trained models for the subtask of toxic identification to detect the samples in the test set .", "Comments": [], "label": []}
{"id": 160348, "text": "The same model with 70M parameters that we use for bilingual evaluation is reused in the multilingual experiments .", "Comments": [], "label": []}
{"id": 154717, "text": "Acknowledgements This work was supported with Cloud TPUs from Google 's TPU Research Cloud ( TRC ) program and Google Cloud Research Credits with the award GCP19980904 .", "Comments": [], "label": []}
{"id": 155409, "text": "Under the large setting , consistently , STABLEMOE outperforms the other MoE methods , and achieves about 2.6 lower perplexity than the standard Transformer .", "Comments": [], "label": []}
{"id": 152657, "text": "For both fine-tuning and training with length reduction , we employed an AdamW optimizer [ Loshchilov and Hutter , 2019 ] with a weight decay rate of 0.1 , warmup proportion 6 % of total training steps and a linear learning rate decay which reaches to zero at the end of training .", "Comments": [], "label": []}
{"id": 156894, "text": "Finally , we use our dataset to finetune and evaluate LED [ Beltagy et al. , , 2020 ] , a pre-trained dialogue agent for answering questions about a long text in generic domains .", "Comments": [], "label": []}
{"id": 155373, "text": "In contrast , when we use our self-learning framework during finetuning , even though there is no MT task , the MT capability can still be preserved .", "Comments": [], "label": []}
{"id": 152749, "text": "] ( https : //docs.microsoft.com/en-us/azure/cognitive-services/speech-service/spx-overview ) [ com/en-us/azure/cognitive-services/ ] ( https : //docs.microsoft.com/en-us/azure/cognitive-services/speech-service/spx-overview ) [ speech-service/spx-overview ] ( https : //docs.microsoft.com/en-us/azure/cognitive-services/speech-service/spx-overview ) .", "Comments": [], "label": []}
{"id": 155534, "text": "Figure 3 : Box plot of survey data from MUSHRA questions comparing Tacotron2 ( TT2 ) and FastSpeech2 ( FS2 ) models with constrained amounts of training data . 'Ref ' refers to reference recordings of natural speech . ing by 33 % on GPU or 64 % on CPU .", "Comments": [], "label": []}
{"id": 151427, "text": "[ https : //github.com/UKPLab/ ] ( https : //github.com/UKPLab/sentence-transformers ) [ sentence-transformers ] ( https : //github.com/UKPLab/sentence-transformers ) The surprisingly strong performance of % -IN-T , GLOVE [ ALIGNED ] and BLEU provide further evidence that the extent to which T repeats words from S is important for uptake [ Tannen , 1987 ] , especially in the context of teaching .", "Comments": [], "label": []}
{"id": 158671, "text": "We use a batch size of 128 and use the default values for other parameters . [ https : //github.com/atcbosselut/ ] ( https : //github.com/atcbosselut/comet-commonsense ) [ comet-commonsense ] ( https : //github.com/atcbosselut/comet-commonsense )", "Comments": [], "label": []}
{"id": 158243, "text": "Did you report the number of parameters in the models used , the total computational budget ( e.g. , GPU hours ) , and computing infrastructure used ?", "Comments": [], "label": []}
{"id": 151452, "text": "Finally , we apply this approach to BART and T5 NLMs trained on text from the English-language Alchemy and TextWorld datasets .", "Comments": [], "label": []}
{"id": 155472, "text": "We use Mean Reciprocal Rank ( MRR ) evaluation metric for the task .", "Comments": [], "label": []}
{"id": 152096, "text": "In this paper , we propose Heterogeneous Graph-based Interaction Model with a Tracker ( GIT ) to solve the aforementioned two challenges .", "Comments": [], "label": []}
{"id": 155940, "text": "Figure 4 : Training time for different model sizes of Vanilla Transformer , Blockwise , and Pyramidion 8k → 512 with the input sequence length of 8192 tokens .", "Comments": [], "label": []}
{"id": 159990, "text": "The w2v-BERT was composed of 24 Conformer layers with 0.6 billions of parameters .", "Comments": [], "label": []}
{"id": 158600, "text": "For both generative experiments , we evaluate the generations with BLEU [ Papineni et al. , , 2002 ] , METEOR [ Lavie ] [ and Agarwal , 2007 ] , ROUGE-L [ Lin , 2004 ] , and < https : //github.com/allenai/comet-atomic-2020 > CIDEr [ Vedantam et al. , , 2015 ] scores .", "Comments": [], "label": []}
{"id": 152769, "text": "We use beam search of size 10 and save all beams .", "Comments": [], "label": []}
{"id": 158747, "text": "In order to obtain the semantic representation of the text contained in an image , we adopt two strong visual encoder architectures : ResNet ( He et al. , 2016 ) and CRNN ( Shi et al. , 2017 ) .", "Comments": [], "label": []}
{"id": 157328, "text": "In order to conduct robustness experiments in Sec . [ A.3 , ] we use the datasets from [ Lin et al. , 2021 ] with 11 entity types .", "Comments": [], "label": []}
{"id": 159725, "text": "During the training stage , we use cross-entropy loss to optimize the parameters for the DFER task : where M is the number of samples in D s .", "Comments": [], "label": []}
{"id": 157286, "text": "To stabilize the performance of each model on ReCoRD , we use the first 40k validation samples to select the best fine-tuned model checkpoints .", "Comments": [], "label": []}
{"id": 152111, "text": "After obtaining document-aware representations of entities and sentences , GIT detects event types and extracts records through the decoding module with a * Tracker * .", "Comments": [], "label": []}
{"id": 159838, "text": "2 ) Then we use the PLM to predict values for [ MASK ] , filter them using each class 's set of label words , and aggregate the properly weighed outputs to produce the final prediction .", "Comments": [], "label": []}
{"id": 152973, "text": "6.5.1 Answer Recalling Model Choices for the Answer Recaller As shown by Table [ 5 , ] though T5-base is commonly recognized as a much weaker model than T5-3b , a recaller based on T5-base can achieve a high coverage of gold answers , leading to competitive end-to-end performance on the test set .", "Comments": [], "label": []}
{"id": 156003, "text": "We use pattern-based data cloze to further improve its performance .", "Comments": [], "label": []}
{"id": 159311, "text": "Following previous work [ Kim et al. , , 2021 ] [ Chen et al. , , 2020 ] [ Li et al. , , 2021a ] [ Dou et al. , , 2022 ] , we adopt four public image-caption datasets for pre-training , including Conceptual Captions ( CC ) [ Sharma et al. , , 2018 ] , SBU Captions ( SBU ) [ Ordonez et al. , , 2011 ] , MSCOCO Captions ( COCO ) [ Chen et al. , , 2015 ] , and Visual Genome ( VG ) [ Krishna et al. , , 2017 ] .", "Comments": [], "label": []}
{"id": 153443, "text": "We use the official crawling script to acquire the dataset and divide some data from the training set as the validation set .", "Comments": [], "label": []}
{"id": 156034, "text": "6 Experiments We plug-in our * core-set * based token selection method and the other competitive methods into encoder layers of a backbone Transformer , and after fine-tuning evaluate their performance on a wide range of natural language classification tasks .", "Comments": [], "label": []}
{"id": 156594, "text": "One interesting note is that , compared to ( ITM+MLM ) , adding ITC loss ( ITM+MLM+ITC ) greatly improves MSRVTT re- Table 4 : SINGULARITY-temporal results on text-to-video retrieval .", "Comments": [], "label": []}
{"id": 155458, "text": "CodeT5 [ Wang et al. , , 2021 ] adapts the T5 [ Raffel et al. , , 2019 ] model that considers the crucial token type information from identifiers and allow for multi-task learning on downstream tasks .", "Comments": [], "label": []}
{"id": 153838, "text": "Another ex- Table 7 : Size of the training/test sets , vocab and training epochs we used for the different datasets .", "Comments": [], "label": []}
{"id": 157692, "text": "While all code was released on GitHub , large artifacts ( e.g. , datasets or pre-trained models ) that are not suitable for git were hosted on other websites .", "Comments": [], "label": []}
{"id": 153404, "text": "The Clustered Vocabulary ( CV ) approach is much worse than BPE .", "Comments": [], "label": []}
{"id": 152370, "text": "For both datasets , we used a stratified 80/10/10 train/dev/test split .", "Comments": [], "label": []}
{"id": 155842, "text": "5 Related Works PLMs and Task-oriented Fine-tuning Recently , various powerful PLMs have been proposed , such as GPT [ Radford et al. , , 2018 ] , BERT [ De ] [ vlin et al. , , 2019 ] , RoBERTa [ Liu et al. , , 2019 ] and T5 [ Raffel et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 159826, "text": "We then showed that we can similarly achieve SOTA performance with the much smaller ( and open-source ) Flan T5 ( Large ) model , when trained using CoT generations produced by GPT-3 .", "Comments": [], "label": []}
{"id": 158898, "text": "From the model perspective , regarding the ratio of performance decay , XGBoost is the least impacted , and MRDM is the most affected .", "Comments": [], "label": []}
{"id": 160000, "text": "S2UT We used a six-layer Transformer unit decoder .", "Comments": [], "label": []}
{"id": 159524, "text": "Concretely we implement our approach on ALBEF by inserting the visually-augmented representations after the VH-words embeddings of the text encoder before the multimodal encoder , and keeping others unchanged .", "Comments": [], "label": []}
{"id": 159712, "text": "Specifically , we use GPT-2 , GPT-2-medium , GPT-2-large and GPT-2-xl for model size ablation and RoBERTa-base pre-trained for 5000∼100000 steps for pre-training ablation , respectively .", "Comments": [], "label": []}
{"id": 155157, "text": "BART [ Lewis et al. , , 2020 ] is a pretrained transformer network considered state-of-the-art for summarization .", "Comments": [], "label": []}
{"id": 153975, "text": "We use their ALBERT-base model finetuned on FEVER [ Thorne et al. , , 2018 ] and their VitaminC dataset .", "Comments": [], "label": []}
{"id": 157914, "text": "In addition , the ablation experiments ( * w/o FF * , * w/o UF * , * w/o FF & UF * , and GPT2-RS ) demonstrate similar results to those on WoW , that is , our proposed heuristic sampling and two-level filtering strategy are essential for generating high-quality and useful synthetic dialogues .", "Comments": [], "label": []}
{"id": 155007, "text": "We used the mGPT model as the backbone LM in all our experiments , which contains 560M parameters .", "Comments": [], "label": []}
{"id": 151338, "text": "We use iBLEU [ Sun and Zhou , 2012 ] as our primary metric , a variant of BLEU [ Papineni et al. , , 2002 ] [ Post , 2018 ] that is penalized by the similarity between the output and the * input * , where α = 0.7 is a constant that weights the tradeoff between fidelity to the references and variation from the input .", "Comments": [], "label": []}
{"id": 156114, "text": "In the gender debias experiments , we use BERT-base-uncased , RoBERTa-base , and ALBERT-large-v2 .", "Comments": [], "label": []}
{"id": 156791, "text": "Q3 : Do bigger T5 models generate better explanations ? * Test : T5-11B vs .", "Comments": [], "label": []}
{"id": 159908, "text": "iii ) HyperMixer requires less hyperparameter tuning than Transformers to reach good results , which is demonstrated by HyperMixer 's higher expected relative improvements at low tuning budgets ( Section [ 4.6 ] .", "Comments": [], "label": []}
{"id": 156031, "text": "We consider a loss function Lw ( · , · ) : X × Y → R which is parametrized over the hypothesis class ( w ) , the parameters of the transformer network ( e.g .", "Comments": [], "label": []}
{"id": 154121, "text": "Popular public KGs include Freebase [ Bollacker et al. , , 2008 ] , Wikidata [ Vran ] deciˇ [ c and Krötzsch ] ´ , [ 2014 ] , YAGO [ Suchanek et al. , , 2007 ] , ConceptNet [ Speer et al. , , 2017 ] , and Word-Net [ Miller , 1992 ] etc .", "Comments": [], "label": []}
{"id": 159790, "text": "3 Fine-tuning Flan-T5 ( large ) is competitive with , but no better than , existing supervised methods , but 4 supervising Flan-T5 with CoT reasoning elicited from GPT-3 substantially outperforms all other models .", "Comments": [], "label": []}
{"id": 157176, "text": "( 2 ) * Multi-TRS * [ Rashkin et al. , , 2019 ] : A multi-task Transformer model that jointly optimizes response generation and emotion prediction .", "Comments": [], "label": []}
{"id": 153893, "text": "Implementation Details We use GPT-2 [ Rad ] [ ford et al. , , 2019 ] in HugginceFace Transformers [ Wolf et al. , , 2020 ] as our backbone .", "Comments": [], "label": []}
{"id": 153036, "text": "We choose ConceptNet Numberbatch due to its advantage of far richer ( 520k ) in-vocabulary multi-grams compared with Word2Vec [ Mikolov et al. , , 2013 ] , GloVe [ Pennington et al. , , 2014 ] , and Counterfitting [ Mrkšic et al . ] ´ , [ 2016 ] , which is especially desirable for multi-gram columns .", "Comments": [], "label": []}
{"id": 157832, "text": "The performance of the QG model is in Appendix [ A.3 . ] 4 Experimentation 4.1 Experimental Settings Dataset We use CoQA [ Reddy et al. , , 2019 ] , a large-scale CQA dataset , in our experiments .", "Comments": [], "label": []}
{"id": 155111, "text": "Next , in the dynamic network convolution step , we utilize the encoded company representations H˜ ( t ) and the weighted adjacency tensor A˜ ( t ) as inputs to a weighted dynamic graph convolution step to encode network representations of companies .", "Comments": [], "label": []}
{"id": 158219, "text": "4.2 Implementation Details UPPAM is produced via continued pre-training on RoBERTa-base model [ Liu et al. , , 2019b ] , where we add parallel FFN modules in each transformer layer with the same initialization as the original one .", "Comments": [], "label": []}
{"id": 157703, "text": "We adopt cross-entropy loss to optimize BIC , * i * .", "Comments": [], "label": []}
{"id": 155648, "text": "However , our DMLM can efficiently apply to Transformer-based models without additional computational complexity .", "Comments": [], "label": []}
{"id": 152061, "text": "For En→Fr , the training dataset contains about 36M sentence pairs , and we use * newstest2013 * with 3000 sentences as validation set and * newstest2014 * with 3003 sentences as test set .", "Comments": [], "label": []}
{"id": 156966, "text": "3.4 Implementation Details We implement the cluster number calculation based on the code of U-k-means [ 7 ] [ Sinaga and Yang , 2020 ] .", "Comments": [], "label": []}
{"id": 157873, "text": "We define the relation vector r ∈ R 2M×|R| as We use the average Hamming distance between each mention pair in cluster Cx , C as Dr : Relation Triple Decoding .", "Comments": [], "label": []}
{"id": 154497, "text": "Experimental results over the Multi-News and WCEP MDS datasets show significant improvements of up to +0.95 pp average ROUGE score and +3.17 pp METEOR score over the baseline , and competitive results with the literature .", "Comments": [], "label": []}
{"id": 154448, "text": "In addition , when the group size grows to 512 , PL-Marker slows down due to the increased complexity of the Transformer .", "Comments": [], "label": []}
{"id": 160156, "text": "We use scareBLEU [ 5 ] [ Post , 2018 ] to compute case-sensitive detokenized BLEU [ Papineni et al. , , 2002 ] scores and the statistical significance of translation results with paired bootstrap resampling [ 6 ] [ Koehn , 2004 ] .", "Comments": [], "label": []}
{"id": 155323, "text": "To calculate the Precision @ 1 of question answering models , we use the first passage that contains the returned spoiler .", "Comments": [], "label": []}
{"id": 156156, "text": "We attribute this to the reduced inference candidate search space due to question augmentation , and smaller input size to the T5 component ( refer to Section [ 3.3 ] for details ) .", "Comments": [], "label": []}
{"id": 153293, "text": "As a precautionary measure , we use a softmax temperature τ to flatten the model predictions , based on a similar technique in knowledge distillation [ Hinton et al. , , 2015 ] and multi-view regularization [ Wang et al. , , 2021 ] .", "Comments": [], "label": []}
{"id": 154993, "text": "To make the notation simpler , we use the following equation to denote the repeated application of fLM over a sequence z : = [ z , . . . , z ] given past activations A : where Zi : = [ ez , . . . , ez ] , Gi : = [ g , . . . , g ] , and Hi : = [ h , . . . , h ] .", "Comments": [], "label": []}
{"id": 158723, "text": "Multi-expert Architecture PaCE adopts an extension of the standard Transformer , which learns multiple semantic experts instead of a single feedforward network ( FFN ) as in the original Transformer [ Bao et al. , , 2021 ] .", "Comments": [], "label": []}
{"id": 156292, "text": "Also , BLEURT 's synthetic dataset is made by perturbing Wikipedia sentences with mask-filling , backtranslation , and word dropping , whereas we use other data sources than Wikipedia such as summarization and translation datasets , and only NLG models to induce perturbations .", "Comments": [], "label": []}
{"id": 158488, "text": "Nevertheless , the PLM in our method can be replaced by other models . 3.4 Idea Verbalization In our public beta system , we employ T5 [ Raf ] [ fel et al. , , 2020 ] , a large pretrained sequence-tosequence model for idea verbalization .", "Comments": [], "label": []}
{"id": 157940, "text": "For fair comparison we use word-level normalization .", "Comments": [], "label": []}
{"id": 152982, "text": "Though a * recall-then-verify * system with a recaller based on T5-base significantly outperforms JPR on WEBQSP , it lags behind the system with a T5-3b recaller on F1 ( all ) on the test set .", "Comments": [], "label": []}
{"id": 151976, "text": "4 G-Transformer An example of G-Transformer is shown in Figure [ 6 , ] where the input document contains more than 3 sentences .", "Comments": [], "label": []}
{"id": 159118, "text": "More detailed guidelines are presented in Appendix [ A . ] 3.3 Quality Assurance We first performed pilot studies on collecting data via crowd-sourced annotators who are native speakers of the languages we use in this study .", "Comments": [], "label": []}
{"id": 158301, "text": "ROUGE Package : evaluate , [ https : ] ( https : //huggingface.co/spaces/evaluate-metric/rouge ) [ //huggingface.co/spaces/evaluate-metric/rouge ] ( https : //huggingface.co/spaces/evaluate-metric/rouge ) .", "Comments": [], "label": []}
{"id": 155869, "text": "We use the original data splits with 17,516/974/973 samples in train/dev/test sets .", "Comments": [], "label": []}
{"id": 151672, "text": "Experiment Details We use the Transformer model in our experiments , with 12 encoder layers and 12 decoder layers .", "Comments": [], "label": []}
{"id": 158681, "text": "Note that we exclude Hy-Transformer [ Yu and Yang , 2021 ] , GRAN [ Wang et al. , , 2021 ] and QUAD [ Shomer et al. , , 2022 ] for comparison because 1 ) they are heavily based on StarE and Transformer ; https : //github.com/xiongbo010/ShrinkE Table 4 : The performance of ShrinkE by removing one relational component on JF17K .", "Comments": [], "label": []}
{"id": 151872, "text": "For this , we use a standard cross entropy loss .", "Comments": [], "label": []}
{"id": 156137, "text": "[ Pytorch : ] ( http : //papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf ) [ An imperative style , high-performance deep learning ] ( http : //papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf ) [ library .", "Comments": [], "label": []}
{"id": 159319, "text": "We denote the corresponding transformer architectures of GHA and GHA-PS as Grouped Head Transformers ( GHT ) and Grouped Head Transformers with the Pillars of Strength ( GHT-PS ) , respectively .", "Comments": [], "label": []}
{"id": 152768, "text": "Examples for comparing different verbalizer outputs are shown in [ Table 12 ] in [ Appendix F. ] Later , we use this best generator to verbalize All-tables .", "Comments": [], "label": []}
{"id": 152932, "text": "Also , in line with previous analyses [ Di Gangi et al. , 2020 ] , SMALL-BPE models outperform the CHAR ones by ∼1 BLEU point .", "Comments": [], "label": []}
{"id": 159435, "text": "( a ) Regular Transformer Encoder Layer .", "Comments": [], "label": []}
{"id": 157699, "text": "* Included pointers to specific homework problems on implementing LSTMs and transformers , and asked about them individually .", "Comments": [], "label": []}
{"id": 159465, "text": "We use ADAM [ Kingma and Ba , 2015 ] for optimization , due to its robustness [ Schmidt et al. , , 2021 ] and popularity , with a learning rate of 0.0004 .", "Comments": [], "label": []}
{"id": 157875, "text": "Besides , we also devise a strong baseline , TableFiller , which ablates the graph module and adopts simple heuristic decoding algorithm , i.e . it only comprises a mention extractor , a biaffine encoder , and a classifier .", "Comments": [], "label": []}
{"id": 151384, "text": "We use the span method to calculate F1-score ( F1 ) , precision ( P ) , and recall ( R ) as the evaluation metrics .", "Comments": [], "label": []}
{"id": 159549, "text": "For text classification , we use AG 's News , Yahoo ( Zhang et al. , 2015 ) and DBPedia ( Lehmann et al. , 2015 ) .", "Comments": [], "label": []}
{"id": 155164, "text": "During inference , we use beam search with beam width = 20 for both models , after considering all values between 1 and 20 .", "Comments": [], "label": []}
{"id": 157333, "text": "We use the processed data from [ Zhang et al. , 2021b ] .", "Comments": [], "label": []}
{"id": 154973, "text": "In the third block , we use all the label sets , but we use * fixed weights * instead of using weight predicted from neural label search [ 4 ] .", "Comments": [], "label": []}
{"id": 151682, "text": "The evaluation results are listed in Appendix and we summarize them in Table [ 3 . ] We find that our mRASP2 significantly outperforms m-Transformer and substantially narrows the gap with pivot-based model .", "Comments": [], "label": []}
{"id": 155034, "text": "Therefore , for postprocessing , we use a rule-based method to replace non-standard punctuation with standard counterparts for Chinese .", "Comments": [], "label": []}
{"id": 155947, "text": "At this stage , we use shallow 4-layer models to perform ablation studies and estimate each approach 's strengths and weaknesses .", "Comments": [], "label": []}
{"id": 158124, "text": "We first build a base model with only front-ends and downstream speech recognition module , which follows Transformer architecture with 24 encoder layers and 9 decoder layers .", "Comments": [], "label": []}
{"id": 159342, "text": "Next , we show the importance of GCT for V2S . w/o GCT denotes that we directly perform V2S at the very beginning without GCT . w/o GC Works optimizing parameters of transformer modules rather than the MHA are not compared .", "Comments": [], "label": []}
{"id": 158955, "text": "And we use the difference between generated summary Yˆ and the ground truth Y as the training objective . 4 SDDS Model In this section , we introduce the Static-Dynamic graph based Dialogue Summarization model ( SDDS ) .", "Comments": [], "label": []}
{"id": 155337, "text": "We use the KL loss in [ Hinton et al. , 2015a ] as the distillation objective .", "Comments": [], "label": []}
{"id": 157265, "text": "1 Introduction BERT ( Bidirectional Encoder Representations from Transformers ) [ Devlin et al. , , 2019 ] is one of the most widely-used language model ( LM ) architectures for natural language understanding ( NLU ) tasks .", "Comments": [], "label": []}
{"id": 159210, "text": "Here , we employ n-gram based metrics ROUGE [ Lin , 2004 ] and BLEU [ Pa ] [ pineni et al. , , 2002 ] , embedding-based metric BERTScore [ Zhang et al. , , 2019 ] , and learningbased metric BLEURT [ Sellam et al. , , 2020 ] to evaluate the candidate summaries .", "Comments": [], "label": []}
{"id": 153360, "text": "We also use a training heuristic specific to two stage models that increases relation coverage across multiple languages .", "Comments": [], "label": []}
{"id": 156126, "text": "To compute these distances , we use the fact that each cluster represents a convex object .", "Comments": [], "label": []}
{"id": 152645, "text": "We used the provided implementations and suggested hyperparameters [ 4 ] to train these baselines .", "Comments": [], "label": []}
{"id": 157508, "text": "Figure [ 2 ] illustrates the performance of GPT-3 and T5 on the copy task , one of the simplest symbolic manipulation operations .", "Comments": [], "label": []}
{"id": 151356, "text": "Unsupervised Paraphrasing Some approaches train neural variational auto-encoders unsupervised to represent source sentences , then decodes from these representations to paraphrase [ Roy ] Depending on parameters we found most baselines took multiple seconds per example vs. 10s of seconds for REFLEC-TIVE DECODING on a multi-gpu machine .", "Comments": [], "label": []}
{"id": 160227, "text": "We use cross-entropy loss LCE to learn to generate the correct output out given input in .", "Comments": [], "label": []}
{"id": 159791, "text": "We find that Flan-T5 ( large ; [ Chung et al. , 2022 ] is not as capable , even when fine-tuned .", "Comments": [], "label": []}
{"id": 158049, "text": "We use these resources for non-commercial research purposes .", "Comments": [], "label": []}
{"id": 154443, "text": "For flat NER , we adopt * robertalarge * encoder .", "Comments": [], "label": []}
{"id": 160026, "text": "To prevent our relation classifier from being misled by poor connectives generated at the early stage of training while alleviating the discrepancy between training and inference , we adopt Scheduled Sampling to the joint learning .", "Comments": [], "label": []}
{"id": 157218, "text": "As can be seen from the table , our approach fares quite well against domain-general semantic parsing approaches and , importantly , significantly outperforms both T5-base and the LSTM .", "Comments": [], "label": []}
{"id": 154754, "text": "For all models we used a transformer with 3 layers and 6 attention heads , input size L = 1 , 024 and memory size 2,048 .", "Comments": [], "label": []}
{"id": 159025, "text": "4.2 Implementation Details For the CQA tasks , we use two types of external knowledge : knowledge graph and dictionary .", "Comments": [], "label": []}
{"id": 152360, "text": "Pre-Trained Transformer Models We test an uncased BERT-base model [ Devlin et al. , , 2019 ] , which has been shown to achieve near state-of-theart performance on several abuse detection tasks [ Tran et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 154406, "text": "For the self-implemented methods PACT , LSQ and LAQ , we adopt the commonly-used distillation loss adopted in [ Hinton et al. , , 2015 ] [ Jiao et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 157561, "text": "As a result , we use the default hyperparameters used by [ Ro et al. , 2020 ] for Multi2OIE .", "Comments": [], "label": []}
{"id": 155425, "text": "GShard [ Lepikhin et al. , , 2021 ] , Switch Transformer [ Fedus et al. , , 2021 ] , and BASE Layer [ Lewis et al. , , 2021 ] follow the learning-toroute paradigm and dynamically learn how to route each input token to experts .", "Comments": [], "label": []}
{"id": 159486, "text": "Pointer Network Following [ Logeswaran et al. , 2018 ] ; [ Lee et al. , 2020 ] , we use a pointer network to predict the original orders of the shuffled sentences .", "Comments": [], "label": []}
{"id": 157588, "text": "Specifically , certain relationship types ( e.g. , nationalities ) allow models We did not explore widely-used encoder-decoder models such as T5 , as their supervised pretraining consists of QA .", "Comments": [], "label": []}
{"id": 151605, "text": "Specifically , we propose to associate attention heads and intermediate layers of the fully-connected sub-layers in a transformer with learnable coefficients , which will be jointly trained with BERT but with an additional ` regularization to promote sparsity .", "Comments": [], "label": []}
{"id": 156240, "text": "For training , we use the filtered version of the four datasets from [ Zhou et al. , 2021a ] , which ensures each dialogue contains at least one commonsense knowledge triple from ConceptNet .", "Comments": [], "label": []}
{"id": 158787, "text": "To use TaPas for our task , we use table caption as a proxy for the input question .", "Comments": [], "label": []}
{"id": 152292, "text": "For each sentence si in the input document , T is applied to obtain a contextual representation for each word : And the representation of a sentence is acquired by applying weighted-pooling : Document-level transformer T takes s as input and yields a contextual representation for each sentence : 3.3 Deep Differential Amplifier In the Transformer model sketched above , intersentence relations are modeled by multi-head attention based on softmax functions , which only capture shallow structural information [ Liu et al. , , 2019 ] .", "Comments": [], "label": []}
{"id": 157958, "text": "In addition , our approach requires a well-trained language model for task identification and a Transformer-based model ( well-trained also ) for parameter efficient tuning .", "Comments": [], "label": []}
{"id": 156203, "text": "Thus , for CELL type token in formula , we use a [ RANGE ] tag as input token and copy all cell-level embeddings ( position , format , numeric , ... ) from the referenced cell to this CELL type token .", "Comments": [], "label": []}
{"id": 159537, "text": "4.1 Zero-shot Evaluation For zero-shot performance , we evaluate the following models on FERMAT without any training : [ 11 ] T0 ( 3B ) [ Sanh et al. , , 2022 ] , FLAN-XL ( 3B ) [ Wei et al. , 2022a ] , BHASKARA ( 2.7B ) ( ¯ [ Mishra et al. , , 2022a ] , FLAN-large ( 770M ) , FLAN-base ( 220M ) , T5-base ( 220M ) [ Raffel et al. , , 2020 ] , BARTbase ( 140M ) [ Lewis et al. , , 2020 ] , and NT5 ( 3M ) [ Yang et al. , , 2021 ] , where the size of the models is given in brackets .", "Comments": [], "label": []}
{"id": 152571, "text": "We use the best checkpoint by validation loss for evaluation , except for Section [ 4.4.2 ] where we average the 10 best checkpoints .", "Comments": [], "label": []}
{"id": 159916, "text": "Of course , the present study can not determine whether HyperMixer is as versatile as Transformers .", "Comments": [], "label": []}
{"id": 160044, "text": "We use the AdamW optimizer with an initial learning rate of 1e-5 , a batch size of 16 , and a maximum epoch number of 10 for training .", "Comments": [], "label": []}
{"id": 154224, "text": "For numbers , instead of using the initial input value ' [ CLS ] ' of the Transformer decoder , we use the input `` [ CLS ] explain : * context * [ SEP ] , '' where the * context * part depends on the number or variable .", "Comments": [], "label": []}
{"id": 153569, "text": "Early approaches mainly attempt to Figure 2 : Architecture of METER framework : ( a ) Multimodally-Enhanced Transformer , which takes user ID u , item ID i , feature word f as initial condition tokens .", "Comments": [], "label": []}
{"id": 155226, "text": "We want to defend four textual NN models ( base models ) of different architectures , namely RNN with GRU cells [ Chung et al . ] , * transformer * -based BERT [ Devlin et al. , , 2019 ] and RoBERTa [ Liu et al. , , 2019b ] .", "Comments": [], "label": []}
{"id": 154992, "text": "The Transformer network produces two outputs : the final output g ∈ R d and the activation h ∈ R 2N×d , [ 4 ] where d denotes the hidden size of the Transformer network and N is the number of layers of the Transformer network .", "Comments": [], "label": []}
{"id": 156060, "text": "For Chinese characters , we use two masking strategies – Whole Word Masking ( WWM ) and Char Masking ( CM ) because a large number of words in Chinese consist of multiple characters [ Cui et al. , , 2019 ] [ Sun et al. , 2021 ] .", "Comments": [], "label": []}
{"id": 151722, "text": "HIF+KATID3 and HIF+KATXGB inducts KAT with ID3 algorithm and xgboost respectively constraining maximum depth to 3 .", "Comments": [], "label": []}
{"id": 160184, "text": "We use the 'LM adapted ' version of T5 models since they have been shown to work better for prompt-based learning [ Lester et al. , , 2021 ] .", "Comments": [], "label": []}
{"id": 155180, "text": "We also find that our model performs competitively with the quantized transformer ( QT ) [ An ] [ gelidis et al. , , 2021 ] and CTRLSUM [ He et al. , , 2020 ] methods in this dataset .", "Comments": [], "label": []}
{"id": 153837, "text": "We observed a similar thing in our experiments , but saw that some Transformer architectures resulted in an increased chance of finding this * needle * .", "Comments": [], "label": []}
{"id": 156295, "text": "BART is a seq2seq autoencoder with a Transformer architecture .", "Comments": [], "label": []}
{"id": 151514, "text": "First , to ensure distributional differences and non-parallelism , we use 5 % of the training data from the source language and augment a different ( nonparallel ) 5 % Table 2 : XNER results on WikiANN . data for the target language .", "Comments": [], "label": []}
{"id": 157583, "text": "[ Petroni et al. , 2019 ] demonstrate that large pretrained LMs such as BERT [ Devlin et al. , , 2019 ] memorize the significant amount of world knowledge in their parameters ( * parametric knowledge * ) , and [ Roberts et al. , 2020 ] show that fine-tuned T5 without any reference documents ( closedbook QA ) can achieve competitive performance on open-domain QA .", "Comments": [], "label": []}
{"id": 151798, "text": "For each encoder , we use the transformer architecture with 2 hidden layers , 8 attention heads , hidden size of 512 and filter size of 1,024 , and the parameters of two encoders are shared with each other .", "Comments": [], "label": []}
{"id": 151995, "text": "On Europarl , G-Transformer outperforms Transformer by 0.77 d-BLEU point , and G-Transformer fine-tuned on sentence-level Transformer enlarges the gap to 0.98 d-BLEU point .", "Comments": [], "label": []}
{"id": 153303, "text": "mosesdecoder/scripts/generic/multi-bleu.perl tensorflow/tensor2tensor/utils/get_ende_bleu.sh SacreBLEU signature : nrefs:1|case : mixed| eff : no|tok:13a|smooth : exp|version:2.0.0 Section [ A.3.3 ] details a supplemental experiment combining CipherDAug with Data Diversification .", "Comments": [], "label": []}
{"id": 158981, "text": "We use AdamW optimizer [ Loshchilov and Hutter , 2019 ] as our optimizing algorithm and employ beam search with size 5 to generate more fluency summary .", "Comments": [], "label": []}
{"id": 157726, "text": "During inference we use τ = −6.5 for ReTraCk A and τ = −7.5 for ReTraCk A+U .", "Comments": [], "label": []}
{"id": 157468, "text": "Instead of using entity segment indexes inferred from ground truth , we use word boxes provided by OCR .", "Comments": [], "label": []}
{"id": 157735, "text": "Since each KG has its own characteristics , it is intuitive that we adopt different mono-graph attentions to weight the neighbor information within each language .", "Comments": [], "label": []}
{"id": 155541, "text": "A pre-training , fine-tuning pipeline could be attractive for this reason ; communities could fine-tune their own models on a laptop if a multilingual/multi-speaker model were pre-trained on GPUs at a larger institution .", "Comments": [], "label": []}
{"id": 153913, "text": "T5 uses independent positional encodings for the encoder and decoder , and relies on multiple sentinel tokens to differentiate the masked spans .", "Comments": [], "label": []}
{"id": 159607, "text": "For lemmatization , we compare the performance of CLTK and UDPIPE with that of our full-fledged T5 models .", "Comments": [], "label": []}
{"id": 153166, "text": "As the MVSA-Multi dataset provides the coarsegrained sentiment labels for all the text-image pairs , we use the sentiment labels as supervision signals of our MSP task .", "Comments": [], "label": []}
{"id": 151417, "text": "We used two different kinds of state-of-the-art Table 2 : The extrinsic evaluation results on tweet sentiment analysis tasks .", "Comments": [], "label": []}
{"id": 154412, "text": "Compared to the vanilla transformer encoder , our encoder has slight differences in position embeddings and self-attention layer in fine-tuning phase , which contains richer location information and will be introduced in § [ 2.7 . ]", "Comments": [], "label": []}
{"id": 154167, "text": "2 : ] • Each auxiliary task can always bring improvement compared with the NCT model * w/o * task ; En↔Zh : https : //www.kexiaoguo.com/ and En↔De : https : //opus.nlpl.eu/OpenSubtitles.php https : //github.com/facebookresearch/LASER Note that , in the last two in-domain stages , we use the conventional multi-task learning to pre-train and fine-tune models rather than the scheduled multi-task learning .", "Comments": [], "label": []}
{"id": 158110, "text": "To produce high-quality instructions that accurately convey the intended tasks , we employ an iterative annotation process involving two expert annotators who have a thorough understanding of the task and the dataset .", "Comments": [], "label": []}
{"id": 151531, "text": "To reduce those tracepoints to an acceptable length due to the limit of GPU memory , we segment the tracepoints sequences uniformly by the same time window τ , and then each trace segment is converted to its minimal bounding rectangle .", "Comments": [], "label": []}
{"id": 157030, "text": "In this setting , all the student and teacher models are of the same model architecture , which is Transformer-base .", "Comments": [], "label": []}
{"id": 156733, "text": "For the pretrained models we used Paracrawl [ Esplà et al. , , 2019 ] for German and French , JParacrawl [ Morishita et al. , , 2020 ] for Japanese and the Backtranslated News from WMT2021 for Chinese .", "Comments": [], "label": []}
{"id": 155345, "text": "Translation Decoder Our * translation decoder * is composed of N transformer decoder layers , which contain an additional cross-attention layer compared with transformer encoder layers .", "Comments": [], "label": []}
{"id": 159217, "text": "OmitR . denotes * Omission Rate * in Eqation [ 1 . ] RG stands for the ROUGE score .", "Comments": [], "label": []}
{"id": 151766, "text": "In the following experiments , we use R = 90 and β = 2 as the default setting for our sampling strategy if not otherwise stated .", "Comments": [], "label": []}
{"id": 154290, "text": "To remedy this , we employ BiLSTM-bert , which treats a timeline as a sequence of posts to be modelled , each being represented via the [ CLS ] representation of BERT ( f ) .", "Comments": [], "label": []}
{"id": 160273, "text": "0.288 Text Similarity Mean 1 5 0 0.7 0.668 0.694 0.667 0.655 0.878 0.242 0.621 3510.933 λ λa λb λc 1 5 0.5 0.7 0.664 0.692 0.664 0.651 0.878 0.241 0.621 3504.548 0 0.621 0.611 0.655 Image Similarity Mean Edit Distance ( BLEU-4 ) Time ( s ) Varying Lambda vs F1 Varying Lambda vs Mean Hypothesis Relevance Varying Lambda vs Mean Hypothesis to Image Relevance λ λa λb λc 10 0.654 0.638 0.61 0.605 0.61 0.615 0.62 0.625 0.63 0.635 0.64 0.645 0.65 0.655 0.66 0.866 0.868 0.87 0.872 0.874 0.876 0.878 0.88 0.882 0.234 0.236 0.238 0.24 0.242 0.244 Mean Hypothesis to Image Similarity Mean Hypothesis Similarity Mean F1 5 0.876 0.873 0.873 10 0.88 0.877 0.868 0.235 0.231 0.242 0.5 0.235 0.231 0.241 0.235 0.231 0.241 0.236 0.235 0.235 0.239 0.239 0.231 0.5 0.282 0.278 0.289 lambda_a lambda_b lambda_c threshold Accuracy Precision Recall F1 λ λa λb λc 0 0.282 0.278 0.29 0.29 0.292 Similarity Figure 5 : Ablations on threshold , t , across task-specific metrics for both tasks .", "Comments": [], "label": []}
{"id": 151777, "text": "We further augment the training task by the introduction of two computationally-lite sentence-level contrastive learning tasks to enhance the alignment of cross-lingual sentence representation space , which compensates for the learning bottleneck of the lightweight transformer for generative tasks .", "Comments": [], "label": []}
{"id": 155988, "text": "For effectiveness , we use exactly the same metrics ( i.e. , accuracy , F1 , and EM ) as PET [ Schick and Schutze , 2021 ] .", "Comments": [], "label": []}
{"id": 160216, "text": "We use the default beam size for PathRetriever , which is 8 , and we use a beam size of 5 for MDR , to closely match PROMPTRANK 's pruning parameter K = 5 .", "Comments": [], "label": []}
{"id": 159515, "text": "( 1 ) PLMs : We choose BERT [ Devlin et al. , , 2019 ] , RoBERTa [ Liu et al. , 2019 ] , BART [ Lewis et al. , , 2020 ] , T5 [ Raffel et al. , 2020 ] as the PLM backbones , and directly fne-tune them as baselines .", "Comments": [], "label": []}
{"id": 153394, "text": "The BPE algorithm grows V incrementally .", "Comments": [], "label": []}
{"id": 160154, "text": "We use Adam optimizer [ Kingma and ] [ Ba , 2015 ] with 4k warm-up steps .", "Comments": [], "label": []}
{"id": 156077, "text": "See Appendix [ B ] for more detailed parameter settings and Appendix [ C ] for all baseline models . 4.3 Evaluation Metrics We use strict evaluation metrics that an entity is confirmed correct when the entity boundary and the entity type are correct simultaneously .", "Comments": [], "label": []}
{"id": 159243, "text": "Specifically , for each target segment t ∈ D , we calculate the ROUGE-2 [ Lin , 2004 ] scores between the target segment t and all reference segments r ∈ Dℓ− and sort the reference segments according to their scores in descending order as S¯ ( t ) = r1 , r2 , .", "Comments": [], "label": []}
{"id": 152244, "text": "Our heuristic functions , combined with the proposed workflow , can be readily deployed to annotate new dialog datasets . 2 Related Work 2.1 Open-Domain Dialog System Evaluation Open-domain dialog system evaluation is a longlasting challenge .", "Comments": [], "label": []}
{"id": 159059, "text": "Data augmentation helps : In the ablation experiments , data augmentation in the supervised scenario shows a significant improvement ( * S-KPM-DA * vs. * S-KPM * ) , by around 4 points on ROUGE-L and up to 3 points on proposed evaluation metrics .", "Comments": [], "label": []}
{"id": 158106, "text": "We use OFA as it was the largest and most powerful open-source multimodal pre-trained model available at the time of our research while other stronger models did n't have publicly available checkpoints at that time .", "Comments": [], "label": []}
{"id": 154309, "text": "This reveals the potential limitations of our method and training using transformers is a future work .", "Comments": [], "label": []}
{"id": 155140, "text": "Following [ Vaswani et al. , 2017 ] , we use sinusoidal positional embedding , and apply layer normalization for word embedding and pre-norm residual connection following [ Wang et al. , 2019a ] .", "Comments": [], "label": []}
{"id": 157960, "text": "Did you report the number of parameters in the models used , the total computational budget ( e.g. , GPU hours ) , and computing infrastructure used ?", "Comments": [], "label": []}
{"id": 160324, "text": "B . * refers to regional bias and * Anti-L+ * is short for anti-LGBTQ . https : //hatebase.org/ https : //zenodo.org/record/4773875 https : //github.com/doccano/doccano 4 Insult Lexicon We divide the insult lexicon built in the process of annotation into five categories according to the attacking object .", "Comments": [], "label": []}
{"id": 154731, "text": "While variations of efficient transformers have been proposed , they all have a finite memory capacity and are forced to drop old information .", "Comments": [], "label": []}
{"id": 157743, "text": "By varying how candidate sets are constructed and optimized , an extra calibration step can unlock large gains in relevance ( via ROUGE [ Liu and Liu , 2021a ] [ Liu et al. , , 2022 ] ) or improve the faithfulness of summaries to the source [ Nan et al. , , 2021b ] [ Cao and Wang , 2021a ] .", "Comments": [], "label": []}
{"id": 157697, "text": "For our expert reproduction of results , we limited the effort to 2 hours of setup time and 4 hours of total code runtime ( for successful runs of the code ) on a single GPU [ 31 ] for any model training or evaluation , while our computing restrictions were dictated by the available hardware and software within the environment used to reproduce results .", "Comments": [], "label": []}
{"id": 154461, "text": "( Table [ 3 , ] columns 6–7 ) , Transformer significantly outperforms Seq2Seq when trained on the timesegmented Train set , but does not when trained on the cross-project Train set .", "Comments": [], "label": []}
{"id": 157785, "text": "We use a learning rate of − for the encoder and − for all other parameters .", "Comments": [], "label": []}
{"id": 153332, "text": "We feed the sentence into BERT [ De ] [ vlin et al. , , 2019 ] and for each word w , we use the last subtoken emebedding of the last layer as its dense representations x .", "Comments": [], "label": []}
{"id": 158213, "text": "Concretely , we use a handcrafted information retriever ( see more details in Appendix [ A.2 ] , to collect statements related to the queried policy area as input to encode the specific representation .", "Comments": [], "label": []}
{"id": 155135, "text": "In this paper , we use ISO language code [ 4 ] to identify each language .", "Comments": [], "label": []}
{"id": 155909, "text": "Token length is the median value . of the sentence transformers with the same batch softmax objective used for fine-tuning the few-shot models and on the same data we used for training the cross attention model .", "Comments": [], "label": []}
{"id": 156350, "text": "D Supplemental Material : Improved Inference D.1 Inference Process In Sec [ 5.1 ] we discussed a potential extension of our user-user inference operator based approach , where instead of adding the top k edges based on only embedding similarity search , we used a global probabilistic inference approach .", "Comments": [], "label": []}
{"id": 158144, "text": "• TM-seq2seq [ Afouras et al. , , 2018a ] : TMseq2seq proposes a Transformer-based AVSR system to model the A-V features separately and then attentively fuse them for decoding , and uses sequence-to-sequence loss [ Watan ] [ abe et al. , , 2017 ] as training criterion .", "Comments": [], "label": []}
{"id": 158283, "text": "If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE , etc. ) ? * Section 4.1 and Appendix B *", "Comments": [], "label": []}
{"id": 151736, "text": "[ Yu et al. , 2020 ] propose a Multimodal Transformer model , which empowers transformer with a multimodal interaction module to capture the inter-modality dynamics between words and images .", "Comments": [], "label": []}
{"id": 157764, "text": "SciFive is a language model pre-trained on diverse biomedical tasks with T5-inspired [ Raffel et al. , , 2020 ] prefixes .", "Comments": [], "label": []}
{"id": 154882, "text": "The central differences between [ Zhang et al. , 2020 ] and our method are as follows : • Last , their method directly uses visual features extracted by ResNet [ He et al. , , 2016 ] , which may introduce too much noise .", "Comments": [], "label": []}
{"id": 156794, "text": "Perhaps as a result , 5-shot GPT-4 ( which also achieves significantly higher BLEU-4/Rouge-L ) is preferred in 64 % of cases .", "Comments": [], "label": []}
{"id": 156318, "text": "To evaluate fake news article detection , we used the dataset released by [ Nguyen et al. , , 2020 ] , put together from Twitter data using related work on rumor classification [ Kochkina et al. , , 2018 ] [ Ma et al. , , 2016 ] and fake news detection [ Shu et al. , , 2018 ] .", "Comments": [], "label": []}
{"id": 152823, "text": "Concretely , we fine-tune a T5-based model on the QED dataset to perform explanation generation following the recipe of [ Lamm et al. , 2021 ] , and use this to identify predicates and references for the question from each ( q , c , a ) triple .", "Comments": [], "label": []}
{"id": 154510, "text": "To implement the reward , we have chosen to use ROUGE-L F1 for the references and a multi-document coverage score for the input documents that we describe hereafter .", "Comments": [], "label": []}
{"id": 158345, "text": "During per-training , we use a global batch size of 512 with 2500 steps warmup using the AdamW optimizer .", "Comments": [], "label": []}
{"id": 151927, "text": "The threshold values we used are given in the appendix .", "Comments": [], "label": []}
{"id": 155060, "text": "We employ early stopping with a patience of 15 evaluation steps and use the best performing checkpoint for the final evaluation .", "Comments": [], "label": []}
{"id": 155746, "text": "Due to our limited GPU resources , we compare the methods built on * GPT-2 Medium * rather than * GPT-2 Large * .", "Comments": [], "label": []}
{"id": 154833, "text": "Apart from the low-dimensional setting that is common in hyperbolic literature , we scale up the model to be the same size as Transformer base ( 512-dimensional input ) [ Vaswani et al. , , 2017 ] .", "Comments": [], "label": []}
{"id": 153317, "text": "In CipherDAug , this is accomplished by Note that in Table [ 6 , ] the BPE vocabularies from the original source and target remain approximately same across the baseline ( 12k ) and CipherDAug ( 11.8k ) even though the final vocabulary sizes of our models vary with the addition of the enciphered source ( s ) .", "Comments": [], "label": []}
{"id": 154348, "text": "Since our MedLAMA contains multiple answers for queries , we use a threshold on the average exact matching score , i.e . avgmatch > 0.1 , to filter out easy examples , where avg-match is calculated by : avg-match = Count ( matched answers ) Count ( total answers ) .", "Comments": [], "label": []}
{"id": 158738, "text": "Multi-Modal Intent Prediction For the PhotoChat dataset , we report the performances of strong baselines as in [ Zang et al. , , 2021 ] , including ALBERT-base [ Lan et al. , , 2019 ] , BERT [ Devlin et al. , 2018 ] , T5-base , and T5-3B [ Raffel et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 157123, "text": "For DIV-F , we use SentBERT [ Reimers ] [ and Gurevych , 2019 ] to encode the inputs of training examples into sentence embeddings , following [ Su et al. , 2022 ] .", "Comments": [], "label": []}
{"id": 154808, "text": "We implement all models in MindSpore .", "Comments": [], "label": []}
{"id": 155727, "text": "As we use multiple input hidden states and partitions , the differences would be enlarged .", "Comments": [], "label": []}
{"id": 159390, "text": "E2E-VLP [ Xu et al. , , 2021 ] : proposes the first endto-end VLP method for both V+L understanding and generation , with a unified Transformer encoderdecoder architecture .", "Comments": [], "label": []}
{"id": 153214, "text": "Therefore , in our work we use arc-based label scores to suit the LAS metric . where ∆ measures the difference between the incorrect tree and gold tree .", "Comments": [], "label": []}
{"id": 152754, "text": "For all the models , we use greedy decoding at inference time .", "Comments": [], "label": []}
{"id": 152248, "text": "The heuristics rules are implemented as heuristic functions based on regular expressions and dialog acts .", "Comments": [], "label": []}
{"id": 159000, "text": "We use the official test set for evaluation . * AG News * [ Zhang et al. , , 2015 ] This is a news classification dataset with four possible labels ( * sports * , * world * , * science/technology * , * business * ) .", "Comments": [], "label": []}
{"id": 159839, "text": "We use PLM 's embedding to search for label words semantically relevant to given label names .", "Comments": [], "label": []}
{"id": 159053, "text": "The results of Aspect Clustering are reported directly from the paper [ Alshomary et al. , , 2021 ] , as their code is not open source . 11B , 3B , Large and Base refer to Flan-T5-xxl , Flan-T5-xl , Flan-T5-Large and T5-effictive-base , respectively .", "Comments": [], "label": []}
{"id": 155497, "text": "For each token , we employ a 2-way classifier MLP h K j to predict its probability of being tagged as p where argmax ( p ) = 1 means positive ( see Appendix [ A ] for more details ) . • * Deriving head * .", "Comments": [], "label": []}
{"id": 159163, "text": "In case of Marathi , NLLB outputs are better , followed by IndicTrans .", "Comments": [], "label": []}
{"id": 158360, "text": "Specifically , in this work we use a total of 2.5M pretraining instances each consists of 2048 tokens .", "Comments": [], "label": []}
{"id": 156826, "text": "E Dataset Splits For FinDATA tasks , we use the official test set and development set if they exist and are publicly available : for TSA and FSRL , we use official test sets ; for NC and NAD , we use both official test and development sets .", "Comments": [], "label": []}
{"id": 158105, "text": "We use OFA [ Wang et al. , , 2022a ] [ 2 ] , a unified model that is pre-trained on a diverse set of multimodal and unimodal tasks in a single Transformerbased sequence-to-sequence framework , as the base pre-trained multimodal language model , and fine-tune it on MULTIINSTRUCT .", "Comments": [], "label": []}
{"id": 154477, "text": "Then we show how we adopt the graph-based stage-1 reranking with DPR retriever to improve passage retrieval ( Section [ 3.2 ] .", "Comments": [], "label": []}
{"id": 156289, "text": "Even when enough GPU memory is available , relying on such large models is still associated with extended runtimes , which can impede the progress of experiments when used once or more per epoch for validation and monitoring purposes .", "Comments": [], "label": []}
{"id": 154105, "text": "The musical style Transformer autoencoder [ Choi et al. , , 2020 ] uses a similar Transformer-based style transfer architecture to conditionally generate new music in controllable styles .", "Comments": [], "label": []}
{"id": 159623, "text": "As the EvaLatin dataset does not provide a validation set , we use 2 % of the training data as the validation set .", "Comments": [], "label": []}
{"id": 154929, "text": "Our code and data are available at < https : //ntunlpsg.github.io/project/coherence-paradigm > With recent advancements in neural methods , claims of fluency in summarization [ Liu et al. , , 2017 ] [ Celikyilmaz et al. , , 2018 ] , language modeling [ Radford et al. , , 2019 ] [ Brown et al. , , 2020 ] , response generation [ Zhang et al. , , 2020 ] [ Hosseini-Asl et al. , , 2020 ] and human parity in machine translation [ Hassan et al. , , 2018 ] have led to calls for finergrained discourse-level evaluations [ Läubli et al. , , 2018 ] [ Sharma et al. , , 2019 ] [ Popel et al. , , 2020 ] , since traditional metrics such as BLEU and ROUGE are unable to measure text quality and readability [ Paulus et al. , , 2018 ] [ Reiter , 2018 ] .", "Comments": [], "label": []}
{"id": 157488, "text": "Next , we use spectral clustering [ Von Luxburg , 2007 ] with the adaptive K-means [ Bhatia et al. , , 2004 ] algorithm to cluster these typical concepts into several groups , each of which corresponds to a topic .", "Comments": [], "label": []}
{"id": 158786, "text": "We also use constraint ( 4 ) for GNN training . 7 Experiments Baseline models : We implement DISCOMAT with LM as MATSCIBERT [ Gupta et al. , , 2022 ] , and the GNNs as Graph Attention Networks [ Velickovi ] ˇ c et al . ´ , [ 2018 ] .", "Comments": [], "label": []}
{"id": 158883, "text": "We employed BERT-Large [ Devlin et al. , , 2019 ] as the backbone and pre-trained them using the same dataset as MetaNER .", "Comments": [], "label": []}
{"id": 152743, "text": "We use greedy decoding for all models .", "Comments": [], "label": []}
{"id": 159873, "text": "In summary , our contributions can be enumerated as follows : learns attention patterns similar to Transformers .", "Comments": [], "label": []}
{"id": 151473, "text": "We explore a collection of heuristic approaches to select the decoy triples and craft adversarial perturbations that use different inference patterns to improve the model 's predictive performance on these decoy triples .", "Comments": [], "label": []}
{"id": 157995, "text": "As a qualitative experiment , we use the flan-t5-xl model to label the subset of the PRIDE dataset ( Section [ 6 ] for the appropriateness of all 49 relationships in our training data .", "Comments": [], "label": []}
{"id": 158199, "text": "A.3 Prompt Format We use the same format for label generation for all datasets , shown in Figure [ 7 , ] but customize the instruction for each dataset , as shown in Figure [ 8 . ] For example generation , we prompt with an example sampled from each class and a random novel label .", "Comments": [], "label": []}
{"id": 154506, "text": "To address this gap , in this paper we propose leveraging a modified coverage reward to improve information coverage across all the documents in the input set , jointly with a principled policy gradient estimator ( RELAX ) and a performing long transformer model ( the BART Longformer Encoder-Decoder , or BART-LED ) , in the hope of benefiting from the synergy between these components .", "Comments": [], "label": []}
{"id": 160334, "text": "Human labeled speech [ https : //github.com/facebookresearch/ ] ( https : //github.com/facebookresearch/fairseq/tree/ust/examples/speech_matrix ) [ fairseq/tree/ust/examples/speech_matrix ] ( https : //github.com/facebookresearch/fairseq/tree/ust/examples/speech_matrix ) data is expensive to create , there are very few data resources providing speech alignments , and the data amount is quite limited .", "Comments": [], "label": []}
{"id": 159670, "text": "T5-base , on the other hand , finds it easier to learn the distinctive ( and more direct ) relationship between u and the short program p .", "Comments": [], "label": []}
{"id": 157500, "text": "We adopt a BERTbase with 12 layers and 12 self-attention heads as the topic classifier and concept extractor in KPCE .", "Comments": [], "label": []}
{"id": 155343, "text": "Translation Encoder Our * translation encoder * is composed of N transformer [ Vaswani et al. , , 2017 ] encoder layers , which includes a self-attention layer , a feed-forward layer , normalization layers , and residual connections .", "Comments": [], "label": []}
{"id": 160172, "text": "However , curating large datasets of question-document pairs is expensive , especially for low-resource languages or domains that require unique expertise ( e.g. , medical or legal documents ) , thus creating a * bottleneck * for building QA pipelines [ Ram et al. , , 2022 ] .", "Comments": [], "label": []}
{"id": 152635, "text": "This work has benefited from financial support to SB by Eric and Wendy Schmidt ( made by recommendation of the Schmidt Futures program ) , Samsung Research ( under the project * Improving Deep Learning using Latent Structure * ) , Apple , and Intuit , and from inkind support by the NYU High-Performance Computing Center and by NVIDIA Corporation ( with the donation of a Titan V GPU ) .", "Comments": [], "label": []}
{"id": 158780, "text": "We use a table parser [ Jensen et al. , , 2019a ] for raw XML tables and captions .", "Comments": [], "label": []}
{"id": 160336, "text": "We use the following score to measure similarity between the source audio , and the target transcriptions or translations : where x and y are the source and target embeddings , and NNk ( x ) denotes the k nearest neighbors of x .", "Comments": [], "label": []}
{"id": 151450, "text": "Simultaneously , in this work , the data resources we use are all from published works and do not involve privacy issues related to data collection .", "Comments": [], "label": []}
{"id": 158195, "text": "In the Gold Data setting , we use CCL with held out data of the gold novel test class ( es ) as a strict upper bound for both label and example generation .", "Comments": [], "label": []}
{"id": 155930, "text": "It has been previously shown that the progressive elimination of word vectors occurring layer after layer can improve inference time of transformer-based language models used in a text classification scenario [ Goyal et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 151735, "text": "[ Chen et al. , 2020 ] use captions to represent images as text and adopt transformer-based sequence labeling models to connect multimodal information .", "Comments": [], "label": []}
{"id": 158962, "text": "Utterance Position Graph . * To capture the position information of utterances , we use the relative distance between utterances as the edge feature of utterance position graph G s p .", "Comments": [], "label": []}
{"id": 154498, "text": "Recently , the release of dedicated datasets [ Fabbri et al. , , 2019 ] [ Gholipour Ghalandari et al. , , 2020 ] , and intelligently designed Transformer models [ Liu et al. , , 2018 ] [ Liu and Lapata , 2019 ] [ Beltagy et al. , , 2020 ] , have helped drive advancements in multi-document summarization , generally improving the accuracy and fluency of the predicted summaries .", "Comments": [], "label": []}
{"id": 154047, "text": "• BART [ Lewis et al. , , 2020 ] adopts a seq2seq Transformer architecture with denoising pretraining objectives .", "Comments": [], "label": []}
{"id": 159178, "text": "All WSL approaches studied in < https : //pytorch.org/ > Table 2 : Detailed data statistics .", "Comments": [], "label": []}
{"id": 156085, "text": "We use standard metrics exact-match ( EM ) and F scores for measuring the quality of predicted answers .", "Comments": [], "label": []}
{"id": 153017, "text": "All baselines ( Transformer and LCDIMT ) and the BiTIIMT models are based on the architecture with dmodel = 512 , dhidden = 2048 , nheads = 8 , nlayers = 6 , and pdropout = 0.1 .", "Comments": [], "label": []}
{"id": 152906, "text": "Note that this strategy relies on the fact that translations in higher-resource languages are included in 푇푒 , which we adopt by using English in our experiments .", "Comments": [], "label": []}
{"id": 153690, "text": "< https : //www.nltk.org/ > 98 misinfo and 98 real headlines in the dev .", "Comments": [], "label": []}
{"id": 153216, "text": "For PTB , we use the Stanford Dependencies conversion software of version 3.3 to obtain dependency trees .", "Comments": [], "label": []}
{"id": 157000, "text": "For both BART and T5 , the largest performance gaps appear when we test on the two non-American dialects , CollSgE and IndE ( -15.3 % and -12.3 % exact match accuracy for T5- 3b ) .", "Comments": [], "label": []}
{"id": 158813, "text": "We use the validation set to choose the weighting between these two types of supervised data . 4.2 Fine-grained Module supervision Using the multiple-choice feedback on the types of improvement , the model can learn to improve those individual components of the model .", "Comments": [], "label": []}
{"id": 154291, "text": "To convert the post-level scores/representations from ( iii ) - ( v ) above into time-sensitive models we used the same BiLSTM from ( vi ) , operating at the timeline-level .", "Comments": [], "label": []}
{"id": 154939, "text": "We use the same training data as the baseline models to train our contrastive model ; the positive documents remain the same , while we use 5 negative documents per instance ( instead of only 1 in the pairwise setup ) .", "Comments": [], "label": []}
{"id": 157788, "text": "Did you report the number of parameters in the models used , the total computational budget ( e.g. , GPU hours ) , and computing infrastructure used ?", "Comments": [], "label": []}
{"id": 160435, "text": "] ( https : //fasttext.cc/docs/en/crawl-vectors.html ) [ html ] ( https : //fasttext.cc/docs/en/crawl-vectors.html ) J.2 Results on Sentence Simplification In this section , we present results on existing test sets , i.e. , * ZEST * [ Mallinson et al. , , 2020 ] ( see [ Ta ] [ ble 16 ] , * APA-LHA C2-A2 * [ Spring et al. , , 2021 ] ( see [ Table 17 ] , * APA-LHA C2-B1 * [ Spring et al. , , 2021 ] ( see [ Table 18 ] , and * TCDE19 * [ Naderi et al. , , 2019 ] ( see [ Table 19 ] .", "Comments": [], "label": []}
{"id": 157461, "text": "Our backbone model consists of a 6-layer GCN encoder to generate structure-aware super-tokens , followed by a 12-layer ETC transformer decoder equipped with Rich Attention for document entity extraction .", "Comments": [], "label": []}
{"id": 153944, "text": "For the question generation task , we use the SQuAD 1.1 dataset [ Rajpurkar et al. , , 2016 ] and follow the dataset split of [ Du et al. , , 2017 ] .", "Comments": [], "label": []}
{"id": 159825, "text": "We then proposed a distillation technique in which we augmented target RE labels with * Chain of Thought * ( CoT ) style explanations elicited from GPT-3 and used this to fine-tune Flan-T5 ; this yielded SOTA performance across all datasets considered , often by wide margins ( 5-10 points in F1 ) .", "Comments": [], "label": []}
{"id": 155440, "text": "Then , the * k * NN-MT aggregates the retrieved tokens to form a probability distribution pkNN ( y |x , yˆ1 : i−1 ) with a softmax with temperature T to the negative L distances [ 2 ] , 2.2 Presenting Examples We used a pair of incorrect and correct sentences stored in the value retrieved for the predicted token yˆ as an example from the correction .", "Comments": [], "label": []}
{"id": 158776, "text": "We adopted two strong visual encoder architecture , ResNet-101 and CRNN ( VGG+BiLSTM ) .", "Comments": [], "label": []}
{"id": 157020, "text": "More formally , we use Q ( y , y < j , x , ϕ ) to quantify how well a student model predicts a target token .", "Comments": [], "label": []}
{"id": 154430, "text": "There are 2 essential components that contribute greatly the success of our model : Firstly , We adopt a newly developed pretrained LM as the initializer and further continue its pretraining pipeline on our dialog dataset ( Reddit ) and thus we have a really powerful encoder-decoder .", "Comments": [], "label": []}
{"id": 155712, "text": "To have the highest but similar dot products for the two options , the transformer encoder in GPT-2 wants to output the hidden state that is close to the average of the * woman * embedding and the * king * embedding .", "Comments": [], "label": []}
{"id": 155393, "text": "By contrast , Switch Transformer and the training stage 1 in STABLE-MOE design balance losses to control the balance of the token-to-expert assignment .", "Comments": [], "label": []}
{"id": 159229, "text": "MELM . [ Zhou et al. , , 2022 ] Masked Entity Language Modeling ( MELM ) proposes fine-tuning a transformer-encoder-based PLM on linearized labeled sequences using masked language modeling .", "Comments": [], "label": []}
{"id": 159935, "text": "For computing expected validation performance , we use the public implementation by [ Dodge et al. , 2019 ] .", "Comments": [], "label": []}
{"id": 159100, "text": "For the other hyper-parameters of GRM , we use grid search to determine the best value , the result is shown in Table A3 .", "Comments": [], "label": []}
{"id": 158039, "text": "We use D & D as our test bed and construct large-scale data G-DRAGON by using IDM to provide quality labels .", "Comments": [], "label": []}
{"id": 156044, "text": "Given a dataset we use the retention generation function in Equation [ 5 ] and random method to generate a fix number of sequencelength configurations , separately .", "Comments": [], "label": []}
{"id": 156544, "text": "We use both image-text and video-text data for pre-training .", "Comments": [], "label": []}
{"id": 152189, "text": "The specific BERT variant we use is BERTBASE ( uncased ) [ Devlin et al. , , 2019 ] .", "Comments": [], "label": []}
{"id": 158428, "text": "We use L-layer stacked R-GCN [ Schlichtkrull et al. , 2018 ] to model the interactions among different nodes through edges with different relation types .", "Comments": [], "label": []}
{"id": 160420, "text": "It was tested on Chinese- ] ( https : //huggingface.co/T-Systems-onsite/cross-en-de-roberta-sentence-transformer ) [ English parallel corpora and showed promising ] ( https : //huggingface.co/T-Systems-onsite/cross-en-de-roberta-sentence-transformer ) [ results .", "Comments": [], "label": []}
{"id": 153034, "text": "[ Xu and Carpuat , 2021 ] propose a novel re-position operator to replace deletion in Levenshtein Transformer to exploit lexical constraints more effectively and efficiently .", "Comments": [], "label": []}
{"id": 155634, "text": "To verify the effectiveness and architecturally generalizability of our method , we conduct the generation tasks with three dominant neural language models , including LSTM , Transformer and GPT-2 .", "Comments": [], "label": []}
{"id": 158489, "text": "We fine-tune the T5 model to find the optimal parameters θ ∗ to encode the input sequence and verbalize it into an idea sequence , i.e. , the item p in the quintuple .", "Comments": [], "label": []}
{"id": 155584, "text": "We apply subword segmentation with a joint inputoutput byte pair encoding model with 32 , 000 operations .", "Comments": [], "label": []}
{"id": 158390, "text": "A Appendix A.1 Dataset Information We use the QMSum and SQuALITY datasets according to their intended research purposes .", "Comments": [], "label": []}
{"id": 158712, "text": "For BERT , RoBERTa , and XLNet , we used the AdamW with a learning rate of 5e-6 .", "Comments": [], "label": []}
{"id": 153349, "text": "For different domains , we use independent soft prompts to represent domain-specific information , thus making them have the * domain-aware * knowledge .", "Comments": [], "label": []}
{"id": 151964, "text": "We follow the standard Transformer big model [ Vaswani et al. , , 2017 ] , using 6 layers , 16 heads , 1024 dimension outputs , and 4096 dimension hidden vectors .", "Comments": [], "label": []}
{"id": 160363, "text": "The HuBERT model consists of 7 convolutional layers and 12 Transformer encoder layers .", "Comments": [], "label": []}
{"id": 158907, "text": "Based on this hypothesis , we approach the problem of hallucination detection as a problem of anomaly detection with an optimal transport ( OT ) formulation [ Kantorovich , 2006 ] [ Peyré et al. , , 2019 ] .", "Comments": [], "label": []}
{"id": 155031, "text": "We used the same tokenization and vocabulary as the mT5 model [ Xue et al. , , 2021 ] .", "Comments": [], "label": []}
{"id": 159968, "text": "We use the S2TT model in B3 for S2TT pretraining . t-mBART and u-mBART stand for text-based mBART and unit-based mBART , respectively .", "Comments": [], "label": []}
{"id": 159544, "text": "References Winter , Chris Hesse , Mark Chen , Eric Sigler , Mateusz Litwin , Scott Gray , Benjamin Chess , Jack Clark , Christopher Berner , Sam McCandlish , Alec Radford , Ilya Sutskever , and Dario Amodei .", "Comments": [], "label": []}
{"id": 156743, "text": "Models and Training We train an LSTM [ Hochreiter and Schmidhuber , 1997 ] and finetune a T5 transformer [ Raffel et al. , , 2020 ] on the sequence-to-sequence prediction problem ( x 1 : N ins , x 0 state ) → x N state Training details may be found in Appendix [ C. ] We compare these baseline models to their LEXSYM-augmented versions as well as the existing compositional data augmentation scheme of [ Liu et al. , 2021b ] .", "Comments": [], "label": []}
{"id": 151539, "text": "Temporal Contrastive Constraints According to the split mentioned above , we aggregate the transformer 's last layer hidden states of trace segments and caption sentences respectively , and denote them as Hts = { h 1 ts , . . . , h ts } and Hcs = { h 1 cs , . . . , h cs } .", "Comments": [], "label": []}
{"id": 154334, "text": "Additionally , we demonstrate that different state-of-the-art PLMs and transformer layers are suited for different types of relational knowledge , and different relations requires different depth of tuning , suggesting that both the layers and tuning depth should be considered when infusing knowledge over different relations .", "Comments": [], "label": []}
{"id": 156367, "text": "For example , we use the simulated dialogues from AgenT and Sys-AgenT to build the GPTAT .", "Comments": [], "label": []}
{"id": 160430, "text": "CATS align each original sentence with the ] ( https : //huggingface.co/T-Systems-onsite/cross-en-de-roberta-sentence-transformer ) [ closest simple sentence by calculating the similar ] ( https : //huggingface.co/T-Systems-onsite/cross-en-de-roberta-sentence-transformer ) [ ity of all of them based on n-grams ( option : C3G ] ( https : //huggingface.co/T-Systems-onsite/cross-en-de-roberta-sentence-transformer ) [ or word vectors ( option : CWASA and WAVG ) .", "Comments": [], "label": []}
{"id": 154276, "text": "B.4 Evaluation Metrics We use a variety of evaluation metrics to compare our approach 's performance on two major facets : ( 1 ) Quality of generated text , and ( 2 ) success on matching the target attribute used for control .", "Comments": [], "label": []}
{"id": 159707, "text": "TyDiQA-GP We use the gold passage version of the Typologically Diverse Question Answering [ Clark et al. , , 2020a ] dataset , a benchmark for information-seeking question answering , which covers nine languages .", "Comments": [], "label": []}
{"id": 156483, "text": "We compare with two models , Q and ANLI based on 11B T5 reported by [ Honovich et al. , 2022 ] .", "Comments": [], "label": []}
{"id": 159580, "text": "Full results on ROUGE-1/2/L are given in Appendix [ D. ] The experimental results on Cross-Sum also verify the superiority of PISCES , which are provided in Appendix [ C.2 . ] PISCES vs .", "Comments": [], "label": []}
{"id": 155526, "text": "We use the the Noam Learning rate scheduler described in [ Vaswani et al. , 2017 ] with the warmup steps of 4000 , and the other hyper-parameter details are shown in Table [ 3 . ] We use the same hyperparameters for fine-tuning with the L2 language .", "Comments": [], "label": []}
{"id": 153117, "text": "For INCLUDE dataset , an XGBoost model is used [ Sridhar et al. , , 2020 ] with direct input as 135 pose-keypoints obtained using OpenPose .", "Comments": [], "label": []}
{"id": 156212, "text": "For TaPEx , we adopt the same table QA strategy in its paper by inputting the table and text as source , and generating the answer as target .", "Comments": [], "label": []}
{"id": 153927, "text": "While we can not directly compare GLM with T5 due to the differences in training data and the number of parameters , the results in Tables [ 1 ] and [ 6 ] have demonstrated the advantage of GLM . 4 Related Work Pretrained Language Models .", "Comments": [], "label": []}
{"id": 158644, "text": "Given the limited number of training samples , we utilize a split method similar to the random method described above .", "Comments": [], "label": []}
{"id": 155092, "text": "To compare with these baselines , we use CodeXGLUE train/valid/test splits for training and testing .", "Comments": [], "label": []}
{"id": 156410, "text": "Since the QA-dataset is relatively small compared with RealToxicityPrompts , we use it to assist evaluating detoxification methods associated with sensitive topics . 4.3 Automatic Evaluation We evaluate our generated outputs for toxicity , fluency and diversity .", "Comments": [], "label": []}
{"id": 152490, "text": "4.4 Model Performance over Selected/Other Data Splits In order to better analyse the behaviour of AdvPicker across data variations , we use the trained language discriminator to split the target language test sets into Selected and Other partitions ( similarly to how the training set is processed ) .", "Comments": [], "label": []}
{"id": 154499, "text": "As for what models are concerned , abstractive MDS has made increasing use of transformers , both `` conventional '' [ Lewis et al. , , 2020 ] [ Zhang et al. , , 2020a ] and modified to accommodate the characteristic input length of multi-document sets [ Beltagy et al. , , 2020 ] [ Za ] [ heer et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 151490, "text": "We use the state-of-art evaluation protocol for data poisoning attacks [ Xu et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 151621, "text": "We use EarlyBERTBASE-Self ( only self-attention heads are pruned when drawing the winning ticket ) as the testing bed .", "Comments": [], "label": []}
{"id": 159103, "text": "We train the GRM model for 5 epochs in total when the Word2Vec model is taken as the background word embedding model .", "Comments": [], "label": []}
{"id": 158206, "text": "* Note that here we use a slight generalization of the notion of sparsity called * compressibility * defined as follows .", "Comments": [], "label": []}
{"id": 158205, "text": "Model architecture For finite-sample ASR-U , we use wav2vec-U [ Baevski et al. , , 2021 ] with several modifications .", "Comments": [], "label": []}
{"id": 160588, "text": "This paper tackles the problem of better utilizing GPUs for this task . . . .", "Comments": [], "label": []}
{"id": 152407, "text": "In dealing with large-scale transformer models like T5 , efficient memory usage is of paramount importance .", "Comments": [], "label": []}
{"id": 152236, "text": "We used the same 90 % split used to train the BERT models , but with automatic tokenization ( ≈ 600M tokens ) .", "Comments": [], "label": []}
{"id": 155953, "text": "DeepPyramidion achieves a ROUGE-2 score indistinguishable from SOTA on arXiv and performs competitively on PubMeb .", "Comments": [], "label": []}
{"id": 153479, "text": "We also conducted the experiments with automatically predicted sentiment labels rather than the gold ones as the reviewer suggested , where we used the mixed sentiment presentation by dotmultiplying the predicted sentiment distribution and the sentiment label representation .", "Comments": [], "label": []}
{"id": 157516, "text": "[ Nogueira et al. , 2021 ] fine-tune T5 with different ways of representing numbers , but even with the best-performing representation , the fine-tuned model can not achieve as good ac- curacy on out-of-distribution testing examples as in-distribution testing examples .", "Comments": [], "label": []}
{"id": 159530, "text": "Besides , we adopt the text encoder of CLIP as the VL-PTM for generating the visually-aligned representation .", "Comments": [], "label": []}
{"id": 156182, "text": "( 2 ) Based on the extracted table hierarchy , we use the source set of top and left header cells to include their indexed data cells , and we also use the source set of data cells to include corresponding header cells .", "Comments": [], "label": []}
{"id": 157153, "text": "To obtain inferential commonsense knowledge , we use COMET [ Bosse ] [ lut et al. , , 2019 ] , a pretrained generative model , to generate rich commonsense statements .", "Comments": [], "label": []}
{"id": 156979, "text": "Second , we use this system as a data augmentation technique to improve the dialect robustness of existing systems .", "Comments": [], "label": []}
{"id": 152265, "text": "Given any nonempty subset * S * ⊆ * D * train , we use the * K * NN classifier to classify * x * dev .", "Comments": [], "label": []}
{"id": 151669, "text": "We report de-tokenized BLEU with Sacre- https : //github.com/facebookresearch/MUSE Bg , Cs , De , El , En , Es , Et , Fi , Fr , Gu , Hi , It , Ja , Kk , Lt , Lv , Ro , Ru , Sr , Tr , Zh , Nl , Pl , Pt http : //data.statmt.org/news-crawl Table 2 : mRASP2 outperforms m-Transformer in unsupervised translation directions by a large margin .", "Comments": [], "label": []}
{"id": 159272, "text": "In light of the fact that the relation in USSA is sensitive to the relative distance of word pairs ( e.g. , the greater the NW span , the more words are spaced for the next word ) , we use distance feature to represent the relative distance information .", "Comments": [], "label": []}
{"id": 152496, "text": "The exact formula we use is shown in Table [ 1 , ] where * Partial Ratio * ( pr ) measures the similarity between each context mention and the description .", "Comments": [], "label": []}
{"id": 158509, "text": "4.2 Multiple Extractive Oracles To provide basic supervised signals for multiple extractors , we employ M separate * extractive oracles * , n X j o o j=1 , thus introducing M * oracle losses * , defined as follows : In our setting , we deploy two different sets of extractive oracles for X ( j ) : ROUGE-based extractive oracles , as in DYLE , and our CES-based extractive oracles , which we clearly specify in Section [ 6.1.1 . ]", "Comments": [], "label": []}
{"id": 152698, "text": "We therefore use a single sample Yˆ ( which takes a large portion of probability mass from the teacher ) instead as in [ Kim and Rush , 2016 ] . 3.3 Re-scaling attention temperatures Both our teacher and student models are Seq2Seq Transformer models .", "Comments": [], "label": []}
{"id": 154153, "text": "50 BPE tokens are given as prefix and the models are to generate the continuation of 100 next BPE tokens .", "Comments": [], "label": []}
{"id": 152848, "text": "Experiment results show that DYLE outperforms all existing methods on GovReport and QMSum , with gains up to 6.1 ROUGE , while yielding strong results on arXiv .", "Comments": [], "label": []}
{"id": 158314, "text": "For the general scenario , we use the same templates as PTR .", "Comments": [], "label": []}
{"id": 159564, "text": "< https : //hf.co/Krystalan/PISCES > We use `` direction '' to denote the summarization direction from the source to the target languages , e.g. , English ( documents ) ⇒ Chinese ( summaries ) .", "Comments": [], "label": []}
{"id": 152516, "text": "When used to extend the pretraining of transformer-based language models , our approach closes the performance gap between unsupervised and supervised pretraining for universal sentence encoders .", "Comments": [], "label": []}
{"id": 158393, "text": "For the ablations , we use batch size of 64 and the same number of steps .", "Comments": [], "label": []}
{"id": 153981, "text": "In Aug-WoW-noctx we do not concatenate the dialogue context , and in Aug-WoW-BertLarge we use the Bert-Large model as base architecture .", "Comments": [], "label": []}
{"id": 158312, "text": "We use REINFORCE [ Williams , 1992 ] , a classical algorithm that is also used in [ Zhang et al. , 2021 ] , to obtain the assessment of sample quality . 3 Method Previous research has demonstrated the effectiveness of prompt-tuning as a practical approach to address few-shot learning in NLP .", "Comments": [], "label": []}
{"id": 157237, "text": "Results are statistically significant with p-value < 0.005. tune BERTbase as the teacher model , and utilize a smaller BERT released by [ Turc et al. , 2019 ] with 6 Transformer layers , 768 hidden neurons and 12 attention heads to instantiate the student model following [ Park et al. , 2021 ] .", "Comments": [], "label": []}
{"id": 159868, "text": "However , the token mixing MLP learns a * fixed-size * set of * position-specific * mappings , arguably making MLPMixer 's architecture too detached from the inductive biases needed for natural language understanding , in contrast to Transformers [ Henderson , 2020 ] .", "Comments": [], "label": []}
{"id": 151600, "text": "By instead using network slimming [ Liu et al. , , 2017 ] on the self-attention and fully-connected sub-layers inside a transformer , we are the first to introduce an effective approach that can identify * structured winning tickets in the early stage of BERT training * , that are successfully applied for efficient language modeling pre-training and fine-tuning .", "Comments": [], "label": []}
{"id": 152976, "text": "Replacing T5-3b with T5-base for the recaller is significantly faster in answer recalling , but is much less precise and produces more answer candidates with the same αneg , which increases the burden on the verifier and thus may fail to reduce the overall computation cost if αneg is not raised .", "Comments": [], "label": []}
{"id": 155071, "text": "Due to computational restrictions , we slightly modify the experimental setup from the main experiments : we use mixed precision training and a more aggressive early stopping patience of 3 evaluation steps .", "Comments": [], "label": []}
{"id": 157506, "text": "Figure [ 2 ] shows the performance of GPT-3 and T5 on addition using the scratchpad version of training data .", "Comments": [], "label": []}
{"id": 159442, "text": "Regardless , most closely related to the presented work are the studies by [ Zhang et al. , 2021 ] and [ Purason and Tättar , 2022 ] . [ Zhang et al. , 2021 ] propose adding Conditional Language-Specific Routing ( CLSR ) layers inside the encoder and decoder Transformer layers .", "Comments": [], "label": []}
{"id": 153678, "text": "For categorical inferences ( e.g . the gold label ) , we use either generative models or BERT-based discriminative models [ Devlin et al. , , 2019 ] .", "Comments": [], "label": []}
{"id": 151715, "text": "We propose Key Attribute Tree , a decision tree , to make the matching decision based on * key attribute * heuristic , in the sense that some attributes are more important than others for EM .", "Comments": [], "label": []}
{"id": 153420, "text": "In Online , we recall a candidate set of code snippets according to the Hamming distance between the query and code , and then we use the original DCS model to re-rank the candidates .", "Comments": [], "label": []}
{"id": 159817, "text": "* * Target * * [ [ Ray , 'Kill ' , King ] ] * * Explanation * * - We then use these explanations along with reference relation labels as targets to fine-tune Flan-T5 ( Large ) , as depicted in Figure [ 3 . ]", "Comments": [], "label": []}
{"id": 155298, "text": "We use a maximum possible batch size that fits inside the GPU , which results in batch size of 192 .", "Comments": [], "label": []}
{"id": 155473, "text": "We use the dataset provided by the CodeXGLUE team [ Lu et al. , 2021 ] for this task .", "Comments": [], "label": []}
{"id": 152582, "text": "Source code for this paper can be found at [ https : //github.com/ ] ( https : //github.com/castorini/transformers-selective ) [ castorini/transformers-selective ] ( https : //github.com/castorini/transformers-selective ) .", "Comments": [], "label": []}
{"id": 157017, "text": "We use x = x1 , . . . , x to denote a * source-language sentence * and y = y1 , . . . , y to denote a * target-language sentence * .", "Comments": [], "label": []}
{"id": 158407, "text": "Specifically , we use the IMDB dataset [ Maas et al. , , 2011 ] and SST-2 [ Socher et al. , , 2013 ] on sentiment analysis , the 20NewsGroups ( 20NG ) dataset [ Lang , 1995 ] on topic classification , the RTE [ Wang et al. , 2018 ] and MNLI [ Williams et al. , , 2018 ] on natural language inference , the English side of Multi30k [ Elliott et al. , , 2016 ] on machine translation , the cross-intent dataset CLINC150 [ Larson et al. , 2019 ] , and the NewsCategory multiclass classification dataset [ Misra , 2018 ] .", "Comments": [], "label": []}
{"id": 159589, "text": "D Full Results on WikiLingua Table [ 12 ] shows the experimental results in terms of ROUGE-1 , ROUGE-2 and ROUGE-L , respectively .", "Comments": [], "label": []}
{"id": 154991, "text": "Initially , the inputs to the Transformer network are z and H0 , where H is an empty sequence .", "Comments": [], "label": []}
{"id": 156102, "text": "In our implementation , we combine the heuristic strategies applied by GRR [ Asai et al. , , 2020 ] and MDR [ Xiong et al. , , 2021 ] .", "Comments": [], "label": []}
{"id": 158555, "text": "The head number is set to 4 for cross-modal transformer and 2 for graph-transformer .", "Comments": [], "label": []}
{"id": 158716, "text": "This might be because transformer models are better at capturing contextual information and claims are usually composed of more contextual information than noun phrases .", "Comments": [], "label": []}
{"id": 154108, "text": "3 Data In this section , we describe the pretraining and fine-tuning datasets we use in our studies .", "Comments": [], "label": []}
{"id": 154900, "text": "We use the script to compare aligned segment and tagging scores between oracle ( gold ) segmentation and realistic ( predicted ) segmentation .", "Comments": [], "label": []}
{"id": 153341, "text": "We use the first 10 % epochs to linearly warmup the learning rates of each components to their maximum value and gradually decay them to zero for the rest of epochs .", "Comments": [], "label": []}
{"id": 160383, "text": "Comparing against dense XM Transformer , GShard with 64 experts has 1.0 BLEU gains on average over 5 directions on EP/VP , and +0.3 BLEU gains for FLEURS .", "Comments": [], "label": []}
{"id": 158889, "text": "A Experiment Details A.1 Datasets for the extraction language model task Rather than randomly generating spans to form target labels in instruction , we use informative spans [ Bian et al. , , 2021 ] as target labels .", "Comments": [], "label": []}
{"id": 152015, "text": "Here , fnt . denotes the model finetuned on sentence-level Transformer .", "Comments": [], "label": []}
{"id": 157959, "text": "Therefore , it is challenging to cooperate our approach with a language model with random initialization or non-transformer architecture .", "Comments": [], "label": []}
{"id": 153528, "text": "Formally , we define a corrupted sentence as x1 , x2 , · · · , xL , and feed it into a Transformers encoder [ Vaswani et al. , , 2017 ] , the hidden states from the final layer are denoted as h1 , h2 , · · · , hL .", "Comments": [], "label": []}
{"id": 157864, "text": "Evaluation Metrics We adopt Precision ( P ) , Recall ( R ) , and F1-score ( F1 ) as evaluation metrics , same as previous methods [ Tran Phu and Nguyen , 2021 ] to ensure comparability . 4.2 Baselines We compare our proposed CHEER with various state-of-the-art SECI and DECI methods .", "Comments": [], "label": []}
{"id": 154289, "text": "( iii ) Emotion Classification We used DeepMoji ( EM-DM ) [ Felbo et al. , , 2017 ] and Twitter-roBERTabase ( EM-TR ) from TweetEval '20 [ Barbieri et al. , , 2020 ] operating on the post-level , to generate softmax probabilities for each emotion ( 64 for EM-DM , 4 for EM-TR ) .", "Comments": [], "label": []}
{"id": 154718, "text": "To monitor training dynamics , we use the Area Under the Margin ( AUM ) statistic [ Pleiss et al. , , 2020 ] which measures how different a true label for a sample is compared to a model 's * beliefs * at each epoch and is calculated as the average difference between the logit values for a sample 's assigned class and its highest non-assigned class across training epochs .", "Comments": [], "label": []}
{"id": 155178, "text": "Significantly , our model generally outperforms keyword matching , demonstrating that semantic match information from training with the BERTScore oracle may be more useful than training with a ROUGE oracle in terms of reproducing annotators ' judgments ; recall that our model has not been trained on any ASPECTNEWS data and only on our synthetic data .", "Comments": [], "label": []}
{"id": 156457, "text": "In addition , we used Berkeley Neural Parser ( Stern et al. , 2017 ; Kitaev and Klein , 2018 ; Kitaev et al. , 2018 ) for the analysis of the original and replaced sentences and showed that all the non-terminal symbols in the pre- and post-replaced phrases matched in our method , whereas different non-terminal symbols were assigned in RoBERTa in some cases ( syntactic structure has changed ) .", "Comments": [], "label": []}
{"id": 155845, "text": "For Chinese experiments , we use four datasets from CLUE [ Xu et al. , 2020 ] ( CMNLI [ 3 ] , OCNLI [ Hu et al. , , 2020 ] , TNews [ 3 ] , C [ Sun et al. , , 2020 ] ) , two sentiment analysis datasets ( ChnSent [ 4 ] and Amazon Reviews [ 4 ] ) , and one extra natural language inference dataset LCQMC [ Liu et al. , , 2018 ] .", "Comments": [], "label": []}
{"id": 154051, "text": "5 Evaluation 5.1 Automatic Evaluation Measures For automatic evaluation of the summary quality , we utilized five measures .", "Comments": [], "label": []}
{"id": 157362, "text": "We compare with existing models ViT , ResNet 152 , BERT , and MMBT .", "Comments": [], "label": []}
{"id": 151447, "text": "Automatic Metrics For dialogue quality , we employ perplexity ( PPL . ) and distinct 1/2 ( Dist.1/2 ) following common practice [ Zhang et al. , , 2018 ] [ Zheng et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 153377, "text": "For Spanish , Portuguese OpenIE we use test sets provided in [ Ro et al. , 2020 ] .", "Comments": [], "label": []}
{"id": 158629, "text": "Apart from vanilla transformer , we here also experiment on LSTM and BART ( a pretrained language model ) . [ Table 5 ] shows the results .", "Comments": [], "label": []}
{"id": 157196, "text": "There are two exceptions : 1 ) In the main experiment ( [ §5.1 ] we use the entire dataset group except STSB .", "Comments": [], "label": []}
{"id": 154071, "text": "We use full table for ToTTo since our task does not contain highlighted cell .", "Comments": [], "label": []}
{"id": 156914, "text": "Secondly , to make better use of the scarce available data , we use curriculum learning [ Bengio et al. , , 2009 ] , which enables the models to gradually proceed from easy training instances to harder ones , when the instances are themselves ordered according to a difficulty measure .", "Comments": [], "label": []}
{"id": 151916, "text": "We use the same Bi-LSTM described in Section [ A.1 ] to encode each knowledge path and generate the K number of path representations { pit } K =1 between the i-th clause and the emotion clause .", "Comments": [], "label": []}
{"id": 153540, "text": "In addition , as shown in Table [ 4 , ] TACO achieves competitive results compared to BERT-NCE and INFOWORD , two similar contrastive methods . [ https : //huggingface.co/models ] ( https : //huggingface.co/models ) Table 3 : GLUE results on BERT-base .", "Comments": [], "label": []}
{"id": 159296, "text": "Next , we adopt a probabilitybased generation strategy to generate the following tokens and their corresponding labels .", "Comments": [], "label": []}
{"id": 156218, "text": "Finally , we use a mix-up strategy to calculate a new input embedding for the target word X′ target as shown in equation [ 1 : ] Where Xtarget is the initial input embedding of the target word , Xsynonyms is the average embedding of all the synonyms .", "Comments": [], "label": []}
{"id": 151495, "text": "We use the 1-K training protocol but without reciprocal relations .", "Comments": [], "label": []}
{"id": 158652, "text": "The objective of Rel-CSKGC is to predict the most reasonable r given h and t. [ 1 ] Main Structure We utilize RoBERTa [ Liu et al. , , 2019 ] to acquire contextual representations of freeform texts describing events .", "Comments": [], "label": []}
{"id": 151817, "text": "( 1 ) Given Φ , a popular approximation for efficient inference of LDA is mean-field variational inference , which tries to maximize the evidence lower bound ( ELBO ) of marginal data log likelihood as where q ( θ ) is the variational posterior .", "Comments": [], "label": []}
{"id": 153167, "text": "We use the cross-entropy loss for the MSP task : where s is the golden sentiment annotated in dataset . 3.3.4 Full Pre-training Loss To optimize all the model parameters , we adopt the alternating optimization strategy to iteratively optimize our five pre-training tasks .", "Comments": [], "label": []}
{"id": 160037, "text": "Similarly , we use cross-entropy for training , and the loss is formulated as : where Y is the ground truth relation of the i-th sample with a one-hot scheme , RN is the total number of relations .", "Comments": [], "label": []}
{"id": 159445, "text": "3 Methods In this section we describe our proposed Language-Specific Transformer Layer , as well as a way to select whether to use shared or language-specific weights for each layer .", "Comments": [], "label": []}
{"id": 156066, "text": "TextFooler is originally designed for English , we reimplement it with corresponding pretrained Chinese-version models . https : //github.com/thunlp/THUCTC https : //github.com/Tencent/PatrickStar We use the small version of training data to test the fewshot capability of models .", "Comments": [], "label": []}
{"id": 156275, "text": "The pretrained RoBERTa-large was used as the base model and then we used the following three methods to fine-tune it : 1 ) CorefGNN : feeding the coreference information into a graph neural network and then fuse the representations ; 2 ) CorefAddAtt : adding the coreference weights with the self-attention weights ; 3 ) CorefMultiAtt : calculating the dot product of the coreference weights with the selfattention weights .", "Comments": [], "label": []}
{"id": 154463, "text": "We found that the findings reported by prior work may not hold in our experiment : for example , the finding `` DeepComHybrid outperforms Seq2Seq '' from [ Hu et al. , 2020 ] does not hold on our dataset ( one reason could be the Seq2Seq code we used is more recent than the version that DeepComHybrid based on ) .", "Comments": [], "label": []}
{"id": 155808, "text": "Following the common practices [ Gu et al. , , 2018 ] [ Qian et al. , , 2021a ] , we measure the decoding latency of each model by decoding sentence by sentence and compute the speedup compared with the autoregressive Transformer ( AT ) model to reflect its decoding efficiency .", "Comments": [], "label": []}
{"id": 153363, "text": "Next we discuss the training and inference of AACTRANS model . * * Training * * : We use parallel sentences of languages * E * and * F * that are available in existing translation corpora for training the AACTRANS model .", "Comments": [], "label": []}
{"id": 160253, "text": "The retrieval occurs for each text over a set of images , in our case we use a subset of 1000 textimage pairs , with the objective being to rank the original/ground-truth image the highest .", "Comments": [], "label": []}
{"id": 157079, "text": "Then , we use the tree reasoning mechanism to capture observation-aware information by dynamically aggregating nodes in the graph .", "Comments": [], "label": []}
{"id": 155414, "text": "Except the standard Transformer , the other models have the same FLOPs .", "Comments": [], "label": []}
{"id": 157476, "text": "For search terms , we used anti-trans bill numbers retrieved from the `` Legislative Tracker : Anti-Transgender Legislation '' website [ 4 ] , which tracks proposed state and federal legislation that would limit the rights of trans people in the United States , as well as hashtags commonly used by those fighting anti-trans legislation .", "Comments": [], "label": []}
{"id": 159186, "text": "To gain deeper insights into the omission problem induced by models with different capacities , we select 6 different model settings [ 3 ] , including BARTlarge/base [ Lewis et al. , , 2020 ] , T5base/small [ Raffel et al. , , 2020 ] , Transformers [ Vaswani et al. , , 2017 ] , and Pegasuslarge [ Zhang et al. , 2020 ] , to generate candidate summaries [ 4 ] .", "Comments": [], "label": []}
{"id": 160185, "text": "We report additional results with the OPT-30B model [ Zhang et al. , , 2022 ] in [ §4.3 . ] Hyperparameters For PROMPTRANK , we use a path length of H = 2 for all experiments .", "Comments": [], "label": []}
{"id": 155103, "text": "Classical methods [ Box and Jenkins , 1990 ] [ Bollerslev , 1986 ] are commonly applied to time-series forecasting .", "Comments": [], "label": []}
{"id": 157820, "text": "We use the Adam optimizer [ Kingma and Ba , 2014 ] with a maximum learning rate of 10− which is warmed up for the first 10 % of steps and subsequently decays linearly .", "Comments": [], "label": []}
{"id": 159909, "text": "Attention-like model Finally , our experiments on a synthetic task indicate that HyperMixer can learn very similar attention patterns as the selfattention mechanism in Transformers ( Section [ 4.7 ] , supporting hypothesis H3 .", "Comments": [], "label": []}
{"id": 151491, "text": "We use the publicly available implementation and the default attack settings [ 4 ] .", "Comments": [], "label": []}
{"id": 156578, "text": "During testing , we use 12 frames per video for MSRVTT and DiDeMo , and 32 frames for ActivityNet Captions since it has longer videos .", "Comments": [], "label": []}
{"id": 156311, "text": "We use inference operators , which build on the similarity metric defined by the learned graph embedding , to increase the number of edges connecting the two node types .", "Comments": [], "label": []}
{"id": 155798, "text": "For discretizing target sentences to latent variables , we use * vector quantization * [ Roy et al. , 2018 ] , which works by dividing a large set of origin vector representations into small groups .", "Comments": [], "label": []}
{"id": 151667, "text": "Both m-Transformer and our mRASP2 have 12 layers for encoder and decoder . tains a large public parallel corpora of 32 Englishcentric language pairs .", "Comments": [], "label": []}
{"id": 154976, "text": "To effectively pre-train the MarkupLM , we use three pretraining strategies .", "Comments": [], "label": []}
{"id": 158346, "text": "For fine-tuning in Lean , we use the tactic dataset in PACT to train the model , with a batch size of 16 , and a warmup step of 1000 with a maximum learning rate of − and a minimum learning rate of − .", "Comments": [], "label": []}
{"id": 158179, "text": "A possible reason is that the volume of social issue discussions on social media is higher than economic issues [ Flores-Saviaga et al. , , 2022 ] [ Ray ] [ mond et al. , , 2022 ] , since the bar for discussing economic issues is higher [ Crawford et al. , , 2017 ] [ Johnston and Wronski , 2015 ] , requiring background knowledge and a deeper understanding of economics .", "Comments": [], "label": []}
{"id": 157091, "text": "Both datasets have been automatically de-identified , and we use the same preprocessing setup of [ Chen et al. , 2020 ] .", "Comments": [], "label": []}
{"id": 153725, "text": "For CSQA2 , we use snippets returned by Google when querying the question .", "Comments": [], "label": []}
{"id": 158440, "text": "PoWER-BERT [ Goyal et al. , , 2020 ] speeds up the inference of text-based Transformers like BERT [ Devlin et al. , , 2019 ] by removing the input text tokens , which are not the main computation bottlenecks for most vision and language tasks .", "Comments": [], "label": []}
{"id": 152378, "text": "We use the encoder-decoder T5 model [ Raffel et al. , , 2020 ] as the underlying model for our experiments and evaluate on the standard GLUE benchmark [ Wang et al. , 2019b ] .", "Comments": [], "label": []}
{"id": 158902, "text": "Hence , we contend that the increase in XGBoost 's returns does not have a strong reference value .", "Comments": [], "label": []}
{"id": 151739, "text": "Finally , to effectively capture long-term dependencies , down-sampled acoustic features flow through the transformer-based encoder [ Vaswani et al. , 2017 ] .", "Comments": [], "label": []}
{"id": 155997, "text": "We use the word embedding version with 300 dimensions and 6 billion words .", "Comments": [], "label": []}
{"id": 157235, "text": "And then the contextualized representations H = Transformer ( E ) ∈ R n×d are obtained after several layers of Transformer blocks .", "Comments": [], "label": []}
{"id": 151982, "text": "Table 2 : Case-sensitive BLEU scores on En-De translation . `` * '' indicates statistically significant at p < 0.01 compared to the Transformer baselines .", "Comments": [], "label": []}
{"id": 151858, "text": "One heuristic simplification of the above equation is to let D = D = Dtr , and the optimization problem in Eq [ 16 ] reduces to the single-level optimization ( SLO ) , which can be solved directly by stochastic gradient descent .", "Comments": [], "label": []}
{"id": 156115, "text": "In the racial debiasing experiments , we use BERT-baseuncased and ALBERT-base-v2 .", "Comments": [], "label": []}
{"id": 159927, "text": "Hence , the scope of our claims does not extend to all * efficient * Transformer models .", "Comments": [], "label": []}
{"id": 159275, "text": "Furthermore , we use 4-layer BiLSTMs with an output size of 768 .", "Comments": [], "label": []}
{"id": 159565, "text": "Table 1 : Results on WikiLingua ( ROUGE-1 / ROUGE-2 / ROUGE-L / BERTSCORE ) .", "Comments": [], "label": []}
{"id": 159051, "text": "Evaluation metrics : We calculate ROUGE [ Lin , 2004 ] scores on the test set , by comparing the concatenation of all generated key points to the concatenation of the reference , averaging for all topic and stance combinations .", "Comments": [], "label": []}
{"id": 155751, "text": "Similar to P1 , we define a penalty function p to enforce P3 : We employ p to update s as follows : [ The Bakersfield Supermarket ] went … [ The business ] closed when [ its ] old … [ The murder ] saddened [ the customers ] … The Bakersfield * * Bidirectional LSTM * * where γ3 , like γ1 , is the hardness coefficient .", "Comments": [], "label": []}
{"id": 155229, "text": "For each dataset , we use * grid-search * to search for the best τ value from { 1.0 , 0.1 , 0.01 , 0.001 } based on the averaged defense performance on the * validation * set under TextFooler [ Jin et al. , , 2019 ] and DeepWord-Bug [ Gao et al . ] .", "Comments": [], "label": []}
{"id": 158334, "text": "Besides its remarkable theoretical value and huge potential to accelerate research in mathematics , ATP already demonstrates excellent application value in , for ex- Work done during internship in Huawei Noah 's Ark Lab Figure 1 : Illustration of a theorem and its proof in Lean [ de Moura et al. , , 2015 ] .", "Comments": [], "label": []}
{"id": 155308, "text": "Since LJP-E only contains the 15 case types of CAIL , when applying * EPM * on the CAIL test set we use the pretrained version of * EPM * ( i.e. , without fine-tuning ) to predict samples that do not belong to the 15 types and use the fine-tuned version of * EPM * to predict samples that belong to one of the 15 types .", "Comments": [], "label": []}
{"id": 152042, "text": "B Transformer B.1 Model Transformer [ Vaswani et al. , , 2017 ] has an encoderdecoder structure , using multi-head attention and feed-forward network as basic modules .", "Comments": [], "label": []}
{"id": 157505, "text": "[ Nogueira et al. , 2021 ] also observe that the finetuned T5 model can not correctly add or subtract arbitrarily long numbers .", "Comments": [], "label": []}
{"id": 151624, "text": "N is the number of transformer layers . and NLP tasks .", "Comments": [], "label": []}
{"id": 152836, "text": "Answer Generation We use a T5-3B model trained on the same subset of Natural Questions ( NQ ) as question generation with same set of hyperparameters and model size described above .", "Comments": [], "label": []}
{"id": 152276, "text": "We also notice that heuristic group 4 ( `` End with non-positive responses '' ) plays a more critical role in the Gunrock Movie dataset than in the ConvAI2 dataset .", "Comments": [], "label": []}
{"id": 151712, "text": "We use circles and rectangles to denote words and vectors , respectively .", "Comments": [], "label": []}
{"id": 160248, "text": "We use these objects and attributes to modify T , by introducing masks ( i.e. , the [ MASK ] token ) , in front of the mentions of the objects in T .", "Comments": [], "label": []}
{"id": 151506, "text": "After training the base task models θ ( 1 ) , θ ( 2 ) , and θ ( 3 ) on source labeled data D ( WarmUp ) , we use two of them ( θ ( j ) , θ ( k ) ) to pseudo-label and co-distill the unlabeled target language data ( D 0 t ) .", "Comments": [], "label": []}
{"id": 156056, "text": "Table 18 : LRA * test * set performances at 30 % space complexity reduction for * Big Bird * ( left ) and * Performers * ( right ) as the backbone Transformer .", "Comments": [], "label": []}
{"id": 159502, "text": "Did you report the number of parameters in the models used , the total computational budget ( e.g. , GPU hours ) , and computing infrastructure used ?", "Comments": [], "label": []}
{"id": 152225, "text": "Figure [ 1 ] shows the accuracy curves ( vs. the macro-average * Sent * and * WV * vectors of the contextualized and static embeddings ) for five Transformers models on Galician , the language with the largest dataset ( see Appendix [ A ] for equivalent figures for the other languages ) .", "Comments": [], "label": []}
{"id": 155394, "text": "4 Experiments 4.1 Tasks and Datasets Language Modeling Following [ Lewis et al. , , 2021 ] and [ Roller et al. , 2021 ] , we use the combination of the corpora in RoBERTa [ Liu et al. , ] Table 2 : Perplexity results of language modeling .", "Comments": [], "label": []}
{"id": 157688, "text": "[ 11 ] We use their responses to characterize each paper 's efforts toward reproducibility .", "Comments": [], "label": []}
{"id": 157974, "text": "Viewing these two trends together , we posit Figure 4 : Performance by the T5 Prompt-based model at rating contextual appropriateness to relationships seen in training versus categories of unseen relationships .", "Comments": [], "label": []}
{"id": 153452, "text": "User summarization is relatively simple on MC , and the baseline methods could achieve high performance ( 5.35 points of ROUGE-2 higher than the best performance in the original paper [ Song et al. , , 2020 ] ) .", "Comments": [], "label": []}
{"id": 154910, "text": "The annotation is carried out by 3 expert English native annotators , each reading the book in sequential order , over a BRAT [ 1 ] based annotation interface .", "Comments": [], "label": []}
{"id": 152746, "text": "We show that it is possible to reduce latency by 30 % –63 % using a strong graphbased semantic parser—either trained to parse prefixes directly or combined with a pre-trained language model for utterance completion—followed by a simple heuristic for subgraph selection .", "Comments": [], "label": []}
{"id": 157042, "text": "To avoid this situation , we use perplexity to filter out bad paraphrases .", "Comments": [], "label": []}
{"id": 154569, "text": "Therefore , a research question arises : * how to fuse these external syntactic and semantic knowledge into our model ? * To address the problem , we use pre-training techniques in programming language processing ( PLP ) , which are trained on massive code corpus to learn programming basics .", "Comments": [], "label": []}
{"id": 152197, "text": "These findings suggest that while transformer-based LMs learn relational knowledge to a meaningful extent , more work is needed to understand how such knowledge is encoded , and how it can be exploited .", "Comments": [], "label": []}
{"id": 155124, "text": "We use model forecasts to compute 10 day 95 % portfolio VaR forecasts , and evaluate model performances by the total number of VaR breaches .", "Comments": [], "label": []}
{"id": 156256, "text": "Soft-Matching We reuse the first several steps of hard-matching to find a set of candidate triples for each dialogue turn , then instead of searching for the exact words in the next turn , we use embedding similarity from SentenceBERT [ Reimers and ] [ Gurevych , 2019 ] ( specifically the `` * all-MiniLM-L6 v2 * '' variant , which is claimed to be a `` All-round model tuned for many use-cases .", "Comments": [], "label": []}
{"id": 155067, "text": "This work would not have been possible without the financial support of Facebook AI Research , Microsoft Research , Google Research , the Institute of Computational Linguistics at the University of Zurich , the NAACL Emerging Regions Fund , Comunidad Elotl , and Snorkel AI .", "Comments": [], "label": []}
{"id": 151771, "text": "As shown , our TRANSFORMER-BIG models trained on the authentic parallel data achieve the performance competitive with or even better than the submissions to WMT competitions .", "Comments": [], "label": []}
{"id": 159548, "text": "Besides heuristic prompt design , some recent works try to optimize the input prompts without gradients .", "Comments": [], "label": []}
{"id": 160138, "text": "For all models , we employ the scheme illustrated in Figure [ 1 ] and report the best results after training with or without the 3 subsampling strategies .", "Comments": [], "label": []}
{"id": 156934, "text": "Speech Encoder We design our speech encoder based on the WavLM structure [ Chen et al. , , 2022 ] with three key modules : a feature extractor , a feature projection module and a Transformer encoder module .", "Comments": [], "label": []}
{"id": 158823, "text": "Our contributions include : * ( i ) * introducing negative and tacit intention categories for questions and designing a rubric to annotate them * ( ii ) * gathering perceived intentions from readers ' point-of-view , * ( iii ) * conducting a meta-analysis , and * ( iv ) * building TF-IDF-based dictionary on intentions and add it to a transformer to benchmark intention classification .", "Comments": [], "label": []}
{"id": 156242, "text": "The testing data consists of around 3k dialogues . 4.2 Compared Methods We use DialoGPT-medium [ Zhang et al. , , 2020a ] as our base model , which is a commonly-used endto-end RG model .", "Comments": [], "label": []}
{"id": 155136, "text": "The detailed correspondence and summary of our pre-training corpora can be seen in Appendix [ A . ] Data pre-processing We directly learn a shared BPE [ Sennrich et al. , , 2016b ] model on the entire data sets after tokenization .", "Comments": [], "label": []}
{"id": 152084, "text": "C Case Study As shown in Figure [ 7 , ] our approach outperforms the base model ( i.e. , the Transformer ) in translation adequacy .", "Comments": [], "label": []}
{"id": 153719, "text": "For finetuned T5 ( including UnifiedQA which is SOTA ) , we use the same setup as [ Khashabi et al. , 2020 ] .", "Comments": [], "label": []}
{"id": 158951, "text": "First , we employ a pre-trained language model to encode all the utterances into vector representations .", "Comments": [], "label": []}
{"id": 153811, "text": "We report the average of at least 3 training runs ( for algorithmic tasks , we use at least 5 training runs , and 10 for set intersection since they have a higher variance ; see Appendix B ) .", "Comments": [], "label": []}
{"id": 154799, "text": "We use 2000 Non-coherent concept : random tweets collected with stop words as queries Explicit anti-Asian abuse : tweets labeled as explicit from * EA * dev and * CH * Implicit abuse ( EA ) : tweets labeled as implicit from * EA * dev Implicit abuse ( CH ) : tweets labeled as implicit from * CH * Generic hate : tweets from the * Hateful * class of * Founta * dev Table 4 : Human-defined concepts and the sources of the tweets used as concept examples .", "Comments": [], "label": []}
{"id": 157607, "text": "This is also an important limitation in latency-constrained realistic use cases that use big tables , while limiting Transformer models even down to 64 input tokens .", "Comments": [], "label": []}
{"id": 155533, "text": "We used the NVIDIA implementation with default hyperparameters apart from a reduced batch size of 32 to fit the memory capacity of our GPU resources .", "Comments": [], "label": []}
{"id": 160197, "text": "For evaluation , we use a set of 4K questions from HotpotQA and T5-Large , and Table 4 : Recall measured on 4K questions from HotpotQA in two settings : reranking each document separately with the LM ( single-hop ) and reranking the full path at once ( multihop ) .", "Comments": [], "label": []}
{"id": 158107, "text": "Among them , VL-T5 [ Cho et al. , , 2021 ] tackles vision-and-language tasks with a unified text-generation objective conditioned on multimodal inputs , while OFA [ Wang et al. , 2022a ] further extends it to image generation tasks by using a unified vocabulary for all text and visual tokens .", "Comments": [], "label": []}
{"id": 151867, "text": "In our implementation [ 4 ] , the following hyperparameters were fixed : On the other hand , we operated an exhaustive grid search over these hyperparameters : Due to its architectural similarity to ESCOFILT , we reimplemented BENEFICT by augmenting it with the pre-trained BERTLARGE model and adopting our model 's fusion and latent vector dimension strategies .", "Comments": [], "label": []}
{"id": 152649, "text": "We used grid search to find the optimal values for these two terms , while keeping the other hyperparameters constant over all datasets .", "Comments": [], "label": []}
{"id": 159119, "text": "Finally , we employed language experts who have [ Bing API ] ( https : //learn.microsoft.com/en-us/azure/cognitive-services/translator/reference/v3-0-reference ) [ Google API ] ( https : //cloud.google.com/translate/docs/reference/rpc/google.cloud.translate.v2 ) Figure 3 : Distribution of the total number of errors per model across each language in consideration .", "Comments": [], "label": []}
{"id": 151689, "text": "The reported accuracy is the average of En→X and X→En accuracy . mRASP2 outperforms m-Transformer on all directions in Englishcentric sentence retrieval task .", "Comments": [], "label": []}
{"id": 155505, "text": "In this study , we adopt a dependency-based latent structure .", "Comments": [], "label": []}
{"id": 156541, "text": "Concurrent work [ Buch et al. , , 2022 ] proposes a new module , atemporal probe , for selecting the best single-frame as inputs to a trained image-text model during inference ; whereas we utilize multiple uniformly sampled frames and study effective ways of ensembling these frames .", "Comments": [], "label": []}
{"id": 153874, "text": "4 Two-Stage Methods Motivated by prior continual sequence generation work [ Madotto et al. , , 2021 ] that uses Adapter [ Houlsby et al. , , 2019 ] to insert new adapter module into every transformer layer for each new coming task , we propose to strategically * decide * whether we can reuse some adapter modules from old tasks before * training * on each new coming task , in a twostage manner : decision stage and training stage , where the former determines the architecture for new tasks and the later trains the model .", "Comments": [], "label": []}
{"id": 152480, "text": "To construct the student model , we used the pretrained cased multilingual BERT ( mBERT ) [ Devlin et al. , 2019 ] as the initialization and a linear layer with softmax function : where Pθ ( Y T-NER ) is the distribution of entity labels probability output from the student model .", "Comments": [], "label": []}
{"id": 160399, "text": "XLS-R model used for the speech encoder initialization is open sourced under Apache-2.0 license .", "Comments": [], "label": []}
{"id": 160340, "text": "Segments are considered to be parallel if the margin score exceeds a threshold , we use 1.06 if not specified otherwise .", "Comments": [], "label": []}
{"id": 157654, "text": "Examples of input and out prompts to the T5 model are demonstrated in Table [ 12 . ]", "Comments": [], "label": []}
{"id": 157108, "text": "In addition , our model increases R-L by 0.6 % on the MIMIC-CXR dataset compared to the best baseline and achieves the second-best result on the IU X-RAY dataset .", "Comments": [], "label": []}
{"id": 156151, "text": "Then , a training instance of ProofWriter , which is a T5 [ Raffel et al. , , 2020 ] model , uses the input { R , F } and output { c , r , f } .", "Comments": [], "label": []}
{"id": 158187, "text": "We use a large online thesaurus [ 3 ] to remove synonyms from the final novel label set .", "Comments": [], "label": []}
{"id": 160088, "text": "We use the following seven transfer tasks to evaluate sentence embeddings : MR [ Pang and Lee , 2005 ] , CR [ Hu and Liu , 2004 ] , SUBJ [ Pang and Lee , 2004 ] , MPQA [ Wiebe et al. , , 2005 ] , SST [ Socher et al. , 2013 ] , TREC [ Voorhees and Tice , 2000 ] , and MRPC [ Dolan and Brockett , 2005 ] .", "Comments": [], "label": []}
{"id": 155540, "text": "The current model for developing speech synthesis systems is not very equitable – models need to be run on GPUs by people with specialized training .", "Comments": [], "label": []}
{"id": 158632, "text": "For the LSTM and BART-Base models we use in [ §6.4 , ] the model sizes are 40M and 251M , respectively .", "Comments": [], "label": []}
{"id": 154988, "text": "GPTs are a series of causal language models based on the Transformer architecture [ Vaswani et al. , , 2017 ] .", "Comments": [], "label": []}
{"id": 154528, "text": "Table 3 : Comparison of the average ROUGE and ME-TEOR scores with different fine-tuning sizes over the Multi-News validation set . 5.3 Configuring RELAX The RELAX gradient estimator introduces two new hyperparameters : the temperature parameter , τ , and the control variate , cφ .", "Comments": [], "label": []}
{"id": 158253, "text": "Existing methods require training , architecture modifications , additional losses to force parallel translation , and distillation from an additional MT transformer model ( `` Big '' indicates the size ) .", "Comments": [], "label": []}
{"id": 159298, "text": "For Domain-Adaptive Pseudo Labeling , the hyper-parameter α in Eq . [ 2 ] is set as 0.01 , and we adopt the Adam algorithm with a learning rate of 3e-5 to optimize the parameters .", "Comments": [], "label": []}
{"id": 152763, "text": "Given paired training examples consisting of a structured data input and a target sentence , we finetune the T5 model to maximize the log-likelihood of generating the corresponding target sentences .", "Comments": [], "label": []}
{"id": 152074, "text": "We re- [ https : //github.com/moses-smt/mosesde ] ( https : //github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl ) [ coder/blob/master/scripts/generic/multi ] ( https : //github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl ) [ bleu.perl ] ( https : //github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl ) [ https : //github.com/moses-smt/mosesde ] ( https : //github.com/moses-smt/mosesdecoder/blob/mast-er/scripts/generic/mteval-v13a.pl ) [ coder/blob/mast-er/scripts/generic/mteva ] ( https : //github.com/moses-smt/mosesdecoder/blob/mast-er/scripts/generic/mteval-v13a.pl ) [ l-v13a.pl ] ( https : //github.com/moses-smt/mosesdecoder/blob/mast-er/scripts/generic/mteval-v13a.pl ) The LM does not need to be state-of-the-art .", "Comments": [], "label": []}
{"id": 153983, "text": "We introduce a new annotated corpus of Spanish newswire rich in unassimilated lexical borrowings—words from one language that are introduced into another without orthographic adaptation—and use it to evaluate how several sequence labeling models ( CRF , BiLSTM-CRF , and Transformer-based models ) perform .", "Comments": [], "label": []}
{"id": 159439, "text": "For the non-shared layers , we propose using Language-Specific Transformer Layers ( LSLs ) , illustrated in Figure [ 1b . ] Quite simply , LSLs are a combination ( i.e. , a dictionary ) of regular Transformer layers ( Figure [ 1a ] , where the sub-layer used depends on the chosen language .", "Comments": [], "label": []}
{"id": 159842, "text": "We use kernel smoothing to aggregate as follows : Where the weight between label word v and class name y is defined as : Finally , the best class prediction is selected from the maximum of all labels : Notice since we use kernel smoothing on logits instead of probability , Q is also unnormalized probability .", "Comments": [], "label": []}
{"id": 156473, "text": "Our metric can be easily accessed from [ https : // ] ( https : //huggingface.co/nightdessert/WeCheck ) [ huggingface.co/nightdessert/WeCheck ] ( https : //huggingface.co/nightdessert/WeCheck ) [ 2022 ] and directly apply them to evaluate factual consistency without any adaption .", "Comments": [], "label": []}
{"id": 159844, "text": "We provide detailed information on our implementation and address several research questions related to NPPrompt .", "Comments": [], "label": []}
{"id": 160113, "text": "[ stable/modules/generated/sklearn.svm .", "Comments": [], "label": []}
{"id": 156258, "text": "For decoding , we use top-p nucleus sampling [ Holtzman et al. , , 2019 ] with temperature T ( p = 0.9 and T = 0.7 ) , and a maximum decoding length of 300 tokens .", "Comments": [], "label": []}
{"id": 154737, "text": "In this paper , we propose the ∞-former ( * infinite former * ; Fig . [ 1 ] : a transformer model extended with an unbounded long-term memory ( LTM ) , which allows the model to attend to arbitrarily long contexts .", "Comments": [], "label": []}
{"id": 155755, "text": "We adopt the entity coreference loss function originally defined by [ Wiseman et al. , 2015 ] .", "Comments": [], "label": []}
{"id": 152030, "text": "Recent studies extend the translation unit to whole document [ Junczys-Dowmunt , 2019 ] [ Liu et al. , , 2020 ] , using large augmented dataset or pretrained models . [ Liu et al. , 2020 ] shows that Transformer trained directly on documentlevel dataset can fail , resulting in unreasonably low BLEU scores .", "Comments": [], "label": []}
{"id": 155531, "text": "* org/go/ [ Instructions_for_implementing_a_new_language_ % ] ( https : //wiki.laptop.org/go/Instructions_for_implementing_a_new_language_ % 22voice % 22_for_Speak_on_the_XO ) [ 22voice % 22_for_Speak_on_the_XO ] ( https : //wiki.laptop.org/go/Instructions_for_implementing_a_new_language_ % 22voice % 22_for_Speak_on_the_XO ) As there are different variations of spelling , we use the spelling used in the communities of Kahnawà : ke and Kahnesetà : ke throughout this paper level using Praat [ Boersma and van Heuven , 2001 ] and ELAN [ Brugman and Russel , 2004 ] .", "Comments": [], "label": []}
{"id": 159695, "text": "In the current setup of training , we use two Transformer encoders in conjunction : a generator G and a discriminator D , where the generator G is trained with masked language modeling objective ( MLM ; [ Devlin et al. , 2019 ] ) and the discriminator is trained on replaced token detection objective ( RTD ; [ Clark et al. , 2020b ] on all the tokens passing through the generator .", "Comments": [], "label": []}
{"id": 158079, "text": "Note that with the same backbone generator ( T5- Base/T5-Large ) , our system surpasses Q-TOD even though it relies on human annotations to train a query generator for knowledge retrieval .", "Comments": [], "label": []}
{"id": 158202, "text": "We use label smoothing factor α = 0.1 and calculate confidence with Max-Prob .", "Comments": [], "label": []}
{"id": 154855, "text": "We adopt the framework of conditional variational auto-encoder ( CVAE ) [ Kingma and Welling , 2014 ] [ Sohn et al. , , 2015 ] to maximize the conditional marginal log-likelihood Figure 1 : Example of extracting < noun phrase , image region > pairs from existing < sentence , image > pairs .", "Comments": [], "label": []}
{"id": 155763, "text": "'s [ 2018 ] rule-based approach , which consists of rules that are built on top of [ Hou et al. , 2014 ] .", "Comments": [], "label": []}
{"id": 158812, "text": "We use this data to train several learning methods using the feedback from the v1 models .", "Comments": [], "label": []}
{"id": 160096, "text": "In this work , we limit ourselves to the three most common types , covering 95 % of all labels revisions in the ClaimRev dataset : clarification , typo/grammar correction , and adding/correcting links .", "Comments": [], "label": []}
{"id": 154325, "text": "We adopt the * spell * data perturbation without any data filter and avoid exhaustive hyper-parameter tuning .", "Comments": [], "label": []}
{"id": 159526, "text": "As shown in Table [ 5 , ] we can see that our approach is better than all the variants , even T5-3b with billionscale parameters .", "Comments": [], "label": []}
{"id": 155850, "text": "C.3 Prompt Pre-Training We use the sampled 10GB data to construct the pre-training data for each task format for prompt pre-training .", "Comments": [], "label": []}
{"id": 157712, "text": "Most recent KBQA models [ Ye et al. , , 2022 ] [ https : //github.com/dair-iitd/GrailQAbility . ] ( https : //github.com/dair-iitd/GrailQAbility.git ) [ git ] ( https : //github.com/dair-iitd/GrailQAbility.git ) [ Chen et al. , , 2021 ] are trained with questions and gold logical forms .", "Comments": [], "label": []}
{"id": 153824, "text": "While the different variations on Transformer models have little affect on the performance , the use of an intermediate representation significantly improves performance , going from around 0.3 accuracy for most Transformer models to over 0.5 , and up to 0.555 for the * rel-eb * model .", "Comments": [], "label": []}
{"id": 151788, "text": "Two transformer encoders and linear layers share parameters , which has been proved effective and necessary for cross-lingual representation learning [ Conneau et al. , , 2020b ] . 3.2 Generative Task SMLM .", "Comments": [], "label": []}
{"id": 155736, "text": "To ensure every model is trained on the same data and accelerate the training in our machines , we split the training data into 20 consecutive partitions and load only one partition at a time during training .", "Comments": [], "label": []}
{"id": 156957, "text": "The decoder takes z and zs as inputs , and generates a Style Transformer [ Dai et al. , , 2019 ] belongs to the second group which directly revises an entangled representation of an input by using an extra style embedding .", "Comments": [], "label": []}
{"id": 156669, "text": "To mitigate error propagation from the filtering step , we use a publicly available DeBERTa-v2 [ He et al. , , 2020 ] model checkpoint ( containing 1.3 billion parameters ) trained on a mixture of NLI datasets , including SNLI [ Bowman et al. , , 2015 ] , MultiNLI [ Williams et al. , , 2018 ] , FEVER [ Thorne et al. , 2018 ] , ANLI [ Nie et al. , , 2020 ] , that achieves SOTA performance on these datasets .", "Comments": [], "label": []}
{"id": 155000, "text": "We use rectangles to denote prompt vectors and rounded rectangles to denote activations . propose multi-stage prompting which divides the procedure of using pre-trained LMs as translators into three separate stages : the encoding , the reencoding , and the decoding stages .", "Comments": [], "label": []}
{"id": 159324, "text": "GHA ( Figure [ 1 ] is developed with GCT that removes head redundancy ; GHA-PS is developed by removing the redundant parameters of GHA in V2S .", "Comments": [], "label": []}
{"id": 154005, "text": "4 Error analysis We compared the different results produced by the best performing model of each type on the test set : ( 1 ) the mBERT model , ( 2 ) the BiLSTM-CRF with BERT+BETO , BPE and character embeddings and ( 3 ) the BiLSTM-CRF model with codeswitch , BPE and character embeddings .", "Comments": [], "label": []}
{"id": 154698, "text": "For one , we use the Kullback–Leibler ( KL ) divergence [ 2 ] loss function to solve a regression task of predicting the N-length affix distribution vector .", "Comments": [], "label": []}
{"id": 158341, "text": "In this paper , we adopt the same model setup from PACT [ Han et al. , ] At this stage , the proof-level value function is not applicable ; thus the state 's preceding step 's prior probability is used as value estimate v .", "Comments": [], "label": []}
{"id": 151337, "text": "4.1 Verification of Separation Inspired by [ Chen et al. , 2019b ] we use a * semantic textual similarity * task and a * template detection * task to confirm that SEPARATOR does indeed lead to encodings { zsem , zsyn } in latent spaces that represent different types of information .", "Comments": [], "label": []}
{"id": 154409, "text": "Zhongyu Wei and Yeyun Gong are corresponding authors . enhanced encoder-decoder pre-training framework and we adopt the optimization techniques based on the VAEs literature to learn the model with continuous latent variables .", "Comments": [], "label": []}
{"id": 154471, "text": "The approximate model training time are : DeepComHybrid 7 days ; Seq2Seq 4 hours ; Transformer 10 hours ; Code2Seq 4 hours ; Code2Vec 15 minutes .", "Comments": [], "label": []}
{"id": 151475, "text": "In this work , we use these patterns to investigate the adversarial vulnerability of KGE models .", "Comments": [], "label": []}
{"id": 151540, "text": "We adopt the NCE loss to learn to discriminate the positive from negative trace-caption pairs .", "Comments": [], "label": []}
{"id": 152798, "text": "We used two pre-trained models as the initial student model : ALBERT-xlarge ( L=24 , H=2048 , A=32 ) and RoBERTa-large ( L=24 , H=1024 , A=16 ) .", "Comments": [], "label": []}
{"id": 154046, "text": "These table representations ( h1 , h in [ Fig . 3b ] are then fed into a 3-layer Transformer encoder-decoder model to generate the target summaries .", "Comments": [], "label": []}
{"id": 155706, "text": "Acknowledgment We thank Siddharth Karamcheti , members of the Stanford P-Lambda , SNAP and NLP groups , as well as our anonymous reviewers for valuable feedback .", "Comments": [], "label": []}
{"id": 157713, "text": "* Train-Test Split : * Since the test questions for GrailQA are unavailable , we use the train and dev questions .", "Comments": [], "label": []}
{"id": 158113, "text": "Specifically , we use the transformer-based encoder of OFA to encode the instruction along with all necessary information and an optional image , and predict the output with the transformer-based decoder .", "Comments": [], "label": []}
{"id": 154462, "text": "We could not find reference points for this finding in prior work ( unfortunately , [ Ah ] [ mad et al. , 2020 ] did not compare Seq2Seq with Transformer though both were provided in their replication package ) .", "Comments": [], "label": []}
{"id": 160207, "text": "In our work , we use instructions to guide to model toward assigning better scores to more relevant document paths .", "Comments": [], "label": []}
{"id": 155055, "text": "4.1 Zero-Shot Learning Pretrained Model We use XLM-R [ Conneau et al. , 2020 ] as the pretrained multilingual model in our experiments .", "Comments": [], "label": []}
{"id": 153304, "text": "See [ A.1 ] for additional details . our method yields stronger improvements over the standard Transformer than any other data augmentation technique ( Table [ 3 ] .", "Comments": [], "label": []}
{"id": 156873, "text": "This is intuitive because given a specific task and a Transformer layer , parameters that contain the layer information and parameters that contain the task information should be both used in the forward pass .", "Comments": [], "label": []}
{"id": 152849, "text": "[ 1 ] 1 Introduction Transformer-based [ Vaswani et al. , , 2017 ] pretrained language models ( PLMs ) such as BART [ Lewis et al. , , 2020a ] and T5 [ Raffel et al. , , 2020 ] , have achieved state-of-the-art performance on short text summarization .", "Comments": [], "label": []}
{"id": 155956, "text": "However , the extent to which it is possible to replace full-attention in Transformer with the sparse attention we propose is unknown .", "Comments": [], "label": []}
{"id": 159674, "text": "The T5-base models are efficient enough , but the prompted GPT3 model is too slow for a responsive interactive experience .", "Comments": [], "label": []}
{"id": 156217, "text": "In order to obtain appropriate synonyms we use Word-Net [ Miller , 1995 ] which is an extensive lexical database where words are grouped into sets of synonyms ( synsets ) .", "Comments": [], "label": []}
{"id": 159694, "text": "We use the Transformer model [ Vaswani et al. , 2017 ] trained with ELECTRA [ Clark et al. , , 2020b ] style of replace token detection ( RTD ) on both monolingual ( MRTD ) and bitext ( TRTD ) data .", "Comments": [], "label": []}
{"id": 160187, "text": "We use the TF-IDF index implemented by [ Asai et al. , 2020 ] and initially retrieved F = 100 documents from TF-IDF .", "Comments": [], "label": []}
{"id": 155233, "text": "B REPRODUCIBILITY B.1 Infrastructure and Source Code B.2 Experimental Settings for Base Models B.2.1 Architectures and Parameters B.2.2 Vocabulary and Input Length Due to limited GPU memory , we set the maximum length of inputs for transformer-based models , i.e. , BERT and RoBERTa , to 128 during training .", "Comments": [], "label": []}
{"id": 153226, "text": "[ Eisner , 1997 ] ; [ Satta and Kuhlmann , 2013 ] use the headsplitting trick to accelerate projective and nonprojective dependency parsing .", "Comments": [], "label": []}
{"id": 157005, "text": "A.7 Agreement We implement the invariant present tense [ [ 170 ] ] ( https : //ewave-atlas.org/parameters/170 ) , as well as the existential dummy * it * [ [ 173 ] ] ( https : //ewave-atlas.org/parameters/173 ) .", "Comments": [], "label": []}
{"id": 158909, "text": "The Wasserstein distance arises from the method of optimal transport ( OT ) [ Kan ] [ torovich , 2006 ] [ Peyré et al. , , 2019 ] : OT measures distances between distributions in a way that depends on the geometry of the sample space .", "Comments": [], "label": []}
{"id": 160321, "text": "For this purpose , we adopt the following measures : We first guarantee the diversity of annotators in terms of background information , including gender , age , race , region , and study .", "Comments": [], "label": []}
{"id": 152294, "text": "Besides , another heuristic method is to make our model pay more attention to 1-class : a weighted cross-entropy function .", "Comments": [], "label": []}
{"id": 158147, "text": "Inspired by the works [ Han et al. , 2020 ] [ Cui et al. , , 2021 ] [ Zhao et al. , , 2022 ] [ Zhang et al. , , 2022 ] [ Hu et al. , , 2022 ] , we adopt the k-means algorithm to cluster the samples of each relation r ∈ Rk .", "Comments": [], "label": []}
{"id": 153759, "text": "* [ 2020 ] that BPE performed better than regexbased tokenization for SMILES on downstream tasks .", "Comments": [], "label": []}
{"id": 154478, "text": "Specifically , we use English Wikipedia as the text corpus and English Wikidata ( Vrandeciˇ [ c and Krötzsch ] ´ , [ 2014 ] as the knowl- edge graph , since there exists an alignment between the two resources [ 1 ] .", "Comments": [], "label": []}
{"id": 153672, "text": "We extract claims with either supported or refuted labels in the original dataset . [ 7 ] Covid-19 Dataset For trustworthy news regarding Covid-19 , we use the CoAID dataset [ Cui and ] [ Lee , 2020 ] and a Covid-19 related subset of NELA-GT-2020 [ Gruppi et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 160594, "text": "This is impossible to be implemented in a general GPU device .", "Comments": [], "label": []}
{"id": 158656, "text": "Dense-ATOMIC benefits the performance of * * COMET * * To empirically demonstrate the knowledge coverage and quality of Dense-ATOMIC , we evaluate Dense-ATOMIC with * * COMET * * [ Bosselut et al. , 2019 ] .", "Comments": [], "label": []}
{"id": 158680, "text": "Baselines We compare ShrinkE against various models , including m-TransH [ Wen et al. , , 2016 ] , RAE [ Zhang et al. , , 2018 ] , NaLP-Fix [ Rosso et al. , , 2020 ] , HINGE [ Rosso et al. , , 2020 ] , NeuInfer [ Guan et al. , , 2020 ] , BoxE [ Abboud et al. , , 2020 ] , Transformer and StarE [ Galkin et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 154932, "text": "We use the original pairwise test set used by [ Moon et al. , 2019 ] with 20 , 411 pairs for testing .", "Comments": [], "label": []}
{"id": 158447, "text": "For a fair comparison , we use the original DynamicViT configurations ( pruning layers and ratios ) for the ViLT model .", "Comments": [], "label": []}
{"id": 156121, "text": "For all our experiments , we use two-layer neural networks as our probe classifiers .", "Comments": [], "label": []}
{"id": 153702, "text": "We use Cohen 's d to compute effect size .", "Comments": [], "label": []}
{"id": 153330, "text": "Table 11 : Results on IWSLT14 De→En with baseline Transformer and CipherDAug using different dropout values .", "Comments": [], "label": []}
{"id": 153840, "text": "Our code is publicly available at [ https : //github.com/ ] ( https : //github.com/facebookresearch/perfect.git ) [ facebookresearch/perfect.git ] ( https : //github.com/facebookresearch/perfect.git ) .", "Comments": [], "label": []}
{"id": 155152, "text": "We further introduce the * aligned code-switching & masking * to align the representation space for words with similar semantics but in different languages , then we use a * dynamic dual-masking * strategy to induce the bidirectional decoder to actively obtain the information from the source side .", "Comments": [], "label": []}
{"id": 158208, "text": "Next , for all experiments except the experiment on different generator averaging strategies , we use a one-layer CNN with |Y| input channels , 1 output channel and a 1 × L kernel with no bias .", "Comments": [], "label": []}
{"id": 156954, "text": "SpokenWoz consists of 204k turns , 5.7k dialog , and 249 hours of recordings.We adopt joint goal accuracy ( JGA ) as the evaluation metric , which compares the predicted and ground- truth dialogue states at each turn .", "Comments": [], "label": []}
{"id": 156771, "text": "We fine-tune CLIP ViT-L/14 @ 366px [ Radford et al. , , 2021 ] ( 428M parameters ) , which consists of a text Transformer [ Vaswani et al. , , 2017 ] and a vision Transformer [ Dosovitskiy et al. , , 2021 ] pretrained to align images/captions in the WebImageText corpus ( 400M pairs ) .", "Comments": [], "label": []}
{"id": 159953, "text": "For text decoder pre-training , we use the same En-Es and 50-language t-mBART models as [ Wang et al. , 2022 ] and [ Tang et al. , 2020 ] , respectively .", "Comments": [], "label": []}
{"id": 153458, "text": "We use Adam optimizer with a warmup of 1000 steps .", "Comments": [], "label": []}
{"id": 153049, "text": "For both TextFooler and BERT-Attack , we skip their word importance ranking ( WIR ) modules while only keeping the word transformer modules for candidate generation [ 8 ] .", "Comments": [], "label": []}
{"id": 158662, "text": "Consequently , we additionally design a simple heuristic sampling rule : a multi-hop path * A * → * . . . * → * C * is chosen only when A and C are also linked in Dense-ATOMIC .", "Comments": [], "label": []}
{"id": 158620, "text": "Apart from transformer , we also validate the effectiveness of our unlearning method in other architecture including LSTM and pretrained language model BART [ Lewis et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 155709, "text": "We use max_seq_length = 128 .", "Comments": [], "label": []}
{"id": 151405, "text": "In the contextual information unit , the roles of H i−1 i and M i in GRU are reversed , i.e. , H i−1 i controls the propagation of M i : The representation of u at the l-th layer is the sum of e i and C l i : 3.3.4 Training and Prediction We take the concatenation of ui 's hidden states at all DAG-ERC layers as the final representation of u , and pass it through a feed-forward neural network to get the predicted emotion : For the training of DAG-ERC , we employ the standard cross-entropy loss as objective function : where M is the number of training conversations , Ni is the number of utterances in the i-th conversation , yi , t is the ground truth label , and θ is the collection of trainable parameters of DAG-ERC .", "Comments": [], "label": []}
{"id": 158768, "text": "Following ItNet , the visual encoder of PEIT is ResNet with Xavier initialization .", "Comments": [], "label": []}
{"id": 152881, "text": "It is a large Transformer trained on huge parallel data across 2200 translation directions and with 7.5B parallel sentences from CC-Matrix and CCAligned as well as additional backtranslations .", "Comments": [], "label": []}
{"id": 160002, "text": "We used the same dmodel , dff , and Nhead as the S2UT model in all the settings .", "Comments": [], "label": []}
{"id": 152831, "text": "Experimental Setting We use the method and implementation from [ Guu et al. , 2020 ] to finetune REALM on ( q , a ) pairs from NQ .", "Comments": [], "label": []}
{"id": 152092, "text": "We use the best F1 scores on the validation set to calculate the MSP and GDA thresholds adaptively .", "Comments": [], "label": []}
{"id": 156882, "text": "The answer is a set of distinct entities in Freebase ; we use the modified versions by [ Min et al. , 2021 ] , which recasts WebQSP as textual question answering based on Wikipedia .", "Comments": [], "label": []}
{"id": 158119, "text": "B.2 NLP Evaluation Tasks Below are the task names of the 20 NLP tasks that we used to test the zero-shot performance of all the methods .", "Comments": [], "label": []}
{"id": 157443, "text": "T5 : when did bouvier enter vassar college ?", "Comments": [], "label": []}
{"id": 153749, "text": "Transformer models trained with self-supervised masked language modeling ( MLM ) loss [ Vaswani et al. , 2017 ] in chemical domain [ Wang et al. , , 2019 ] [ Chithrananda et al. , , 2020 ] [ Elnaggar et al. , , 2020 ] [ Rong et al. , , 2020 ] [ Schwaller et al. , , 2021 ] [ Bagal et al. , 2021 ] have also been used for molecular representation learning .", "Comments": [], "label": []}
{"id": 152371, "text": "We used cross-entropy loss with class weights emphasising the hateful minority class .", "Comments": [], "label": []}
{"id": 160011, "text": "We use 1k and 2k units for the subword vocabulary on Fisher and multi-domain Es→En corpora , respectively .", "Comments": [], "label": []}
{"id": 153797, "text": "Section [ 2 ] provides some background on compositional generalization and Transformers .", "Comments": [], "label": []}
{"id": 154067, "text": "Table 6 : Results measured by BLEU for transferability based on the T5 model .", "Comments": [], "label": []}
{"id": 159824, "text": "6 Conclusions and Future Directions We have evaluated the capabilities of modern large language models ( LLMs ) —specifically GPT-3 and Flan T5 ( Large ) —on the task of Relation Extraction ( RE ) .", "Comments": [], "label": []}
{"id": 155115, "text": "The transformer encoded sequence of representations are combined with temporal attention fusion , which weights contributions of each time step t − k based on its importance .", "Comments": [], "label": []}
{"id": 151503, "text": "For this , we use a vectorized implementation similar to KGE evaluation protocol .", "Comments": [], "label": []}
{"id": 153738, "text": "On top of the T5-11b inference model , The 6.7B knowledge model gives Figure 5 : Human evaluation of generated knowledge .", "Comments": [], "label": []}
{"id": 158703, "text": "Did you report the number of parameters in the models used , the total computational budget ( e.g. , GPU hours ) , and computing infrastructure used ?", "Comments": [], "label": []}
{"id": 158101, "text": "Compared with the standard bilingual Transformer and confidence-based KD [ Zhou et al. , , 2022 ] , PBD-MT significantly improves the performance , which verifies the effectiveness of pretrained bidirectional distillation on bilingual NMT .", "Comments": [], "label": []}
{"id": 153861, "text": "We use this choice in all the experiments when removing handcrafted patterns .", "Comments": [], "label": []}
{"id": 155151, "text": "Non-autoregressive Neural Machine Translation [ Gu et al. , 2018 ] first introduced a transformer-based method to predict the complete target sequence in parallel .", "Comments": [], "label": []}
{"id": 159685, "text": "Due to error propagation , the T5 model is ∼5–8 % worse when asked to jointly perform ASR repair and interpretation from noisy ASR , than when simply asked to interpret normalized utterances .", "Comments": [], "label": []}
{"id": 151762, "text": "Then we used the bilingual dictionary to compute the data uncertainty expressed in Equation [ 3 ] for the sentences in the monolingual data set .", "Comments": [], "label": []}
{"id": 154580, "text": "We use the term * model * from here on to align with the QA literature .", "Comments": [], "label": []}
{"id": 152903, "text": "This formulation encompasses several different previous works in data augmentation for MT , such as monolingual data copying [ Currey et al. , 2017 ] , where 푔 ( · ) is the identity function , back translation [ Sennrich et al. , , 2016 ] , where 푔 ( · ) is a backwards translation model , as well as heuristic noise functions [ Song et al. , , 2019 ] [ Lewis et al. , , 2020 ] [ Liu et al. , , 2020 ] that randomly sample noise according to manually devised heuristics .", "Comments": [], "label": []}
{"id": 156385, "text": "The subset of a datastore can be loaded into GPU memory since it is significantly smaller than the original kNN-MT datastore , so we retrieved k-nearest-neighbor tokens from a subset on a GPU .", "Comments": [], "label": []}
{"id": 151538, "text": "Here N denotes the number of pre-detected visual objects , T denotes the number of tokens in a caption sentence after padding , L denotes the number of transformer layers , and H denotes the number of attention heads in transformer layers .", "Comments": [], "label": []}
{"id": 158036, "text": "Finally , following Section [ 3.2 , ] we use a trained player model to provide reward signals for DM models for RL .", "Comments": [], "label": []}
{"id": 152421, "text": "For identifying personal attacks , we use a lexicon released by [ Wiegand et al. , 2018 ] , which contains a vocabulary of abusive words .", "Comments": [], "label": []}
{"id": 158490, "text": "For this purpose , we use the maximum likelihood estimation objective : During the inference process ( production environment ) , we use the predicted connection of concepts cu , cv , and their corresponding sentences of papers p , p to construct the input sequence , which is encoded by our fine-tuned T5 to generate an idea sequence .", "Comments": [], "label": []}
{"id": 159179, "text": "Table 3 : Examples of two keyword based and two regular expression based rules for the IMDb dataset . this paper can fit into one single GPU .", "Comments": [], "label": []}
{"id": 152872, "text": "To investigate this , we report the decomposed precision and recall of ROUGE scores in Table [ 6 . ] We observe that the extracted snippets have much higher recall than the generated summaries , while the generated summaries have higher precision .", "Comments": [], "label": []}
{"id": 154789, "text": "For the compressive transformer , we follow [ Rae et al. , 2019 ] and use a compression rate of 4 and the attention reconstruction auxiliary loss .", "Comments": [], "label": []}
{"id": 154772, "text": "In this paper , we propose an extension to a transformer model that makes the attention complexity independent of the length of the context being attended to .", "Comments": [], "label": []}
{"id": 156303, "text": "We use evaluation datasets containing ( reference , candidate ) sequence pairs annotated with human scores assessing the quality of the candidates given the references .", "Comments": [], "label": []}
{"id": 151508, "text": "To model such dependencies , we use successive cross , which uses cross-product of two successive max applied independently to each component .", "Comments": [], "label": []}
{"id": 157482, "text": "Models were finetuned for one epoch each , with instantaneous batch size determined by GPU capacity , gradient accumulation over 10 steps , and all other hyperparameters at default settings , following [ Felkner et al. , 2022 ] .", "Comments": [], "label": []}
{"id": 151865, "text": "Finally , the MLP 's output is fed to one more linear layer to produce the predicted rating : 4 Empirical Evaluation 4.1 Research Questions In this section , we detail our experimental setup designed to answer the following research questions ( RQs ) : 4.2 Datasets , Baselines , and Evaluation Metric Table [ 2 ] summarizes the four public datasets [ 1 ] that we utilized in our study .", "Comments": [], "label": []}
{"id": 154774, "text": "C Experimental details C.1 Sorting For the compressive transformer , we consider compression rates of size 2 for sequences of length 4,000 , from 2 to 6 for sequences of length 8,000 , and from 2 to 12 for sequences of length 16,000 .", "Comments": [], "label": []}
{"id": 151560, "text": "We then find the most similar edited image * I * in the training set T via cosine similarity : We use the captions associated with the most similar image in the training set . b .", "Comments": [], "label": []}
{"id": 157557, "text": "However , We find that augmented with the relative position embeddings , DeBERTa tends to face more severe over-fitting than T5 during fine-tuning .", "Comments": [], "label": []}
{"id": 158541, "text": "* Turn-level Oracle [ Mao et al. , , 2022 ] Table 10 : Example of a * turn-level * ROUGE-based extractive oracle for a gold summary in Table [ 8 ] the bold-faced numbers refer to the turn-based sentence ids of the annotated CESs or PESs .", "Comments": [], "label": []}
{"id": 158362, "text": "We use a batch size of 16 and learning rate of 2e-5 for the one-pass gradient descent with an Adam optimizer [ Kingma and Ba , 2014 ] .", "Comments": [], "label": []}
{"id": 156234, "text": "We use the same mapping as that used in COMET [ Bosselut et al. , , 2019 ] , covering all standard types of relations in ConceptNet .", "Comments": [], "label": []}
{"id": 155624, "text": "We used 50 labeled samples per class for training ( i.e. , K=50 ) , and the class distributions of the test sets were adjusted to 2:8 ( negative : positive ) . `` ' denotes that the performance is statistically similar to the BUS-stop ( i.e. , * p * -value over 0.05 ) .", "Comments": [], "label": []}
{"id": 152445, "text": "We use its output and metrics to evaluate the end-to-end system , but also evaluate relation extraction separately from upstream components to isolate its performance .", "Comments": [], "label": []}
{"id": 155500, "text": "For target fact selection , we utilize the word overlap between the assumption and the context to locate the target fact .", "Comments": [], "label": []}
{"id": 151705, "text": "We used the * AdamW * optimizer with a linear learning rate scheduler with no warm up steps .", "Comments": [], "label": []}
{"id": 151844, "text": "To satisfy the maximum capacity of the encoder in the base model , such as 512 for BertSUM , we use truncated document as the encoder input .", "Comments": [], "label": []}
{"id": 155718, "text": "As the orange box in [ Figure 2 , ] we first concatenate a W × H block of input hidden states into ⊕i=0 ... W−1 , m=0 ... H−1hM− ct− , where M −m is the transformer layer index and t−i is the index of the ith to the last word in the context .", "Comments": [], "label": []}
{"id": 155496, "text": "We use h K j 7→ c to represent the mapping between token and fact , which is true if token j belongs to fact c .", "Comments": [], "label": []}
{"id": 152457, "text": "In this paper , we develop * DeepRapper * , a Transformer-based rap generation system that can model both rhymes and rhythms .", "Comments": [], "label": []}
{"id": 158604, "text": "COMET ( GPT2- XL ) generators are fine-tuned on the ATOMIC subset , augmented by a mixture of annotated and pseudo-labeled abstract triples .", "Comments": [], "label": []}
{"id": 154265, "text": "Sentiment transfer ( Yelp ) : We use Yelp [ Shen et al. , 2017 ] dataset 's test-set for the task of sentiment transfer .", "Comments": [], "label": []}
{"id": 157895, "text": "We fine-tune a pre-trained sequence-to-sequence model , T5 , by a dialogue reconstruction task as our dialogue content realization model .", "Comments": [], "label": []}
{"id": 158857, "text": "We use a soft label distribution instead of single label because of the high overlap across patterns .", "Comments": [], "label": []}
{"id": 155687, "text": "We use k= 5 . 4.2 Pretraining tasks Creating input instances .", "Comments": [], "label": []}
{"id": 152999, "text": "Interestingly , we see that sparse models show no acceleration on GPU even when the sparsity is high ( e.g. , 90 % ) .", "Comments": [], "label": []}
{"id": 156507, "text": "Prompts In order to increase the probability for chatbots to surface unsafe responses for rewriting , we use the Jigsaw checker ( described in subsection [ 3.2 ] to search unsafe responses from 50,000 prompt-response pairs from LCCC-large [ Wang et al. , 2020 ] and 50,000 from PChatbotW [ Qian et al. , 2021 ] and only keep their prompts .", "Comments": [], "label": []}
{"id": 158516, "text": "By merging the two types of extractors , Multi-DYLE ( XROG o , XCES o ) leads to non-trivial improvements over runs with a single extractor ( i.e. , Multi-DYLE ( XROG o ) or Multi-DYLE ( XCES o ) ) , finally achieving 37.55 of ROUGE-1 .", "Comments": [], "label": []}
{"id": 158772, "text": "We apply byte pair encoding to segment all sentences with merge operations of 32K .", "Comments": [], "label": []}
{"id": 158442, "text": "Finally , the cross-modal encoder takes the concatenated text and image tokens and fuses information between image and text modalities via Transformer-style [ Vaswani et al. , , 2017 ] cross-attention interactions .", "Comments": [], "label": []}
{"id": 160128, "text": "We use Krippendorff 's α instead of Cohen 's κ to measure inter-rater agreement here because it is applicable to ordinal annotation data .", "Comments": [], "label": []}
{"id": 153922, "text": "Additionally , we use the CNN/DailyMail [ See et al. , , 2017 ] and XSum [ Narayan et al. , , 2018 ] datasets for abstractive summarization as the benchmarks for models pretrained on larger corpora .", "Comments": [], "label": []}
{"id": 159641, "text": "We implement a vanilla Transformer for original languages as the initial model which is trained on multiple parallel data jointly [ John ] [ son et al. , , 2017 ] .", "Comments": [], "label": []}
{"id": 158952, "text": "• Comprehensive experiments conducted on three benchmark datasets show SDDS achieves significant improvement over strong baselines .", "Comments": [], "label": []}
{"id": 154183, "text": "] ( https : //www.zimutiantang.com/ ) https : //stanfordnlp.github.io/CoreNLP/index.html https : //github.com/Unbabel/BConTrasT www.unbabel.com set the step of fine-tuning stage 5,000 .", "Comments": [], "label": []}
{"id": 157816, "text": "In order to improve span representations for down-stream applications to DST we pre-train SPLAT in a self-supervised manner using a modified recurrent span selection objective [ Ram et al. , , 2021 ] .", "Comments": [], "label": []}
{"id": 159227, "text": "We use XLM-RoBERTa-large with a linear head as our NER model .", "Comments": [], "label": []}
{"id": 158454, "text": "References [ TPrune : Efficient Transformer Pruning for Mobile De ] ( https : //dl.acm.org/doi/abs/10.1145/3446640 ) [ vices : ACM Transactions on Cyber-Physical Sys ] ( https : //dl.acm.org/doi/abs/10.1145/3446640 ) [ tems : Vol 5 , No 3 .", "Comments": [], "label": []}
{"id": 160121, "text": "COVID-19 treatments make an ideal testbed for human-in-the-loop misinformation extraction because Twitter has provided clearly defined policies in this area , which we use as guidelines in a realistic human evaluation of a system 's output .", "Comments": [], "label": []}
{"id": 159362, "text": "Mask Infilling with BART To perform infilling , we took an approach similar to that of [ Donahue et al. , 2020 ] , but we used BART [ Lewis et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 152150, "text": "GIT also uses a * Tracker * to track the extracted records to consider global interdependency during extraction .", "Comments": [], "label": []}
{"id": 155966, "text": "We employed block attention with window size and stride equal to 512 .", "Comments": [], "label": []}
{"id": 158300, "text": "A.6 Hyperparameters BLEU Package : evaluate , [ https : //huggingface . ] ( https : //huggingface.co/spaces/evaluate-metric/bleu ) [ co/spaces/evaluate-metric/bleu ] ( https : //huggingface.co/spaces/evaluate-metric/bleu ) .", "Comments": [], "label": []}
{"id": 155983, "text": "This is followed by employing a pretrained T5 model [ Raffel et al. , , 2020 ] to fill the blanks to form a new sample x 0 ( see Appendix [ A.3 ] for more details ) .", "Comments": [], "label": []}
{"id": 158933, "text": "All data used in this paper is licensed under a MIT License . < https : //github.com/Unbabel/COMET > [ https : //scikit-learn.org ] ( https : //scikit-learn.org ) Table 3 : Examples of hallucination types .", "Comments": [], "label": []}
{"id": 152125, "text": "Specifically , GIT improves 2.8 micro F1 compared with the previous state-of-the-art , Doc2EDAG , especially 4.5 improvement in * Equity Underweight * ( EU ) event type .", "Comments": [], "label": []}
{"id": 153909, "text": "Transformers rely on positional encodings to inject the absolute and relative positions of the tokens .", "Comments": [], "label": []}
{"id": 153568, "text": "Recently , DALL-E [ Ramesh et al. , , 2021 ] merges text and visual tokens as a single stream of data and employs a universal Transformer to autoregressively model the multimodal stream .", "Comments": [], "label": []}
{"id": 154393, "text": "From Table [ 6 , ] `` decoder-last '' also has better ROUGE 1 , 2 , L values than other counterparts . 5.2 Ablation on Dynamic Scaling Figure [ 8 ] shows the learned scaling γ of different modules in the 2-bit GPT-2 model .", "Comments": [], "label": []}
{"id": 157993, "text": "The different prompts that we used before finaliz- Table 9 : Performance ( Binary F1 ) at recognizing whether a message was * in * appropriate in a relationship context on the test set .", "Comments": [], "label": []}
{"id": 151880, "text": "Surprisingly , we find that simply modeling the conversation history through a standard two-level hierarchical sequence model does not consistently < https : //convex.mpi-inf.mpg.de/ > [ https : //amritasaha1812.github.io/ ] ( https : //amritasaha1812.github.io/CSQA/ ) [ CSQA/ ] ( https : //amritasaha1812.github.io/CSQA/ ) [ https : //demo.allennlp.org/ ] ( https : //demo.allennlp.org/named-entity-recognition ) [ named-entity-recognition ] ( https : //demo.allennlp.org/named-entity-recognition ) [ https : //nlp.stanford.edu/projects/ ] ( https : //nlp.stanford.edu/projects/glove/ ) [ glove/ ] ( https : //nlp.stanford.edu/projects/glove/ ) Table 1 : F1 results on development and Acc/F1 results on test of ConvQuestions and ConvCSQA .", "Comments": [], "label": []}
{"id": 152614, "text": "This corresponds to the heuristic based data augmentation works where they typically modify tokens with a 30 % to 40 % probability .", "Comments": [], "label": []}
{"id": 155143, "text": "Configuration We adopt a dropout rate of 0.1 for extremely high-resource En→Fr , En→De ( WMT19 ) ; for all other language pairs , we set the value of 0.3 .", "Comments": [], "label": []}
{"id": 157436, "text": "T5 : what did the bus driver say ?", "Comments": [], "label": []}
{"id": 152629, "text": "We use ALBERT-XXL-v2 [ Lan et al. , , 2020 ] , RoBERTaLarge and RoBERTaBase [ Liu et al. , , 2019 ] , BERTLarge and BERTBase [ Devlin et al. , , 2019 ] , XLM-R [ Conneau et al. , 2020 ] , and 12 MiniBERTas [ Zhang et al. , , 2021b ] .", "Comments": [], "label": []}
{"id": 154652, "text": "For ALBEF ITM , we use the third layer of the multi-modal transformer for GradCAM ( following [ Li et al. , 2021 ] ) .", "Comments": [], "label": []}
{"id": 160061, "text": "We use Ebias , Evar , EvarO , and Evar for Stage-I , and adopt classic evaluation metrics ( P , R , and F1 ) for Stage-I and Stage-II .", "Comments": [], "label": []}
{"id": 152362, "text": "For both datasets , we use a stratified 80/10/10 train/dev/test split .", "Comments": [], "label": []}
{"id": 152160, "text": "Our GIT consistently outperform other baselines . * * EquityUnderweight EquityOverweight * * Figure 7 : The original complete document corresponding to the running example in Figure [ 1 . ] Sentences in red color are presented in Figure [ 1 . ]", "Comments": [], "label": []}
{"id": 154250, "text": "Formality transfer ( GYAFC ) : We use 1051 sentences from the entertainment and music domain subset of the GYAFC [ Rao and Tetreault , 2018 ] dataset , which contains formal and informal sentences for the task of formality transfer ( both directions of formal to informal and informal to formal ) .", "Comments": [], "label": []}
{"id": 151775, "text": "[ https : //github.com/pytorch/fairseq/ ] ( https : //github.com/pytorch/fairseq/tree/master/examples/backtranslation ) [ tree/master/examples/backtranslation ] ( https : //github.com/pytorch/fairseq/tree/master/examples/backtranslation ) [ https : //github.com/clab/fast_align ] ( https : //github.com/clab/fast_align ) < https : //kheafield.com/code/kenlm/ >", "Comments": [], "label": []}
{"id": 158221, "text": "Note that for vote prediction tasks , we use of bills and corresponding voting records , instead of of records .", "Comments": [], "label": []}
{"id": 159720, "text": "Next , the emotion-aware visual representation is then integrated with textual and acoustic representations via Cross-Modal Transformer [ Tsai et al. , 2019 ] for utterance-level emotion recognition .", "Comments": [], "label": []}
{"id": 153168, "text": "We adopt two benchmark datasets annotated by [ Yu and Jiang , 2019 ] , namely TWITTER-2015 and TWITTER-2017 to evaluate our model .", "Comments": [], "label": []}
{"id": 159870, "text": "In contrast to Transformer 's quadratic complexity , HyperMixer 's complexity is linear in the input length .", "Comments": [], "label": []}
{"id": 159605, "text": "[ 11 ] As a baseline , we use the currently best-performing system , UDPIPE [ Straka et al. , , 2019 ] , a transformer-based multitask architecture that utilizes multilingual BERT , trainable word embeddings , and character embeddings .", "Comments": [], "label": []}
{"id": 153761, "text": "A Transformer encoder has two sub-layers , a multi-head attention layer and a fully-connected feed-forward layer .", "Comments": [], "label": []}
{"id": 158929, "text": "We show the performance of both these versions in Table [ 4 . ] CometKiwi significantly outperforms the previous iteration of COMET-QE .", "Comments": [], "label": []}
{"id": 155754, "text": "Task Losses We employ a max-margin loss for the bridging and coreference resolution tasks .", "Comments": [], "label": []}
{"id": 154904, "text": "Previous SOTA reported by [ Minh Van Nguyen and Nguyen , 2021 ] Table 8 : Morpheme-Based NER F1 on the NEMO corpus .", "Comments": [], "label": []}
{"id": 152454, "text": "Based on these results , we used early fusion in our final CitationIE models for mention identification and relation extraction .", "Comments": [], "label": []}
{"id": 159034, "text": "Established evaluation metrics such as ROUGE [ Lin , 2004 ] and BLEU [ Papineni et al. , , 2002 ] rely on the n-gram overlap between candidate and reference sentences , but are not concerned with the * semantic similarity * of predictions and gold-standard ( reference ) data .", "Comments": [], "label": []}
{"id": 156172, "text": "For TaPas , we use the pre-trained version and follow its weak supervised training process on WTQ . 3.2.3 Partial Supervision Given labeled entity links , quantity links , and calculations ( from the formula ) , we further explore to guide training in a * partially supervised * way .", "Comments": [], "label": []}
{"id": 155521, "text": "We adopt the biaffine graph-based parser [ Dozat and Manning , 2017 ] with the Transformer encoder .", "Comments": [], "label": []}
{"id": 152328, "text": "To close this gap , we use a rating deviation between the source reviews and the target as an additional input feature of the text decoder , inspired by [ Brazinskas et al . ] ˇ [ 2020 ] .", "Comments": [], "label": []}
{"id": 152105, "text": "We summarize our contributions as follows : • Experiments show GIT outperforms the previous state-of-the-art model by 2.8 F1 on the large-scale public dataset [ Zheng et al. , , 2019 ] with 32 , 040 documents , especially on crosssentence events and multiple events scenarios ( with 3.7 and 4.9 absolute increase on F1 ) .", "Comments": [], "label": []}
{"id": 152927, "text": "[ 11 ] In terms of BLEU score – 34.12 on enes , 40.3 on en-fr , 27.7 on en-it – our LARGE-BPE models compare favorably with recently published results on MuST-C test data [ Le et al. , 2021 ] [ 12 ] and [ Bentivogli et al. , 2021 ] [ 13 ] ) .", "Comments": [], "label": []}
{"id": 159710, "text": "We use RoBERTa-large [ Liu et al. , , 2019 ] and GPT-2 [ Rad ] [ ford et al. , , 2019 ] as the backbones .", "Comments": [], "label": []}
{"id": 154411, "text": "Figure [ 1 ] gives a overview of our model . 2.2 Encoder We use multi-layer Transformer-based [ Vaswani et al. , 2017 ] encoder to encode the dialogue context .", "Comments": [], "label": []}
{"id": 156321, "text": "We use MAP inference to identify the solution edge set .", "Comments": [], "label": []}
{"id": 151725, "text": "For example , on Phone datasets our methods can inference in a single batch , while Hier-Matcher can only run in a batch size of 4 with 24GiB RAM .", "Comments": [], "label": []}
{"id": 153857, "text": "For FINETUNE and PET , we use the default learning rate of 10− , while for our method , as required by adapter-based methods [ Mahabadi et al. , , 2021a ] , we set the learning rate to a higher value of 10− . [ 7 ] Through all experiments , we fix the adapter bottleneck size to 64 .", "Comments": [], "label": []}
{"id": 155330, "text": "All student models have the same architecture of 66M parameters , 6 Transformer layers and 1.94× speed-up .", "Comments": [], "label": []}
{"id": 153133, "text": "For both pretraining and finetuning , we used Adam optimizer with an initial learning rate of 1e-3 .", "Comments": [], "label": []}
{"id": 155368, "text": "As shown in Table [ 3 , ] the end-to-end baseline W2V2-Transformer is inferior to the cascaded system , but our method significantly outperforms it , which shows the potential of our STEMM method . 4.2 Ablation Studies Is Each Learning Objective Effective ?", "Comments": [], "label": []}
{"id": 156837, "text": "We use the output of the last Res-SE block , h l , and the normalized attention scores w.r.t . the maximum weight , ascaled , as inputs to two linear layers that share parameters for all the labels .", "Comments": [], "label": []}
{"id": 153158, "text": "The encoder of our model is a multilayer bidirectional Transformer .", "Comments": [], "label": []}
{"id": 153065, "text": "For the two image classification tasks CIFAR-10 and CIFAR-100 , our implementation is based on the source code released by [ Chen et al. , 2020 ] [ 4 ] .", "Comments": [], "label": []}
{"id": 158089, "text": "Here , we use X and Y to denote the sentence in source and target language , respectively , and x denotes the t-th position of X. w is a word in the vocabulary V .", "Comments": [], "label": []}
{"id": 151454, "text": "Except where noted , BART is pretrained on OpenWebText , BookCorpus , CC-News , and Stories [ Lewis et al. , , 2020 ] , T5 is pretrained on C4 [ Raffel et al. , , 2020 ] , and both are fine-tuned on the TextWorld or Alchemy datasets described below .", "Comments": [], "label": []}
{"id": 157453, "text": "[ 10 ] from multiple components : the usual transformer attention score ( Eq .", "Comments": [], "label": []}
{"id": 151456, "text": "Results Results are shown in Table [ 1 . ] A probe on T5 can exactly recover 14.3 % of information states in Alchemy , and 53.8 % in TextWorld .", "Comments": [], "label": []}
{"id": 154122, "text": "Pre-trained Language Models including BERT [ Devlin et al. , , 2019 ] , GPT [ Radford et al. , , 2018 ] , and T5 [ Raffel et al. , , 2019 ] have led to a learning paradigm shift in NLP .", "Comments": [], "label": []}
{"id": 157051, "text": "For each dataset , we use the learned syntactically controlled paraphrase generators from Section [ 5.2 ] to generate three augmented examples with different parses for each training instance .", "Comments": [], "label": []}
{"id": 153999, "text": "Although character embeddings seem to make little difference in overall F1 , recall was consistently higher in models that included character embeddings , and in fact , the model with BETO+BERT embeddings , BPE embeddings and character embeddings produced the highest recall overall ( 77.46 ) .", "Comments": [], "label": []}
{"id": 158218, "text": "In the first stage , we adopt the MLM task on the original statement sentences and activate text modules , to urge the model to understand the political text .", "Comments": [], "label": []}
{"id": 157847, "text": "We replicate this experiment by reporting the performance of the T5 model with a different number of the previous history turns in Table [ 7 . ] We derive the same observation as [ Do et al. , 2022 ] , which is the model performs the best with a maximum of two or three conversational previous turns .", "Comments": [], "label": []}
{"id": 152413, "text": "For the zero-shot setting , we use English for training and evaluate the model on different target languages .", "Comments": [], "label": []}
{"id": 155536, "text": "This is a relevant and important finding for low-resource speech synthesis because it shows that a FastSpeech2 voice built with 3 hours of data can achieve subjective naturalness ratings which are not significantly different from a Tacotron2 voice built with 24 hours of data .", "Comments": [], "label": []}
{"id": 152063, "text": "For Zh→En , the number of merge operations in byte pair encoding ( BPE ) [ Sennrich et al. , , 2016a ] is set to 32K for both source and target languages .", "Comments": [], "label": []}
{"id": 159901, "text": "On all datasets , the relative improvement of HyperMixer over Transformers is larger when training with 10 % of the dataset than with the full dataset .", "Comments": [], "label": []}
{"id": 156432, "text": "[ com/facebookresearch/ ] ( https : //github.com/facebookresearch/fairseq ) [ fairseq ] ( https : //github.com/facebookresearch/fairseq ) Table 4 : Performance on ROC Stories , Writing Prompt , and WikiPlots .", "Comments": [], "label": []}
{"id": 155347, "text": "We use dev set for validation and tst-COMMON set for test .", "Comments": [], "label": []}
{"id": 156090, "text": "FID * indicates that the reported results are obtained by our implementation following the training details in the paper .", "Comments": [], "label": []}
{"id": 156680, "text": "We use a contrastive loss over the final layers of the object and text decoders . 3.2 Baselines Groundless Baseline .", "Comments": [], "label": []}
{"id": 154400, "text": "We use BART-base with 6 encoder layers , 6 decoder layers and hidden state dimension as 768 for experiments in Section [ 4 . ]", "Comments": [], "label": []}
{"id": 156434, "text": "As for the multi-paragraph opinionated articles writing , we utilize B-n , R-n , and METEOR [ Banerjee and Lavie , 2005 ] to evaluate the results .", "Comments": [], "label": []}
{"id": 153038, "text": "Orig . denotes performance without defense from Table [ 2 . ] Table 4 : Schema linking analysis of ETA on Spider . with three baseline adversarial training approaches : Word2Vec ( W2V ) , TextFooler ( TF ) [ Jin et al. , , 2020 ] , and BERT-Attack ( BA ) [ Li et al. , , 2020 ] ( details found in [ D. ] ) .", "Comments": [], "label": []}
{"id": 152758, "text": "We use circled numbers ( e.g . ① , ② , · · · ) to denote unique function calls being executed ( in exec . means ongoing execution , done means finished execution ) .", "Comments": [], "label": []}
{"id": 159895, "text": "For short input sequences , the number of FOPs is dominated by the size of the hidden layer and hence slightly lower for Transformers than for HyperMixer .", "Comments": [], "label": []}
{"id": 153923, "text": "The results for models trained on larger corpora are shown in Table [ 2 . ] GLMRoBERTa can achieve performance matching the seq2seq BART model , and outperform T5 and UniLMv2 .", "Comments": [], "label": []}
{"id": 156782, "text": "< https : //beta.openai.com/docs/guides/fine-tuning > [ ; for ex ] [ planation , we use the default settings ; for multiple choice , we ] set prompt loss weight [ to zero .", "Comments": [], "label": []}
{"id": 156120, "text": "1 Introduction Pre-trained transformer-based language models ( e.g. , [ Devlin et al. , , 2019 ] form the basis of state-of-the-art results across NLP .", "Comments": [], "label": []}
{"id": 156900, "text": "As a baseline for rationale selection , we use a retrieval-based method .", "Comments": [], "label": []}
{"id": 156035, "text": "For the backbone Transformer , we choose BERTBase [ De ] [ vlin et al. , , 2018 ] for the GLUE benchmarks , and two state-of-the-art long-text Transformers * Big Bird * [ Zaheer et al. , , 2020 ] and * Performers * [ Choro ] [ manski et al. , , 2020 ] for the LRA .", "Comments": [], "label": []}
{"id": 156602, "text": "Table 14 : SINGULARITY hyper-parameters for text-to-video retrieval tasks .", "Comments": [], "label": []}
{"id": 154489, "text": "For other baselines , we compare with representative methods from each category : ( 1 ) not using external knowledge source : T5 [ Roberts et al. , , 2020 ] and GPT-3 [ Brown et al. , , 2020 ] ; ( 2 ) reranking-based methods : RIDER [ Mao et al. , , 2021 ] and RECON-SIDER [ Iyer et al. , , 2021 ] ; ( 3 ) leveraging knowledge graphs or graph information between passages : Graph-Retriever [ Min et al. , , 2019 ] , Path-Retriever [ Asai et al. , , 2020 ] , KAQA [ Zhou et al. , , 2020 ] , and UniK-QA [ Oguz et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 157150, "text": "Did you report the number of parameters in the models used , the total computational budget ( e.g. , GPU hours ) , and computing infrastructure used ?", "Comments": [], "label": []}
{"id": 158577, "text": "GPT2 [ Radford et al. , , 2019 ] models are trained with a text generation objective only on positive examples , and we use perplexity as the prediction scores to calculate AUC .", "Comments": [], "label": []}
{"id": 153384, "text": "In this paper , we propose Overlap BPE ( OBPE ) .", "Comments": [], "label": []}
{"id": 159629, "text": "We adopt a divide-and-conquer approach to use two caches and adopt different eviction policies .", "Comments": [], "label": []}
{"id": 159131, "text": "For example , SacreBLEU has a scoring range between 0 to 100 .", "Comments": [], "label": []}
{"id": 157271, "text": "After the 4th transformer layer , we insert the layers L4 , k to transform the hidden states before inputting them to the 5th layer .", "Comments": [], "label": []}
{"id": 159786, "text": "Note that the shadow datasets we use for backdoor injection are the same as introduced in [ Section 3.3 . ] Table 8 : Impact of frozen layers on attack performance .", "Comments": [], "label": []}
{"id": 152814, "text": "We use a T5 [ Raffel et al. , , 2020 ] model fine-tuned An alternative approach would be to make direct , targeted edits to the original context c .", "Comments": [], "label": []}
{"id": 157649, "text": "For the scoring function that measures similarity between the conversation history and the text of recipe steps , we use two simple approaches : ( 1 ) WordMatch ( Word Matching ) : the scoring function computes the unigram F1 overlap between two input texts .", "Comments": [], "label": []}
{"id": 151615, "text": "To evaluate model performance , we use Matthew 's correlation score for CoLA , matched accuracy for MNLI , F1-score for SQuAD v1.1 , and accuracy in percentage for other tasks on GLUE .", "Comments": [], "label": []}
{"id": 157168, "text": "Since k xReact i , j ∈ KxReact i is usually an emotion word ( e.g. , happy , sad ) , we concatenate KxReact i and feed it into the Transformer-based encoder ( reaction encoder ) to get the representation of the emotional reaction Her i .", "Comments": [], "label": []}
{"id": 153669, "text": "As shown by the examples , large-scale pretrained models ( GPT-2 , T5 ) miss nuances present in perceptions of informed readers even when they correctly predict whether the headline is from real news or not .", "Comments": [], "label": []}
{"id": 155991, "text": "Moreover , T5-style blank filling could produce samples that are more compatible with label flipping .", "Comments": [], "label": []}
{"id": 152438, "text": "When rendering utterances , we use a templatebased language generator as in [ He et al. , , 2018 ] , and insert population-specific tokens in utterances by sampling according to different opponent types .", "Comments": [], "label": []}
{"id": 154169, "text": "Strictly speaking , it is unfair to directly compare with them since we use additional data .", "Comments": [], "label": []}
{"id": 157666, "text": "For fair comparison [ Xu and ] [ Zhao , 2021 ] [ Zhang and Zhao , 2021 ] [ Xu et al. , , 2021 ] , we utilize two public dialogue datasets , JDDC [ Chen et al. , , 2020 ] and ECD [ Zhang et al. , , 2018 ] , to conduct all experiments .", "Comments": [], "label": []}
{"id": 158264, "text": "Increasing the number of available resources ( number of CPU cores ) allows the methods to decrease the parallel overheads .", "Comments": [], "label": []}
{"id": 157494, "text": "We use Adam [ Kingma and Ba , 2015 ] to optimize L. 5 Experiments 5.1 Datasets CN-DBpedia From the latest version of Chinese KG CN-DBpedia [ Xu et al. , , 2017 ] and Wikipedia , we randomly sample 100,000 instances to construct our sample pool .", "Comments": [], "label": []}
{"id": 153654, "text": "We adopt three popular evaluation metrics for clustering : normalized mutual information ( NMI ) , adjusted rand index ( ARI ) , and accuracy ( ACC ) .", "Comments": [], "label": []}
{"id": 154496, "text": "To implement the approach , we utilize * RELAX * [ Grathwohl et al. , , 2018 ] , a contemporary gradient estimator which is both low-variance and unbiased , and we fine-tune the baseline in a few-shot style for both stability and computational efficiency .", "Comments": [], "label": []}
{"id": 156902, "text": "The average Fleiss ' Kappa [ Fleiss , 1971 ] is 0.89 , and the Krippendorf 's alpha [ Krippendorff , 2019 ] is 0.83 , showing that the quality of annotations is high enough to be used in our analysis of the EXP We use all-mpnet-base-v2 as the current best performing model for sentence representation .", "Comments": [], "label": []}
{"id": 152726, "text": "When setting the coefficient of the cross attention module to λcross = 1.0 , the ROUGE scores drop most .", "Comments": [], "label": []}
{"id": 159398, "text": "Researchers often leverage popular Natural Language Generation ( NLG ) metrics such as BLEU [ Papineni et al. , , 2002 ] and ROUGE [ Lin , 2004 ] to evaluate the similarity between model-generated and human-annotated explanations , with a strong assumption that humanannotated ones are the gold standard .", "Comments": [], "label": []}
{"id": 157784, "text": "We use 512 for maximum segment length with batch size of one document similar to [ Lee et al. , 2018 ] .", "Comments": [], "label": []}
{"id": 159054, "text": "Here , the annotators were asked to perform pair-wise comparison between two sets of generated KPs for which the difference between ROUGE scores and the soft-F1 metric was the highest .", "Comments": [], "label": []}
{"id": 152812, "text": "Instead , we use post-hoc filtering to reduce noise , select minimal candidates , or select for specific semantic phenomena based on the relation between q and q 0 .", "Comments": [], "label": []}
{"id": 160045, "text": "We use standard accuracy ( Acc , % ) and F1-macro ( F1 , % ) as evaluation metrics .", "Comments": [], "label": []}
{"id": 152242, "text": "The heuristic functions primarily consist of regular expressions ( Regexes ) and incorporate open-sourced natural language understanding ( NLU ) services .", "Comments": [], "label": []}
{"id": 156324, "text": "For any results that we discuss on the data we use , we will not include account information and all results will be anonymous .", "Comments": [], "label": []}
{"id": 157601, "text": "We use the preprocessed data from the MRQA shared task [ Fisch et al. , 2019 ] for our experiments .", "Comments": [], "label": []}
{"id": 154850, "text": "The context encoder is a Lorentz Transformer encoder that shares the same embedding module with mention encoder .", "Comments": [], "label": []}
{"id": 160177, "text": "Therefore , we leverage automated instruction search [ Gao et al. , 2021 ] , where we use an encoder-decoder LM , e.g. , a T5-Base model [ Raf ] [ fel et al. , , 2020 ] , that is trained to fill masked text spans to generate instructions .", "Comments": [], "label": []}
{"id": 151512, "text": "XNLI We use the standard dataset [ Conneau et al. , 2018 ] .", "Comments": [], "label": []}
{"id": 152473, "text": "N-gram Rhyme To highlight the advantage of DeepRapper in modeling N-gram rhyme , we use Combo-N to measure the ability of each design in DeepRapper to model N-gram rhyme .", "Comments": [], "label": []}
{"id": 155182, "text": "BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding .", "Comments": [], "label": []}
{"id": 157730, "text": "Generation is performed by a transformer-based seq2seq model - T5 ( base ) [ Raf ] [ fel et al. , , 2020a ] .", "Comments": [], "label": []}
{"id": 155234, "text": "For CNN and RNN-based models , we use all the vocabulary tokens that can be extracted from the training set , and we use all of the vocabulary tokens provided by pre-trained models for BERT and RoBERTa-based models .", "Comments": [], "label": []}
{"id": 155161, "text": "BART : While both model classes perform well , BART models tend to perform better overall , demonstrating the value of BART 's larger transformer-based architecture and pretraining .", "Comments": [], "label": []}
{"id": 153300, "text": "We compare CipherDAug to back-translation-based data-diversification [ Nguyen et al. , , 2019 ] , word replacement techniques like SwitchOut [ Wang et al. , , 2018 ] , WordDrop [ Sennrich et al. , , 2016a ] , and RAML [ Norouzi et al. , , 2016 ] , and the subword-regularization technique BPE-Dropout [ Provilkov et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 157470, "text": "OCR serialization also distorts strong cues of semantic relatedness – a word that is just above another word may be related to it , but if there are many tokens to the right of the upper word or to the left of the lower word , they will intervene between the two after serialization , and the model will be unable to take advantage of the heuristic that nearby tokens tend to be related .", "Comments": [], "label": []}
{"id": 156947, "text": "Both text and speech encoders have 12 Transformer layers with a hidden size d of 768 .", "Comments": [], "label": []}
{"id": 158552, "text": "Semantic Information Refinement . 2.3 Emotion Classifier The output of graph transformer is fed into a MLP with fully connected layers and get the prediction values of the utterance u under each emotion label : where yˆ is the emotion label predicted for the utterance u .", "Comments": [], "label": []}
{"id": 151631, "text": "We use CoLA , MRPC , QNLI , and SST-2 with their original training and development sets for our analysis .", "Comments": [], "label": []}
{"id": 156201, "text": "We use XLParser [ Aivaloglou ] * et al . * , [ 2015 ] , a highly-compatible formula parser with compact grammar , to analyze formula .", "Comments": [], "label": []}
{"id": 155557, "text": "To achieve a high compression ratio while maintaining the competitiveness in relevance , we consider the ranking contribution of a contextual token embedding for soft matching containing two components : 1 ) document specific component derived from the self attention among context in a document , 2 ) document-independent and corpus-specific component generated by the transformer model .", "Comments": [], "label": []}
{"id": 155915, "text": "We use a learning rate of 1e-5 , a batch size of 8 and run the training for 10 epochs .", "Comments": [], "label": []}
{"id": 153478, "text": "6.2 Metrics For multimodal chat translation , following previous work [ Liang et al. , , 2021c ] [ Ive et al. , , 2019 ] , we use the SacreBLEU [ 6 ] [ Post , 2018 ] , METEOR [ Denkowski and Lavie , 2014 ] and TER [ Snover et al. , , 2006 ] with the statistical significance test [ Koehn , 2004 ] for fair comparison .", "Comments": [], "label": []}
{"id": 154615, "text": "The last column of [ Table 3 ] shows the percentage of responses where the input to the Transformer is truncated for session 4 , for each truncation length .", "Comments": [], "label": []}
{"id": 151582, "text": "Given the true claims and the 5 evidence documents for each claim ( Section [ 2.1 ] we use cosine similarity on SBERT sentence embeddings [ Reimers and ] [ Gurevych , 2019 ] to extract the top 5 sentences most similar to the true claim .", "Comments": [], "label": []}
{"id": 158094, "text": "On each GPU , the number of tokens in each batch is at most 32K .", "Comments": [], "label": []}
{"id": 155165, "text": "We use 20 as it achieved the best performance on the validation set while completing in reasonable time .", "Comments": [], "label": []}
{"id": 152003, "text": "Compared to the document-level Transformer baseline , G-Transformer gives 1.74 , 1.22 , and 0.31 higher d-BLEU points , respectively .", "Comments": [], "label": []}
{"id": 157295, "text": "With the advances in NLP , there has been considerable research into the field of AA that has demonstrated the success of TFIDF-based clustering and classification techniques [ Agarwal et al. , , 2019 ] [ ˙Izzet Bozkurt et al. , , 2007 ] , CNNs [ Rhodes , 2015 ] [ Shrestha et al. , , 2017 ] , RNNs [ Zhao et al. , , 2018 ] [ Jafariakinabad et al. , , 2019 ] [ Gupta et al. , , 2019 ] , and transformers architectures [ Fabien et al. , , 2020 ] [ Ordoñez et al. , , 2020 ] [ Uchendu et al. , , 2020a ] .", "Comments": [], "label": []}
{"id": 152405, "text": "For low-resource fine-tuning in [ §3.3 , ] we use reduction factors of { 16,32,64 } .", "Comments": [], "label": []}
{"id": 159170, "text": "Acknowledgments We thank Vagrant Gautam for thoughtful suggestions and insightful discussions .", "Comments": [], "label": []}
{"id": 159408, "text": "T5-base : ECQA > CoS-E v1.11 > CoS-E v1.0 > e-SNLI > ComVE BART-base : ECQA > CoS-E v1.11 > CoS-E v1.0 > ComVE > e-SNLI From Table [ 3 , ] the Simulatability score ranks e-SNLI and ComVE reversely on BART compared with T5 models , indicating Simulatability score could be more affected by different model architectures even with the unified data structure .", "Comments": [], "label": []}
{"id": 155109, "text": "The time vector P num ( t ) ∈ R |V |×K×d is learned by combining functional forms and learnable weights and could be viewed as a time-sensitive version of positional encodings used in transformers [ Vaswani et al. , 2017 ] .", "Comments": [], "label": []}
{"id": 157032, "text": "To justify that , we replace the Transformer-base teacher models with RNN [ Bahdanau et al. , , 2014 ] and Transformer-big [ Vaswani et al. , , 2017 ] models , and repeat the experiments in Table [ 2 ] with other settings remaining identical .", "Comments": [], "label": []}
{"id": 156004, "text": "We will always augment in the following three steps : ( 1 ) mask the sentence , ( 2 ) generate the new label ( preserve or flip the label ) , and ( 3 ) fill in the blanks by T5 .", "Comments": [], "label": []}
{"id": 152713, "text": "Following [ Durrett et al. , 2016 ] ; [ Liu and Lapata , 2019 ] , we report limited-length recall based ROUGE-1 , ROUGE-2 , and ROUGE-L , where generated summaries are truncated to the lengths of gold summaries .", "Comments": [], "label": []}
{"id": 153584, "text": "For simplicity , we employ greedy decoding as the sampling method to select the word/code with the highest probability .", "Comments": [], "label": []}
{"id": 158529, "text": "The results show that Multi-DYLE ( XROG o , XCES o ) with dual extractors again achieves the best performance under the joint settings involving E3 , exhibiting increases of approximately 2 in the Joint ROUGE scores and of approximately 4 in the F1 score of E3 , compared to those of Figure 3 : Example of ExplainMeetSum of comparing the summary and evidence sentences between the predicted results and gold ones for the query in QMSum ( i.e. , the transcript name : IS1006c , the query id : specific-6 ) ; G-Sent-n/P-Sent-n refer to n-th gold/predicted summary sentence .", "Comments": [], "label": []}
{"id": 158096, "text": "Settings We follow the model configurations used in CeMAT [ Li et al. , , 2022 ] to train a * Transformer-big * [ Vaswani et al. , , 2017 ] size NMT model , which will compare with models using the pretrain-finetune paradigm .", "Comments": [], "label": []}
{"id": 160255, "text": "Specifically , we use the imagecaption pairs from 2017 's validation split .", "Comments": [], "label": []}
{"id": 155900, "text": "As the objective we use the so called * batch softmax * [ Henderson et al. , , 2017 ] : Where B is the batch size and S ( x , y ) = f ( x ) ·f ( y ) the similarity between input x and label text y under the current model f .", "Comments": [], "label": []}
{"id": 152820, "text": "Over-generating potential counterfactuals based on latent dimensions identified in retrieval and using a simple filtering heuristic avoids biasing the model toward a narrow set of perturbation types [ Joshi and He , 2021 ] .", "Comments": [], "label": []}
{"id": 153391, "text": "We use LLRL to denote the subset of the n languages that are low-resource , the remaining languages L − LLRL are denoted as the set LHRL of high resource languages .", "Comments": [], "label": []}
{"id": 154075, "text": "First , we utilize the CRAFT model [ Baek et al. , 2019a ] to recognize the texts of the detected textual chart elements ( * x-axis labels * , and * legend labels * ) .", "Comments": [], "label": []}
{"id": 155916, "text": "We use a learning rate of 2e-5 and of 2e-4 for the Bit-Fit experiments .", "Comments": [], "label": []}
{"id": 160176, "text": "This process is shown in Figure [ 2 ] a ) . 2.1 Path Reranking with PROMPTRANK Given a question q and a reasoning path or chain c , we use an LM to score c according to its relevance to q .", "Comments": [], "label": []}
{"id": 159295, "text": "For each sample ( x , ) ∈ D ∪ DP T , we optimize the parameters for DALM by minimizing the combination of cross entropy losses for the output token sequence and label sequence as follows : 3.5 Target-Domain Data Generation After training the DALM , we employ it to generate target-domain data with fine-grained annotations in an autoregressive manner .", "Comments": [], "label": []}
{"id": 158753, "text": "To make synthesized images lifelike , we used a set of backgrounds randomly extracted from the ECOIT dataset and the font sizes are ranging from 30 pixels to 60 pixels .", "Comments": [], "label": []}
{"id": 160196, "text": "We use the checkpoint provided by [ Xiong et al. , 2021 ] , and the same inference setting .", "Comments": [], "label": []}
{"id": 159522, "text": "Furthermore , we also perform parameter-effcient tuning on T5- 3B-encoder with our approach and boost its performance .", "Comments": [], "label": []}
{"id": 158755, "text": "5.2 Settings and Baselines Model Configuration The shared encoder-decoder backbone network contains 6 Transformer encoder blocks and 6 Transformer decoder blocks , where the model dimension is 256 , and the number of attention heads is 8 .", "Comments": [], "label": []}
{"id": 153946, "text": "The evaluation metrics are the scores of BLEU-1 , BLEU-2 , BLEU-3 , BLEU-4 [ Papineni et al. , , 2002 ] , METEOR [ Denkowski ] [ and Lavie , 2014 ] and Rouge-L [ Lin , 2004 ] .", "Comments": [], "label": []}
{"id": 158480, "text": "We use such quintuples to train a sequence-to-sequence text generation model .", "Comments": [], "label": []}
{"id": 159144, "text": "We use the subset of the dataset that only contains Indian languages and follow [ Amrhein et al. , 2022 ] to report performance with Kendall 's tau-like correlations .", "Comments": [], "label": []}
{"id": 157169, "text": "Similar to [ Majumder et al. , , 2020 ] and [ Sabour et al. , , 2022 ] , we use the average pooling to represent the reaction sequence , i.e. , h er = Average ( Her i ) .", "Comments": [], "label": []}
{"id": 154916, "text": "We use a linear schedule with warmup , with the number of warmup steps set to 0 .", "Comments": [], "label": []}
{"id": 160215, "text": "We run each method with the maximum batch size that fits within the GPU .", "Comments": [], "label": []}
{"id": 153343, "text": "It also outperforms all span- [ nested-ner-tacl2020-transformers ] ( https : //github.com/yahshibu/nested-ner-tacl2020-transformers ) < https : //nlp.cs.nyu.edu/evalb > [ https : //github.com/yahshibu/ ] ( https : //github.com/yahshibu/nested-ner-tacl2020-transformers ) In their paper , they claim an O ( n ) time complexity , which treats the complexity of a single pointing operation as O ( 1 ) .", "Comments": [], "label": []}
{"id": 152600, "text": "Instead of human annotators to modify the labels we use the teacher . 3 Methodology We propose an algorithm that involves co-training and deploy an adversarial text generator while training a student network using KD .", "Comments": [], "label": []}
{"id": 153590, "text": "The metrics are organized into four groups – text explainability ( FMR , FCR ) , text diversity ( DIV , USR ) , text quality ( BLUE , ROUGE ) , and image consistency ( CS ) .", "Comments": [], "label": []}
{"id": 158571, "text": "To study conceptualization over CSKBs , we use the AbstractATOMIC dataset provided by [ He et al. , 2022 ] as the benchmark .", "Comments": [], "label": []}
{"id": 154660, "text": "For our task we use PER , ORG , DATE , and LOC entities .", "Comments": [], "label": []}
{"id": 157178, "text": "We use Adam optimizer [ Kingma and Ba , 2015 ] with β = 0.9 and β = 0.98 .", "Comments": [], "label": []}
{"id": 155076, "text": "Researchers have been passionate about pre-training Transformer models [ Vaswani et al. , , 2017 ] for source code with two categories : encoder-only and encoderdecoder [ Ahmad et al. , , 2021 ] [ Wang et al. , , 2021 ] [ Rozière et al. , , 2021 ] [ Phan et al. , , 2021 ] .", "Comments": [], "label": []}
{"id": 158869, "text": "We used Adam optimizer [ Kingma and Ba , 2014 ] , learning rate of 1e-05 , with linear warmup of 100 steps .", "Comments": [], "label": []}
{"id": 156641, "text": "Interestingly , when looking at hallucinations , internal ALTI performs on par with COMET-QE : the differences between these two methods are not statistically significant .", "Comments": [], "label": []}
{"id": 156014, "text": "This is determined by the characteristic of T5 .", "Comments": [], "label": []}
{"id": 158980, "text": "In our graph transformer , we use 4 self-attention layers with 1024 hidden size and 8 attention head .", "Comments": [], "label": []}
{"id": 158353, "text": "We estimate it takes 64 GPU hours to run a single evaluation on the mathlib test set .", "Comments": [], "label": []}
{"id": 153502, "text": "To summarize , our contributions are as follows : state-of-the-art results are achieved on the Long Range Arena benchmark . 2 Related Works 2.1 Efficient Transformer [ Tay et al. , 2020c ] have provided a comprehensive overview of existing efficient Transformers .", "Comments": [], "label": []}
{"id": 158915, "text": "We use this dataset to construct ( offline ) a set of held-out source attention distributions Rheld = { πM ( x ) ∈ △|x : x ∈ Dheld } .", "Comments": [], "label": []}
{"id": 152245, "text": "Example proxy metrics include conversation length like number of dialog turns [ Venkatesh et al. , , 2018 ] [ Ram et al. , , 2018 ] , and conversational breadth like topical diversity [ Guo et al. , 2018 ] .", "Comments": [], "label": []}
{"id": 158371, "text": "Did you report the number of parameters in the models used , the total computational budget ( e.g. , GPU hours ) , and computing infrastructure used ?", "Comments": [], "label": []}
{"id": 151537, "text": "Vision Box : person Trace Utterance : horse Number of Tracepoints in Box : Total Number of Points : 67 Supervision Attention Score=14/67=0.209 Attention-guided Grounding A cross-attention matrix is generated in shape ( N , T , L , H ) during the transformer 's decoding steps .", "Comments": [], "label": []}
{"id": 151442, "text": "We employ an Adam optimizer with a learning rate of varying from 5e-6 to 5e-5 .", "Comments": [], "label": []}
{"id": 153031, "text": "Effect of Random Sampling Strategy When training the model for BiTIMT in section 3.2.2 , we employ a random strategy to sample Y¯ such that it contains a random number of constraint segments .", "Comments": [], "label": []}
{"id": 151496, "text": "For TransE scoring function , we use L2 norm and a margin value of 9.0 .", "Comments": [], "label": []}
{"id": 157401, "text": "( c ) The various Transformer models – COREF ( from Sec .", "Comments": [], "label": []}
{"id": 157908, "text": "Unfortunately , the explicit correspondence between persona sentences and utterances is not given in PersonaChat , so we use the same method as in Cao et al . ( 2022 ) to predict the correspondence by a RoBERTa-based model first .", "Comments": [], "label": []}
{"id": 153258, "text": "Based on the pretrained T5 model [ Raffel et al. , , 2019 ] , UnifiedQA is continually trained on various QA tasks , including three yes/no datasets .", "Comments": [], "label": []}
{"id": 158583, "text": "The commonsense generation results are presented in Table [ 4 . ] Similar to COMET [ Bosselut et al. , , 2019 ] , all models are evaluated on the original ATOMIC 's full validation and testing sets .", "Comments": [], "label": []}
{"id": 155942, "text": "Analogously , we use v 0 = wivi+wjv as the new-round 's scores .", "Comments": [], "label": []}
{"id": 158876, "text": "We use `` one-step gradient '' because it strikes a balance between cost and effectiveness .", "Comments": [], "label": []}
{"id": 152323, "text": "Besides , we employ another weighted cross-entropy function to compensate for the imbalance .", "Comments": [], "label": []}
{"id": 160226, "text": "We use pre-trained text generation model T5-large [ Raffel et al. , 2020 ] as the backbone LM .", "Comments": [], "label": []}
{"id": 154923, "text": "4.2 Chinese Dataset For the definition generation dataset , we use the Chinese WordNet ( CWN ) [ Huang et al. , , 2010 ] , which is a semantic lexicon aiming to provide a knowledge base of sense distinction .", "Comments": [], "label": []}
{"id": 157923, "text": "Our criterion for including a language-script in Glot500-c is that it includes more than 30,000 sentences . * * Model training . * * To train Glot500-m , we employ vocabulary extension and continued pretraining .", "Comments": [], "label": []}
{"id": 153224, "text": "[ 2005 , 2009 ] adapt the hook trick to accelerate machine translation decoding .", "Comments": [], "label": []}
{"id": 158940, "text": "In our experiments , we utilize the weighted average similarity scores of two teachers as pseudo ranking labels : S T ( xi ) = αS 1 ( xi ) + ( 1 − α ) S T 2 ( xi ) where α is a hyperparameter to balance the weight of the teachers .", "Comments": [], "label": []}
{"id": 151515, "text": "We used a different seed each time to retrieve this 5 % data .", "Comments": [], "label": []}
{"id": 152142, "text": "As Table [ 5 ] shows , the F1 in GIT-OT/GIT-OP decreases by 0.5/1.2 , suggesting the interdependency among records of both the same and different event types do play an essential role .", "Comments": [], "label": []}
{"id": 151342, "text": "First , in the * contextualization * step , we use LMs to generate ensembles of past and future contexts which collectively capture the input ( e.g . the source sentence for paraphrasing ) .", "Comments": [], "label": []}
{"id": 153828, "text": "References A Implementation Details We used a standard Transformer implementation [ 4 ] , and added all the proposed variations on top of it .", "Comments": [], "label": []}
{"id": 151544, "text": "The embedding size d , number of transformer layers , hidden size of the transformer feed-forward layer are 768 , 2 , and 768 , respectively .", "Comments": [], "label": []}
{"id": 155397, "text": "Language Modeling We adopt the tokenizer of GPT-2 [ Radford et al. , , 2019 ] , which uses byte-pair encoding [ Sennrich et al. , , 2016 ] with a vocabulary size of 50,257 .", "Comments": [], "label": []}
{"id": 154245, "text": "3.4 Controlled generation Tasks We use the expert black-box factors and the sampling scheme described above in our framework to perform two kinds of controlled generation tasks .", "Comments": [], "label": []}
{"id": 154533, "text": "Table 4 : Comparison of the average ROUGE and coverage scores over the Multi-News validation set with different hidden sizes of the control variate . 5.4 Coverage Reward Trajectory In a reinforcement learning framework , it could be useful to monitor the value of the reward over the training steps .", "Comments": [], "label": []}
{"id": 159769, "text": "Such injection works at the encoder level since it misleads the transformer blocks in the encoder to focus on the presence of triggers and target anchors .", "Comments": [], "label": []}
{"id": 154970, "text": "For more in-depth analysis , we note that : 1 ) As the input of a model , the translation-based documents are prone to the error propagation , therefore , we should avoid Table 6 : Ablation Study , Zero-Shot ROUGE-L Results on Validation Dataset of MLSUM to encode these noise documents .", "Comments": [], "label": []}
{"id": 154836, "text": "Because HATT is a partially hyperbolic Transformer , intuitively , its ability to capture the structured information should be better than Euclidean Transformer , but worse than HYBONET .", "Comments": [], "label": []}
{"id": 158123, "text": "Following Transformer [ Vaswani et al. , 2017 ] architecture , they first employ selfattention modules to capture the contextual dependencies within each modality , followed by crossattention modules for interaction between two modalities , which can initially narrow their gap to benefit the subsequent modality-invariant representation learning .", "Comments": [], "label": []}
{"id": 157524, "text": "If we take the action sequence `` rmov , end=F , . . . `` to train a Transformer for copying , the hypothesis space is simplified , thus making it possible to find the simplest model that can simulate the whole action sequence .", "Comments": [], "label": []}
{"id": 155312, "text": "I Experimental Details For training , we utilize the Adam optimizer with learning rate of 10− and the batch size is 32 .", "Comments": [], "label": []}
{"id": 157490, "text": "Identifying Topic Prompt for Each Entity We adopt a topic classifier to assign the topic prompt to the input text X , which is one of the 17 typical topics in Table [ 6 . ]", "Comments": [], "label": []}
{"id": 160247, "text": "Object and Attribute Detection : For each image I we use a pre-trained bottom-up attention model [ Anderson et al. , , 2018 ] [ Yu et al. , , 2020 ] to extract objects and their associated attributes .", "Comments": [], "label": []}
{"id": 154149, "text": "To measure the isotropy of the token embedding distribution , we adopt the partition function Z ( a ) = P =1 exp ( wia T ) defined in [ Arora et al. , 2016 ] , where w denotes the embedding vector of token v , and a represents a unit vector .", "Comments": [], "label": []}
{"id": 155789, "text": "Glancing Transformer .", "Comments": [], "label": []}
{"id": 152634, "text": "We use all three variants in producing responses for IRT .", "Comments": [], "label": []}
{"id": 155977, "text": "D.3 Detailed Results Table [ 9 ] reports ROUGE scores for all of the evaluated models .", "Comments": [], "label": []}
{"id": 159344, "text": "Table 4 : Abstractive Summarization results on CNN-DailyMail in terms of F1-Rouge and efficiency ( parameter , inference speed , and FLOPs ) .", "Comments": [], "label": []}
{"id": 152202, "text": "For instance , given a query `` word : language '' and a candidate `` note : music '' , the prompting function produces T * to-as * ( `` word '' , `` language '' , `` note '' , `` music '' ) = `` word is to language as note is to music '' where we use the template type * to-as * here .", "Comments": [], "label": []}
{"id": 157495, "text": "To justify it , we compare KPCE with another variant , namely KPCE LDA , where the topics are the keywords obtained by running Latent Dirichlet Allocation ( LDA ) [ Blei et al. , , 2001 ] over the abstracts of all entities .", "Comments": [], "label": []}
{"id": 155187, "text": "Using BERTScore [ Zhang et al. , , 2020b ] to obtain oracle extractive summaries for training data produces models that are significantly stronger than models trained on sentences selected by maximizing ROUGE score .", "Comments": [], "label": []}
{"id": 155950, "text": "The decoder also has six layers , making our model directly comparable to the deeper Blockwise Transformer .", "Comments": [], "label": []}
{"id": 157656, "text": "We use AdamW to optimize and consider the initial learning rate ∈ { 1e-5 , 5e-5 , 1e-4 } .", "Comments": [], "label": []}
{"id": 151838, "text": "More detailed settings and implementation details can be found in Appendix [ B.3 . ] Results ROUGE scores on CNN/DM , XSum have been exhibited in Tables [ 4 , ] respectively .", "Comments": [], "label": []}
{"id": 158479, "text": "Did you report the number of parameters in the models used , the total computational budget ( e.g. , GPU hours ) , and computing infrastructure used ?", "Comments": [], "label": []}
{"id": 156236, "text": "The mappings we use for these two types of representations are shown in Appendix [ A.2 . ]", "Comments": [], "label": []}
{"id": 159653, "text": "We use Adam [ Kingma and Ba , 2014 ] and a half-precision training scheme to optimize the parameters of all MNMT models .", "Comments": [], "label": []}
{"id": 159026, "text": "In the experiment , we use RoBERTa-large [ Liu et al. , , 2019 ] as the encoder and Adamw [ Loshchilov and Hutter , 2019 ] as the model optimizer .", "Comments": [], "label": []}
{"id": 155667, "text": "We use the small labeled dataset D to identify hard instances , i.e. , where the model tends to make cumulative mistakes during iterative learning .", "Comments": [], "label": []}
{"id": 152930, "text": "Table 3 : BLEU , coverage and gender accuracy ( percentage ) scores computed on MuST-SHE . 5 Word-level Evaluation 5.1 Overall quality and gender translation Table [ 3 ] presents SacreBLEU [ Post , 2018 ] , [ 15 ] coverage , and gender accuracy scores on the MuST-SHE test sets .", "Comments": [], "label": []}
{"id": 152372, "text": "We used the range of possible values recommended by [ Devlin et al. , 2019 ] : [ 2 , 3 , 4 ] for epochs , [ 2e-5 , 3e-5 , 5e-5 ] for learning rate and [ 16 , 32 ] for batch size .", "Comments": [], "label": []}
{"id": 154326, "text": "We adopt a 4-gram Kneser-Ney language model to compute perplexity scores for the fluency data filter .", "Comments": [], "label": []}
{"id": 155775, "text": "Similar to AMR parsing , we use finetuned T5 [ 3 ] that is shown to be effective for the AMR-to-Text generation task [ Mager et al. , , 2020 ] [ Ribeiro et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 160344, "text": "We use off-the-shelf ASR models to transcribe the speech generated by vocoders .", "Comments": [], "label": []}
{"id": 157357, "text": "The performance metrics are macro and micro F1 The code is available at [ https : //github.com/IvonneMont/ ] ( https : //github.com/IvonneMont/Dynamic-UDA-for-Transformers-in-Multimodal-Classification.git ) [ Dynamic-UDA-for-Transformers-in-Multimodal-Classification.git ] ( https : //github.com/IvonneMont/Dynamic-UDA-for-Transformers-in-Multimodal-Classification.git ) Figure 2 : Augmented examples using word replacement and RandAugment for Moviescope and macro and micro average precision for MM-IMDb .", "Comments": [], "label": []}
{"id": 158444, "text": "Assume for t1 , t3 , t5 , t in E , the most similar tokens in O are t4 , t4 , t6 , t respectively , and t −t4 , t −t6 , t −t are the edges ( darker and thicker lines mean larger similarity values ) with top-r ′ ( r ′ = 3 ) most similarity , then we merge ( t3 , t4 ) into one token t m , ( t5 , t7 , t6 ) into one token t m , and keep t1 , t2 , t8 , in this case , we reduce three ( 3/8=37.5 % ) tokens .", "Comments": [], "label": []}
{"id": 158512, "text": "To clarify the different setups for using extractive oracles , with the abuse of notation , XROG o , XCES o , and XP ES o refer to the sets of ROUGE-based , CESbased , and PES-based extractive oracles ( in ExplainMeetSum ) , respectively .", "Comments": [], "label": []}
{"id": 158707, "text": "To ensure the annotation quality , we employ strict requirements for annotators : 1 ) Annotators should reside in China ; 2 ) Annotators should have college degrees .", "Comments": [], "label": []}
{"id": 152620, "text": "We use the 3-parameter ( 3PL ) IRT model , where item behavior is governed by discrimination , difficulty , and guessing parameters .", "Comments": [], "label": []}
{"id": 159452, "text": "If we have L different languages , then each layer will have as many parameters as L + 1 `` regular '' Transformer layers .", "Comments": [], "label": []}
{"id": 160132, "text": "We use an NBOW model and a relative position encoding ( RPE ) [ Shaw et al. , , 2018 ] to indicate the position of the claim in the input sentence .", "Comments": [], "label": []}
{"id": 154541, "text": "To this aim , Table [ 7 ] shows the values of the average ROUGE and coverage scores over the Multi-News validation set .", "Comments": [], "label": []}
{"id": 158557, "text": "CTNet [ Lian et al. , , 2021 ] utilizes transformer to obtain the multimodal rerpesentation by modeling the intra-modal and cross-modal interactions .", "Comments": [], "label": []}
{"id": 156747, "text": "We use the same algorithm as previous section to extract lexical relations .", "Comments": [], "label": []}
{"id": 156611, "text": "We use the model released by [ Guerreiro et al. , 2022 ] that has been used to generate the hallucinations we analyze .", "Comments": [], "label": []}
{"id": 160240, "text": "For the MI module , we use a window size of 10 words , with a sliding step of 4 words .", "Comments": [], "label": []}
{"id": 160254, "text": "Dataset : For this task , we use the MSCOCO dataset [ Lin et al. , , 2014 ] , which contains imagescaption pairs .", "Comments": [], "label": []}
{"id": 159907, "text": "While HyperMixer yields results that are on par with Transformer 's results , it reduces the cost of all three cost factors : i ) The cost of processing a single example ( E ) is lower , particularly for long inputs due to its linear complexity compared to the quadratic complexity of self-attention ( Section [ 4.4 ] .", "Comments": [], "label": []}
{"id": 157535, "text": "As shown in Figure [ 9 ] a ) , GPT3 achieves 100 % accuracy on the in-distribution testing data ( 1-5 digits ) but the fine-tuned T5 achieves 78 % accuracy on the 5-digit repeating numbers although they are indistribution .", "Comments": [], "label": []}
{"id": 153556, "text": "T5 ( in-domain ) On quantity typing , T5 improves by a large margin over the naive baseline in all domains .", "Comments": [], "label": []}
{"id": 156769, "text": "To complement these human evaluations , we also report in [ Ap ] [ pendix E ] automatic metrics that take into account the human-written reference : ( a ) BLEU-4 [ Pap ] [ ineni et al. , , 2002 ] using [ Post , 2018 ] +ROUGE-L [ Lin , 2004 ] ; and ( b ) word-level perplexity .", "Comments": [], "label": []}
{"id": 160233, "text": "We use the same data pre-processing , data splits and metrics as prior works [ Wadden et al. , 2019 ] [ Lin et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 154387, "text": "`` Bits ( W-E-A ) '' represents the bit-width for weights of Transformer layers , word embedding , and activations .", "Comments": [], "label": []}
{"id": 160256, "text": "We use the test set of the dataset , comprising 17 , 859 image ( premise ) & text ( hypothesis ) pairs .", "Comments": [], "label": []}
{"id": 156624, "text": "Note that we take such a large percentage because in the hallucination dataset we use , about 10 % of translations are hallucinations and about 30 % more are errors .", "Comments": [], "label": []}
{"id": 154188, "text": "https : // [ huggingface.co ] ( https : //huggingface.co/datasets/nlpaueb/finer-139 ) /datasets/nlpaueb/finer-139 https : // [ huggingface.co ] ( https : //huggingface.co/nlpaueb/sec-bert-base ) /nlpaueb/sec-bert-base https : // [ huggingface.co ] ( https : //huggingface.co/nlpaueb/sec-bert-num ) /nlpaueb/sec-bert-num https : // [ huggingface.co ] ( https : //huggingface.co/nlpaueb/sec-bert-shape ) /nlpaueb/sec-bert-shape Financial ner : Previous financial ner applications use at most 9 ( generic ) class labels .", "Comments": [], "label": []}
{"id": 151846, "text": "To avoid overfitting , we utilize the dropout and set its rate as 0.5 .", "Comments": [], "label": []}
{"id": 151622, "text": "We use F1 score for SQuAD , Matthew 's correlation score for CoLA and accuracy for all other tasks on GLUE as the metric .", "Comments": [], "label": []}
{"id": 155783, "text": "‡ 1 Introduction Non-autoregressive Transformer ( NAT , [ Gu et al. , , 2018 ] introduce a parallel decoding paradigm with higher decoding efficiency ( > 10× ) than autoregressive models [ Bahdanau et al. , , 2015 ] [ Gehring et al. , 2017 ] [ Vaswani et al. , , 2017 ] .", "Comments": [], "label": []}
{"id": 153818, "text": "It assumes that the input and output vocabularies are the same ( we use the union of input and output vocabularies in our experiments ) .", "Comments": [], "label": []}
{"id": 160127, "text": "We employ two in-house annotators , who act as mock content-moderators , to evaluate these 300 treatments and determine ( 1 ) if the extraction is a treatment ( 2 ) if the treatment is unapproved and ( 3 ) the earliest publication date of a news article debunking the treatment as effective for COVID-19 using the Google News engine .", "Comments": [], "label": []}
{"id": 153629, "text": "However , due to the batch-wise dependency of the contrastive loss , it requires fitting the large batch into GPU ( accelerator ) memory .", "Comments": [], "label": []}
{"id": 158513, "text": "The various types of Multi-DYLE are defined as follows : Some variants of DYLE using XROG o are denoted as follows : Interestingly , by performing inference only at sentence-level utterances without any finetuning , DYLE ( XROG o ) achieves a ROUGE-1 of 35.41 , with an increase of approximately 1 in ROUGE-1 over the original turn-level DYLE .", "Comments": [], "label": []}
{"id": 159788, "text": "Here we push the limits of this approach , using larger language models ( GPT-3 and Flan-T5 large ) than considered in prior work and evaluating their performance on standard RE tasks under varying levels of supervision .", "Comments": [], "label": []}
{"id": 158502, "text": "We use all graph snapshots , including the year 2021 , for training to mine potential connections that may appear in the future .", "Comments": [], "label": []}
{"id": 152864, "text": "For the QMSum dataset , we used λ * * = 1 , λ * * = 1 , λ * * = 1 .", "Comments": [], "label": []}
{"id": 159383, "text": "When training ALBEF [ Li et al. , , 2021 ] , we directly remove false negatives samples in a heuristic way from a mini-batch ( more details in Section [ 4.3 ] , achieving a new pre-trained model ALBEF++ .", "Comments": [], "label": []}
{"id": 156505, "text": "We use the test set of SAFECONV for evaluation , in which the human annotation of unsafe spans provides a reference .", "Comments": [], "label": []}
{"id": 155491, "text": "We use the Adam optimizer to fine-tune the model for 30 epochs and perform early stopping on the development set .", "Comments": [], "label": []}
{"id": 153650, "text": "Then , we employ a joint pre-training loss as in [ Zhang et al. , 2021d ] .", "Comments": [], "label": []}
{"id": 160149, "text": "Table [ 4 ] in Appendix [ A ] lists the statistics of all datasets . 5.2 Experimental Setups Pre-processing For * speech * input , we use the raw 16-bit 16kHz mono-channel audio wave .", "Comments": [], "label": []}
{"id": 156603, "text": "For SINGULARITY-temporal , we train it with a similar setup , except that we set training frames to be 4 .", "Comments": [], "label": []}
{"id": 158639, "text": "Here we use BART [ Lewis et al. , , 2019 ] as the fMRI-Text decoder D and cross-entropy loss ( CE ) like most seq2seq tasks as the training target .", "Comments": [], "label": []}
{"id": 156887, "text": "4.4 Implementation details We choose T5-Base [ Raffel et al. , , 2020 ] as the backbone of the answering model .", "Comments": [], "label": []}
{"id": 159443, "text": "We also do not add extra parameters to the layer , meaning we have the same inference time complexity as regular Transformer layers .", "Comments": [], "label": []}
{"id": 152013, "text": "These observations demonstrate that G-Transformer converges faster and better .", "Comments": [], "label": []}
{"id": 157830, "text": "Firstly , following [ Do et al. , 2022 ] , we train a T5 model on CoQA [ Reddy et al. , 2019 ] as our CQA model to answer the generated questions .", "Comments": [], "label": []}
{"id": 154402, "text": "We use beam search to generate summaries , with beam size 6 and length penalty 1 .", "Comments": [], "label": []}
{"id": 159894, "text": "In Appendix [ C.2 , ] we discuss ablations such as untied HyperMixer . 4.4 Time per Example In order to assess the efficiency of our model , we measure the wallclock-time of processing a single input ( repeated 1,000 times ) through the token mixing stages of HyperMixer and Transformer , respectively .", "Comments": [], "label": []}
{"id": 152110, "text": "Then GIT constructs a heterogeneous graph interaction network with mention nodes and sentence nodes , which captures the global interactions among them based on GCNs .", "Comments": [], "label": []}
{"id": 152299, "text": "NYT Following previous work [ Zhang et al. , , 2019b ] [ Xu and Durrett , 2019 ] , we use 137,778 , 17,222 and 17,223 samples for training , validation , and test , respectively .", "Comments": [], "label": []}
{"id": 159875, "text": "Furthermore , we reiterate the arguments of [ Henderson , 2020 ] for inductive biases in language and apply them to our model design . [ Henderson , 2020 ] attributes the Transformer 's success to two concepts : * variable binding * and * systematicity * .", "Comments": [], "label": []}
{"id": 152492, "text": "The recently proposed BLINK [ Logeswaran et al. , , 2019 ] [ Wu et al. , , 2020 ] uses powerful transformer-based encoder architectures trained on massive amounts of data ( such as Wikipedia , Wikia ) to achieve SotA performance on entity disambiguation tasks , and is shown to be especially effective in zero-shot settings .", "Comments": [], "label": []}
{"id": 158231, "text": "We use a beam size of 5 .", "Comments": [], "label": []}
{"id": 157238, "text": "For the IG steps m described in Section [ 3.1 , ] we adopt m = 1 in the main results due to the huge computational overhead .", "Comments": [], "label": []}
{"id": 155431, "text": "We used the same versions of the data as [ Shwartz et al. , 2020 ] to allow for a direct comparison — COPA [ Gordon et al. , , 2012 ] , CommonsenseQA [ Talmor et al. , , 2019 ] , MCTACO [ Zhou et al. , 2019 ] , SocialIQA [ Sap et al. , , 2019 ] , PIQA [ Bisk et al. , , 2020 ] , WinoGrande [ Sakaguchi et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 159618, "text": "For example , we could not comprehensively evaluate the effects of ( i ) the pre-training corpora , as we did not re-train a BERT model for Ancient Greek , to pin down the exact difference between prior BERT models ( which were trained on smaller data before ) and our own models , which are based on inherently stronger model types ; similarly , we did not induce Latin ROBERTA and T5 models , to confirm the differences between mono- and multilingual models for language-specific Latin tasks .", "Comments": [], "label": []}
{"id": 153563, "text": "We use a batch size of 32 and run 20 epochs for each setting .", "Comments": [], "label": []}
{"id": 153141, "text": "For each of the 4 models , we report the model size and latency measured on 4 different CPUs in Table [ 7 . ]", "Comments": [], "label": []}
{"id": 156160, "text": "We use these datasets to quantify the robustness of FAIRR and compare it with baselines .", "Comments": [], "label": []}
{"id": 158212, "text": "General and Specific Statements Collection In practice , we use all statements a political actor has posted to get his general representation , characterizing the broad political leaning .", "Comments": [], "label": []}
{"id": 157927, "text": "* * 3.4 Data Cleaning * * To remove noise , we use chunk-level and corpuslevel filters .", "Comments": [], "label": []}
{"id": 158246, "text": "1 Introduction In recent years there have been dramatic improvements in Machine Translation ( MT ) [ Edunov et al. , , 2018 ] [ Liu et al. , , 2020 ] thanks to the transition to neural models and the advent of the Transformer architecture [ Vaswani et al. , , 2017 ] .", "Comments": [], "label": []}
{"id": 152048, "text": "We use the same settings as Transformer to train G-Transformer , using label-smoothing of 0.1 , dropout of 0.3 , Adam optimizer , and a learning rate of 5e − 4 with 4000 warmup steps .", "Comments": [], "label": []}
{"id": 155613, "text": "Our implementation of scheduled sampling for transformer is parallel as in [ Mihaylova and Mar ] [ tins , 2019 ] and [ Duckworth et al. , 2019 ] .", "Comments": [], "label": []}
{"id": 153131, "text": "We use 4 windows of data as input to predict the embeddings of the next 3 windows , each window spanning 10 frames , which we empirically found to be the best setting .", "Comments": [], "label": []}
{"id": 155630, "text": "We conduct experiments with different neural language models including LSTM [ Hochreiter and ] [ Schmidhuber , 1997 ] , Transformer [ Vaswani et al. , , 2017 ] , and GPT-2 [ Radford et al. , , 2019 ] across different tasks in conditional text generation , unconditional text generation , and language modeling .", "Comments": [], "label": []}
{"id": 152562, "text": "We use the same ASR model described in Section [ 2.2.2 ] and a language model ( Section [ 2.2.4 ] to decode the segmented target audio .", "Comments": [], "label": []}
{"id": 156057, "text": "We use ChineseGPT [ Zhang et al. , , 2021 ] as the language model , so `` word '' here means the subword token defined in the vocabulary of ChineseGPT . wher ◦ means applying a new attacking algorithm A to the output of the last step .", "Comments": [], "label": []}
{"id": 155979, "text": "Based on this observation , FlipDA first generates data using word substitution based on a pretrained T5 [ Raffel et al. , , 2020 ] and uses a classifier to select label-flipped The authors have contributed equally to this work .", "Comments": [], "label": []}
{"id": 154059, "text": "We also observe that while the fluency of the model outputs is comparable to the gold summary , their factual correctness and coherence were significantly worse , especially for the OCR-T5 model . 5.3 Error Analysis and Challenges We manually analyzed 200 random samples from Statista and Pew .", "Comments": [], "label": []}
{"id": 158939, "text": "2 Related Work Unsupervised Sentence Representation Learning Early works typically augment the idea of word2vec [ Mikolov et al. , , 2013 ] to learn sentence representations , including Skip-Thought [ Kiros et al. , 2015 ] , FastSent [ Hill et al. , , 2016 ] and Quick-Thought [ Logeswaran and Lee , 2018 ] .", "Comments": [], "label": []}
{"id": 156872, "text": "Table 11 : Results of different mask combining methods based on T5 multi-task learning on GLUE .", "Comments": [], "label": []}
{"id": 154734, "text": "However , while humans process information sequentially , updating their memories continuously , and recurrent neural networks ( RNNs ) update a single memory vector during time , transformers do not – they exhaustively query every representation associated to the past events .", "Comments": [], "label": []}
{"id": 153436, "text": "At last , we use the role attention result and multiple context attention results to predict the word probability distribution of the summary .", "Comments": [], "label": []}
{"id": 151571, "text": "For all models relying on VLP as their underlying infrastructure , we use 30 training epochs , 0.1 warmup proportion , 0.01 weight decay , 64 batch size .", "Comments": [], "label": []}
{"id": 155938, "text": "Blockwise sparse attention improved the vanilla Transformer 's complexity by limiting the number of tokens each attends to from n ( input length ) to m ( block size ) as seen in Table [ 1 . ] As we keep the encoding of blockwise attention , the m improvement also applies to our self-attention .", "Comments": [], "label": []}
{"id": 159731, "text": "UniMSE [ Hu et al. , , 2022b ] unifies multimodal sentiment analysis and ERC tasks with a unified framework based on T5 [ Raffel et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 156766, "text": "[ https : //github.com/explosion/spacy-models/ ] ( https : //github.com/explosion/spacy-models/releases/tag/en_core_web_lg-3.5.0 ) [ releases/tag/en_core_web_lg-3.5.0 ] ( https : //github.com/explosion/spacy-models/releases/tag/en_core_web_lg-3.5.0 ) COGS Since the extracted lexicon is large for semantic parsing , we present only some of the equivalance relations that we extracted from COGS training data for reference .", "Comments": [], "label": []}
{"id": 153699, "text": "Furthermore , while transformer models have contributed to much of the recent algorithmic progress in NLP research and are the most powerful computational models available to us , work has highlighted shortcomings in their performance on domain-specific text [ Moradi et al. , 2021 ] and noted that these models can easily detect their own machine-generated misinformation [ Zellers et al. , , 2019 ] .", "Comments": [], "label": []}
{"id": 153421, "text": "3.1 Offline Stage Multiple Code Hashing Design with Code Classification Module Since the capacity of binary hashing space is very limited compared to Euclidean space , the Hamming distance between similar code snippets will be too small to be distinguishable if we adopt a single Hashing model .", "Comments": [], "label": []}
{"id": 153834, "text": "We used the same names as used in the paper ( as models * rel2-eb-c * and * small-2 * both refer to the same model , we included the row twice , with both names , for clarity ) .", "Comments": [], "label": []}
{"id": 157962, "text": "For example , swearing is often considered a norm violation [ Jay ] [ and Janschewitz , 2008 ] , but can also be viewed as a signal of solidarity between close friends [ Mon ] [ tagu , 2001 ] or co-workers [ Baruch and Jenkins , 2007 ] .", "Comments": [], "label": []}
{"id": 156839, "text": "Next , we used different portions of the remaining 80 % data to create 12.5 % , 25 % , 50 % , 75 % , and 100 % training sets to train the attention weights of Table 8 : Our new Code+Evidence data splits based on the splits of [ Mullenbach et al. , 2018 ] for code prediction and our evidence dataset splits .", "Comments": [], "label": []}
{"id": 155480, "text": "B Pre-training Setting UniXcoder uses 12 layers of Transformer with 768 dimensional hidden states and 12 attention heads .", "Comments": [], "label": []}
{"id": 152479, "text": "Considering a lot of helpful information can be carried in soft targets instead of hard targets [ Hinton et al. , , 2015 ] , we use the soft labels of the selected pseudo-data to train a student model h T stu via knowledge distillation .", "Comments": [], "label": []}
{"id": 155341, "text": "To justify this motivation , we conduct experiments using a teacher optimized for the ResNet-32 student to statically distill to the ResNet-20 student , and also in reverse .", "Comments": [], "label": []}
{"id": 152904, "text": "In particular , as our baseline we focus on the mBART method [ Liu et al. , , 2020 ] , a popular method with two types of heuristic noise functions being used sequentially on each text segment .", "Comments": [], "label": []}
{"id": 160054, "text": "Bias can be reduced by modifying the model architecture , e.g. , making the neural networks wider and deeper as in transformer-based LPMs .", "Comments": [], "label": []}
{"id": 158521, "text": "Although Multi-DYLE ( XCES o ) shows weak performance in correctly extracting XROG o , it shows better ROUGE scores than Multi-DYLE ( XROG o ) .", "Comments": [], "label": []}
{"id": 156756, "text": "We use batch size of 512 , and learning rate 0.0003 with the Adam optimizer [ Kingma and ] [ Ba , 2015 ] .", "Comments": [], "label": []}
{"id": 151517, "text": "For this , we use the XNER task and analyze the model based on the results in Table [ 1 . ] 4.1 Analysis of distillation methods Model confidence vs. clustering We first analyze the performance of our * single-model distillation * methods ( [ §2.3 ] to see which of the two alternatives works better .", "Comments": [], "label": []}
{"id": 156712, "text": "To obtain the P-CXMI for words in the data , we train a small Transformer [ Vaswani et al. , , 2017 ] model for every target language and incorporate the target context by concatenating it with the current target sentence [ Tiedemann and Scherrer , 2017 ] .", "Comments": [], "label": []}
{"id": 158727, "text": "The joint loss in stage I can be formulated as : Stage II : Image-Context Matching In stage II , we use multi-modal dialogue data D to pre-train PaCE , which aims to model dialogue context for multi-modal dialogue tasks .", "Comments": [], "label": []}
{"id": 159097, "text": "B.3 Training Details We use the same background model and synonyms as our GRM model to train the context-free baselines .", "Comments": [], "label": []}
{"id": 159727, "text": "To achieve interactions between different modalities , we apply the Cross-Model Transformer ( CMT ) layer [ Tsai et al. , , 2019 ] .", "Comments": [], "label": []}
{"id": 158351, "text": "For training the policy model and the critic model , we use 8 GPUs with data parallel to speed up the training , with estimated 1100 GPU hours to run the training .", "Comments": [], "label": []}
{"id": 158058, "text": "Ziegler , Jeffrey Wu , Clemens Winter , Christopher Hesse , Mark Chen , Eric Sigler , Mateusz Litwin , Scott Gray , Benjamin Chess , Jack Clark , Christopher Berner , Sam McCandlish , Alec Radford , Ilya Sutskever , and Dario Amodei .", "Comments": [], "label": []}
{"id": 156326, "text": "The dataset we use has 859 sources : 452 * high * factuality , 245 * mixed * , and 162 * low * , and was released publicly by [ Baly et al. , , 2020b ] [ 4 ] .", "Comments": [], "label": []}
{"id": 159343, "text": "Figure 4 : Intra-group homogeneity ( upper ) and intergroup diversity ( lower ) of GHT and vanilla transformer by training steps .", "Comments": [], "label": []}
{"id": 155825, "text": "We use the classification accuracy ( % ) for evaluation . in Figure [ 1 ] ( c ) and ( d ) , the verbalizer maps the label `` Positive '' to `` great '' .", "Comments": [], "label": []}
{"id": 156725, "text": "Results are reported with respect to standard MT metrics BLEU [ Papineni et al. , , 2002 ] and COMET [ Rei et al. , , 2020 ] , as well as the MuDA benchmark .", "Comments": [], "label": []}
{"id": 153762, "text": "For each of the 6 Transformer encoder layers , we choose the number of self-attention heads as 12 and hidden size of 768 .", "Comments": [], "label": []}
{"id": 155683, "text": "We then train the LM with two joint objectives : We use masked language modeling ( MLM ) to encourage learning multi-hop knowledge of concepts brought into the same context by document links ( e.g . `` Tidal Basin '' and `` Japanese cherry '' in Figure [ 1 ] .", "Comments": [], "label": []}
{"id": 158227, "text": "If we also set the amateur temperature to be very small , then it approaches the canonical heuristic of forbidding repeated n-grams [ Paulus et al. , , 2018 ] .", "Comments": [], "label": []}
{"id": 154946, "text": "During pre-training , these transformer models project representations of different languages †Corresponding authors Table 1 : Monolingual Bias for Different Languages .", "Comments": [], "label": []}
{"id": 153083, "text": "For each test language , we bold its in-distribution ( e.g. , en → en ) , and underline the best out-of-distribution ( e.g. , ru → en ) numbers . 4.3 Generalization Experiments As our PLMs , we use XLM-R [ Conneau et al. , , 2020 ] for cross-lingual and BERT for cross-dataset experiments .", "Comments": [], "label": []}
{"id": 159883, "text": "H3 ( Section [ 4.7 ] : Due to its inductive biases mirroring those of Transformers , HyperMixer also learns similar patterns as the attention mechanism . 4.1 Datasets We evaluate on four sentence-pair classification tasks and one single-sentence classification task .", "Comments": [], "label": []}
{"id": 159002, "text": "We use ATINTER which was trained to remove adversarial perturbations for the BERT classifier on the SST-2 dataset and employ it , without retraining , to remove adversarial perturbations for other classifiers on the same dataset .", "Comments": [], "label": []}
{"id": 153113, "text": "We release all 28 trained models along with scripts for efficient deployment which demonstrably achieve real-time performance on CPUs and GPUs . 3 .", "Comments": [], "label": []}
{"id": 151597, "text": "As more transformer layers are stacked with larger self-attention blocks , model complexity increases rapidly .", "Comments": [], "label": []}
{"id": 156848, "text": "LoRA [ Hu et al. , , 2022 ] injected tunable rank decomposition matrices into each Transformer layer .", "Comments": [], "label": []}
{"id": 154211, "text": "For bert , we used grid-search to select the optimal learning rate from { 1 * e * -5 , * e * -5 , * e * -5 , * e * -5 , * e * -5 } , fine-tuning for 10 epochs , using early stopping with patience 2 .", "Comments": [], "label": []}
{"id": 160028, "text": "We share the embedding layer and transformer blocks between two modules and train the whole model in an end-to-end manner .", "Comments": [], "label": []}
{"id": 155675, "text": "We use gray background to show the results of WLS baselines fine-tuned on the clean data .", "Comments": [], "label": []}
{"id": 155813, "text": "Therefore , we use the average gains from the knowledge distillation to reflect the ability of the NAT models to overcome this issue .", "Comments": [], "label": []}
{"id": 155799, "text": "We use the embedding as repr ( yi ) , refer to [ Bao et al. , 2021 ] .", "Comments": [], "label": []}
{"id": 157114, "text": "Ethics Statement The IU X-RAY [ Demner-Fushman et al. , , 2016 ] and MIMIC-CXR [ Johnson et al. , , 2019 ] datasets have been automatically de-identified to protect patient privacy .", "Comments": [], "label": []}
{"id": 156616, "text": "The scores are computed by the same model that produced the translations ( Section [ 2.1 ] . 3.3 External models Baseline : COMET-QE .", "Comments": [], "label": []}
{"id": 156589, "text": "In Table [ 4 ] and Table [ 5 ] we show results of SINGULARITY-temporal on existing textto-video retrieval and video question answering datasets .", "Comments": [], "label": []}
{"id": 152607, "text": "We used mixedprecision training [ Micikevicius et al. , , 2018 ] to expedite the training procedure .", "Comments": [], "label": []}
{"id": 151552, "text": "We will then * topologically sort * this graph ; each region is then given an embedding corresponding to its sorted position – similar to the position embedding in a Transformer .", "Comments": [], "label": []}
{"id": 159007, "text": "For modeling , our method ATINTER uses another neural network based language model T5 [ Raffel et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 159386, "text": "Therefore , in the beginning , we use the highquality human-annotated dataset [ Chen et al. , , 2015 ] to train another model denoted as H which shares the same structure with our VLP model Sγ .", "Comments": [], "label": []}
{"id": 154820, "text": "Dependency tree probing is also done on both Lorentz and Euclidean Transformers to compare their capabilities of representing structured information .", "Comments": [], "label": []}
{"id": 159149, "text": "It also shows higher correlations than COMET-MQM across all languages .", "Comments": [], "label": []}
{"id": 156116, "text": "We use different ALBERT models in the two experiments to allow a fair comparison with existing benchmarks .", "Comments": [], "label": []}
{"id": 156798, "text": "B.3 OFA We use validation-set early stopping on crossentropy loss , and fine-tune OFA separately for each cross-validation split .", "Comments": [], "label": []}
{"id": 153607, "text": "The standard adapter layer can be formalized as : where Wdown , Wup , bdown and bup are model parameters , which are much smaller than the parameters of transformer in scale , and the dimension size of hmid is also smaller than that of the corresponding transformer dimension .", "Comments": [], "label": []}
{"id": 160300, "text": "To train our multilingual AMRSim , we employed the multilingual version of BERT base model and added a Chinese AMR dataset containing about 2,500 Chinese AMR graphs from [ Li et al. , 2016 ] to English training data in Section [ 4.1 . ]", "Comments": [], "label": []}
{"id": 151724, "text": "HIF-EMB replaces the outputs of HIF with the mean pooling of word embeddings . 4.1.3 Evaluation Metrics We use F score as the evaluation metric .", "Comments": [], "label": []}
{"id": 155984, "text": "We find it beneficial to remove the sample if T5 does not predict y given x 0 .", "Comments": [], "label": []}
{"id": 154493, "text": "A.3 Training Process For training our framework , we adopt the separatetraining strategy to avoid out-of-memory issue : we first train the DPR model following its original paper , then freeze the DPR model to train the stage-1 reranking module , and finally jointly train stage-2 reranking and reader part .", "Comments": [], "label": []}
{"id": 156402, "text": "Table 13 : COMET and chrF scores in the German-to-English domain adaptation .", "Comments": [], "label": []}
{"id": 158173, "text": "We use 10 random seeds for prompted generation , filter low-confidence responses using the stance detector , and average the stance detection scores for a more reliable evaluation .", "Comments": [], "label": []}
{"id": 152350, "text": "For some attributes ( 'Ambience ' , 'BusinessParking ' , 'GoodForMeal ' ) that have subordinate attributes , we used each subordinate attribute .", "Comments": [], "label": []}
{"id": 156051, "text": "For an example , each pixel of an image ( CIFAR-10 ) or a character in the byte-level text classification represents a token in the input for the Transformer .", "Comments": [], "label": []}
{"id": 152462, "text": "Denote the word sequence of a lyric https : //github.com/deezer/spleeter https : //github.com/bootphon/phonemizer https : //github.com/MontrealCorpusTools/Montreal-Forced-Aligner 4 https : //github.com/librosa/librosa sentence as W = { w1 , w2 , · · · , w|W| } , and its beat sequence as B = { b1 , b2 , · · · , b|B| } , where w and b represent i-th word and j-th beat .", "Comments": [], "label": []}
{"id": 152701, "text": "All datasets are tokenized with the GPT-2 tokenizer [ Radford et al. , , 2019 ] , which is based on UTF-8 BPE [ Sennrich et al. , , 2016 ] .", "Comments": [], "label": []}
{"id": 160319, "text": "The total environmental cost , according to the Jean Zay supercomputer documentation [ 4 ] is equivalent to 6,604,500 Wh or 376.45 kg CO2eq based on the carbon intensity of the energy grid mention by BLOOM environmental cost study also made on Jean Zay [ Luccioni et al. , , 2022 ] .", "Comments": [], "label": []}
{"id": 154162, "text": "Table 15 : Generated texts on the Wikitext-103 test set and uniq tokens for each texts . 50 bpe tokens are given as prefix and the models are to generate the continuation of 100 next bpe tokens .", "Comments": [], "label": []}
{"id": 160034, "text": "Like the connective generation module , we feed X¯ into an Embedding layer and L stacked Transformer blocks .", "Comments": [], "label": []}
{"id": 155671, "text": "For the initial weak supervision sources , we use the labeling rules provided by existing works : [ Zhou et al. , 2020 ] for TACRED , [ Meng et al. , 2020 ] for DBPedia , and [ Zhang et al. , 2021 ] for Chemprot and AG News .", "Comments": [], "label": []}
{"id": 158343, "text": "We use a concrete theorem as an example to demonstrate how DT-Solver ( left ) differs from an approach using best-first search ( right ) .", "Comments": [], "label": []}
{"id": 152308, "text": "Therefore , we only train a large version DifferSum for a fair comparison . 5.2 Results on NYT Results on NYT are summarized in Table [ 3 . ] Note that we use limited-length ROUGE recall as [ Dur ] [ rett et al. , 2016 ] , where the selected sentences are truncated to the length of the human-written summaries .", "Comments": [], "label": []}
{"id": 154181, "text": "In * Transformer-Big * , we use 1024 as hidden size , 4096 as filter size , and 16 heads in multihead attention .", "Comments": [], "label": []}
{"id": 151992, "text": "Compared to this strong baseline , our randomly initialized model of G-Transformer improves the s-BLEU by 0.81 point on the large dataset Europarl .", "Comments": [], "label": []}
{"id": 154486, "text": "For the GNN reranking models , we adopt 3-layer Graph Attention Networks ( GAT ) [ Velickovic et al. , , 2018 ] .", "Comments": [], "label": []}
{"id": 159401, "text": "2.2 Evaluation Metric for Explanations Many commonly used evaluation metrics for textbased content like BLEU [ Papineni et al. , , 2002 ] and ROUGE [ Lin , 2004 ] treat human-annotated answers as the absolute gold standard without questioning or attempting to evaluate their quality .", "Comments": [], "label": []}
{"id": 157031, "text": "`` Base '' and `` Big '' denote Transformer-based and Transformer-big models , respectively .", "Comments": [], "label": []}
{"id": 157465, "text": "The backbone for these studies is a 4 layer 1-attention-head GCN encoder followed by a 4-layer 8-attention-head ETC transformers decoder with 512 hidden units .", "Comments": [], "label": []}
{"id": 158548, "text": "After an intra-modal transformer to capture the global temporal dependencies of unimodal features , we apply a cross-modal locality-constrained transformer to capture the local contextual information focusing on correspondences between different modalities .", "Comments": [], "label": []}
{"id": 160251, "text": "In order to do so , we utilize embedding-based similarities between each predicted token and the detected attribute string .", "Comments": [], "label": []}
{"id": 155601, "text": "Table 2 : SacreBLEU for different models on WMT16 En-Ro and WMT17 Zh-En tasks .", "Comments": [], "label": []}
{"id": 157061, "text": "To score retrieval units , we will use an early interaction ( cross-attention ) Transformer network , to which we will feed the question and a retrieval unit , suitably encoded into text .", "Comments": [], "label": []}
{"id": 153560, "text": "With practicality in mind , we dive deep into the semantics of quantity events and propose a meta-framework for spatiotemporal quantity extraction : we formulate the problem as four information extraction tasks which lead to quick and reliable data annotation via crowdsourcing ; we also build a T5 baseline to study the difficulties of the task and discuss transfer learning opportunities .", "Comments": [], "label": []}
{"id": 159084, "text": "We choose the Word2Vec model mentioned before as the static pre-trained embedding models .", "Comments": [], "label": []}
{"id": 159799, "text": "CoNLL As an instructional prefix for CoNLL , we use : * List the entities of the types [ LOCATION , ORGANIZATION , PERSON ] and relations of types [ Organization Based In , Work For , Located In , Live In , Kill ] among the entities in the given text * .", "Comments": [], "label": []}
{"id": 160346, "text": "For the training efficiency of extensive S2ST experiments , we use a subset of mined data as the train set .", "Comments": [], "label": []}
{"id": 152346, "text": "In particular , our model achieved state-of-the-art results on the Amazon dataset and outperformed the comparable model by a large margin in the R-L representing the ROUGE scores on the Yelp dataset .", "Comments": [], "label": []}
{"id": 152019, "text": "These observations imply that the potential conflict of local sentence and document context can be mitigated by G-Transformer . 5.4 Discussion of G-Transformer Document Context .", "Comments": [], "label": []}
{"id": 157334, "text": "We use the News20 and Social21 from two SemEval shared tasks [ Da San Martino et al. , , 2020 ] [ Dimitrov et al. , , 2021 ] and follow the official data split strategy .", "Comments": [], "label": []}
{"id": 154790, "text": "We set τ = 0.5 and for the KL regularization we used λKL = 1 × 10− and σ = 0.1 .", "Comments": [], "label": []}
{"id": 152198, "text": "After the initial focus on understanding pre-trained LSTM-based LMs [ Peters et al. , , 2018b ] , attention has now shifted toward transformer-based models .", "Comments": [], "label": []}
{"id": 159500, "text": "In contrast , baselines such as Style Transformer have better content preservation but hardly transfer the style .", "Comments": [], "label": []}
{"id": 157511, "text": "The results indicate that there are two problems intervening : Transformers are not good at handling repeating symbols and OOD generalization .", "Comments": [], "label": []}
{"id": 154999, "text": "N } in the Transformer network .", "Comments": [], "label": []}
{"id": 159153, "text": "Specifically , we use the facebook/mbart-large-50-many -to-many-mmt model .", "Comments": [], "label": []}
{"id": 158899, "text": "This can be because traditional machine learning models , such as XGBoost , have fewer parameters than deep learning models and are therefore less affected by Table 3 : Performance on the trading simulation . '+Fin-Trust ' represents the performance using the input after the transformation . artefacts .", "Comments": [], "label": []}
{"id": 152400, "text": "The total number of parameters for Adapters for L Transformer layers in both an encoder and a decoder across T tasks is , therefore , 4TL ( 2hd+ 2h ) , which scales linearly with the number of tasks times the number of layers .", "Comments": [], "label": []}
{"id": 153628, "text": "Here we use random spans as surrogates of passages and enforce the distributional hypothesis through NCE , as word embedding learning in Word2Vec [ Mikolov et al. , , 2013 ] .", "Comments": [], "label": []}
{"id": 155899, "text": "We use the dot product as the similarity function .", "Comments": [], "label": []}
{"id": 152941, "text": "Surprisingly , we find that BPE models – in spite of their higher BLEU scores – accumulate more translation errors than their CHAR counterparts .", "Comments": [], "label": []}
{"id": 152747, "text": "For PREFIXTOGRAPH , we use an explicit MASK token for unrevealed future tokens , and force the model to copy MASK to the predicted program instead of freely generating text .", "Comments": [], "label": []}
{"id": 159255, "text": "Specifically , we use the Pearson correlation coefficient ( PCC ) for evaluation .", "Comments": [], "label": []}
{"id": 157794, "text": "We adopt ontology-based negative sampling to train classifiers for FB15k-237 , YAGO3-10 , and ogbl-wikikg2 , and embeddingbased negative sampling for WN18RR .", "Comments": [], "label": []}
{"id": 158724, "text": "Formally , the unique information , which is obtained by switching experts in each block , can be formulated as : where Hl− ( l ∈ [ 1 , L ] ) represents the output representation of the l-1 layer and L is the number of Transformer blocks .", "Comments": [], "label": []}
{"id": 152986, "text": "3 Background In this section , we briefly review the structure of XLM-R [ Conneau et al. , , 2020 ] , a Transformer encoder [ Vaswani et al. , , 2017 ] pre-trained by masked language modeling task [ Devlin et al. , , 2019 ] .", "Comments": [], "label": []}
{"id": 157445, "text": "Generated Question with Knowledge Transferred from MS MARCO PEGASUS : how many drunk drivers in nyc BART : drinking drivers in nyc T5 : how many drunk drivers in larchmont nyc SPARTA ( PEGASUS ) : how bad is traffic in larchmont ny on new years eve SPARTA ( BART ) : would traffic be bad at larchmont nyc SPARTA ( T5 ) : would traffic be worse in larchmont nyc on Generated Question with Knowledge Transferred from NewsQA PEGASUS : what should you be cautious about ?", "Comments": [], "label": []}
{"id": 155887, "text": "It might be because that `` ` Wikipedia : Copyrights `` ` < https : //www.gao.gov/ > < https : //crsreports.congress.gov/ > [ https : //creativecommons.org/licenses/ ] ( https : //creativecommons.org/licenses/by/4.0/ ) [ by/4.0/ ] ( https : //creativecommons.org/licenses/by/4.0/ ) [ https : //en.wikipedia.org/wiki/ ] ( https : //en.wikipedia.org/wiki/Wikipedia : WikiProject_Biography ) [ Wikipedia : WikiProject_Biography ] ( https : //en.wikipedia.org/wiki/Wikipedia : WikiProject_Biography ) < https : //scrapy.org > [ https : //github.com/attardi/ ] ( https : //github.com/attardi/wikiextractor ) [ wikiextractor ] ( https : //github.com/attardi/wikiextractor ) .", "Comments": [], "label": []}
{"id": 157284, "text": "To reduce the computational cost , we use two pretraining random seeds and four fine-tuning random seeds in our ablation study in [ Table 2 . ]", "Comments": [], "label": []}
{"id": 152851, "text": "First , we compute the * extractive oracle * based on the reference summary with a greedy search over the best ROUGE scores .", "Comments": [], "label": []}
{"id": 152269, "text": "( 1 ) * Heuristics * uses the labeling heuristic function with both Regex and dialog act to predict the test set .", "Comments": [], "label": []}
{"id": 159767, "text": "Experimental results show the stability of NOTABLE and it reveals that backdoor effects suggest shortcut attentions in the transformer-based encoders .", "Comments": [], "label": []}
{"id": 159903, "text": "Results In Figure [ 4 , ] we show the * relative * expected validation performance , i.e. , the relative performance change of HyperMixer compared to Transformer , for all five datasets .", "Comments": [], "label": []}
{"id": 159188, "text": "RG denotes ROUGE .", "Comments": [], "label": []}
{"id": 152580, "text": "The CMU Wilderness dataset [ Black , 2019 ] collects readings from the New Testament , with 700 different languages avail- https : //www.caito.de/2019/01/the-m-ailabs-speechdataset http : //www.voxforge.org https : //gitlab.com/Jaco-Assistant/deepspeech-polyglot able .", "Comments": [], "label": []}
{"id": 155150, "text": "We use learned positional embeddings [ Ghazvininejad et al. , , 2019 ] and set the max-positions to 10,000 .", "Comments": [], "label": []}
{"id": 155619, "text": "To this end , we use a fixed-size queue Queue , and its size nque as a hyperparameter , as shown in Alg . 2 . 3.3 Calibration of Class Distribution In this section , we describe the calibration of the predicted class distribution .", "Comments": [], "label": []}
{"id": 154123, "text": "Instead of directly using the hidden state of the first token , we use mean pooling followed by L normalization to get the relation-aware embedding ehr , as mean pooling has been shown to result in better sentence embeddings [ Gao et al. , , 2021 ] [ Reimers ] [ and Gurevych , 2019 ] .", "Comments": [], "label": []}
{"id": 160091, "text": "Inspired by this finding , we use MLM loss when training RankEncoder .", "Comments": [], "label": []}
{"id": 159973, "text": "We also show the results on Fisher in Appendix [ I . ] We removed three long utterances from the mTEDx dev set to fit the GPU memory .", "Comments": [], "label": []}
{"id": 155296, "text": "We use three corruption procedure for generating incorrectly marked slots , namely , invert , randomize and switch .", "Comments": [], "label": []}
{"id": 153743, "text": "A.3 Human Evaluation Guidelines [ Table 11 ] and [ 12 ] shows the detailed guidelines we use for human evaluation of generated knowledge .", "Comments": [], "label": []}
{"id": 156406, "text": "The probability distribution of language model PLM is used to guarantee fluency , while the toxicity distribution is used to avoid toxicity . 4 Experiments We use GPT-2 medium as our base pre-trained LM .", "Comments": [], "label": []}
{"id": 160422, "text": "Following its ] ( https : //huggingface.co/T-Systems-onsite/cross-en-de-roberta-sentence-transformer ) [ outperforming results in the sentence transformers ] ( https : //huggingface.co/T-Systems-onsite/cross-en-de-roberta-sentence-transformer ) [ experiment , we used the LaBSE sentence trans ] ( https : //huggingface.co/T-Systems-onsite/cross-en-de-roberta-sentence-transformer ) [ former model in this experiment .", "Comments": [], "label": []}
{"id": 156207, "text": "( 2 ) A large proportion of table headers are not properly detected ( mentioned in its paper ) , while we adopt ranges and headers detected by TableSense [ Dong ] * et al .", "Comments": [], "label": []}
{"id": 156465, "text": "Model We employ a Transformer-based encoderdecoder language model * i.e . * , T5 [ Raffel et al. , , 2020 ] , as the model architecture for UIE .", "Comments": [], "label": []}
{"id": 154431, "text": "Recently , various new pre-trained language models have been pre-trained including BART [ Lewis et al. , , 2020 ] , ProphetNet [ Qi et al. , , 2020 ] , T5 [ Raffel et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 153413, "text": "The experiments reported in the paper were made possible by a Tensor Flow Research Cloud ( TFRC ) TPU grant .", "Comments": [], "label": []}
{"id": 152858, "text": "For Transformers [ Vaswani et al. , , 2017 ] and encoderdecoder with attention models [ Bahdanau et al. , , 2015 ] , h x t is usually the model 's output before the final language model head .", "Comments": [], "label": []}
{"id": 158409, "text": "We use embeddings of the beginning-of-sentence ( BOS ) token as the sentence representation , and compare this to alternate approaches in Appendix [ C. ] Following [ Zhou et al. , 2021 ] , we fine-tune RoBERTa-base on downstream datasets for 10 epochs .", "Comments": [], "label": []}
{"id": 160195, "text": "We adopt an extractive reader model based on ELEC-TRA Large [ Clark et al. , , 2020 ] with two heads to predict the start and end of the answer span .", "Comments": [], "label": []}
{"id": 154057, "text": "Table [ 5 ] shows that the TAB-T5 performed significantly better than OCR-T5 based on all three criteria , especially on factual correctness .", "Comments": [], "label": []}
{"id": 159992, "text": "For multilingual experiments on CVSS-C , we use mBART50 [ Tang et al. , , 2020 ] with multilingual fine-tuning to En .", "Comments": [], "label": []}
{"id": 155786, "text": "Non-autoregressive Transformer .", "Comments": [], "label": []}
{"id": 159146, "text": "Overall , we observe that fine-tuning the COMET metric on our MQM dataset not only Table 4 : Kendall-tau ( τ ) correlations for the zero-shot performance of Indic-COMETMQM .", "Comments": [], "label": []}
{"id": 160406, "text": "A human evaluation is required in order to obtain a more reliable assessment of For our experiments , we use all default parameters as reported on GitHub [ https : //github.com/a-rios/ ] ( https : //github.com/a-rios/ats-models ) [ ats-models ] ( https : //github.com/a-rios/ats-models ) .", "Comments": [], "label": []}
{"id": 156995, "text": "For each dataset , we use fixed hyperparameters without early stopping and report all performances on dialect variants of the * evaluation * data , since public test sets are not available for the original datasets .", "Comments": [], "label": []}
{"id": 157886, "text": "When generating each synthetic dialogue , SynDG ( a ) constructs a dialogue flow by heuristic sampling from unstructured knowledge data ; ( b ) incrementally realizes every knowledge piece in the flow as an utterance by a fine-tuned T5 model , producing a synthetic dialogue ; ( c ) scores the synthetic dialogue both at the flow-level and utterance-level with two T5-based scorer .", "Comments": [], "label": []}
{"id": 153868, "text": "PrefixTuning [ Li and Liang , 2021 ] prepends key-value pairs to each transformer layer as activations .", "Comments": [], "label": []}
{"id": 159310, "text": "The masking strategy and MLM task head we use are the same as RoBERTa .", "Comments": [], "label": []}
{"id": 154802, "text": "We used this technique to compare the generalizations of abusive language classifiers , trained with pre-pandemic data , to explicit and implicit COVID-related anti-Asian racism .", "Comments": [], "label": []}
{"id": 154355, "text": "[ Fan et al. , 2019 ] randomly dropped Transformer layers to sample small sub-networks from the larger model during training which are selected as the inference models .", "Comments": [], "label": []}
{"id": 160337, "text": "We used k = 4 .", "Comments": [], "label": []}
{"id": 152604, "text": "We used the AdamW [ Loshchilov and Hutter , 2017 ] optimizer with the default values .", "Comments": [], "label": []}
{"id": 156502, "text": "Toxicity Mitigation To detect unsafe contents , transformer-based classifiers [ Devlin et al. , , 2019 ] [ Liu et al. , , 2019 ] are the predominant methods due to their strong representation power , upon which some datasets [ Davidson et al. , , 2017 ] [ Hartvigsen et al. , 2022 ] can be leveraged to train decent and powerful toxicity detectors .", "Comments": [], "label": []}
{"id": 154899, "text": "For task ( iv ) the NER task , we use the morphologically-annotated data files of the aforementioned SPMRL-based NEMO corpus [ Bareket ] [ and Tsarfaty , 2020 ] .", "Comments": [], "label": []}
{"id": 156490, "text": "For words in the sentence , also as the nodes in the AMR , we utilize BERT as an encoder to get contextual embeddings H = { h1 , h2 , ... , hn } like lots of previous works .", "Comments": [], "label": []}
{"id": 157442, "text": "SPARTA ( T5 ) : when did she go to college ?", "Comments": [], "label": []}
{"id": 156449, "text": "Our implementation used for this paper is available at https : //github.com/Ryosuke-Yamaki/Hol-CCG.git .", "Comments": [], "label": []}
{"id": 154490, "text": "A Appendix A.1 Dataset The datasets we use are Natural Questions ( NQ ) and TriviaQA .", "Comments": [], "label": []}
{"id": 158773, "text": "We use the schedule strategy with 4,000 warmup steps .", "Comments": [], "label": []}
{"id": 158836, "text": "B TF-IDF based dictionary To build out TF-IDF induced dictionary , we took the following steps : The results of the Transformer could be improved using a dictionary since the lexicon in the dictionary gathered by TF-IDF could emphasize the word/phrase in contrast to the attention mechanism in which the transformer set the weights based on the pre-training .", "Comments": [], "label": []}
{"id": 157807, "text": "Results on triple classification are shown in Table [ 13 . ] We adopt TransE as the baseline KGe model and reduce it from 512 dimensions to 128 dimensions in GreenKGC .", "Comments": [], "label": []}
{"id": 151487, "text": "For each side , we select the negative triple that is ranked just below the target triple ( that Table 2 : A summary of heuristic approaches used for different steps of the adversarial attack with symmetry ( Sym ) , inversion ( Inv ) and composition ( Com ) pattern .", "Comments": [], "label": []}
{"id": 155302, "text": "Inspired by previous work [ Chalkidis et al. , 2019b ] [ Rios and Kavuluru , 2018 ] , we employ an attention mechanism to incorporate article semantics into the model .", "Comments": [], "label": []}
{"id": 152603, "text": "The teacher and the student are 110M and 66M parameters respectively with a vocabulary size of 30,522 extracted using BPE .", "Comments": [], "label": []}
{"id": 158793, "text": "Since , the TAPAS [ Herzig et al. , , 2020 ] architecture has been used for QA over tables and we do not have any questions in the composition extraction task , we use table caption as a proxy for the question .", "Comments": [], "label": []}
{"id": 160163, "text": "We use µ = 15 in our experiments .", "Comments": [], "label": []}
{"id": 151402, "text": "The framework is shown in Figure [ 3 . ] 3.3.1 Utterance Feature Extraction DAG-ERC regards each utterance as a graph node , the feature of which can be extracted by a pretrained Transformer-based language model .", "Comments": [], "label": []}
{"id": 159614, "text": "In Figure [ 3 ] we compare the PoS tagging validation accuracy of GRεTA-ENC to that of a randomly initialized T5 encoder ( same size ) .", "Comments": [], "label": []}
{"id": 151983, "text": "The complexities of both the self-attention and cross-attention in Transformer are O ( N ) .", "Comments": [], "label": []}
{"id": 155936, "text": "Table 1 : Time complexity of attention in the Transformer models .", "Comments": [], "label": []}
{"id": 159863, "text": "The template we use is `` [ LABEL_NAME ] can also be called [ MASK ] ∗n . `` , where n is the length of the candidate .", "Comments": [], "label": []}
{"id": 156497, "text": "The time consumption and GPU memory used for multiple operations are expensive .", "Comments": [], "label": []}
{"id": 154685, "text": "We address these issues by proposing a simple yet effective two-tier transformer encoder architecture .", "Comments": [], "label": []}
{"id": 154612, "text": "Retriever and Documents In this work the set of passages in the memory is not large enough to require a FAISS index , but it is large enough that retrieval may be useful .", "Comments": [], "label": []}
{"id": 153116, "text": "Transformer architectures with encoder-only models like BERT [ Vaswani et al. , , 2017 ] have also been studied for pose-based ISLR models [ De Coster et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 158989, "text": "Instead , we employ a separate model that is explicitly trained to remove adversarial perturbations .", "Comments": [], "label": []}
{"id": 154037, "text": "Recent methods have primarily used an LSTM-based encoder-decoder architecture [ Mei et al. , , 2016 ] [ Le ] [ bret et al. , , 2016 ] [ Wiseman et al. , , 2017 ] . [ Gong et al. , 2019 ] found that transformers [ Vaswani et al. , , 2017 ] yielded more fluent and coherent outputs compared to their LSTM counterparts .", "Comments": [], "label": []}
{"id": 156186, "text": "Replacing the traditional LSTM with transformers shows large increasing .", "Comments": [], "label": []}
{"id": 151941, "text": "Baselines We use the following pretrained crosslingual LMs as baselines .", "Comments": [], "label": []}
{"id": 152054, "text": "Similar as the finetuning on sentence-level Transformer , we also copy parameters from mBART25 [ Liu et al. , , 2020 ] to G-Transformer , leaving the global multi-head attention and the gates randomly initialized .", "Comments": [], "label": []}
{"id": 151665, "text": "Results for Transformer-6 ( 6 layers for encoder and decoder ) are from [ Lin et al. , 2020 ] .", "Comments": [], "label": []}
{"id": 157919, "text": "The datasets we use are publicly available and contain no personal identifiable information .", "Comments": [], "label": []}
{"id": 152983, "text": "We conjecture that this is because with the same αneg=0.1 , a T5-base recaller obtains lower recall ( 69.5/61.4 ) than a T5-3b recaller ( 72.1/63.3 ) ; a T5-base recaller may need an even lower value of αneg to obtain a higher coverage of gold answers .", "Comments": [], "label": []}
{"id": 156946, "text": "Besides , we use pre-trained models RoBERTabase and WavLM-base+ to initialize our text and speech encoder , respectively .", "Comments": [], "label": []}
{"id": 156322, "text": "We assigned labels to users using the same heuristic described in Sec . [ 5.1 . ] We see that the users cluster better after the inference operators are applied ( even via bias labels ) , showing our ability to use them to form information communities . 5.4 What Does our Model Learn to Connect ?", "Comments": [], "label": []}
{"id": 151954, "text": "As a solution , we propose G-Transformer , introducing locality assumption as an inductive bias into Transformer , reducing the hypothesis space of the attention from target to source .", "Comments": [], "label": []}
{"id": 157660, "text": "We use AdamW to optimize and set the initial learning equal to 1e-5 , except for transfer learning experiments with CookDial , in which we use the learning rate of 5e-6 .", "Comments": [], "label": []}
{"id": 159112, "text": "We observe that pre-trained metrics show the highest correlations with the annotator scores , with the COMET metric performing the best ( [ §5.1 ] .", "Comments": [], "label": []}
{"id": 155985, "text": "Note that using T5 to generate augmented samples does introduce additional knowledge and reduce grammatical errors , but naively using T5 for augmentation without label flipping and selection does not work well ( see ablation study in Section [ 4 ] .", "Comments": [], "label": []}
{"id": 151975, "text": "We propose to use locality properties [ Rizzi , 2013 ] [ Hardmeier , 2014 ] [ Jawahar et al. , , 2019 ] of both the language itself and the translation task as a constraint in Transformer , regulating the hypothesis space of the self-attention and target-to-source attention , using a simple group tag method .", "Comments": [], "label": []}
{"id": 159905, "text": "These results indicate that HyperMixer is substantially easier to tune than Transformers . 4.7 HyperMixer Learns Attention Patterns We hypothesized that the token mixing layer of HyperMixer offers a mechanism similar to attention .", "Comments": [], "label": []}
{"id": 153920, "text": "T5 has no direct match in the number of parameters for BERTLarge , so we present the results of both T5Base ( 220M parameters ) and T5Large ( 770M parameters ) .", "Comments": [], "label": []}
{"id": 157781, "text": "We use pre-trained SpanBERT for our encoder .", "Comments": [], "label": []}
{"id": 155911, "text": "In addition , we use several German and Spanish datasets for the multilingual experiments .", "Comments": [], "label": []}
{"id": 151752, "text": "For each language pair , we applied Byte Pair Encoding ( BPE , [ Sennrich et al. , , 2016b ] with 32K merge operations .", "Comments": [], "label": []}
{"id": 160169, "text": "We use the same pre-processing , model configuration , and hyper-parameters as MuST-C ( see details in Section [ 5.2 ] .", "Comments": [], "label": []}
{"id": 151668, "text": "Evaluation Datasets For supervised directions , most of our evaluation datasets are from WMT and IWSLT benchmarks , for pairs that are not available in WMT or IWSLT , we use OPUS-100 instead .", "Comments": [], "label": []}
{"id": 156405, "text": "The last step is to interpolate the toxicity distribution Ptoxicity with the LM distribution PLM with a tuned hyper-parameter λ and normalize to produce the final distribution we use to sample the next token [ Khandelwal et al. , , 2019 ] : Figure [ 3 ] illustrates the overall procedure of MIL-Decoding .", "Comments": [], "label": []}
{"id": 156787, "text": "Caption-only T5-11B . * Answer : Yes .", "Comments": [], "label": []}
{"id": 157417, "text": "T5 has better zero-shot generalization performance on conversational QG task .", "Comments": [], "label": []}
{"id": 152038, "text": "Experiments show that the resulting G-Transformer converges fast and stably on small and large data , giving the state-of-the-art results compared to existing models under both pre-training and random initialization settings .", "Comments": [], "label": []}
{"id": 156951, "text": "We use MIntRec [ Zhang et al. , , 2022a ] as the experimental dataset for SLU and adopt classification accuracy for the evaluation metric .", "Comments": [], "label": []}
{"id": 152642, "text": "While self-attention is one of the most white-box components in transformer-based models , relying on raw attention weights as an explanation could be misleading given that they are not necessarily responsible for determining the contribution of each token in the final classifier 's decision [ Jain and Wal ] [ lace , 2019 ] [ Serrano and Smith , 2019 ] [ Abnar and ] [ Zuidema , 2020 ] .", "Comments": [], "label": []}
{"id": 154101, "text": "Starting from a pretrained BERT model , it is further pretrained to predict a number of pre-existing metrics , such as BLEU , ROUGE and BERTScore .", "Comments": [], "label": []}
{"id": 157640, "text": "As an initial baseline model , we used this data to fine-tune GPT-J following a similar protocol to [ Peng et al. , 2022 ] .", "Comments": [], "label": []}
{"id": 158850, "text": "Here , we use the strategies identified by the workers in the fourth task of evaluating reframed thoughts .", "Comments": [], "label": []}
{"id": 153030, "text": "First , we use the post-editing data from a valid dataset of WMT21 Automatic Post-Editing Shared Task on En-De [ Sharma et al. , , 2021 ] , where the words are edited by humans are natural constraints instead of simulated constraints .", "Comments": [], "label": []}
{"id": 158686, "text": "The output is obtained as a weighted sum of V , whose weights are computed through a compatibility function between Q and K that , in the case of the scaled dot-product attention used in the original Transformer formulation , is : where d is the dimension of K .", "Comments": [], "label": []}
{"id": 157963, "text": "We use plausibility to avoid judging appropriateness for messages that would likely never be said , e.g. , `` would you cook me a ham- Table 1 : The organization of relationships into folk categories .", "Comments": [], "label": []}
{"id": 158542, "text": "Table 11 : Example of a * sentence-level * ROUGE-based extractive oracle for a gold summary in Table [ 8 ; ] the bold-faced numbers refer to the turn-based sentence ids of the annotated CESs or PESs .", "Comments": [], "label": []}
{"id": 154788, "text": "For this experiment , we use a transformer with 16 layers , 10 heads , embeddings of size 410 , and a feed-forward hidden size of 2100 .", "Comments": [], "label": []}
{"id": 151878, "text": "We employ GloVe [ 7 ] as our initialized word embeddings and set the maximum number of GCN layers as 10 .", "Comments": [], "label": []}
{"id": 157909, "text": "On PersonaChat , we use the code released by Cao et al .", "Comments": [], "label": []}
{"id": 153869, "text": "For example , AdapterDrop [ Rücklé et al. , 2021 ] shows that removing adapters from lower transformer layers can almost maintain the original performance while reducing computational overhead .", "Comments": [], "label": []}
{"id": 157399, "text": "Actor groups are generated by clustering TAMPA-trained node embeddings via HDBSCAN [ McInnes et al. , , 2017 ] . 5 Evaluation and Discussion To assess the effectiveness of our framework , we use a two-step evaluation approach .", "Comments": [], "label": []}
{"id": 159345, "text": "Figure [ 4 ] shows the trends of intra-group homogeneity ( given by the * 1st * term of Eq [ .4 ] B ) ) and inter-group diversity ( given by the * 2nd * term of Eq [ .4 ] B ) ) of GHT and vanilla transformer in the training process on five IWSLT datasets .", "Comments": [], "label": []}
{"id": 153527, "text": "Concerning possible relationships between context-dependent and context-independent representations , we adopt the simplest probing method to extract global information via the gap between context-dependent and context-independent representations of a token for simplification , as shown in Figure [ 1 . ]", "Comments": [], "label": []}
{"id": 157747, "text": "For relevance calibration , models are typically calibrated to the ROUGE scores of their own outputs after an initial fine-tuning step [ Liu and Liu , 2021b ] [ Liu et al. , , 2022 ] .", "Comments": [], "label": []}
{"id": 156740, "text": "In recent years , more generic model architectures such as recurrent neural networks ( RNNs ) and transformers , with no explicit compositional scaffolding , have consistently outperformed compositional models in language processing tasks with natural data [ Wu et al. , , 2016 ] .", "Comments": [], "label": []}
{"id": 153820, "text": "These are higher than previously reported results in the literature ( using the original Transformer , which is a much larger model ) : 0.50 and 0.72 [ Hupkes et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 159692, "text": "We then propose leveraging * X-Y * bitexts in conjunction with the improved sampling strategy , as well as a VoCAP [ Zheng et al. , , 2021 ] style sentencepiece vocabulary re-construction for improving multilingual representation learning ( [ §3.1 ] .", "Comments": [], "label": []}
{"id": 159785, "text": "We have mentioned that we use 10 % as the default poisoning rate to inject backdoors .", "Comments": [], "label": []}
{"id": 153769, "text": "MM-Deacon and DMP performed comparably on the four classification tasks , while DMP used 12 layers of Transformer blocks for SMILES and a 12-layer GNN to encode a molecule 2D graph , which is nearly twice the size of MM-Deacon model .", "Comments": [], "label": []}
{"id": 155289, "text": "We use a similar sampling procedure as described previously .", "Comments": [], "label": []}
{"id": 154894, "text": "We used a version of the Hebrew Facebook Sentiment dataset ( henceforth FB ) of [ Amram et al. , 2018 ] which we corrected by removing leaked samples .", "Comments": [], "label": []}
{"id": 154905, "text": "The two initial data sources we used to pre-train the language models are Oscar and Wikipedia .", "Comments": [], "label": []}
{"id": 158064, "text": "For the shallow/deep MP , we use an Adam optimizer with learning rate of 1e-2/3e-3 for tuning the router ( s ) ( stage I ) and learning rate of 3e-4/2e-5 for tuning the prompts ( stage II ) .", "Comments": [], "label": []}
{"id": 152844, "text": "We observe a monotonic relation between retrieval rank and edit distance ( which we use for filtering our training data ) .", "Comments": [], "label": []}
{"id": 154178, "text": "We use Word2Vec [ 11 ] [ Mikolov et al. , 2013 ] trained on a dialogue dataset [ 12 ] to obtain the distributed word vectors whose dimension is set to 100 . [ Tab . 7 ] shows the measured coherence of different models on validation set of BMELD in En→Zh direction .", "Comments": [], "label": []}
{"id": 158995, "text": "In summary , any adversarial modification made to the input at any stage should be reversed by AT-INTER , i.e. , Finally , on non-adversarial inputs , we do not need to make any changes , and the function T should therefore act as an identity function on these inputs : Training Details We use the T5 model [ Raf ] [ fel et al. , , 2020 ] as the starting point for our text rewriter ATINTER .", "Comments": [], "label": []}
{"id": 158967, "text": "And we use the cross-entropy loss between generated summary and ground truth summary as the training objective to optimize all the parameters of SDDS .", "Comments": [], "label": []}
{"id": 155146, "text": "The Effectiveness of Dynamic Masking In the pre-training phase , we use a dynamic strategy when doing dual-masking on the encoder and decoder respectively .", "Comments": [], "label": []}
{"id": 159965, "text": "The detailed training hyperparameters are described in Appendix [ H. ] 3.7 Decoding We use a beam width of 10 for ASR , S2TT , and S2UT models .", "Comments": [], "label": []}
{"id": 155050, "text": "2 Background and Related Work 2.1 Pretrained Multilingual Models Prior to the widespread use of pretrained transformer models , cross-lingual transfer was mainly achieved through word embeddings [ Mikolov et al. , , 2013 ] [ Pennington et al. , , 2014 ] [ Bojanowski et al. , , 2017 ] , either by aligning monolingual embeddings into the same embedding space [ Lample et al. , ] [ 2018b , ] [ a ; ] [ Grave et al. , , 2018 ] or by training multilingual embeddings [ Ammar et al. , , 2016 ] [ Artetxe and ] [ Schwenk , 2019 ] .", "Comments": [], "label": []}
{"id": 158754, "text": "For fine-tuning ( see Section 4.6 ) , we used the curated ECOIT dataset .", "Comments": [], "label": []}
{"id": 153342, "text": "We batch sentences of similar lengths to make full use of GPUs and the number of tokens in a single batch is set to 3000 .", "Comments": [], "label": []}
{"id": 152839, "text": "QED Training We use a T5-large model finetuned on the Natural Questions subset with QED annotations [ Lamm et al. , , 2021 ] . [ 9 ] We refer the reader to the QED paper for details on the linearization of explanations and inputs in the T5 model .", "Comments": [], "label": []}
{"id": 158708, "text": "For subtask B , we use the data from six domains ( source ) for training and validation , and the data from the left-out domain ( zero-shot ) as the test set .", "Comments": [], "label": []}
{"id": 154553, "text": "Therefore , we adopt a simple and universal detection algorithm LOF algorithm [ Breunig et al. , , 2000 ] and compute LOF score following [ Lin and Xu , 2019 ] , see Appendix [ A.3 ] for specific calculation steps .", "Comments": [], "label": []}
{"id": 159201, "text": "We use the sequence labeling framework equipped with RoBERTabase for these experiments and use the word-level omission recall ( WR ) for evaluation . would bring benefit to the refinement model .", "Comments": [], "label": []}
{"id": 155487, "text": "We use the same hyperparameters as CSN dataset but fine-tune the model .", "Comments": [], "label": []}
{"id": 155836, "text": "We use the Yelp-5 [ Zhang et al. , , 2015a ] dataset to train the RoBERTaBASE model mentioned in Section [ 3.2.3 . ]", "Comments": [], "label": []}
{"id": 152027, "text": "When all the biases removed , the model downgrades to a document-level Transformer .", "Comments": [], "label": []}
{"id": 152750, "text": "E Implementation Details All of our parsers are based on a 6-layer-4-head Transformer architecture [ Vaswani et al. , , 2017 ] , with 256 hidden dimensions and 512 dimensions for fully connected layers , without additional modules .", "Comments": [], "label": []}
{"id": 160360, "text": "We adopt the best models in Slavic-to-English translation , i.e. , multilingual XM Transformer with both dense and sparse architectures .", "Comments": [], "label": []}
{"id": 157312, "text": "Figure [ 6 ] illustrates the word attributions of the same advertisement from a vendor , `` pckabml '' , generated through the [ captum ] ( https : //captum.ai/ ) [ Kokhlikyan et al. , , 2020 ] and [ transformers ] ( https : //pypi.org/project/transformers-interpret/ ) [ interpret ] ( https : //pypi.org/project/transformers-interpret/ ) [ Pierse , 2021 ] frameworks .", "Comments": [], "label": []}
{"id": 154919, "text": "[ Huang et al. , 2021 ] leveraged the T5 [ Raffel et al. , 2019 ] model for this task and introduced a re-ranking mechanism to model specificity in definitions .", "Comments": [], "label": []}
{"id": 160333, "text": "A Sample We adopt JSON file to store TOXICN dataset , which is a mainstream coding specification to facilitate machine-readable .", "Comments": [], "label": []}
{"id": 156908, "text": "B Dialogue Diversity Supplementary Table [ 7 ] shows the diversity scores for each group of messages across different diversity threshold values and sentence transformer models .", "Comments": [], "label": []}
{"id": 151518, "text": "In our main experiments ( Tables [ 1- ] [ 4 ] and subsequent analysis , we use model confidence for distillation .", "Comments": [], "label": []}
{"id": 156401, "text": "Note that Table [ 13 ] only shows COMET and chrF scores and the BLEU scores are shown in Table [ 2 ] due to space limitations .", "Comments": [], "label": []}
{"id": 159864, "text": "Abstract evaluation of a result R as following : Transformer-based architecture are the model of choice for natural language understanding , but they come at a significant cost , as they have quadratic complexity in the input length , re-quaring at a lot of training data , and can be difficult to tune .", "Comments": [], "label": []}
{"id": 156411, "text": "Given a prompt from the dataset , we use the language model to generate n = 25 continuations with different detoxification methods , where each continuation is limited to a maximum length of 20 tokens .", "Comments": [], "label": []}
{"id": 160016, "text": "Weights w1 , . . . , w for the weighted sum of the 12 Transformer encoder outputs are trainable and satisfy P =1 w = 1. v v v v a a a a d d d d v v v v a a a a d d d d ϵ = ϵ = ϵ = 1/3 .", "Comments": [], "label": []}
{"id": 157142, "text": "We use three classical pre-trained language models in our experiments : BERTBASE [ Devlin et al. , , 2019 ] , RoBERTaLARGE [ Liu et al. , , 2019 ] and GPT-2 [ Radford et al. , , 2019 ] , which have 12/24/24 layers with hidden size of 768/1024/1024 and 110/380/354M trainable parameters , respectively .", "Comments": [], "label": []}
{"id": 151755, "text": "For the unconstrained scenario ( [ §3.3 ] , we adopted the TRANSFORMER-BIG model .", "Comments": [], "label": []}
{"id": 158530, "text": "The performance gain obtained through both Multi DYLE ( XROG o , XCES o ) and the EE model tends to be further enlarged compared to the gain of ROUGE only by Multi-DYLE ( XROG o , XCES o ) for meeting summarization .", "Comments": [], "label": []}
{"id": 158785, "text": "We use tables in Figure [ 4 ] as our running examples .", "Comments": [], "label": []}
{"id": 159633, "text": "We adopt an off-the-shelf mention detection model [ Toshni ] [ wal et al. , , 2021 ] to detect mentions and use ground truth to replace the relation classifier to get rid of the model difference , except the cache structure .", "Comments": [], "label": []}
{"id": 156785, "text": "OFA→T5 exemplifies the effect of subop- timal visual recognition when shifting from the * from pixels * setting to the * from description * setting .", "Comments": [], "label": []}
{"id": 154437, "text": "Moreover , to further alleviate the inference cost , we adopt PL-Marker as a post-processing module of a two-stage model , in which it is used to identify entities from a small number of candidate entities proposed by a simpler and faster model .", "Comments": [], "label": []}
{"id": 160304, "text": "Its underlying network structure incorporates GNNs into a transformer network and employs self-supervised learning .", "Comments": [], "label": []}
{"id": 153956, "text": "We use teacher forcing and consider the prediction correct only when all the predicted tokens are correct .", "Comments": [], "label": []}
{"id": 154193, "text": "We used the iob2 annotation scheme to distinguish tokens at the beginning , inside , or outside of tagged expressions , which leads to 279 possible token labels .", "Comments": [], "label": []}
{"id": 154380, "text": "Specifically , we use superscripts s and t to denote the quantized student network and fullprecision teacher network , respectively .", "Comments": [], "label": []}
{"id": 155348, "text": "The detailed statistics of all datasets included are shown in Table [ 1 . ] 3.2 Experimental setups Pre-processing For speech input , we use the raw 16-bit 16kHz mono-channel audio wave .", "Comments": [], "label": []}
{"id": 155707, "text": "R56LM013365 ; Stanford Data Science Initiative , Wu Tsai Neurosciences Institute , Chan Zuckerberg Biohub , Amazon , JPMorgan Chase , Docomo , Hitachi , Intel , KDDI , Toshiba , NEC , Juniper , and UnitedHealth Group .", "Comments": [], "label": []}
{"id": 160047, "text": "Also , note that our method has a similar number of parameters to the RoBERTa baseline since we share embedding layers and transformers between the connection generation and relation classification modules in our approach .", "Comments": [], "label": []}
{"id": 156296, "text": "We used OPUS-MT because of its ready-to-use checkpoints available for many language pairs .", "Comments": [], "label": []}
{"id": 153406, "text": "Figure 1 : Percentage reduction in Pseudo perplexity [ Salazar et al. , , 2020 ] for different LRLs as we go from BPE to OBPE vocabulary . ( Section [ 4.2 ] sentences drops by 2.6 % when we go from the BPE to OBPE vocabulary .", "Comments": [], "label": []}
{"id": 153469, "text": "By doing so , we build the MSCTD [ 5 ] . 3.3 Annotation Quality Assessment To evaluate the quality of annotation , we use Fleiss ' Kappa to measure the overall annotation consistency among three annotators [ Fleiss and Cohen , 1973 ] .", "Comments": [], "label": []}
{"id": 160593, "text": "But in practice , if we do this , the gradients of all potential scores , which is O ( n 4 ) ( n is the sentence length ) , should be stored in the GPU memory .", "Comments": [], "label": []}
{"id": 157728, "text": "And we do inference on a CPU machine with 80GB RAM which takes 3 hours .", "Comments": [], "label": []}
{"id": 152697, "text": "The Seq2Seq Transformer model can be trained by minimizing the negative log-likelihood of gold document-summary pairs : where |Y | is the number of tokens in summary Y .", "Comments": [], "label": []}
{"id": 160008, "text": "We adopted crosslingual semantic textual similarity ( XSTS ) [ Licht et al. , 2022 ] and percent acceptable translations .", "Comments": [], "label": []}
{"id": 157134, "text": "As such , conventional fine-tuning of the larger models could require hundreds of GPU hours .", "Comments": [], "label": []}
{"id": 154141, "text": "For learning rate , we use 5 × 10− , 10− , and 3 × 10− for WN18RR , FB15k-237 , and Wikidata5M datasets , respectively .", "Comments": [], "label": []}
{"id": 159673, "text": "Note that GPT3 is called via an external API , while T5 is run on a local GPU .", "Comments": [], "label": []}
{"id": 155421, "text": "These two cases support that the fixed routing strategy , which addresses the routing fluctuation problem , can bring better performance for MoEbased Transformers .", "Comments": [], "label": []}
{"id": 156152, "text": "We use cross entropy loss to train the rule selector , and binary cross entropy loss to train the fact selector .", "Comments": [], "label": []}
{"id": 155791, "text": "Latent Transformer .", "Comments": [], "label": []}
{"id": 156529, "text": "Following up on prior studies on argumentative and pragmatic labeling schemata for free-form peer reviews [ Hua et al. , , 2019 ] [ Kuznetsov et al. , ] [ 2022 , ] etc . ) , we use NLPEER to explore the connection of pragmatic labels and structured review forms for the purpose of this assistance scenario .", "Comments": [], "label": []}
{"id": 156573, "text": "Given the encoded vision embedding Fv ( fi , t ) , we use a projection head ( with pooling ) ϕ to project the embedding sequence into a vector representation ϕv ( Fv ( fi , t ) ) ∈ R .", "Comments": [], "label": []}
{"id": 158859, "text": "We use the metrics described in [ §5.3.2 ] namely BLEU , ROUGE , BERTScore , and Distinct-n metrics .", "Comments": [], "label": []}
{"id": 152365, "text": "We demonstrated the utility of HATECHECK as a diagnostic tool by testing near-state-of-the-art transformer models as well as two commercial models for hate speech detection .", "Comments": [], "label": []}
{"id": 160139, "text": "We use the same 1 million randomly sampled sentences [ 4 ] as SimCSE for training , be- https : //github.com/princeton-nlp/SimCSE https : //huggingface.co/datasets/princeton-nlp/datasetsfor-simcse Table 1 : The performance on STS tasks ( Spearman 's correlation ) for different sentence embedding models .", "Comments": [], "label": []}
{"id": 158306, "text": "For regressive alignment ( CTC-R ) , we use BLEURTAligner .", "Comments": [], "label": []}
{"id": 157639, "text": "We recall that for TaPas , we use N=1 as , due to joint tasks of cell selection and aggregation classification , it is not straightforward to determine the probability of the output making it unfeasible to compare N > 1 predictions .", "Comments": [], "label": []}
{"id": 153918, "text": "Due to resource limitations , we only pretrain the model for 250,000 steps , which are half of RoBERTa and BART 's training steps and close to T5 in the number of trained tokens .", "Comments": [], "label": []}
{"id": 154388, "text": "We use perplexity ( PPL ) to evaluate the performance for language modeling .", "Comments": [], "label": []}
{"id": 152533, "text": "For Sentence Transformers , we compare to `` roberta-base-nli-mean-tokens '' [ 6 ] , which , like DeCLUTR-base , uses the RoBERTabase architecture and pretrained weights .", "Comments": [], "label": []}
{"id": 156048, "text": "Next , for each token selection method we select the accuracy scores when there are speedup 1.5X , 2X , 3X , and 3.5X over the Transformers without any sequence reduction .", "Comments": [], "label": []}
{"id": 153535, "text": "We adopt optimizer AdamW [ Loshchilov and Hut ] [ ter , 2019 ] with learning rate 1e-4 .", "Comments": [], "label": []}
{"id": 153781, "text": "The pronoun ranking bias ( PRB ) for this template is the difference in log probabilities : 3.3 Toxicity Classification For toxicity classification , we use the WIKI dataset , which consists of just under 130,000 comments from the online forum Wikipedia Talks Pages ( Dixon et al. , 2018 ) .", "Comments": [], "label": []}
{"id": 156739, "text": "DeepL ( doc ) BLEU , COMET and word f-measures statistically significantly higher than DeepL ( sent ) are underlined .", "Comments": [], "label": []}
{"id": 153305, "text": "By contrast , CipherDAug trains a single model , and improves the baseline transformer by 2.9 BLEU points on IWSLT14 De→En and about 2.2 BLEU points on the smaller datasets .", "Comments": [], "label": []}
{"id": 154523, "text": "The inclusion of the coverage term with RELAX has not been able to increase the ROUGE scores , but has increased ME-TEOR by +1.64 pp .", "Comments": [], "label": []}
{"id": 158760, "text": "We reimplemented this model and pre-trained & fine-tuned it on our datasets . 5.3 Main Results For evaluating translation performance , we used two automatic evaluation metrics sacreBLEU9 and METEOR10 ( Denkowski and Lavie , 2014 ) .", "Comments": [], "label": []}
{"id": 151556, "text": "We use the backbone feature extractor φ ( Faster-RCNN feature extractor [ Ren et al. , , 2015 ] [ Anderson et al. , , 2018 ] to extract * N * regions of interest for each region : We note that some of these regions in 1 , ... , * * are provided to the model ( as annotated regions in the image ) ; the rest are detected by φ .", "Comments": [], "label": []}
{"id": 159497, "text": "As for Style Transformer , the results demonstrate that only an attention-based model hardly removes style features in overwhelming tokens information , leading to degenerate into an auto-encoder .", "Comments": [], "label": []}
{"id": 152348, "text": "From the results , we conclude that both automatic evaluation and human evaluation performances should be supported to be a good summarization model and BERT-score can complement ROUGE-score in automatic evaluation .", "Comments": [], "label": []}
{"id": 154726, "text": "We use threshold values for splitting data into two groups Dhigh and Dlow ( the median AUM over full training samples ) as 3.5/4.4/2.5 for BERT , 3.4/4.0/2.7 for RoBERTa on SNLI/QQP/SWAG , respectively .", "Comments": [], "label": []}
{"id": 151984, "text": "In contrast , the complexity of group attention in G-Transformer is O ( N2/M ) given the fact that the attention is restricted to a local sentence .", "Comments": [], "label": []}
{"id": 160305, "text": "AMRSim is alignment-free , allowing for a significant reduction in inference time when utilizing GPUs .", "Comments": [], "label": []}
{"id": 153600, "text": "It is built upon a modified attention mask of the Transformer model .", "Comments": [], "label": []}
{"id": 153410, "text": "Setting p = 1 gives the original BPE algorithm .", "Comments": [], "label": []}
{"id": 159160, "text": "From the plots , we observe that high-performing metrics such as BERTScores and COMET-DA correlate positively with the metric scores as the human scores increase .", "Comments": [], "label": []}
{"id": 157309, "text": "Bowman . 2021 . [ Fine-tuned transformers show clus ] ( http : //arxiv.org/abs/2109.08406 ) [ ters of similar representations across layers . ] ( http : //arxiv.org/abs/2109.08406 ) * CoRR * , abs/2109.08406 .", "Comments": [], "label": []}
{"id": 155429, "text": "To analyze the impact of such attributes , we use a macro F1 score which takes into account the partitions created by an attribute .", "Comments": [], "label": []}
{"id": 153648, "text": "They also proposed to align clusters to accelerate the learning of top layers .", "Comments": [], "label": []}
{"id": 160076, "text": "3.4 Model Training We use similarities predicted by rank vectors to train another sentence encoder , E [ 2 ] ; rank vectors capture a better semantic similarity than their vector representation computed by base encoder E1 .", "Comments": [], "label": []}
{"id": 159389, "text": "Specifically , we use the contrastive weights , negatively correlated with cross-modal similarity , to roughly approximate whether a negative sample is false .", "Comments": [], "label": []}
{"id": 159845, "text": "5.1 Datasets , Prompt Templates , and Experimental Setup We adopt sentiment classification tasks on two datasets , IMDB [ Maas et al. , , 2011 ] and Amazon [ McAuley and Leskovec , 2013 ] , and topic classification tasks on another two datasets , AG News [ Zhang et al. , , 2015 ] and DBPedia [ Lehmann et al. , , 2015 ] .", "Comments": [], "label": []}
{"id": 155354, "text": "For the * translation encoder * , we use N = 6 transformer encoder layers .", "Comments": [], "label": []}
{"id": 153653, "text": "The dataset statistics are summarized in Table [ 1 . ] We use the same splits of BANKING and StackOverflow as in [ Zhang et al. , 2021b ] .", "Comments": [], "label": []}
{"id": 160259, "text": "CheckList [ Ribeiro et al. , , 2020 ] : Developed to generate a diverse set of evaluation examples , CheckList works by coalescing name replacement , < https : //github.com/QData/TextAttack > location replacement , number alteration , and word contraction/extension .", "Comments": [], "label": []}
{"id": 155516, "text": "As expected , pretraining with natural languages ( English , Spanish and Japanese ) provides better encoders for language modeling than the artificial languages both with LSTM and Transformer .", "Comments": [], "label": []}
{"id": 156007, "text": "The T5 model will fill the blanks in the masked sentences .", "Comments": [], "label": []}
{"id": 154227, "text": "For variables , we use the variable index because variables do not appear in the problem .", "Comments": [], "label": []}
{"id": 154023, "text": "We use the same BERT and SciBERT base models as described in the DyGIE++ architecture above , both with a maximum input sequence length of 512 tokens .", "Comments": [], "label": []}
{"id": 156246, "text": "We compare TBS models with other knowledge-augmented baselines that retrieve knowledge from ConceptNet using embedding scores ( KS-SBERT ) or a trained selector ( KS-RoBERTa ) , or generate from * another * model ( KG-COMET ) .", "Comments": [], "label": []}
{"id": 157289, "text": "Without using the diversity tricks we developed ( e.g. , inserting linear layers into the transformer encoder ) , this metric would be greater than 0.99 and the improvement would be greatly reduce in downstream applications ( see our ablation study in Table [ 2 ] .", "Comments": [], "label": []}
{"id": 151436, "text": "In the annotation framework , we used the term `` active listening '' to refer to uptake , since we found that active listening is more interpretable to raters , while uptake is too technical .", "Comments": [], "label": []}
{"id": 152345, "text": "R-1 , R-2 , R-L , and FBERT refer to ROUGE- { 1,2 , L } , and BERT-score , respectively .", "Comments": [], "label": []}
{"id": 155793, "text": "For clarity , we use words and tokens interchangeably in the paper .", "Comments": [], "label": []}
{"id": 156600, "text": "For SINGULARITY-temporal , we train with a similar setup , except that we set training frames to be 4 .", "Comments": [], "label": []}
{"id": 153726, "text": "For QASC , we use the associated fact sentences that are used to create each question .", "Comments": [], "label": []}
{"id": 157039, "text": "It is worth noting that our proposed method is not limited to this dataset but can be applied to any general texts for constructing syntactically diverse paraphrases . 3.2 Translating Texts to AMR Graphs We use a pre-trained AMR parser to encode source sentences to AMR graphs .", "Comments": [], "label": []}
{"id": 155981, "text": "3.3 Effectiveness : Manual Label Flipping Improves Performance Since previous methods are not sufficiently effective and robust in our preliminary experiments ( see Tables [ 5 ] and [ 6 ] in Section [ 4 ] for details ) , we use manual augmentation to investigate what kind of augmented data is beneficial for large pretrained models in the few-shot setting .", "Comments": [], "label": []}
{"id": 154907, "text": "The SPMRL [ Seddah et al. , , 2013 ] and UD [ Sadde et al. , , 2018 ] datasets we used for evaluating segmentation , tagging and parsing , were used to both train our morphological extraction model as well as provide us with the test data to evaluate on morphological level tasks .", "Comments": [], "label": []}
{"id": 159040, "text": "To that end , we use BERTopic as our clustering model [ Grootendorst , 2022 ] , which facilitates the clustering of sentences based on their contextualised embeddings obtained from a pre-trained language model [ Reimers and ] [ Gurevych , 2019 ] , as well as fine-tuning them further for the clustering task .", "Comments": [], "label": []}
{"id": 157889, "text": "For Wizard of Wikipedia , similar to Li et al . ( 2022a ) , we use the chosen topic passage and the retrieved passages in the first turn as the knowledge corpus K .", "Comments": [], "label": []}
{"id": 157724, "text": "Same as ReTraCk , we use pre-trained BERT-base-uncased model as a schema retriever and fine-tune it on GrailQAbility for 10 epochs with a learning rate of 1e-5 .", "Comments": [], "label": []}
{"id": 156103, "text": "More precisely , a single training example becomes impossible to fit into GPU memory ( 40GB ) even for top-25 paths for PATHFID+ model with T5-large initialization .", "Comments": [], "label": []}
{"id": 155417, "text": "We also attempt to insert the MoE layer before the first Transformer block ( bottom ) , and after the last Transformer block ( top ) .", "Comments": [], "label": []}
{"id": 156200, "text": "We use the same spreadsheet table corpus as TUTA : ( 1 ) 13.5 million public spreadsheet files are crawled from 1.75 million websites ; ( 2 ) table ranges and headers are detected using TableSense [ Dong ] * et [ al .", "Comments": [], "label": []}
{"id": 157710, "text": "[ Wei and Nguyen , 2019 ] propose https : //github.com/Bjarten/early-stopping-pytorch https : //github.com/Lyken17/pytorch-OpCounter Figure 10 : Accuracy of BIC with different settings of considering semantic consistency .", "Comments": [], "label": []}
{"id": 157143, "text": "For BERT and RoBERTa , we evaluate on the GLUE benchmarks [ Wang et al. , , 2018 ] , and for GPT-2 we use E2E [ Novikova et al. , , 2017 ] , WebNLG [ Gardent et al. , , 2017 ] and DART [ Nan et al. , 2021 ] .", "Comments": [], "label": []}
{"id": 154865, "text": "We use Adam optimizer [ Kingma and Ba , 2015 ] to tune the parameters .", "Comments": [], "label": []}
{"id": 152033, "text": "The locality bias we introduce to G-Transformer is different from the ones in Longformer [ Beltagy et al. , 2020 ] and Reformer [ Kitaev et al. , , 2020 ] in the sense that we discuss locality in the context of representing the alignment between source sentences and target sentences in document-level MT .", "Comments": [], "label": []}
{"id": 154805, "text": "We use Roberta-base model , which includes 12 layers , 768 hidden nodes , 12 head nodes , 125M parameters , and add a linear layer with two nodes for binary classification .", "Comments": [], "label": []}
{"id": 157003, "text": "Despite both the 1.3B and 615M NLLB models being distilled from the same larger model , we see that the dialectal gap is smaller for German , Gujurati , and Russian .", "Comments": [], "label": []}
{"id": 153574, "text": "To balance efficiency and perceptual quality , we adopt VQ-GAN [ Esser et al. , , 2021 ] as the visual encoder and decoder in our framework .", "Comments": [], "label": []}
{"id": 157001, "text": "This supports our claim that the discrepancies are caused by model mismatch , rather Table 5 : Dialect Translation Stress Test : SacreBLEU Score [ Post , 2018 ] on each VALUE-transformed validation set of the WMT19 benchmark at 2 distilled scales of the NLLB Translation model [ Costa-jussà et al. , , 2022 ] .", "Comments": [], "label": []}
{"id": 152914, "text": "* * Architecture : * * We use a standard sequence-tosequence Transformer model [ Vaswani et al. , , 2017 ] with 12 layers each for the encoder and decoder .", "Comments": [], "label": []}
{"id": 157371, "text": "When we use only the image , the performance is poor .", "Comments": [], "label": []}
{"id": 156517, "text": "Approximately 2,000 GPU hours were required to train all hyperparameter variations of RBE plus the Bert baseline across all 3 test sets .", "Comments": [], "label": []}
{"id": 151654, "text": "Specifically , we compare the Autoencoder and the CRA structure in MMIN , and we adopt the same parameter scale to ensure the fairness of the comparison .", "Comments": [], "label": []}
{"id": 152337, "text": "During testing , we used beam search with early stopping and discarded hypotheses that contain twice the same trigram .", "Comments": [], "label": []}
{"id": 155441, "text": "3.1 Datasets and Evaluation Metrics We used the official datasets of BEA-2019 Shared Task [ Bryant et al. , , 2019 ] , W & Itrain [ Granger , 1998 ] [ Yannakoudakis et al. , , 2018 ] , In Equation [ 4 , ] we do not use the input and output sentences in the value , and thus represent them as _ .", "Comments": [], "label": []}
{"id": 158965, "text": "To unify the static and dynamic graph structures into a final utterance representation , we employ a self-attention layer as shown in Figure [ 2 . ]", "Comments": [], "label": []}
{"id": 151677, "text": "mRASP2 w/o AA only adopts contrastive learning on the basis of m-Transformer .", "Comments": [], "label": []}
{"id": 159364, "text": "Often , * loaded language * uses sensational adverbs or adjectives to exaggerate a statement . 8 We use the fine-tuned NLI model from [ https : // ] ( https : //huggingface.co/roberta-large-mnli ) [ huggingface.co/roberta-large-mnli ] ( https : //huggingface.co/roberta-large-mnli ) .", "Comments": [], "label": []}
{"id": 155221, "text": "Presv . ) * , or both of them . algorithm , named as SHIELD , which patches only the last layer of an already deployed textual NN model ( e.g. , CNN , RNN , transformers [ Vaswani et al. , 2017 ] [ Bahdanau et al . ] ) and transforms it into an ensemble of multi-experts or * prediction heads * ( Fig . [ 1C ] ) .", "Comments": [], "label": []}
{"id": 154901, "text": "For each word , counts are based on multiset intersections of the gold and predicted labels ignoring the order of the segments while account- Available here : [ https : //github.com/OnlpLab/ ] ( https : //github.com/OnlpLab/NEMO-Corpus ) [ NEMO-Corpus ] ( https : //github.com/OnlpLab/NEMO-Corpus ) [ https : //universaldependencies.org/ ] ( https : //universaldependencies.org/conll18/results-alltags.html ) [ conll18/results-alltags.html ] ( https : //universaldependencies.org/conll18/results-alltags.html ) [ https : //universaldependencies.org/ ] ( https : //universaldependencies.org/conll18/results.html ) [ conll18/results.html ] ( https : //universaldependencies.org/conll18/results.html ) respectively referred to as 'Segmented Words ' and 'UPOS ' in the CoNLL18 evaluation script Table 4 : Word-based NER F1 .", "Comments": [], "label": []}
{"id": 154843, "text": "[ https : //github.com/OpenNMT/ ] ( https : //github.com/OpenNMT/OpenNMT-tf/tree/master/scripts/wmt ) [ OpenNMT-tf/tree/master/scripts/wmt ] ( https : //github.com/OpenNMT/OpenNMT-tf/tree/master/scripts/wmt ) [ https : //github.com/pytorch/fairseq/ ] ( https : //github.com/pytorch/fairseq/tree/master/examples/translation ) [ tree/master/examples/translation ] ( https : //github.com/pytorch/fairseq/tree/master/examples/translation ) We keep our data preprocessing method for knowledge graph completion the same as [ López and ] [ Strube , 2020 ] .", "Comments": [], "label": []}
{"id": 159304, "text": "LT , where L is the number of layers of the textual encoder . 2.3 Cross-Modal Encoder We adopt the transformer encoder [ Vaswani et al. , , 2017 ] with the co-attention mechanism as the cross-modal encoder [ Lu et al. , , 2019 ] .", "Comments": [], "label": []}
{"id": 159150, "text": "We then use our dataset to train an Indic specific COMET metric that outperforms existing metrics in terms of both correlations and robustness scores ( [ §6.2 ] .", "Comments": [], "label": []}
{"id": 160424, "text": "The method uses a ] ( https : //huggingface.co/T-Systems-onsite/cross-en-de-roberta-sentence-transformer ) [ vicinity-driven approach with a similarity matrix ] ( https : //huggingface.co/T-Systems-onsite/cross-en-de-roberta-sentence-transformer ) [ based on a TF-IDF model .", "Comments": [], "label": []}
{"id": 159563, "text": "In detail , following recent CLS work [ Ladhak et al. , , 2020 ] [ Perez-Beltrachini and Lapata , 2021 ] , we use * Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics Volume 1 : Long Papers * , pages 15127–15143 July 9-14 , 2023 ©2023 Association for Computational Linguistics Work was done when Jiaan Wang was interning at Pattern Recognition Center , WeChat AI , Tencent Inc , China .", "Comments": [], "label": []}
{"id": 151509, "text": "We use a 1d twocomponent Gaussian Mixture Model ( GMM ) to model * per-sample loss distribution * and cluster the samples based on their * goodness * .", "Comments": [], "label": []}
{"id": 152418, "text": "We report the Exact Match and F1 score ( EM / F1 ) on 7 languages . : our implementation by official code ; 1 : ( Hu et al. , 2020 ) ; 2 : ( Liang et al. , 2020 ) ; 3 : ( Yuan et al. , 2020 ) ; 4 : ( Nooralahzadeh et al. , 2020 ) ; 5 : ( Phang et al. , 2020 ) .", "Comments": [], "label": []}
{"id": 157924, "text": "Alternatively , parameter-efficient fine-tuning adapts pre-trained models to new languages by training a small set of weights effectively [ Zhao et al. , 2020 ] [ Pfeiffer et al. , , 2021 ] [ Ansell et al. , , 2022 ] .", "Comments": [], "label": []}
{"id": 155169, "text": "Traditionally , this is done by finding a set of sentences that maximize ROUGE-2 ( R2 ) with respect to the reference : argmaxER2 ( E , S ) [ Gillick and Favre , 2009 ] [ Nallapati et al. , , 2017 ] .", "Comments": [], "label": []}
{"id": 155261, "text": "For the encoding we use one-hot vectors , similar to their implementation .", "Comments": [], "label": []}
{"id": 153086, "text": "Moreover , pre-trained state-of-the-art Transformers are not leveraged despite showing strong performance when fine-tuned in downstream tasks such as single-document summarization [ Liu and ] [ Lapata , 2019b ] [ Lewis et al. , , 2020a ] [ Raffel et al. , , 2020 ] [ Beltagy et al. , , 2020 ] [ Zaheer et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 157422, "text": "As shown in Table [ 4 , ] most variants lead to worse performance and yet still outperform the baseline model T5 .", "Comments": [], "label": []}
{"id": 159521, "text": "Therefore , for effciency , in the following experiments , we utilize the SBS strategy in our approach for comparison .", "Comments": [], "label": []}
{"id": 152813, "text": "We describe each component of RGF below ; additional implementation details are provided in Appendix [ A . ] 3.2 Retrieval We use REALM retrieve-and-read model of [ Guu et al. , 2020 ] .", "Comments": [], "label": []}
{"id": 152143, "text": "Moreover , the performances are similar between GIT-OP and GIT-NT , which also provides evidence that other records do help .", "Comments": [], "label": []}
{"id": 154845, "text": "C.1 Initialization We use different initialization method for different parameters , see Table [ 8 . ] Geoopt [ Kochurov et al. , , 2020 ] initialize the parameter with Gaussian distribution in the tangent space , and map the embedding to hyperbolic space with exponential map .", "Comments": [], "label": []}
{"id": 154022, "text": "For the ProMED dataset , we use a different clustering heuristic that ensures that each template has exactly one role filler for the COUNTRY and DISEASE roles , as detailed in the dataset annotation guidelines .", "Comments": [], "label": []}
{"id": 153466, "text": "Then , we utilize the original English utterance to automatically select its Chinese translation by perfectly matching the English subtitle in the constructed bilingual database .", "Comments": [], "label": []}
{"id": 159407, "text": "The tables above and below correspond to models fine-tuned on T5-base and BART-base , respectively .", "Comments": [], "label": []}
{"id": 157953, "text": "We use Web of Science ( WOS ) from [ Kowsari et al. , 2017 ] and 20 Newsgroups from [ Lang , 1995 ] to assess our method for datasets with high inter-task relevance .", "Comments": [], "label": []}
{"id": 154125, "text": "We propose a simple graph-based re-ranking strategy : increase the score of candidate tail entity t by α ≥ 0 if t is in k-hop neighbors Ek ( h ) of the head entity h based on the graph from training set : 3.5 Training and Inference During training , we use InfoNCE loss with additive margin [ Chen et al. , , 2020 ] [ Yang et al. , , 2019 ] : The additive margin γ > 0 encourages the model to increase the score of the correct triple ( h , r , t ) .", "Comments": [], "label": []}
{"id": 158466, "text": "We employ a masked language model to suggest word-level perturbations that constrain the search space .", "Comments": [], "label": []}
{"id": 152870, "text": "gains of 4.15/6.21/4.00 of ROUGE-1/2/L scores compared to the previous best method .", "Comments": [], "label": []}
{"id": 151581, "text": "[ https : //huggingface.co/amoux/ ] ( https : //huggingface.co/amoux/roberta-cord19-1M7k ) [ roberta-cord19-1M7k ] ( https : //huggingface.co/amoux/roberta-cord19-1M7k ) Figure 2 : Most frequent POS tags of salient words .", "Comments": [], "label": []}
{"id": 157689, "text": "As this assignment required implementing and training language models in PyTorch , this suggests that these hands-on NLP skills may save students ' time when reproducing NLP results .", "Comments": [], "label": []}
{"id": 151378, "text": "Therefore , we choose the more informative Structural Components ( SC ) in Table [ 1 ] as radicallevel features of Chinese characters and use Convolutional Neural Network ( CNN ) to extract character [ https : //ai.tencent.com/ailab/nlp/en/ ] ( https : //ai.tencent.com/ailab/nlp/en/embedding.html ) [ embedding.html ] ( https : //ai.tencent.com/ailab/nlp/en/embedding.html ) Figure 2 : The proposed MECT method : ( a ) the overall structure of MECT ; ( b ) the Cross-Transformer module .", "Comments": [], "label": []}
{"id": 159124, "text": "We observe that COMET-MQM is the best-performing metric overall for all languages in consideration .", "Comments": [], "label": []}
{"id": 157473, "text": "Our dataset was developed using a novel community-in-the-loop method for benchmark development .", "Comments": [], "label": []}
{"id": 156792, "text": "T5-Large . * Answer : Yes .", "Comments": [], "label": []}
{"id": 156267, "text": "Recent studies also explored the possibilities of prompt-based learning in machine reading comprehension , including a new pre-training scheme that changed the question answering into a few-shot span selection model [ Ram et al. , , 2021 ] and a new model that fine-tuned the prompts with knowledge [ Chen et al. , , 2021 ] .", "Comments": [], "label": []}
{"id": 155107, "text": "The resultant representations are then combined and sequentially encoded with a transformer , followed by attention-based temporal and multimodal fusion .", "Comments": [], "label": []}
{"id": 153624, "text": "As shown in [ Figure 1 , ] these Transformer blocks are divided into three groups , early backbone encoder layers , late backbone encoder layers , and head layers .", "Comments": [], "label": []}
{"id": 159683, "text": "We use the same set of metrics described in [ §6.2 ] ( state EM , program EM ) .", "Comments": [], "label": []}
{"id": 152806, "text": "We employed advanced models as the student model and verified the future applicability of our framework to emerging language models by achieving even higher performances than the teacher model .", "Comments": [], "label": []}
{"id": 153994, "text": "The purpose was to assess whether the combination of different embeddings that encode different linguistic information could outperform the Transformer-based models in Section [ 3.2 . ]", "Comments": [], "label": []}
{"id": 158389, "text": "In [ Table 3 , ] we observe a large improvement of +3.1 Rouge-1 over the BART SegEnc baseline .", "Comments": [], "label": []}
{"id": 157038, "text": "K , L , M , N are all Transformer-base models .", "Comments": [], "label": []}
{"id": 153348, "text": "Specifically , we use soft prompts composed of multiple learnable vectors and the [ MASK ] token instead of hard templates for tuning .", "Comments": [], "label": []}
{"id": 152315, "text": "Specifically , each document is truncated to 2000 BPE tokens to involve more sentences , but this can not cover those whole documents with more than 55-sentences .", "Comments": [], "label": []}
{"id": 159215, "text": "Pointer Network The autoregressive decoder of our pointer network is implemented by a Transformer decoder , which is proposed by Zou et al . [ 2021b ] and was previously used for extractive summarization .", "Comments": [], "label": []}
{"id": 151664, "text": "Especially , it obtains substantial gains on zero-shot directions . 3.1 Settings and Datasets Parallel Dataset PC32 We use the parallel dataset PC32 provided by [ Lin et al. , 2020 ] .", "Comments": [], "label": []}
{"id": 151399, "text": "KET [ Zhong et al. , , 2019 ] uses hierarchical Transformers [ Vaswani et al. , , 2017 ] with external knowledge .", "Comments": [], "label": []}
{"id": 153451, "text": "For PGN-based methods , the best method PGN-both utilizes two interactions and achieves 2.84 and 1.53 higher points on ROUGE-L for user summary and agent summary than PGN-single .", "Comments": [], "label": []}
{"id": 154373, "text": "* , one clipping factor for elements in each weight matrix ) for all weight matrices in the Transformer layers and rowwise quantization ( * i.e .", "Comments": [], "label": []}
{"id": 159631, "text": "< github.com/shtoshni/fast-coref > < github.com/raghavlite/Scalable-Coreference > The script for calculating MAC is adapted from [ https : ] ( https : //github.com/Lyken17/pytorch-OpCounter/ ) [ //github.com/Lyken17/pytorch-OpCounter/ ] ( https : //github.com/Lyken17/pytorch-OpCounter/ ) Table 3 : Results on three benchmarks .", "Comments": [], "label": []}
{"id": 151633, "text": "References A Appendix A.1 Datasets TAE Table [ 8 ] presents the data statistics of the TAE datasets we used in § [ 3.1 . ] GLUE Table [ 9 ] presents the statistics and descriptions of GLUE tasks .", "Comments": [], "label": []}
{"id": 153371, "text": "For Spanish we use parallel sentences from EuroParl corpus [ Koehn et al. , , 2005 ] , and for Portuguese we use a subset of the ParaCrawl corpus [ Bañón et al. , 2019 ] , as chosen by [ Lopes et al. , 2020 ] .", "Comments": [], "label": []}
{"id": 157856, "text": "Then , we use the labels to train the model to predict central events : More analysis can be seen in Section [ 4.5 . ] 3.4 EIG Reasoning Network In this section , we first describe a general GNNbased DECI model , then instantiate our implementation by considering causal structures .", "Comments": [], "label": []}
{"id": 153802, "text": "The standard Transformer model consists of two main components ( see the center of Figure [ 2 ] : an * encoder * and a * decoder * , each of which consists of a series of layers .", "Comments": [], "label": []}
{"id": 155881, "text": "Experiments on hierarchical question-summary generation and full summary generation show that HIBRIDS produces question-summary hierarchies of higher quality as measured by both automatic metrics and human judges , and achieves higher content coverage of summaries than competitive comparisons as reported by ROUGE .", "Comments": [], "label": []}
{"id": 156184, "text": "T5 [ Raffel ] * et al . * , [ 2019 ] A transformer-based pretrained model .", "Comments": [], "label": []}
{"id": 158472, "text": "More training details can be found in Appendix [ §A . ] 4.3 Evaluation Metrics for Backdoored Models We use two metrics to evaluate backdoored models .", "Comments": [], "label": []}
{"id": 156187, "text": "Lastly , between pretrained transformers , T5 reports higher scores over BART , probably for T5 is more extensively tuned during pre-training .", "Comments": [], "label": []}
{"id": 157657, "text": "For all the models , we employ beam search with a beam size of 5 for decoding .", "Comments": [], "label": []}
{"id": 159484, "text": "And we use a style classifier loss to control the style of generated texts [ Lee et al. , , 2021 ] .", "Comments": [], "label": []}
{"id": 158260, "text": "MBart50 is a large multilingual transformer model fine-tuned for translation on 50 languages ( 610M parameters ) .", "Comments": [], "label": []}
{"id": 151713, "text": "We use `` Scaled Dot-Product Attention '' [ Ashish et al. , 2017 ] to propagate information among different attribute values .", "Comments": [], "label": []}
{"id": 158913, "text": "4.2 Wass-to-Data : A data-driven scenario In this scenario , instead of using a single reference distribution , we use a set of reference source mass distributions , Rx , obtained with the same model .", "Comments": [], "label": []}
{"id": 157514, "text": "We hope this could shed light on the capability of Transformer-based LMs in addition to providing large training datasets or scaling up the size of these models .", "Comments": [], "label": []}
{"id": 155220, "text": "In particular , the state-of-the-art transformer models ( e.g. , BERT , RoBERTa ) require great time and computation resources .", "Comments": [], "label": []}
{"id": 158459, "text": "However , unlike [ Fincke et al. , 2022 ] that uses an integer string to represent features in a categorical style , we use a naturallanguage-styled indicator to better exploit the semantics of the condition .", "Comments": [], "label": []}
{"id": 151699, "text": "Therefore , we used crowdsourcing to annotate our data .", "Comments": [], "label": []}
{"id": 153931, "text": "For trade-off of training speed and fair comparison with BERT ( batch size 256 and 1,000,000 training steps ) , we use batch size of 1024 and 200,000 training steps for GLMLarge .", "Comments": [], "label": []}
{"id": 152705, "text": "This student is randomly initialized and denoted by Transformer .", "Comments": [], "label": []}
{"id": 152965, "text": "First , we used the retriever from [ Izacard and Grave , 2021a ] .", "Comments": [], "label": []}
{"id": 159009, "text": "For reporting the results , we try four values of Table 6 : Summary of the black-box adversarial attacks : Comparing the adversarial attacks we use in this work along with related information such as attach type , human perceptibility , and an example input for each attack .", "Comments": [], "label": []}
{"id": 154198, "text": "We use the word2vec embeddings [ Mikolov et al. , 2013a ] [ , b ] of [ Loukas et al. , 2021 ] . bert : This is similar to bilstm , but now we finetune bert-base [ Devlin et al. , , 2019 ] to extract contextualized embeddings of subwords .", "Comments": [], "label": []}
{"id": 159286, "text": "Besides it , another three systems are obtaining relatively large models : T5-1.1-XL_SSM ( 45.27GB ) [ Roberts et al. , , 2020 ] , UnitedQA ( 8.36GB ) [ Cheng et al. , 2021 ] and R2-D2+reranker ( 5.16GB ) [ Faj ] [ cik et al. , , 2021 ] , while the system with the smallest model ( 0.04GB ) is achieved by RePAQ-base [ Lewis et al. , , 2021 ] .", "Comments": [], "label": []}
{"id": 155018, "text": "Table [ 4 ] shows the comparison between Transformer and our mGPT model with MSP on the En- We used the * transformer-big * setting .", "Comments": [], "label": []}
{"id": 154161, "text": "For each task in the experiments , we use the same model architecture and train it with different objectives ( * i.e. , * MLE , AGG , UL ) .", "Comments": [], "label": []}
{"id": 153617, "text": "2 Related Work Dense Retrieval Transformer LM has advanced the state-of-the-art of many NLP tasks [ Devlin et al. , 2019 ] [ Liu et al. , , 2019 ] [ Yang et al. , , 2019 ] [ Lan et al. , , 2020 ] including dense retrieval .", "Comments": [], "label": []}
{"id": 155023, "text": "For the inference time , we found longer prompts do not significantly affect the decoding speed on GPUs as the computation of attention layers are highly parallel , which is also consistent with the findings of [ Li and Liang , 2021 ] .", "Comments": [], "label": []}
{"id": 154856, "text": "To be able to update the parameters using backpropagation , we use the reparameterization trick [ Kingma and Welling , 2014 ] to sample z from qφ : The decoder pθ ( p|z , v ) is also implemented by a single-layer unidirectional RNN .", "Comments": [], "label": []}
{"id": 154353, "text": "Our contributions are summarized as follows : 2 Related work There has been much prior literature on improving the efficiency of Transformers .", "Comments": [], "label": []}
{"id": 152317, "text": "For comparison , we train our DifferSum ( 12 layers ) from scratch , and each document is truncated to 2000 BPE tokens too .", "Comments": [], "label": []}
{"id": 156879, "text": "Prompt-tuning prepends learnable prompting vectors E to K ( x ) j and V ( x ) j to modify the attention distribution as well as the output x of the j-th layer as follows : where x denotes the output of layer , Attn ( · ) represents the attention operation in the transformer , and Cat ( · ) is the concatenation function .", "Comments": [], "label": []}
{"id": 152281, "text": "We use the default hyperparameters of BERT .", "Comments": [], "label": []}
{"id": 151774, "text": "Then we used the alignment model to force-align the monolingual sentences and the synthetic target sentences .", "Comments": [], "label": []}
{"id": 158893, "text": "We use the predictions to determine whether to buy or sell a stock after n days .", "Comments": [], "label": []}
{"id": 151477, "text": "Threat Model : To ensure reliable vulnerability analysis , we use a white-box attack setting where the attacker has full knowledge of the target KGE model [ Joseph et al. , , 2019 ] .", "Comments": [], "label": []}
{"id": 157135, "text": "However , identifying appropriate sparse masks can be burdensome : fine-tuning a large pre-trained language model like GPT-3 for just one step consumes at least 1.2TB of VRAM and requires 96 pieces of NVIDIA Tesla [ Hu et al. , , 2021 ] , and these methods either require access to pre-trained weights or introduce additional learnable coefficients ( such as importance scores of attention heads ) .", "Comments": [], "label": []}
{"id": 154090, "text": "We show that our model is robust to data scarcity , exceeding previous state-of-the-art performance using only 50 % of the available training data and surpassing BLEU , ROUGE and METEOR with only 40 labelled examples .", "Comments": [], "label": []}
{"id": 153698, "text": "We compare this performance against the T5-large and GPT-2 large model finetuned on only 574 cancer examples ( GPT-2 large + sup and T5-large + sup ) , and observe that this leads to a performance increase of up to * 43.49 * % , achieving similar F1 performance to our domains with full data supervision .", "Comments": [], "label": []}
{"id": 155087, "text": "We use cosine similarity as the sim ( ) function and τ is the temperature parameter to scale the loss , and we use τ = 0.05 .", "Comments": [], "label": []}
{"id": 157483, "text": "Evaluation and Finetuning We used similar , but not identical , scoring functions to evaluate masked and autoregressive language models .", "Comments": [], "label": []}
{"id": 158730, "text": "For all experiments , we use AdamW optimizer [ Loshchilov and Hutter , 2017 ] with base learning rate of 10− and weight decay of 10− .", "Comments": [], "label": []}
{"id": 152036, "text": "7 Conclusion We investigated the main reasons for Transformer training failure in document-level MT , finding that target-to-source attention is a key factor .", "Comments": [], "label": []}
{"id": 154256, "text": "As a second baseline for style transfer , we use UNMT [ Lample et al. , , 2018 ] , an unsupervised machine translation framework that demonstrates high performance for sentiment transfer .", "Comments": [], "label": []}
{"id": 159830, "text": "We use a processed version of NYT [ Zeng et al. , , 2018 ] containing three overlapping entity types ( * LOC , PER , ORG * ) and 24 relation types .", "Comments": [], "label": []}
{"id": 152612, "text": "Our model is able to achieve stateof-the-art results for a 6 layer transformer model on the GLUE leaderboard .", "Comments": [], "label": []}
{"id": 157882, "text": "Then , we employ T5 to incrementally turn the dialogue flow into a dialogue .", "Comments": [], "label": []}
{"id": 156099, "text": "To provide a baseline with a smaller model for future research , here we include the results of PATHFID+ with T5-base initialization using the same setting reported in Table [ 6 ] in the main paper .", "Comments": [], "label": []}
{"id": 154884, "text": "1 Introduction Contextualized word representations provided by models such as BERT [ Devlin et al. , , 2019 ] , RoBERTa [ Liu et al. , , 2019 ] , GPT3 [ Brown et al. , , 2020 ] , T5 [ Raffel et al. , , 2020 ] and more , were shown in recent years to be a critical component for obtaining state-of-the-art performance on a wide range of Natural Language Processing ( NLP ) tasks , from surface syntactic tasks as tagging and parsing , to downstream semantic tasks as question answering , information extraction and text summarization .", "Comments": [], "label": []}
{"id": 156153, "text": "Additionally , using a text-totext transformer model adds to the problem since it is usually quite expensive to run at inference time .", "Comments": [], "label": []}
{"id": 153907, "text": "the probability of generating the span s is factorized as : We implement the autoregressive blank infilling objective with the following techniques .", "Comments": [], "label": []}
{"id": 157809, "text": "In this work we introduce the SPan-Selective Linear Attention Transformer , short SPLAT , a novel architecture designed to achieve better generalization , robustness and efficiency in DST than existing approaches .", "Comments": [], "label": []}
{"id": 152058, "text": "We validate the effectiveness and superiority of our approaches on the Transformer [ Vaswani et al. , , 2017 ] , and conduct experiments on large-scale WMT14 English-to-German , WMT19 Chinese-to-English , and WMT14 English-to-French translation tasks .", "Comments": [], "label": []}
{"id": 156069, "text": "This might be because the bpe split is trained on clean data only .", "Comments": [], "label": []}
{"id": 151601, "text": "Others aim to reduce the training time of transformer-based models via large-batch training and GPU model parallelism [ You et al. , , 2020b ] [ Shoeybi et al. , , 2019 ] .", "Comments": [], "label": []}
{"id": 159610, "text": "We use equal amounts of synonyms and antonyms ( a run with 20 samples includes 10 synonyms and 10 antonyms ) .", "Comments": [], "label": []}
{"id": 158262, "text": "This difference in terms of BLEU is expected , as beam search is a heuristic search strategy , while our method is a decoding algorithm .", "Comments": [], "label": []}
{"id": 157133, "text": "For example , T0 [ Sanh et al. , , 2021 ] , FLAN-T5 [ Chung et al. , , 2022 ] , and OPT-IML [ Iyer et al. , , 2022 ] are all trained on several tasks in our paper .", "Comments": [], "label": []}
{"id": 159681, "text": "We suspect this is due to the fact that GPT3 was pretrained on much more ( English ) language data than T5 , giving GPT3 a greater ability to produce grammatically coherent and permissible English sentences , and likely also a better sense of common disfluencies .", "Comments": [], "label": []}
{"id": 151360, "text": "Clark , Christopher Berner , Sam McCandlish , A .", "Comments": [], "label": []}
{"id": 158365, "text": "Experimented with sequences of image-label pairs , [ Chan et al. , 2022 ] find that a skewed class distribution ( high burstiness ) and a large number of rarely occurring classes in training data promote the ICL ability of Transformer models [ Vaswani et al. , , 2017 ] .", "Comments": [], "label": []}
{"id": 155715, "text": "The first enhancement considers the hidden states of multiple positions and multiple transformer layers when determining the probability in each softmax ; the second enhancement uses different contextualized embeddings to compute the probabilities of different subsets of words in the vocabulary .", "Comments": [], "label": []}
{"id": 155622, "text": "We used 50 labeled samples per class for all stop-criteria except for Val-stopadd ( 25 ) . * Note that the Val-stopadd ( 25 ) has an unfair advantage : for each class , it used 25 additional labeled samples for validation while using 50 labeled samples for training .", "Comments": [], "label": []}
{"id": 152797, "text": "Table 2 : The DoKTra framework 's main experimental results . ( ft : fine-tuned ) 4.2 Experimental details For the experiments , we used the pre-trained BioBERT-base model ( L=12 , H=768 , A=12 ) as the initial teacher model .", "Comments": [], "label": []}
{"id": 156158, "text": "A Depth Dataset Details For training and evaluation of FAIRR and ProofWriter , we use the D * datasets and the ParaRules dataset [ Clark et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 154115, "text": "For baselines we use the ALBERTlarge [ Lan et al. , , 2019 ] and the RoBERTalarge [ Liu et al. , , 2019 ] models fine-tuned using their respective hyperparameters .", "Comments": [], "label": []}
{"id": 155917, "text": "We use macro F1-score as the evaluation metric .", "Comments": [], "label": []}
{"id": 155831, "text": "In practice , we use a RoBERTaBASE [ Liu et al. , , 2019 ] model fine-tuned on a 5-class sentiment classification dataset other than the few-shot datasets we evaluate on .", "Comments": [], "label": []}
{"id": 154825, "text": "4.2 Experiments on Deep Networks In this part , we build a Transformer [ Vaswani et al. , , 2017 ] with our Lorentz components introduced in [ §3 . ]", "Comments": [], "label": []}
{"id": 155932, "text": "Typically , this can be achieved by defining a scoring function S : R → R ( which we allow to depend on additional parameters , thus making it trainable ) that assigns a usefulness score to every embedding vector , and putting Next , we use our soft top-k operator Γ : R × × R → R k×d to reduce the number of embeddings from The encoder layer is followed by representation pooling .", "Comments": [], "label": []}
{"id": 155048, "text": "We use entities of new classes to construct corresponding prototypes .", "Comments": [], "label": []}
{"id": 159189, "text": "For BLEURT , we use * BLEURT-20 * . 3.1 Distribution of Omission Information To explain the importance of the omission problem , we answer the following two questions .", "Comments": [], "label": []}
{"id": 153251, "text": "Here , we use the pre-trained RoBERTa-base model . 5 negative samples are randomly chosen for each claim during training .", "Comments": [], "label": []}
{"id": 152103, "text": "To tackle the aforementioned two challenges , in this paper , we propose a Heterogeneous Graphbased Interaction Model with a Tracker ( GIT ) for document-level EE .", "Comments": [], "label": []}
{"id": 156167, "text": "K Hyperparameters We use RoBERTa-large models [ Liu et al. , , 2019 ] to model the rule selector and fact selector in FAIRR .", "Comments": [], "label": []}
{"id": 158618, "text": "A DistilBERT is a distillation version of BERT [ Devlin et al. , , 2019 ] model that contains multiple transformer encoder layers to extract features .", "Comments": [], "label": []}
{"id": 160014, "text": "The parameters of the pre-trained model were frozen and the weighted sum of the outputs of the 12 Transformer encoder blocks was used as the speech embeddings and fed into the downstream model .", "Comments": [], "label": []}
{"id": 153385, "text": "Table [ 1 ] shows an example to highlight this difference between OBPE and BPE .", "Comments": [], "label": []}
{"id": 159957, "text": "S2SpecT We build a direct S2ST model that predicts spectrogram with a single Transformer decoder , similar to [ Lee et al. , 2022a ] ( Figure [ 2a ] .", "Comments": [], "label": []}
{"id": 151721, "text": "For * Real * datasets , we remove matching pairs with large Jaccard similarity ( 0.32 for Phone , 0.36 for others ) and non-matching pairs with small Jaccard similarity ( 0.3 for Phone , 0.332 for others ) . 4.1.2 Baselines We implement 3 variants of our methods with different KAT Induction algorithms .", "Comments": [], "label": []}
{"id": 156808, "text": "We utilize its quantitative semantic role labels .", "Comments": [], "label": []}
{"id": 157010, "text": "We set the learning rate at 5e-5 for T5 models and 1e-5 for BARTs .", "Comments": [], "label": []}
{"id": 151595, "text": "By slimming the self-attention and fullyconnected sub-layers inside a transformer , we are the first to identify * structured * winning tickets in the early stage of BERT training .", "Comments": [], "label": []}
{"id": 157376, "text": "In the test dataset , we used data points from all six data sources . [ 2 ] We note that our test set contains all data points from the two new sources , which are more recent , making them even more challenging .", "Comments": [], "label": []}
{"id": 152311, "text": "From the results shown in Table [ 5 , ] it is obvious that DifferSum is better in relevance compared with others . 5.5 Trigram Blocking Strategy Trigram Blocking leads to a great improvement on all ROUGE metrics for many extractive approaches [ Liu and Lapata , 2019 ] [ Wang et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 158865, "text": "We used a batch size of 32 .", "Comments": [], "label": []}
{"id": 157761, "text": "Ziegler , Jeffrey Wu , Clemens Winter , Christopher Hesse , Mark Chen , Eric Sigler , Mateusz Litwin , Scott Gray , Benjamin Chess , Jack Clark , Christopher Berner , Sam McCandlish , Alec Radford , Ilya Sutskever , and Dario Amodei .", "Comments": [], "label": []}
{"id": 157755, "text": "As in [ Gunel et al. , 2021 ] ; [ Khosla et al. , 2020 ] ; [ Cao and Wang , 2021a ] , we use contrastive learning to minimize the latent distance between pairs of positive summaries vis-a-vis negative ones : Table 3 : Methods to create negative and positive candidates in support of relevance and faithfulness calibration , respectively .", "Comments": [], "label": []}
{"id": 159705, "text": "We use the subset of the PAWS dev and test sets translated to six other languages by professional translators , dubbed as PAWS-X [ Yang et al. , , 2019 ] for evaluation , while using the PAWS set for training .", "Comments": [], "label": []}
{"id": 154522, "text": "As for Multi-News , the best results have been achieved with the RELAX gradient estimator , with improvements of up to +1.32 ROUGE-1 pp and +3.17 METEOR pp over the NLL baseline .", "Comments": [], "label": []}
{"id": 158746, "text": "The visual encoder encodes the input image v to its semantic representation V , which is then fed into the shared encoder-decoder backbone network ( a standard Transformer ) for translation .", "Comments": [], "label": []}
{"id": 159925, "text": "To this end we compare our proposed HyperMixer model to a range of other MLP-based models , and Transformers .", "Comments": [], "label": []}
{"id": 160164, "text": "In this paper , we adopt this approach to approximate the inference mode due to its training stability and low training cost .", "Comments": [], "label": []}
{"id": 153715, "text": "[ 1 ] For inference , we use off-the-shelf T5 [ Raffel et al. , 2019 ] and GPT-3 , as well as finetuned models that are state-of-the-art on each dataset , including UnifiedQA ( UQA ) [ Khashabi et al. , , 2020 ] and Unicorn [ Lourie et al. , , 2021 ] .", "Comments": [], "label": []}
{"id": 156772, "text": "For multiple choice , we use InfoNCE [ Oord et al. , , 2018 ] to encourage the cosine similarity of the cartoon/correct answer to be higher than the incorrect ones .", "Comments": [], "label": []}
{"id": 160401, "text": "The implementations of Textless model , XM Transformer , HuBERT and Vocoder are open sourced under MIT license .", "Comments": [], "label": []}
{"id": 159358, "text": "We use F1-Rouge [ Lin , 2004b ] to evaluate the performance , including Rouge-1 , Rouge-2 and Rouge-L .", "Comments": [], "label": []}
{"id": 158608, "text": "Commonsense Inference Modeling ( COMET ) .", "Comments": [], "label": []}
{"id": 157760, "text": "Diversity may help calibration with increased exploration and smooth out some noise from ROUGE / BERTScore defined rankings .", "Comments": [], "label": []}
{"id": 155488, "text": "We use the dataset provided by CodeXGLUE team [ Lu et al. , , 2021 ] for this task .", "Comments": [], "label": []}
{"id": 159391, "text": "VILT [ Kim et al. , , 2021 ] : adopts linear projection and word embedding as the visual and textual encoders , and uses the visual transformer as the crossmodal encoder to align and fuse the features of both modalities in an end-to-end manner .", "Comments": [], "label": []}
{"id": 155923, "text": "] ( https : //developer.nvidia.com/nvidia-triton-inference-server ) Therefore we think that LT is an interesting alternative for fast and scalable few-shot learning .", "Comments": [], "label": []}
{"id": 151904, "text": "A clause graph is built with the clause representations Cˆ used to initialise the graph nodes .", "Comments": [], "label": []}
{"id": 152006, "text": "The results are shown in Figure [ 7a . ] Unlike Transformer , which fails to train on long input , G-Transformer shows stable scores for inputs containing 512 and 1024 tokens , suggesting that with the help of locality bias , a long input does not impact the performance obviously .", "Comments": [], "label": []}
{"id": 152052, "text": "We copy the parameters of the multi-head attention in Transformer to the group multi-head attention in G-Transformer , leaving the global multi-head attention and the gates randomly initialized .", "Comments": [], "label": []}
{"id": 153085, "text": "Randomly initialized Transformers with the same architecture as PLMs are common baselines in the community .", "Comments": [], "label": []}
{"id": 152168, "text": "We set the weight decay rate , the momentum , the batch size , and the number of epochs to be 10− , 0.5 , 32 , and 100 respectively , especially we use batch size 64 on the GENIA dataset .", "Comments": [], "label": []}
{"id": 151697, "text": "For the first time , we use comparative annotations to detect offensive language .", "Comments": [], "label": []}
{"id": 155448, "text": "We used the final layer of the decoder feedforward network as the datastore key .", "Comments": [], "label": []}
{"id": 151974, "text": "Such back-and-forth oscillation of the attention range may also result in unstable training and slow down the training process . 3.3 Conclusion The above experiments show that training failure on Transformer can be caused by local minima .", "Comments": [], "label": []}
{"id": 153259, "text": "We use UnifiedQAlarge , which is comparable with our synthesis and reasoning model ( ISM w/ VinVL ) in size .", "Comments": [], "label": []}
{"id": 154314, "text": "Following prior studies [ Rao and Tetreault , 2018 ] [ Zhang et al. , , 2020 ] [ Chawla and Yang , 2020 ] [ Lai et al. , 2021 ] on FST , we employ the supervised baseline as a seq2seq encoder-decoder model that directly learns the conditional probability P ( y|x ) from parallel corpus D comprising ( x , y ) pairs .", "Comments": [], "label": []}
{"id": 154296, "text": "This problem setting is an important application While the literature primary utilizes the term * style transfer * , we adopt the more general term * attribute * as suggested by [ Jin et al. , 2020a ] .", "Comments": [], "label": []}
{"id": 159409, "text": "For instance , we observe that the Treu scores for e-SNLI with T5 and BART models are both negative , indicating that the helpfulness of explanations in e-SNLI could be limited .", "Comments": [], "label": []}
{"id": 159960, "text": "We use the same reduction factor as S2SpecT .", "Comments": [], "label": []}
{"id": 158576, "text": "These experiments demonstrate that CAT has a strong capability in conceptualizing CSKBs , and better conceptualization modeling can help populate more novel and diverse commonsense knowledge and thus help commonsense modeling ( COMET ) .", "Comments": [], "label": []}
{"id": 153496, "text": "Great efforts have been made to develop Transformer 's variants for long-range sequence modeling tasks .", "Comments": [], "label": []}
{"id": 158916, "text": "For those experiments , we use the RO-EN ( mid-resource ) and NE-EN ( low-resource ) translations from the MLQE-PE dataset [ Fomicheva et al. , , 2022 ] .", "Comments": [], "label": []}
{"id": 152828, "text": "To ensure that the random sampling of Wikipedia paragraphs has a similar distribution , we employ the learned passage selection model from [ Lewis et al. , 2021 ] , [ 4 ] .", "Comments": [], "label": []}
{"id": 153162, "text": "Specifically , for aspect extraction , we employ the pre-trained model from a wellknown Named Entity Recognition ( NER ) tool for tweets [ Ritter et al. , , 2011 ] to perform NER on each tweet in the dataset , and regard the recognized entities as aspect terms .", "Comments": [], "label": []}
{"id": 157201, "text": "During finetuning , we use the following hyperparameters : learning rate of 5e-5 with linear decay 0.0006 and batch size 256 .", "Comments": [], "label": []}
{"id": 156287, "text": "For instance , it makes little sense to use ROUGE for the evaluation of abstractive summarization systems ( which are becoming the norm ) , or whenever the generated text paraphrases the original text .", "Comments": [], "label": []}
{"id": 159338, "text": "Although GHT-PS reduces 32.1 % parameters , it significantly outperforms both 44M and 30M vanilla transformers , which is comparable to GHT on all datasets .", "Comments": [], "label": []}
{"id": 155600, "text": "As presented in table [ 1 , ] training only S gets better results than existing indiscriminate training , while training only S gets worse results than vanilla transformer .", "Comments": [], "label": []}
{"id": 153174, "text": "We employ a model explainability tool to explore the features that characterize hedges in peer-tutoring conversations , and we identify some novel features , and the benefits of such a hybrid model approach .", "Comments": [], "label": []}
{"id": 153668, "text": "Here , we use Misinfo Reaction Frames to understand how the reader perceives and reacts to the headline .", "Comments": [], "label": []}
{"id": 159167, "text": "We use the subscript `` W '' to emphasize that this finetuning is done on the weakly annotated data and to distinguish it from the fine-tuning experiments in Section [ 6 ] which are done on clean data .", "Comments": [], "label": []}
{"id": 157157, "text": "For constructing the heterogeneous commonsense cognition graph GCS , we use the utterance set U and the commonsense cognition knowledge set KCS = S i=0 S ∈R i as vertices , i.e. , vertex set VCS = U ∪ KCS .", "Comments": [], "label": []}
{"id": 157887, "text": "Besides , we use U = [ u t 1 , u 2 , . . . ] to denote all the utterances in a dialogue .", "Comments": [], "label": []}
{"id": 158033, "text": "We use standard natural language generation ( NLG ) metrics such as BLEU [ Papineni et al. , 2002 ] and ROUGE [ Lin , 2004 ] , etc . to measure the overlap between the output and reference .", "Comments": [], "label": []}
{"id": 158214, "text": "In subsequent pre-training and downstream tasks , we use general sequences as input when the goal is to profile the characters broadly , e.g. , estimating ideology .", "Comments": [], "label": []}
{"id": 157418, "text": "We have observed that T5 achieves better results than BART and PEGASUS in most transfer settings .", "Comments": [], "label": []}
{"id": 157095, "text": "BLEU [ Papineni et al. , , 2002 ] , METEOR [ Banerjee and Lavie , 2005 ] , and ROUGE [ Lin , 2004 ] are selected as NLG Metrics , and we use the MS-COCO caption evaluation tool [ 7 ] to compute the results .", "Comments": [], "label": []}
{"id": 151935, "text": "Formally , we use the following query positions in the pointer network : where M is the set of masked positions .", "Comments": [], "label": []}
{"id": 155073, "text": "We propose to pre-train the Transformer model with such automatically generated program contrasts to better identify similar code in the wild and differentiate vulnerable programs from benign ones .", "Comments": [], "label": []}
{"id": 155656, "text": "In this comparison , we apply the same training settings with Transformer-base models except that learning rate is set to 5e − 5 .", "Comments": [], "label": []}
{"id": 155908, "text": "For the multilingual experiments , we trained a model using the code [ https : //code.google.com/archive/p/ ] ( https : //code.google.com/archive/p/word2vec ) [ word2vec ] ( https : //code.google.com/archive/p/word2vec ) < https : //tinyurl.com/pp-mpnet > < https : //tinyurl.com/nli-mpnet > Table 1 : Overview of the evaluated datasets .", "Comments": [], "label": []}
{"id": 152655, "text": "We used soft scores for computing AP and binary scores for computing FPR .", "Comments": [], "label": []}
{"id": 152929, "text": "4.2 Evaluation method We employ the enriched MuST-SHE corpus to assess generic performance and gender translation at several levels of granularity .", "Comments": [], "label": []}
{"id": 157679, "text": "3.1.3 Reproduction Process In reproducing results themselves , students were required to use the same GPU computing resources within our university 's shared cluster to control for the potential effect of computing infrastructure on results .", "Comments": [], "label": []}
{"id": 157572, "text": "Q2 : * Does mixing features and labels in the first stage lead to better label distribution learning in the second ? * We use the label distributions obtained from the first-stage models from Q1 as feedback for supervised learning .", "Comments": [], "label": []}
{"id": 152735, "text": "We propose a general framework with first a learned prefix-to-program prediction module , and then a simple yet effective thresholding heuristic for subprogram selection for early execution .", "Comments": [], "label": []}
{"id": 151567, "text": "For GPT-2 [ Radford et al. , , 2019 ] , we use the 117M parameter model , taking 5 hours to train .", "Comments": [], "label": []}
{"id": 157846, "text": "[ Do et al. , 2022 ] showed that by training the T5 model with the whole context and the shortened conversational history , the performance of the model is improved .", "Comments": [], "label": []}
{"id": 152047, "text": "C G-Transformer C.1 Training Settings We generate the corresponding group tag sequence dynamically in the model according to the special sentence-mark tokens < s > and < /s > .", "Comments": [], "label": []}
{"id": 154691, "text": "Inspired by [ Tsuruoka and Tsujii , 2005 ] , we devise a greedy heuristic for decoding y using the same first order Markov assumptions but with bidirectional decoding .", "Comments": [], "label": []}
{"id": 159474, "text": "We use data where both the source and target languages are in the same group . requirements constant .", "Comments": [], "label": []}
{"id": 155638, "text": "For all models , we generate stories using nucleus sampling [ Holtzman et al. , ] [ https : //github.com/DoodleJZ/ ] ( https : //github.com/DoodleJZ/HPSG-Neural-Parser ) [ HPSG-Neural-Parser ] ( https : //github.com/DoodleJZ/HPSG-Neural-Parser ) We use the preprocessed data in https : //github.com/thucoai/Stylized-Story-Generation-with-Style-Guided-Planning Table 2 : Automatic evaluation results for the conditional text generation task on Rocstories dataset .", "Comments": [], "label": []}
{"id": 156897, "text": "Consequently , we define two evaluation tasks for this agent : We use the LED agent [ Beltagy et al. , , 2020 ] to conduct this experiment .", "Comments": [], "label": []}
{"id": 155317, "text": "The question answering models used in our experiments are ALBERT [ Lan et al. , , 2020 ] , AllenAI-Document-QA [ Clark and Gardner , 2018 ] , BERT ( cased/uncased ) [ Devlin et al. , , 2019 ] , Big Bird [ Za ] [ heer et al. , , 2020 ] , DeBERTa ( large ) [ He et al. , , 2021 ] , ELECTRA [ Clark et al. , , 2020 ] , Funnel-Transformer [ Dai et al. , , 2020 ] , MPNet [ Song et al. , , 2020 ] , and RoBERTa ( base/large ) [ Liu et al. , , 2019 ] .", "Comments": [], "label": []}
{"id": 158145, "text": "We use the hidden representations of [ E11 ] and [ E21 ] as the representations of head and tail entities .", "Comments": [], "label": []}
{"id": 153111, "text": "We implement these ideas and release several datasets and models in an open-source library OpenHands with the following key contributions : 1 .", "Comments": [], "label": []}
{"id": 151549, "text": "LoopCAG methods Our model comprises of four components : 1 ) the transformer encoderdecoder framework ; 2 ) the trace input ; 3 ) Attention Guidance ( +AG for short ) grounding loss ; 4 ) Contrastive constraints ( +C for short ) .", "Comments": [], "label": []}
{"id": 157539, "text": "GPT3 + fine-grained steps ( + ordered marker ) : The exemplar we use is as shown in Figure [ 4 . ] GPT3 + callable programs : The exemplar is shown in Figure [ 6 . ] DeBERTa / T5 : The training data follows the format of the exemplar for GPT3 .", "Comments": [], "label": []}
{"id": 158642, "text": "For briefings , we use F ij ∼T to represent the fMRI series of length T starting at the k th frame { f ij k , fij k+1 , ... , fij k+T −1 } .", "Comments": [], "label": []}
{"id": 152224, "text": "To shed light on the contextualization process of Transformers , we have analyzed their performance across layers .", "Comments": [], "label": []}
{"id": 153514, "text": "Even so , among all compared Transformer variants , FSAT achieves promising results in time and memory efficiency .", "Comments": [], "label": []}
{"id": 155812, "text": "Previous researches [ Gu et al. , , 2018 ] [ Ma et al. , , 2019 ] [ Qian et al. , 2021a ] [ Bao et al. , , 2021 ] always utilize a Transformer model as a teacher for training NAT models , namely sequence-level knowledge distillation [ Kim and Rush , 2016 ] , which can Table 3 : BLEU scores of NAT models trained with ( or without ) knowledge distillation ( KD ) on translation tasks .", "Comments": [], "label": []}
{"id": 158944, "text": "Alignment and Uniformity Following previous works [ Wang and Isola , 2020 ] , we use alignment and uniformity to measure the quality of representation space .", "Comments": [], "label": []}
{"id": 152707, "text": "For Transformer , the hyperparameters of the Adam optimizer is a bit different , and we use β = 0.9 , β = 0.98 .", "Comments": [], "label": []}
{"id": 152651, "text": "To this end , we used the ERASER benchmark [ DeY ] [ oung et al. , , 2020 ] , which contains multiple tasks for which important spans of the input text have been highlighted as supporting evidence ( aka `` rationale '' ) by human .", "Comments": [], "label": []}
{"id": 158580, "text": "Commonsense Inference Modeling ( COMET ) .", "Comments": [], "label": []}
{"id": 157780, "text": "Unlike the following datasets we use , the annotation style excludes singleton clusters .", "Comments": [], "label": []}
{"id": 157681, "text": "< https : //slurm.schedmd.com/ > 5 Papers identified at reviewers ' request .", "Comments": [], "label": []}
{"id": 157686, "text": "This hands-on experience could contribute to their ability to reproduce results from the selected papers , each of which applied transformer-based models .", "Comments": [], "label": []}
{"id": 157414, "text": "Therefore , we used three commonly used encoder-decoder style pretrained LMs : T5 [ Raffel et al. , , 2020 ] , BART [ Lewis et al. , 2020 ] , and PEGASUS [ Zhang et al. , , 2020 ] , as baselines .", "Comments": [], "label": []}
{"id": 159096, "text": "The experimental setting of the skipgram model is the same as the background Word2Vec model .", "Comments": [], "label": []}
{"id": 151675, "text": "For multilingual vocabulary , we follow the shared BPE [ Sennrich et al. , 2016 ] vocabulary of [ Lin et al. , 2020 ] , which includes 59 languages .", "Comments": [], "label": []}
{"id": 155555, "text": "The transformer architecture based on BERT [ Devlin et al. , , 2019 ] has been adopted to re-ranking tasks by using BERT 's [ CLS ] token representation to summarize query and document interactions [ Nogueira and ] [ Cho , 2019 ] [ Yang et al. , , 2019 ] [ Dai and Callan , 2019 ] [ Nogueira et al. , , 2019a ] [ Li et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 160403, "text": "Hence , we concluded that MASSAlign is the most suitable aligner for our use case as it i ) produces n : m < https : //huggingface.co/sentence-transformers/LaBSE > [ https : //huggingface.co/T-Systems-onsite/cross-en-de ] ( https : //huggingface.co/T-Systems-onsite/cross-en-de-roberta-sentence-transformer ) [ roberta-sentence-transformer ] ( https : //huggingface.co/T-Systems-onsite/cross-en-de-roberta-sentence-transformer ) More details on our adaptations can be found in [ Ap ] [ pendix H. ] n and m are > 1 in this context where n or m equals to 1 in this context , but not both alignments and ii ) has fairly high scores for 1:1 and n : m alignments .", "Comments": [], "label": []}
{"id": 157167, "text": "For each u ∈ U , we infer the commonsense knowledge about emotional reaction KxReact k xReact i,1 , . . . , kxReact i , l using COMET , which is regarded as the user 's possible emotional reaction to the current cognitive situation .", "Comments": [], "label": []}
{"id": 154341, "text": "For the mask predict approach , we use the original implementation of X-FACTR [ Jiang et al. , , 2020a ] , and set the beam size and the number of masks to 5 .", "Comments": [], "label": []}
{"id": 153261, "text": "We use 1 ) VQGAN [ Esser et al. , , 2021 ] , which takes in a vector , and outputs a highresolution image ; and 2 ) CLIP [ Radford et al. , , 2021 ] , which can encode both text and images , and map them into a multi-modal embedding space .", "Comments": [], "label": []}
{"id": 152778, "text": "All metrics are computed on the corresponding test set . ter the first DPR is trained , we used it to retrieve passages from a joint index of text+structured knowledge .", "Comments": [], "label": []}
{"id": 151930, "text": "mT5 [ Xue et al. , , 2020 ] learns a multilingual version of T5 [ Raffel et al. , , 2020 ] with text-totext tasks .", "Comments": [], "label": []}
{"id": 154629, "text": "Specifically , we use the RGB values ( 240 , 0 , 30 ) and transparency 127/255 that [ Yao et al. , 2021 ] say works best with their method .", "Comments": [], "label": []}
{"id": 151881, "text": "To verify if our predicted focal entities are meaningful , we use two concrete examples to conduct a case study .", "Comments": [], "label": []}
{"id": 151680, "text": "The language pairs of En-Nl , En-Pt , and En-Pl are never observed by m-Transformer . m-Transformer sometimes achieves reasonable BLEU for X→En , e.g . 10.7 for Pt→En , since there are many similar languages in PC32 , such as Es and Fr .", "Comments": [], "label": []}
{"id": 158953, "text": "The second category of research works proposes to incorporate manifold information to help the dialogue summarization . [ Feng et al. , 2021a ] and [ Chen ] [ and Yang , 2020 ] propose using a discourse parsing tool or heuristic structure extraction method to help the model capture the dialogue structures .", "Comments": [], "label": []}
{"id": 152306, "text": "We report the ROUGE-1 , ROUGE-2 , and ROUGE-L of DifferSum by * ROUGE-1.5.5.pl * , which calculates the overlap lexical units of extracted sentences and ground-truth . 5 Results and Analysis 5.1 Results on CNN/DM Table [ 2 ] shows the results on CNN/DailyMail .", "Comments": [], "label": []}
{"id": 158420, "text": "This practice is standard for most BERT-like models , including RoBERTa , which we use for our experiments .", "Comments": [], "label": []}
{"id": 155207, "text": "In Table [ 7 , ] we list the keywords that we use to refer to these tasks along with the URLs to the datasets/tables .", "Comments": [], "label": []}
{"id": 153144, "text": "To obtain negative samples , we use a Memory Bank to obtain the embeddings from samples of recent previous batches , which is essentially a FIFO queue of fixed size .", "Comments": [], "label": []}
{"id": 153281, "text": "Under the Transformerbase setting , CBMI-based adaptive training can respectively improve +0.91 and +0.85 BLEU scores on En-De and Zh-En tasks compared to the Transformer baseline .", "Comments": [], "label": []}
{"id": 155318, "text": "The passage retrieval models used in our experiments are MonoBERT [ Nogueira and Cho , 2019 ] [ Nogueira et al. , , 2019 ] and MonoT5 [ Nogueira et al. , , 2020 ] ( both topped the MS MARCO passage retrieval leaderboard once ) , and the classic baseline models BM25 [ Robertson and Zaragoza , 2009 ] and Query Likelihood [ Ponte and Croft , 1998 ] , implemented in Anserini [ Yang et al. , , 2017 ] .", "Comments": [], "label": []}
{"id": 157407, "text": "To validate the zero-shot performance of our proposed SPARTA , we conduct extensive experiments on three conversational datasets : CoQA [ Reddy et al. , , 2019 ] , QuAC [ Choi et al. , , 2018 ] and DoQA [ Campos et al. , , 2020 ] by transferring knowl- edge from three single-turn datasets : MS MARCO [ Nguyen et al. , , 2016 ] , NewsQA [ Trischler et al. , , 2017 ] and SQuAD [ Rajpurkar et al. , , 2016 ] based on different pre-trained LMs : T5 [ Raffel et al. , , 2020 ] , BART [ Lewis et al. , , 2020 ] and PEGASUS [ Zhang et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 155450, "text": "Moreover , we used two baseline methods for example selection , tokenbased retrieval and BERT-based retrieval .", "Comments": [], "label": []}
{"id": 153696, "text": "For the misinformation detection task , we evaluate gold F1 using the Prop-BERT zero-shot model , MRFfinetuned BERT-large , Covid-BERT , T5-large and GPT-2 large models .", "Comments": [], "label": []}
{"id": 156738, "text": "BLEU , COMET , word f-measures statistically significantly higher than no-context ( p < 0.05 ) are underlined .", "Comments": [], "label": []}
{"id": 151793, "text": "The sentence alignment loss aims to force the transformer model to recognize the sentence pair , where one sentence is the translation of the other .", "Comments": [], "label": []}
{"id": 157184, "text": "Missing match The NER component detects no part of an existing NE mention . 4 Experimental Setup Dataset As our dataset , we use AIDA CoNLL-YAGO [ Hoffart et al. , , 2011 ] .", "Comments": [], "label": []}
{"id": 155162, "text": "We use a batch size of 16 .", "Comments": [], "label": []}
{"id": 156532, "text": "Review writing assistance implemented in a real reviewing system should always be accompanied by carefully designed guidelines and policies .", "Comments": [], "label": []}
{"id": 158054, "text": "For black-box tuning , we adopt the Bayesian optimization ( BO ) [ Mockus , 1974 ] in stage I to optimize the routers , and adopt the CMA-ES [ Hansen and Ostermeier , 2001 ] to optimize the selected intrinsic prompts z while freezing the projection matrices A .", "Comments": [], "label": []}
{"id": 155773, "text": "We use Figure 2 : Overall architecture of DEAM metric trained on positive ( green box ) interactions and negative ( red box ) conversations generated from AMR-based manipulations ( orange box ) I read a few of his plays when I was in school .", "Comments": [], "label": []}
{"id": 151606, "text": "Some studies [ Michel et al. , , 2019 ] [ Voita et al. , , 2019 ] find that the multi-head self-attention module of transformer can be redundant , presenting the possibility of pruning some heads from each layer of BERT without hurting model capacity .", "Comments": [], "label": []}
{"id": 156262, "text": "Results comparing to an end-to-end RG model and [ transfer-learning-conv-ai ] ( https : //github.com/huggingface/transfer-learning-conv-ai ) a knowledge-selection model are shown in Table [ 9 . ]", "Comments": [], "label": []}
{"id": 152104, "text": "In this way , GIT jointly models the entities and sentences in the document from a global perspective .", "Comments": [], "label": []}
{"id": 160229, "text": "To gently bridge the discrepancy between gold and predicted upstream results ( ED results passed to EAE , trigger/argument MI results passed to ED/EAE ) , we adopt the scheduled sampling technique to perform curriculum learning [ Bengio et al. , , 2015 ] .", "Comments": [], "label": []}
{"id": 153411, "text": "5 Conclusion In this paper , we address the problem of crosslingual transfer from HRLs to LRLs by exploiting Table 8 : Percentage rise over BPE in representation of LRL , HRL and Shared ( percentage of tokens shared between HRL and LRL weighted by frequency ) in vocabulary generated by OBPE and BPE_overSample and OBPE_overSample ( Section [ 4.4 ] .", "Comments": [], "label": []}
{"id": 152509, "text": "Note that XLnet tokenizer uses byte pair encoding .", "Comments": [], "label": []}
{"id": 152744, "text": "The uncertainty of this guess is reflected in the low probabilities of the actions , and our simple thresholding heuristic can filter out the incorrect subgraphs .", "Comments": [], "label": []}
{"id": 158118, "text": "The performance is reported in Rouge-L and the best performance is in bold . to OFASeqInstruct .", "Comments": [], "label": []}
{"id": 157564, "text": "Evaluation Metrics : We use * OIE2016 * 's , * WiRe57 * 's , and * CaRB * 's metrics for evaluation .", "Comments": [], "label": []}
{"id": 154707, "text": "Pre-training on TPU took 2.3 hours per 1000 steps .", "Comments": [], "label": []}
{"id": 157749, "text": "As opposed to contrasting positives and negatives in a latent space , these models are instructed to calibrate decoder likelihoods to ROUGE or BERTScore-defined rankings .", "Comments": [], "label": []}
{"id": 153980, "text": "We experiment with three evidence retrieval settings - 1 ) Oracle Evidence , where we use gold evidence , 2 ) Wiki-Evidence , where we use Wiki-ctx for document retrieval and Ret-with-context for evidence selection , and 3 ) DPR-Evidence , where we use DPR-WoWft-ctx for document retrieval and Ret-withcontext for evidence selection .", "Comments": [], "label": []}
{"id": 155498, "text": "In particular , we adopt a tagging head to identify the premise and a multi-way classifier for choosing operators , which is formulated as : o = sof tmax ( MLP ( hCLS ) ) .", "Comments": [], "label": []}
{"id": 153812, "text": "We use * sequence-level accuracy * as the evaluation metric : an output sequence with even just a single wrong token is considered wrong .", "Comments": [], "label": []}
{"id": 159702, "text": "We use XNLI ( Cross-Lingual Natural Language Inference ) and PAWS-X for classification and XQuAD , MLQA and TyDiQA-GP for question answering .", "Comments": [], "label": []}
{"id": 156806, "text": "We train/validate two T5-Large models based on this split for the binary classification task .", "Comments": [], "label": []}
{"id": 155292, "text": "On the CARB English benchmark we use results for baselines reported in [ Ro et al. , , 2020 ] and [ Kolluru et al. , , 2020a ] .", "Comments": [], "label": []}
{"id": 158591, "text": "However , this paper only leverages these pseudolabeled examples in the commonsense inference generation task ( COMET ) as baselines .", "Comments": [], "label": []}
{"id": 154080, "text": "Therefore , we adopt a weighted loss for training : L = P −wy ( y log ˆy + ( 1 − yi ) log ( 1 − yˆi ) ) where wy denotes the fixed weights of the two classes , which is inversely proportional to the number of instances from the corresponding class , balancing the training weights of the two classes .", "Comments": [], "label": []}
{"id": 157708, "text": "For early stopping , we utilize the package provided by Bjarten [ 1 ] .", "Comments": [], "label": []}
{"id": 153587, "text": "METER is then trained on J in an end-to-end manner . 4 Experiments and Discussions 4.1 Building Datasets To conduct experiments , we adopt two publicly available explainable recommendation datasets proposed in [ Li et al. , 2020 ] .", "Comments": [], "label": []}
{"id": 159433, "text": "To this end , and combined with the observation from [ Kudugunta et al. , 2019 ] that the translation process in Transformer models starts in the top encoder layers , we propose an architecture with shared and language-specific weights .", "Comments": [], "label": []}
{"id": 158897, "text": "Regarding the forecasting models , we evaluate several baselines , including the traditional machine learning and state-of-the-art transformer-based methods , detailed as follows .", "Comments": [], "label": []}
{"id": 153323, "text": "While smaller models converged within 100k updates , [ https : //github.com/neulab/ ] ( https : //github.com/neulab/word-embeddings-for-nmt ) [ word-embeddings-for-nmt ] ( https : //github.com/neulab/word-embeddings-for-nmt ) < https : //github.com/VProv/BPE-Dropout > [ https : //github.com/nxphi47/data_ ] ( https : //github.com/nxphi47/data_diversification ) [ diversification ] ( https : //github.com/nxphi47/data_diversification ) [ https : //github.com/pytorch/fairseq/ ] ( https : //github.com/pytorch/fairseq/blob/main/examples/translation/prepare-iwslt14.sh ) [ blob/main/examples/translation/ ] ( https : //github.com/pytorch/fairseq/blob/main/examples/translation/prepare-iwslt14.sh ) [ prepare-iwslt14.sh ] ( https : //github.com/pytorch/fairseq/blob/main/examples/translation/prepare-iwslt14.sh ) The official IWSLT17 evaluation campaign : [ https : // ] ( https : //wit3.fbk.eu/2017-01-c ) [ wit3.fbk.eu/2017-01-c ] ( https : //wit3.fbk.eu/2017-01-c ) [ https : //github.com/pytorch/fairseq/ ] ( https : //github.com/pytorch/fairseq/blob/main/examples/translation/prepare-wmt14en2de.sh ) [ blob/main/examples/translation/ ] ( https : //github.com/pytorch/fairseq/blob/main/examples/translation/prepare-wmt14en2de.sh ) [ prepare-wmt14en2de.sh ] ( https : //github.com/pytorch/fairseq/blob/main/examples/translation/prepare-wmt14en2de.sh ) Table 9 : Results on IWSLT14 De→En with baseline Transformer and CipherDAug using ALONE embeddings [ Takase and Kobayashi , 2020 ] .", "Comments": [], "label": []}
{"id": 159963, "text": "Otherwise , we use ( 6 , 6 , 2 ) .", "Comments": [], "label": []}
{"id": 157810, "text": "Unless denoted otherwise these shared target values T are special tokens [ NONE ] and [ DONTCARE ] which correspond to the `` none '' and `` dontcare '' slot values in SGD and MultiWOZ . 2.2 Joint Encoding with Linear Attention Linear Attention Transformers .", "Comments": [], "label": []}
{"id": 159052, "text": "The second task ( HT2 ) evaluates how well the generated set of key points summarises the corpus Note that only key point matching is described in their published paper , but their key point generation code can be found on Github at [ https : //github.com/manavkapadnis/ ] ( https : //github.com/manavkapadnis/Enigma_ArgMining ) [ Enigma_ArgMining ] ( https : //github.com/manavkapadnis/Enigma_ArgMining ) Table 1 : Test set ROUGE scores and the proposed set-based evaluation metrics .", "Comments": [], "label": []}
{"id": 153704, "text": "Average training time for generative models ranges from approx . 1 hour per epoch for T5-base to 4 hours for GPT-2 large .", "Comments": [], "label": []}
{"id": 151317, "text": "We use Twitter due to its international and widespread usage that ensures a sufficient database and the several challenges for the automatic identification of opinions and stance it poses from an NLP perspective [ Imran et al. , , 2016 ] [ Mohammad et al. , , 2016 ] [ Gorrell et al. , , 2019 ] [ Conforti et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 153356, "text": "Firstly , we use domainspecific soft prompts instead of hard templates to represent domain-specific knowledge .", "Comments": [], "label": []}
{"id": 151485, "text": "We use the pre-trained embeddings to group the entities o into k clusters using K-means clustering and determine a decoy entity with minimum soft truth score for each cluster .", "Comments": [], "label": []}
{"id": 155411, "text": "Further , among the MoE methods , STABLEMOE has the fastest convergence speed . 4.3.2 Multilingual Machine Translation We compare STABLEMOE with Switch Transformer , BASE Layer , Hash Layer , the standard Transformer , and a larger Transformer .", "Comments": [], "label": []}
{"id": 159230, "text": "* sam is reading on the Beach * where * on the beach * might be a name of a movie ) .", "Comments": [], "label": []}
{"id": 159080, "text": "And we use synonyms from WordNet1 , which are all in the background vocabulary .", "Comments": [], "label": []}
{"id": 154100, "text": "BLEU , ROUGE , METEOR and chrF++ [ Popovic´ , 2017 ] are widely used n-grambased methods , working at the word , subword or character level .", "Comments": [], "label": []}
{"id": 156248, "text": "We use human evaluation and focus on three dimensions : does the model generate * novel * knowledge that does not appear in ConceptNet ? does the gen- erated knowledge statement * make sense * as a standalone fact ? and is the generated knowledge * relevant * to the dialogue context ?", "Comments": [], "label": []}
{"id": 159222, "text": "We hypothesize that attention heads in a transformer when fine-tuned for NER , formulated as a token-level tagging task , tend to pay the highest attention to the most contextually relevant tokens around it .", "Comments": [], "label": []}
{"id": 151377, "text": "The proposed two-stream model uses a Cross-Transformer module similar to the self-attention structure to fuse the information of Chinese character components .", "Comments": [], "label": []}
{"id": 153676, "text": "Quality Control We use a three-stage annotation process for ensuring quality control .", "Comments": [], "label": []}
{"id": 151604, "text": "Inspired by this , we set out to explore whether there are structured winning tickets in the early stage of BERT training that can significantly accelerate language model pre-training and fine-tuning .", "Comments": [], "label": []}
{"id": 153115, "text": "Fifth , we observe that the trained checkpoints of the pose-based models can be directly integrated with pose estimation models to create a pipeline that can provide real-time inference even on CPUs .", "Comments": [], "label": []}
{"id": 151433, "text": "We use data from Fall 2019 , with 338 sessions representing 117 teachers .", "Comments": [], "label": []}
{"id": 155283, "text": "This is given to a BERT-based transformer , which outputs a hidden state for each token .", "Comments": [], "label": []}
{"id": 151882, "text": "With the advance of computing power , PLMs such as OpenAI GPT [ Radford et al. , , 2018 ] , BERT [ Devlin et al. , , 2018 ] and XLNet [ Yang et al. , , 2019 ] based on deep Transformer [ Vaswani et al. , , 2017 ] architecture demonstrate their superiority in various downstream NLP tasks .", "Comments": [], "label": []}
{"id": 160171, "text": "[ https : //dl.fbaipublicfiles.com/fairseq/ ] ( https : //dl.fbaipublicfiles.com/fairseq/wav2vec/wav2vec_small.pt ) [ wav2vec/wav2vec_small.pt ] ( https : //dl.fbaipublicfiles.com/fairseq/wav2vec/wav2vec_small.pt ) sacreBLEU signature : nrefs:1 | bs:1000 | seed:12345 | case : mixed | eff : yes | nc:6 | nw:0 | space : no | version:2.0.0 Table 6 : BLEU scores on CoVoST 2 En→De test set .", "Comments": [], "label": []}
{"id": 157474, "text": "We used an intentionally broad definition of harm : `` emotional and psychological discomfort , as well as physical violence , discrimination , bullying and cyberbullying , adverse material or financial impacts , and loss of personal or professional opportunities . ''", "Comments": [], "label": []}
{"id": 153175, "text": "Compared to that work , our rule-based classification model is directly detecting hedge classes , and we employ the predictions of the rule-based model as a feature for stronger machine learning models , designed to lessen the impact of the imbalance between classes .", "Comments": [], "label": []}
{"id": 157264, "text": "A similar line of prior work [ Dong et al. , , 2019 ] also proposed combining multiple pre-training objectives using the same Transformer model through a Cloze-type formulation .", "Comments": [], "label": []}
{"id": 152056, "text": "Experiments on WMT14 Englishto-German , WMT19 Chinese-to-English , and WMT14 English-to-French translation tasks demonstrate the effectiveness of our approach , with 1.36 , 1.50 , and 0.63 BLEU improvements , respectively , compared to the Transformer baseline .", "Comments": [], "label": []}
{"id": 153118, "text": "For sequence-based models we consider RNN and Transformer based architectures .", "Comments": [], "label": []}
{"id": 155263, "text": "Hence we use this setup in all following experiments .", "Comments": [], "label": []}
{"id": 153616, "text": "We use the recently proposed Condenser pre-training architecture , which learns to condense information into the dense vector through LM pre-training .", "Comments": [], "label": []}
{"id": 153309, "text": "CipherDAug , on the other hand , only uses a vanilla Transformer that is trained end-to-end .", "Comments": [], "label": []}
{"id": 159359, "text": "Additionally , we use the NLI model to filter out generated sentences that can be inferred from the replaced ones .", "Comments": [], "label": []}
{"id": 154435, "text": "3.1 Background : Levitated Marker Levitated marker is used as an approximation of solid markers , which allows models to classify multiple pairs of entities simultaneously to accelerate the inference process [ Zhong and Chen , 2021 ] .", "Comments": [], "label": []}
{"id": 157231, "text": "1 Introduction Transformer-based pre-trained language models ( PLMs ) , such as BERT [ Devlin et al. , , 2019 ] and RoBERTa [ Liu et al. , , 2019 ] , have aroused widespread interest among Natural Language Processing ( NLP ) researchers in recent years .", "Comments": [], "label": []}
{"id": 160167, "text": "In practice , we use HuBERT to build our systems , since we believe that developing on a strong baseline will make our results more convincing and demonstrate the robustness of our approach .", "Comments": [], "label": []}
{"id": 152079, "text": "In conclusion , our approaches improve up to +1.36 BLEU scores on En→De compared with the Transformer baseline and substantially outperforms the existing NMT systems .", "Comments": [], "label": []}
{"id": 153325, "text": "Following previous work [ Vaswani et al. , 2017 ] [ Nguyen et al. , , 2019 ] [ Xu et al. , , 2021 ] , we compute tokenized BLEU with multi_bleu.perl [ 22 ] for IWSLT14 and TED datasets , additionally apply compound-splitting for WMT14 En-De [ 23 ] and SacreBLEU [ Post , 2018 ] ( Signature : nrefs:1|case : mixed| eff : no|tok:13a|smooth : exp|version :2.0.0 for IWSLT17 datasets .", "Comments": [], "label": []}
{"id": 154298, "text": "3.2.1 Cooperative Contrastive Learning We use contrastive representation learning to regularize the latent space , such that encoders bring two sentences sharing similar constraints closer together ( positive pairs ) , and force dissimilar ones away ( negative pairs ) .", "Comments": [], "label": []}
{"id": 155287, "text": "We use a language specific dependency tagger for obtaining the tags .", "Comments": [], "label": []}
{"id": 158830, "text": "In the left branch , the question , together with its context , is given to a transformer , which is pre-trained and fine-tuned on the polarity labels of our dataset ( positive/neutral vs. negative intentions ) .", "Comments": [], "label": []}
{"id": 152312, "text": "In fact , in CNN/DailyMail dataset , there are plenty of documents with a different num- Table 6 : ROUGE Scores about Trigram-Blocking on CNN/DM Test Set .", "Comments": [], "label": []}
{"id": 152186, "text": "Here , we devise a novel lexical-semantic probing task : we use BERT 's representations for complex words to predict semantic dimensions , specifically sentiment and topicality ( see Figure [ 1 ] .", "Comments": [], "label": []}
{"id": 154483, "text": "4.1 Implementation Details Knowledge Source : Following [ Karpukhin et al. , , 2020 ] [ Izacard and Grave , 2021 ] , we use the English Wikipedia as the text corpus , and apply the same preprocessing to divide them into disjoint passages with 100 words , which produces 21M passages in total .", "Comments": [], "label": []}
{"id": 159733, "text": "The number of heads in self-attention layers and cross-modal transformer layers is 12 , and the learning rates for MERMC and DFER is set to 7e-6 and 5e-5 , respectively .", "Comments": [], "label": []}
{"id": 153061, "text": "We learn a joint BPE model with 32K operations .", "Comments": [], "label": []}
{"id": 156213, "text": "For formula prediction , we set max sequence length 512 and fine-tune 800K steps with batch size 2 on single GPU .", "Comments": [], "label": []}
{"id": 154368, "text": "Concretely , we adopt the Euclidean distance of the CLS representation between BERT and the compressed models as the evaluation metric , which is proportional to the information loss caused by model compression , formally : where M is the number of the instances in corresponding dataset .", "Comments": [], "label": []}
{"id": 152260, "text": "We then apply the heuristic functions on each segment to detect disengaged intents .", "Comments": [], "label": []}
{"id": 158373, "text": "Sentences with the highest self-Rouge with the document are selected for masking , ensuring that there is high information overlap with the rest of the document .", "Comments": [], "label": []}
{"id": 159356, "text": "Following [ Baevski and Auli , 2019 ] , we use perplexity as an evaluation metric and a context window of 2047 at the inference stage .", "Comments": [], "label": []}
{"id": 151783, "text": "Inspired by previous work , for our lightweight model , we propose a robust sentence-level contrastive task by leveraging similarity relationships arising from translation pairs . 3 Methodology We perform cross-lingual sentence representation learning by a lightweight dual-transformer framework .", "Comments": [], "label": []}
{"id": 153344, "text": "This calculation , however , assumes full GPU parallelization .", "Comments": [], "label": []}
{"id": 153092, "text": "We choose the DPR method because it does not interrupt the backpropagation , differently from other solutions , e.g. , BM25 , TF-IDF [ Domeniconi et al. , , 2014b ] or LSA [ Domeniconi et al. , , 2016a ] .", "Comments": [], "label": []}
{"id": 158465, "text": "Since the encoding stage of the input text and the verbalized relationship is separated , we can accelerate the inference time of our modified TAG-PRIME through parallel encoding .", "Comments": [], "label": []}
{"id": 153978, "text": "We use the * Lexical * baseline from section [ 5.1 ] to filter out Non-Verifiable claims , which leads to 46,934 SUPPORTED claims .", "Comments": [], "label": []}
{"id": 160022, "text": "As shown in Figure [ 4 , ] outputs from the text Transformer were concatenated with the outputs from the audio Transformer encoder and fed into the evidential output layer .", "Comments": [], "label": []}
{"id": 155776, "text": "Each baseline metric fuses multiple manipulations , hence we use their citations [ Vakulenko et al. , , 2018 ] , [ Mesgar et al. , 2020 ] to easily refer them in later sections .", "Comments": [], "label": []}
{"id": 154017, "text": "A relevant document has one or more templates . to create the training data for both the DyGIE++ and GTT models , we use the first mention of each role filler in the document as the mention to be extracted .", "Comments": [], "label": []}
{"id": 158603, "text": "C.2.3 Additional Experiment Results We conduct a more comprehensive study on the commonsense inference generation task by experi- Figure 5 : Performance ( % ) curve by COMET ( GPT2- XL ) on commonsense inference generation task with different thresholds for determining positive pseudo labels .", "Comments": [], "label": []}
{"id": 158083, "text": "C Pre-training for Entity Selector Given a dialogue context and the system response , we use the entity with the most occurrences of its attribute values in the dialogue context and system response as the label .", "Comments": [], "label": []}
{"id": 152199, "text": "For a more complete overview on analyses of the different properties of transformer-based LMs , we refer to [ Rogers et al. , 2021 ] .", "Comments": [], "label": []}
{"id": 158742, "text": "To train PEIT , we employ a twostage pre-training strategy with an auxiliary MT task : ( 1 ) pre-training the MT model on the MT training data to initialize the shared encoder-decoder backbone network ; and ( 2 ) pre-training PEIT with the aligner and regularizer on a synthesized dataset with rendered images containing text from the MT training data .", "Comments": [], "label": []}
{"id": 155678, "text": "Following the common practice [ Zhang ] [ et al. , ] [ 2017 , 2021 ] , we use F1 score for TACRED and accuracy for other datasets . 5.2 Main Results Table [ 2 ] shows the performance of PRBOOST and the baselines on the four datasets .", "Comments": [], "label": []}
{"id": 157791, "text": "WN18RR is a subset of Word-Net [ Miller , 1995 ] containing lexical relationships between word senses .", "Comments": [], "label": []}
{"id": 159660, "text": "Change Propagation The ASR engine we use for module ( a ) sometimes revises its results .", "Comments": [], "label": []}
{"id": 154174, "text": "Dia-Trans . [ Maruf et al. , , 2018 ] : A Transformer-based model where an additional encoder is used to introduce the mixed-language dialogue history , re-implement by [ Liang et al. , 2021a ] .", "Comments": [], "label": []}
{"id": 157174, "text": "We use the negative log-likelihood loss Lgen to optimize the decoder : Finally , we jointly optimize the alignment loss , emotion prediction loss , generation loss , and diversity loss proposed by [ Sabour et al. , 2022 ] as : L = γ1Lalign+γ2Lemo+γ3Lgen+γ4Ldiv , where γ1 , γ2 , γ and γ are hyper-parameters .", "Comments": [], "label": []}
{"id": 158533, "text": "In particular , Joint ROUGE scores inherit the limitations of the original ROUGE scores .", "Comments": [], "label": []}
{"id": 156632, "text": "First , MC dropout is easy to use : one has to apply standard inference with dropout on with- Figure 6 : COMET scores for each generation method and number of hypotheses .", "Comments": [], "label": []}
{"id": 151833, "text": "In the first group , Transformer-XL gets better result , which shows that the transformer-based model have better modeling capabilities .", "Comments": [], "label": []}
{"id": 152452, "text": "For each taskspecific model , we use a product of validation loss and corpus-level binary F1 score on the validation set as the validation metric .", "Comments": [], "label": []}
{"id": 156138, "text": "] ( http : //papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf ) In H .", "Comments": [], "label": []}
{"id": 159737, "text": "The baselines with italics only use textual modality . ▲ indicates the model uses T5 [ Raffel et al. , , 2020 ] as the textual encoder .", "Comments": [], "label": []}
{"id": 156374, "text": "We use the greedy decoding strategy for generating word-tokens in the inference phrase .", "Comments": [], "label": []}
{"id": 151368, "text": "This paper presents a novel Multi-metadata Embedding based Cross-Transformer ( MECT ) to improve the performance of Chinese NER by fusing the structural information of Chinese characters .", "Comments": [], "label": []}
{"id": 156956, "text": "Concretely , we adopt an entropy-based method to find an optimal number of clusters for each style automatically , and then design a neural clustering algorithm to cluster the sentences into different transfer patterns .", "Comments": [], "label": []}
{"id": 159279, "text": "To reduce processing time , we can accelerate evidence searching and reading .", "Comments": [], "label": []}
{"id": 160352, "text": "We replace every other Transformer layer with an MoE layer .", "Comments": [], "label": []}
{"id": 155202, "text": "We use the following types of rules that differ in structure and complexity ( c denotes i th clause and l denotes a label ) : We also consider other linguistic variations of rules by inserting quantifiers ( such as 'always ' , 'likely ' ) .", "Comments": [], "label": []}
{"id": 154133, "text": "With in-batch negatives only , the performance of SimKGCIB is already quite strong thanks to the large batch size ( 1024 ) we use .", "Comments": [], "label": []}
{"id": 159601, "text": "While [ Zhang et al . ' ] s [ 2017 ] DENSE parser was based on a bi-directional LSTM , we define the model on top of the final hidden states of the transformer encoders .", "Comments": [], "label": []}
{"id": 157171, "text": "A gating mechanism is designed to capture affective representation raf f : We project raf f to predict the user 's emotion e : which is supervised by the ground truth emotion label e ∗ using the cross-entropy loss : 3.3 Empathy-aware Response Generation We employ a Transformer-based decoder to generate the response .", "Comments": [], "label": []}
{"id": 154009, "text": "We have used the dataset to explore several sequence labeling models ( CRF , BiLSTM-CRF , and Transformer-based models ) for the task of extracting lexical borrowings in a high-OOV setting .", "Comments": [], "label": []}
{"id": 153006, "text": "C Language Family Table [ 4 ] is the language family information we used in Section [ 4 . ]", "Comments": [], "label": []}
{"id": 159440, "text": "We consider two cases : source-indexed LSLs , and target-indexed LSLs , distinguished by whether we use the source or the target language to select the appropriate sub-layer .", "Comments": [], "label": []}
{"id": 157267, "text": "To diversify the CLS embeddings , we modify the transformer encoder in Section [ 2.5 ] and propose a new reparametrization method during the fine-tuning in Section [ 2.6 . ]", "Comments": [], "label": []}
{"id": 151423, "text": "Table 1 : Examples from our annotated data , showing the majority label for each example . 4.1 Methods We use several algorithms to better understand if word- or utterance-level similarity is a better measure of uptake .", "Comments": [], "label": []}
{"id": 155910, "text": "4.2 Datasets We use a number of English text classification datasets used in the zero-shot and the few-shot literature [ Yin et al. , , 2019 ] [ Gao et al. , , 2021 ] [ Wang et al. , 2021 ] .", "Comments": [], "label": []}
{"id": 157158, "text": "We use a Transformer-based sentence encoder ( cognition encoder ) to first encode the vertices VCS of the graph GCS .", "Comments": [], "label": []}
{"id": 151610, "text": "Specifically , we prune 4 heads from each transformer layer in BERTBASE and Early-BERT .", "Comments": [], "label": []}
{"id": 157905, "text": "We adopt pair-wise comparison to conduct human evaluation , where we compare models before and after using our synthetic dialogue data .", "Comments": [], "label": []}
{"id": 151348, "text": "We use finite-sample cross entropy as an effective empirical proxy for DKL : Where c ∼ −→LM ( c|sinput ) indicates sampling contexts for input from −→LM .", "Comments": [], "label": []}
{"id": 154278, "text": "As a measure of meaning preservation , we use the F1 BertScore metric [ Zhang et al. , , 2020 ] to compare the semantic similarity of the provided reference sentence with the generated output .", "Comments": [], "label": []}
{"id": 153750, "text": "However , pre-training objectives like MLM loss tend to impose task-specific bias on the final layers of Transformers [ Carlsson et al. , 2020 ] , limiting the generalization of the embeddings .", "Comments": [], "label": []}
{"id": 155503, "text": "[ Artetxe et al. , 2020 ] showed that a Transformer encoder pretrained only on L1 exhibits strong cross-lingual transfer performance simply by aligning the L2 embeddings to the encoder .", "Comments": [], "label": []}
{"id": 160354, "text": "A learnable gating function routes input tokens to different experts ( NLLB Team et al. , 2022 ) .", "Comments": [], "label": []}
{"id": 152206, "text": "Second , we use a method based on each word pair 's PMI in a given corpus .", "Comments": [], "label": []}
{"id": 160327, "text": "Because BTC is an online API with no training required , we use it to perform the toxic identification of all the samples in TOXICN . and backward directions to obtain the final sentence embedding .", "Comments": [], "label": []}
{"id": 157509, "text": "GPT-3 and T5 still can not perform well on OOD .", "Comments": [], "label": []}
{"id": 154124, "text": "We use NIB , NPB , and NSN to denote the aforementioned three types of negatives .", "Comments": [], "label": []}
{"id": 158491, "text": "N/A means all cases have been predicted to be negative . 4.1.2 Human Assessment of Link Prediction in the Future We use all graph snapshots , including the year 2021 , for training to mine potential connections that may appear in the future .", "Comments": [], "label": []}
{"id": 159550, "text": "For natural language inference ( NLI ) , we adopt RTE ( Dagan et al. , 2005 ; Haim et al. , 2006 ; Giampiccolo et al. , 2007 ; Bentivogli et al. , 2009 ) , SNLI ( Bowman et al. , 2015 ) and MNLI ( Williams et al. , 2018 ) .", "Comments": [], "label": []}
{"id": 160312, "text": "We employ a vocabulary size of 32k subword tokens .", "Comments": [], "label": []}
{"id": 158794, "text": "We use the same rank embeddings for every table cell since there is no rank relation among the table cells for our case .", "Comments": [], "label": []}
{"id": 158825, "text": "To find reliable annotators , we adopted a hierarchical strategy : * i ) * using worker profiles , we limited our workers to those who completed over 700 tasks on MTruk with over 99 % acceptance rate , * ii ) * conducting pre-qualification test and filtering those who earn low scores ( < 80 ) , and * iii ) * pilot testing to check the quality of workers ' annotation .", "Comments": [], "label": []}
{"id": 159856, "text": "While NPPrompt-T5-base and NPPrompt-GPT2-base demonstrate commendable performance , they do not surpass the performance of NPPrompt-RoBERTa-large .", "Comments": [], "label": []}
{"id": 152967, "text": "5.2 Answer Recaller Our answer recaller , based on T5 , is trained to predict all gold answer ( s ) in sequence ( separated by a [ SEP ] token ) from each retrieved positive passage p ∈ B that cover some gold answer ( s ) .", "Comments": [], "label": []}
{"id": 154147, "text": "3 Empirical Study : Token Embedding Training Dynamics led by Rare Tokens 3.1 Initial Training Dynamics of Embeddings To analyze the training procedure of token embeddings , we train a Transformer language model at the WikiText-103 dataset from scratch .", "Comments": [], "label": []}
{"id": 160210, "text": "A.2 Instruction Search The actual templates we feed T5 are * '' Task : * < X > * documents * < Y > question based on them .", "Comments": [], "label": []}
{"id": 153412, "text": "We propose Overlap BPE ( OBPE ) , a simple yet effective modification to the BPE algorithm , which chooses a vocabulary that maximizes overlap across languages .", "Comments": [], "label": []}
{"id": 154814, "text": "Table 3 : GPU time consumption ( seconds ) of training one epoch on the whole dataset .", "Comments": [], "label": []}
{"id": 155088, "text": "For example , while CodeBERT and GraphCodeBERT are trained on 20GB data , we used an order of magnitude less data .", "Comments": [], "label": []}
{"id": 156855, "text": "When integrated with the prototype network , PROPETL updates a total of nL + n parameters during the training phase , which is marginally more than the conventional PEFT methods which have nL extra parameters .", "Comments": [], "label": []}
{"id": 153729, "text": "Previous SOTA and retrieval-based methods are also based on the inference model in their corresponding column : * T5-11b 1.1 +digits ( Submission by ISI Waltham ) ; * * T5-11b + IR [ Yan , 2021 ] ; UQA-11b-ft [ Khashabi et al. , , 2020 ] ( SOTA of single-model methods without referencing ConceptNet ) ; † Unicorn-ft [ Talmor et al. , , 2021 ] ; †† Unicorn-ft + Google snippets [ Talmor et al. , , 2021 ] ; ‡ UQA-11b-ft [ Khashabi et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 151906, "text": "Similar to the original transformer incorporating the position embedding with the word embedding , we utilise the clause position information to enrich the clause representation .", "Comments": [], "label": []}
{"id": 155867, "text": "To cover more documents and avoid collecting shallow hierarchies , each summary paragraph is annotated by one annotator and we select highquality summary paragraphs for annotation based on heuristic rules , e.g. , each paragraph should have at least 3 sentences and 70 words and an adequate level of abstractiveness as measured by normalized density of extractive fragments [ Grusky et al. , , 2018 ] ( with a threshold of < 0.15 ) .", "Comments": [], "label": []}
{"id": 155738, "text": "We set our hyperparameters ( e.g. , facet number K = 3 and W × H = 3 × 3 when using multiple input hidden states ) based on the validation performance in Wikipedia 2016 , the resulting model size , and the memory constraint in GPUs .", "Comments": [], "label": []}
{"id": 154954, "text": "greedy heuristic algorithm [ Nallapati et al. , , 2017 ] , which adds one sentence at a time to the extracted summary , skipping some sentences to maximize the ROUGE score of S and the extracted sentences .", "Comments": [], "label": []}
{"id": 152536, "text": "We use mean pooling on the pretrained transformers token-level output to produce sentence embeddings – the same pooling strategy used in our method . 5 Results In [ subsection 5.1 , ] we compare the performance of our model against the relevant baselines .", "Comments": [], "label": []}
{"id": 159078, "text": "Instead of a simple summation , we use a self-attention network SA ( · ) ( Vaswani et al. , 2017 ) to emphasize the important character components .", "Comments": [], "label": []}
{"id": 157598, "text": "We use this simple technique because the focus of this work is on postabstention i.e . the next step of selective prediction .", "Comments": [], "label": []}
{"id": 157270, "text": "To solve the collapsing problem , we insert multiple linear layers Ll , k into the transformer encoder .", "Comments": [], "label": []}
{"id": 156736, "text": "Table 10 : BLEU , COMET , and Word f-measure per tag for base context-aware models .", "Comments": [], "label": []}
{"id": 153790, "text": "In this paper we explore the design space of Transformer models showing that the inductive biases given to the model by several design decisions significantly impact compositional generalization .", "Comments": [], "label": []}
{"id": 156194, "text": "In our implementation , a function by default takes the selected region of last function as input region to prune search space .", "Comments": [], "label": []}
{"id": 155039, "text": "However , many current state-of-the-art NER models are built with a CRF module [ Liu et al. , , 2019 ] [ Chen et al. , , 2020 ] [ Wang et al. , , 2021 ] .", "Comments": [], "label": []}
{"id": 156734, "text": "Due to the sheer number of experiments , we use a single seed per experiment .", "Comments": [], "label": []}
{"id": 155198, "text": "Table 7 : Automatic evaluation of the chosen finetuned T5 baseline .", "Comments": [], "label": []}
{"id": 158114, "text": "To maximize the effectiveness of the NATURAL INSTRUCTIONS dataset , we use all instances in English-language tasks to tune the model in the first training stage . 5 Experimental Setup Evaluation Metrics We report the accuracy for classification tasks and ROUGE-L [ Lin , 2004 ] for all generation tasks .", "Comments": [], "label": []}
{"id": 160419, "text": "] ( https : //huggingface.co/T-Systems-onsite/cross-en-de-roberta-sentence-transformer ) BertAlign [ Liu and Zhu , 2022 ] [ ) is a attempt to ] ( https : //huggingface.co/T-Systems-onsite/cross-en-de-roberta-sentence-transformer ) [ allow sentence-transformer-based methods to pro ] ( https : //huggingface.co/T-Systems-onsite/cross-en-de-roberta-sentence-transformer ) duce n : m [ alignments .", "Comments": [], "label": []}
{"id": 152658, "text": "We use two separate resources as we find that no dictionary encapsulates both slang and nonslang words .", "Comments": [], "label": []}
{"id": 153652, "text": "We implement the contrastive loss following [ Khosla et al. , 2020 ] .", "Comments": [], "label": []}
{"id": 152794, "text": "We use an existing PLM as the initial student model , which is only pre-trained in the general domain .", "Comments": [], "label": []}
{"id": 153546, "text": "3.2 Domain-specific Typing Following the general guidelines in [ §2.2 , ] we used the following domain-specific types in this study .", "Comments": [], "label": []}
{"id": 156904, "text": "A Dialogue Collection Tool Supplementary We provide details about our implementation of the described data collection methodology below .", "Comments": [], "label": []}
{"id": 154104, "text": "Common to ParaBLEU and FSET is the use of a Transformer for paraphrase style transfer , with differing architectural details .", "Comments": [], "label": []}
{"id": 152855, "text": "Second , current methods mostly use sentence-level ROUGE [ Chen and Bansal , 2018 ] or summary-level ROUGE [ Bae et al. , , 2019 ] as training rewards .", "Comments": [], "label": []}
{"id": 156527, "text": "For readability , we use s to denote sentences , * par * for paragraphs , and * sec * to denote sections for reviews ( e.g . s ) and papers ( e.g . s ) , respectively .", "Comments": [], "label": []}
{"id": 152319, "text": "Note that more truncated BPE tokens will increase the final average ROUGE slightly , for it may lose some summary sentences when truncating too many tokens .", "Comments": [], "label": []}
{"id": 156950, "text": "We adopt the accuracy over positive/negative sentiments classification ( denoted as Acc2 ) as the evaluation metric for our model and baselines .", "Comments": [], "label": []}
{"id": 152227, "text": "Table 4 : Summary of the BERT and * fastText * results . * Macro * and * Micro * refer to the macro-average and microaverage results across the four experiments , respectively . * Full * are the micro-average values on the whole dataset .", "Comments": [], "label": []}
{"id": 153059, "text": "For IWSLT15 En-Vi , we use the pre-processed data used in [ Luong and Manning , 2015 ] [ 2 ] .", "Comments": [], "label": []}
{"id": 155191, "text": "Full details as well as train and dev results can be found in Appendix [ C.1 . ] QCPG : We use the pre-trained T5-base [ Raffel et al. , 2020 ] as the encoder-decoder model .", "Comments": [], "label": []}
{"id": 152551, "text": "The addition of the segmentation and glossing was inspired by [ Moeller and ] [ Hulden , 2021 ] . 3 Data We use published data in ten languages and unpublished data in five low-resource languages .", "Comments": [], "label": []}
{"id": 155265, "text": "4.2.1 Intelligibility To compare intelligibility between our baseline models and our low-resource models , we use the word error rate ( WER ) of an automatic speech recognition system ( ASR ) as a proxy .", "Comments": [], "label": []}
{"id": 153035, "text": "We utilize the Tapas-based [ Herzig et al. , 2020 ] dense retriever in this module [ Herzig et al. , 2021 ] , due to its better tabular contextualization expressiveness over classical retrieval methods such as Word2Vec [ Mikolov et al. , , 2013 ] and BM25 [ Robertson , 2009 ] .", "Comments": [], "label": []}
{"id": 153883, "text": "The statistics/metrics for each dataset and the finetuing results are in Appendix [ A . ] We use `` e2e '' for E2ENLG , `` rest '' for RNNLG ( restaurant ) , `` hotel '' for RNNLG ( hotel ) , `` tv '' for RNNLG ( tv ) , `` laptop '' for RNNLG ( laptop ) , `` wiki '' for WikiSQL , `` cnn '' for CNN/DailyMail , `` woz '' for MultiWOZ .", "Comments": [], "label": []}
{"id": 152952, "text": "The LARGE-BPE model is trained on all the available ( ST and distilled ) data for a total of ∼1.25M pairs , while the SMALL-BPE and SMALL-CHAR are trained only on the MuST-C data for a total of 250-275k pairs .", "Comments": [], "label": []}
{"id": 153076, "text": "Therefore , we expect some transferability across datasets but with differences aligned with their mismatches . 4 Experimental Setup and Results 4.1 Datasets and Setup Datasets We use four metaphor detection datasets in our study .", "Comments": [], "label": []}
{"id": 153659, "text": "The batch sizes are chosen based on available GPU memory .", "Comments": [], "label": []}
{"id": 156299, "text": "For MoverScore , still following the authors [ Zhao et al. , , 2019 ] , we used the variant operating on unigrams and the IDF to compute the vectors of weights .", "Comments": [], "label": []}
{"id": 152529, "text": "We use the Slanted Triangular LR scheduler [ Howard and Ruder , 2018 ] with a number of train steps equal to training instances and a cut fraction of 0.1 .", "Comments": [], "label": []}
{"id": 154021, "text": "For the SciREX dataset , we adopt a heuristic approach that assumes there is only one template per document , and in that template , we assign the named entities predicted by DyGIE++ for a document to the predicted role types .", "Comments": [], "label": []}
{"id": 152996, "text": "[ 15 ] has a closed-form solution for α and θ : [ 6 ] Before training , we use gradient-based pruning to initialize α and θ via Eq .", "Comments": [], "label": []}
{"id": 155931, "text": "A similar approach has been taken in the Funnel Transformer proposed concurrently to our work [ Dai et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 158305, "text": "For discriminative alignment ( CTC-D ) , we use `` roberta-large '' .", "Comments": [], "label": []}
{"id": 151963, "text": "Figure 2 : Transformer on various input length and data scale .", "Comments": [], "label": []}
{"id": 153656, "text": "For MTP , we first train until convergence on the external dataset , and then when training on Dlabeled known , we use a development set to validate early-stopping with a patience of 20 epochs following [ Zhang et al. , 2021c ] .", "Comments": [], "label": []}
{"id": 160109, "text": "Flair + SVM We used the 2,048-dimension `` news-forward '' embeddings , produced by a forward bi-LSTM , trained on the One Billion Word Benchmark [ Chelba et al. , , 2013 ] and feed the obtained embeddings to a linear SVM classifier .", "Comments": [], "label": []}
{"id": 153754, "text": "X-Mol [ Xue et al. , , 2021 ] was pretrained by taking as input a pair of SMILES variants for the same molecule and generating one of the two input SMILES as output with Transformers on 1.1 billion molecules .", "Comments": [], "label": []}
{"id": 156978, "text": "First , we use this system to stress tests question answering , machine translation , and semantic parsing .", "Comments": [], "label": []}
{"id": 153889, "text": "We also provide a comparison in Appendix [ B ] to demonstrate the effect of reusing modules from different transformer layers .", "Comments": [], "label": []}
{"id": 151903, "text": "Clauses are fed into a word-level Bi-LSTM and a clause-level Transformer to obtain the clause representations Cˆ i .", "Comments": [], "label": []}
{"id": 156018, "text": "The complexity of Transformers is mainly due to a pipeline of encoders , each of which contains a multi-head self-attention layer .", "Comments": [], "label": []}
{"id": 156572, "text": "SINGULARITY-temporal .", "Comments": [], "label": []}
{"id": 158805, "text": "3.1 Problem Definition Given a Chinese sentence X = { x1 , x2 , .. , xn } of n characters that may include erroneous characters , we use Y = { y1 , y2 , .. , yn } to represent the corresponding correct sentence .", "Comments": [], "label": []}
{"id": 154636, "text": "Supervised SOTA refers to MDETR [ Kamath et al. , , 2021 ] ; we use the EfficientNet-B3 version .", "Comments": [], "label": []}
{"id": 157421, "text": "4.4 Ablation Studies We conduct ablation experiments over different variants of the best-performing model SPARTA ( T5 ) to better understand the relative importance of the proposed SPARTA framework .", "Comments": [], "label": []}
{"id": 155144, "text": "For inference , we use beam search with a beam size of 5 for all translation directions .", "Comments": [], "label": []}
{"id": 157052, "text": "Then we use the generated full parse trees as the target parse for the syntactically controlled paraphrase generator .", "Comments": [], "label": []}
{"id": 157377, "text": "Fine-tuned models Our baselines include several prominent multilingual encoder-only pretrained Transformer models .", "Comments": [], "label": []}
{"id": 152214, "text": "The results suggest that the best monolingual models based on Transformers [ Vaswani et al. , , 2017 ] can identify homonyms having different meanings adequately .", "Comments": [], "label": []}
{"id": 158841, "text": "We adopted linear SVM to classify the intention categories .", "Comments": [], "label": []}
{"id": 155539, "text": "We used A/B tests for more targeted comparisons between different systems , namely cold-start vs. fine-tuned and neural vs. concatenative .", "Comments": [], "label": []}
{"id": 159676, "text": "C.1 Segmentation Model We use BIOES for the segmentation model .", "Comments": [], "label": []}
{"id": 158506, "text": "We further evaluate the baseline transformer-based models for the E3 task and present the initial experiment results under separate and joint evaluation settings that unify the meeting summarization and E3 .", "Comments": [], "label": []}
{"id": 155138, "text": "Following [ Liu et al. , 2020 ] , we balance the vocabulary size of languages by up/down-sampling text based on their data size when learning BPE .", "Comments": [], "label": []}
{"id": 158593, "text": "In our implementation , dropout or dynamic model depth is introduced as noise to the model .", "Comments": [], "label": []}
{"id": 158460, "text": "Therefore , the representation z is aware of the relation- If a token x is split into multiple word pieces , we use the average embeddings of all its word pieces to be zi , following the practice of [ Lin et al. , 2020 ] . ship r and specific for predicting spans with relationship r to the condition c .", "Comments": [], "label": []}
{"id": 152374, "text": "Abstract State-of-the-art parameter-efficient fine-tuning methods rely on introducing adapter modules between the layers of a pretrained language model .", "Comments": [], "label": []}
{"id": 152383, "text": "Each layer of a transformer model consists of an attention block and a feed-forward block , each followed by a skip connection .", "Comments": [], "label": []}
{"id": 160211, "text": "We use this formulation when computing our recall metrics in [ §3.1 . ] C Hyperparameters C.1 ELECTRA Reader We use the same reader setting as in [ Xiong et al. , 2021 ] , where the top-100 retrieved paths are fed to the reader to obtain an answer from each path .", "Comments": [], "label": []}
{"id": 158818, "text": "Opt : Open pre-trained transformer language models . * arXiv preprint arXiv:2205.01068 * .", "Comments": [], "label": []}
{"id": 152258, "text": "Therefore , we take a conservative approach : we label the dialog turn as disengaged only if no more responses follow the non-positive response . 5.1.2 Heuristic Functions Implementation Next , we discuss how to use heuristic functions to auto-label disengaged user utterances .", "Comments": [], "label": []}
{"id": 155733, "text": "However , our preliminary experi- < https : //huggingface.co/ > ment suggests that the dropout trick can not improve the perplexity of DOC .", "Comments": [], "label": []}
{"id": 159756, "text": "A Proof of Propositions 1 The score function of TeAST is defined as : Following ComplEx [ Trouillon et al. , , 2016 ] , we employ the standard componentwise multilinear dot product < a , b , c > : = P k akbkc in Eq . [ 13 . ] For symmetric pattern , we have r ( s , o , τ ) ∧ r ( o , s , τ ) according to Definition 1 .", "Comments": [], "label": []}
{"id": 152094, "text": "On the other hand , we use the Multiple classifier to calculate the confidence for tokens that are of type * NS * , to distinguish * NS * from all predefined non- * O * slot types .", "Comments": [], "label": []}
{"id": 155326, "text": "Also , our method achieves state-of-the-art performance compressing BERT [ Devlin et al. , , 2019 ] on the GLUE benchmark [ Wang et al. , , 2019 ] and shows competitive results compressing ResNet [ He et al. , , 2016 ] and VGG [ Simonyan & Zisserman , 2015 ] on CIFAR-100 [ Krizhevsky et al. , , 2009 ] .", "Comments": [], "label": []}
{"id": 159426, "text": "( 27 ) Attention Operation for Node Filtering and Edge Adjusting In Section [ 3.3 , ] we utilize the l-order context to determine whether a node should be filtered or an edge should be adjusted since the nodes and edges in a graph have local dependence , Figure 12 : The l-order context for a node and an edge .", "Comments": [], "label": []}
{"id": 156221, "text": "This is the reason why we employ a data augmentation technique in order to produce the examples needed for this task .", "Comments": [], "label": []}
{"id": 152861, "text": "Specifically , we start with an empty set , and we iteratively select a snippet from the input such that the concatenation of that snippet and the already selected snippets maximizes the average of ROUGE-1 , ROUGE-2 , and ROUGE-L scores given the gold summary .", "Comments": [], "label": []}
{"id": 155264, "text": "The texts we use for the following experiments are disjunct from any training data used .", "Comments": [], "label": []}
{"id": 160175, "text": "At each step , we first use TF-IDF similarity to ob- We use * path * and * chain * interchangeably throughout the paper . tain an initial set of supporting document paths . [ 3 ] We then use PROMPTRANK to rerank the current document chains based on their relevance to the question ( [ §2.1 ] .", "Comments": [], "label": []}
{"id": 157992, "text": "In this framework , we use a batch size of 16 , the maximum sequence length was set to 256 , decoder_max_length=3 , truncate_method= '' head '' , and teacher_forcing and predict_eos_token were set to default values .", "Comments": [], "label": []}
{"id": 152287, "text": "Then , residual is introduced to the natural language process by Transformer [ Vaswani et al. , , 2017 ] .", "Comments": [], "label": []}
{"id": 154641, "text": "We use [ https : //github.com/airsplay/ ] ( https : //github.com/airsplay/py-bottom-up-attention ) [ py-bottom-up-attention ] ( https : //github.com/airsplay/py-bottom-up-attention ) to compute the features for RefGTA .", "Comments": [], "label": []}
{"id": 152376, "text": "These costs are substantially reduced by training a single ∗Work done while the author was at Google . * * Multi-head attention * * Sebastian Ruder DeepMind ruder @ google.com James Henderson Idiap Research Institute james.henderson @ idiap.ch Figure 1 : Left : Adapter integration in the T5 model .", "Comments": [], "label": []}
{"id": 160396, "text": "As for expert-specific parameters , we use 64 experts with each running on a single GPU with the frequency of 2 so that every other Transformer decoder layer becomes an MoE layer .", "Comments": [], "label": []}
{"id": 158543, "text": "The Joint ROUGE score defined in Section [ 6.3 ] is also provided .", "Comments": [], "label": []}
{"id": 158635, "text": "In our case , we use a CNN-based model similar to [ Malkiel et al. , 2021 ] as the Snapshot Encoder .", "Comments": [], "label": []}
{"id": 156937, "text": "We pass a as the input of the Transformer encoder module to get the speech sequence representations : where Hs , i ∈ R ( mi−1+mi+2 ) ×d denotes the hidden states of the last Transformer layer .", "Comments": [], "label": []}
{"id": 152182, "text": "We use a twolayer FFN in NNC .", "Comments": [], "label": []}
{"id": 157111, "text": "On the IU X-RAY dataset , B-2 increases by 2.4 % and 3.7 % and B-4 rises by 1.0 % and 1.5 % .", "Comments": [], "label": []}
{"id": 151952, "text": "We use the same vocabulary with XLM-R [ Conneau et al. , , 2020 ] . [ https : //github.com/facebookresearch/ ] ( https : //github.com/facebookresearch/cc_net ) [ cc_net ] ( https : //github.com/facebookresearch/cc_net ) Table 9 : The statistics of Wikipedia dump used for pretraining .", "Comments": [], "label": []}
{"id": 159195, "text": "We use BARTlarge and T5small as the backbone model , and the results are shown in Figure [ 3 . ] The results show that performances are significantly enhanced by the refinement using omissions compared to that Figure 4 : Change of the post-editing results by perturbing input omissions .", "Comments": [], "label": []}
{"id": 156588, "text": "SINGULARITY-temporal Results on Existing Datasets .", "Comments": [], "label": []}
{"id": 156585, "text": "For example , we use validation splits for DiDeMo retrieval and ActivityNet-QA , and we use the test split for MSRVTT retrieval , val1 split for ActivityNet Captions retrieval , and test split for SSv2-label .", "Comments": [], "label": []}
{"id": 160124, "text": "Finally , as the T5 model achieves the best F regardless of QA pre-training , we use T5SQuADv2 Pre-train as our final [ https : //noisy-text.github.io/2020/ext ] ( https : //noisy-text.github.io/2020/extract_covid19_event-shared_task.html ) [ ract_covid19_event-shared_task.html ] ( https : //noisy-text.github.io/2020/extract_covid19_event-shared_task.html ) Table 1 : Token-level F scores for claim extraction experiments on COVID-19 treatment dataset .", "Comments": [], "label": []}
{"id": 151345, "text": "Our implementation details follow those of past work retraining GPT-2 [ Zellers et al. , , 2019 ] . 2.5 Application : Paraphrasing To paraphrase , we begin by generating candidate outputs .", "Comments": [], "label": []}
{"id": 157571, "text": "And in Stage 2 , we use these labels from Stage 1 to train and evaluate with a supervised learning model .", "Comments": [], "label": []}
{"id": 159403, "text": "With the advantage of sequence-to-sequence models like T5 that can map different types of language tasks into generation tasks , we can control and minimize the influence of varying task formats on model performance while evaluating the helpfulness of explanations by leveraging a unified data format .", "Comments": [], "label": []}
{"id": 154852, "text": "2 Phrase-Guided Visual Representation We use phrase-level visual representation to improve NMT .", "Comments": [], "label": []}
{"id": 153796, "text": "The main contributions of this paper are : ( 1 ) A study of the Transformer design space , showing which design choices result in compositional learning biases across a variety of tasks .", "Comments": [], "label": []}
{"id": 154568, "text": "After node embeddings are generated , we use a READ-OUT function to obtain the graph embedding G : We repeat the above process on each subgraph to derive a list of subgraph embeddings L = [ G1 , G2 , · · · , Gn ] , where n is the number of subgraphs .", "Comments": [], "label": []}
{"id": 152508, "text": "In this paper we use parse tree of sentences to construct the meaningful feature structures .", "Comments": [], "label": []}
{"id": 159726, "text": "We feed E and Eˆ v to two separate self-attention Transformer layers [ Vaswani et al. , , 2017 ] to model the intra-modal interactions within audio features and visual features as follows : Inter-Modal Interactions .", "Comments": [], "label": []}
{"id": 151458, "text": "( For all experiments * not * using a pretrained model checkpoint , we experimented with both a BART-like and T5-like choice of depth and hidden size , and found that the BART-like model performed better . )", "Comments": [], "label": []}
{"id": 156559, "text": "Frozen uses a space-time transformer , CLIP4Clip is an extension based on the CLIP [ Radford et al. , , 2021 ] with an extra 4-layer temporal transformer encoder .", "Comments": [], "label": []}
{"id": 153608, "text": "The dimension sizes of hin and hout are consistent with the corresponding transformer hidden states .", "Comments": [], "label": []}
{"id": 154611, "text": "The DPR model is thus used to both retrieve from the FAISS index , and then score the top N candidates .", "Comments": [], "label": []}
{"id": 158732, "text": "We initialize the Transformer weights with the pre-trained ViT [ Dosovitskiy et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 153562, "text": "D Reproducibility For T5-based experiments related to model performances in Table [ 4 , ] we choose the learning rate from [ 5e-2 , 5e-3 , 5e-4 ] and select 5e-3 for final experiments .", "Comments": [], "label": []}
{"id": 159561, "text": "It is worth noting that RL-Prompt adopts the test set of SST2 and we use the `` ` Black-Box-Tuning `` ` `` ` 5 https : //github.com/mingkaid/rl-prompt `` ` Figure 5 : Accuracy heatmap of DecT transferred across different sentiment analysis tasks under 16-shot setting .", "Comments": [], "label": []}
{"id": 157288, "text": "We use the metric to test our diversification methods and detect the collapsing during pretraining .", "Comments": [], "label": []}
{"id": 153145, "text": "We use Facebook 's * [ MoCo codebase ] ( https : //github.com/facebookresearch/moco ) * to implement the setup , by plugging-in our ST-GCN as the encoder .", "Comments": [], "label": []}
{"id": 152367, "text": "Annotator Compensation We employed a team of ten annotators to validate the quality of the HATECHECK dataset .", "Comments": [], "label": []}
{"id": 152152, "text": "Further analysis verifies the effectiveness of GIT especially in cross-sentence events extraction and multi-event scenarios .", "Comments": [], "label": []}
{"id": 160209, "text": "A Instructions A.1 Best Instructions Table [ A1 ] shows the top 10 performing instructions found by instruction search ( [ §2.2 ] based on R @ 2 and using T5-XL .", "Comments": [], "label": []}
{"id": 156193, "text": "We use a beam size of 5 to search decoded outputs ( sequence lengths range from 8 to 60 tokens ) C Hierarchical Table QA C.1 Logical Form Function List We list our logical form functions in Table [ 7 . ] Union selection is required for comparative and arithmetic operations .", "Comments": [], "label": []}
{"id": 153573, "text": "3.2 Visual Encoder To introduce visual signals into the Transformer structure , we follow the idea of VQ-VAEs [ van den ] [ Oord et al. , , 2017 ] to encode an image I ∈ R H×W×3 into a sequence of discrete patch-level visual tokens z ∈ R h×w×d , where H and W is the original size of the input image , h·w is the number of visual patches , and d is the patch-level feature dimensionality .", "Comments": [], "label": []}
{"id": 151642, "text": "As illustrated in Figure [ 2 ] a ) , we employ the Cascade Residual Autoencoder ( CRA ) [ Tran et al. , , 2017 ] structure , which has sufficient learning capacity and more stable convergence than the standard autoencoder .", "Comments": [], "label": []}
{"id": 156939, "text": "We use squared error loss to optimize the TPP task : where Wstart , Wend ∈ R dh×1 are learnable parameters .", "Comments": [], "label": []}
{"id": 159079, "text": "We choose a Word2Vec model trained from a Wikipedia snapshot of 2019 as the pre-trained background word embedding model for the quantitative evaluation by following a previous work ( Kabbach et al. , 2019 ) .", "Comments": [], "label": []}
{"id": 152975, "text": "Answer verifiers were all initialized with T5-3b .", "Comments": [], "label": []}
{"id": 157729, "text": "To train schema-retriever for A model we use all answerable questions while for A+U model we use questions with valid logical forms .", "Comments": [], "label": []}
{"id": 157257, "text": "We leave the extension to more realistic efficient learning scenarios , such as in languages where there is only a limited number of monolingual data , or where there is a hard limit on what pre-training computational resources we can use ( * e.g. , * 1 GPU for 3 days ) , to future work .", "Comments": [], "label": []}
{"id": 155456, "text": "UniXcoder is based on a multi-layer Transformer and follows [ Dong et al. , 2019 ] to utilize mask attention matrices with prefix adapters to control the access to context for each token .", "Comments": [], "label": []}
{"id": 157605, "text": "An Inner Table Retriever for Robust Table Question Answering Weizhe Li [ n ] ∗ , Rexhina Blloshmi , Bill Byrne , , Adrià de Gispert , and Gonzalo Iglesias Amazon Alexa AI University of Cambridge wl356 @ cam.ac.uk { blloshmi , willbyrn , agispert , gjii } @ amazon.com Abstract Recent years have witnessed the thriving of pretrained Transformer-based language models for understanding semi-structured tables , with several applications , such as Table Question Answering ( TableQA ) .", "Comments": [], "label": []}
{"id": 154758, "text": "Both the compressive transformer and ∞-former also lead to smaller accuracies when increasing the sequence length , as expected .", "Comments": [], "label": []}
{"id": 153822, "text": "For the sequence tagging formulation , we used only the encoder part of the Transformer and added five prediction heads , to predict each tag .", "Comments": [], "label": []}
{"id": 155873, "text": "To allow efficient encoding , we use LONG-FORMER [ Beltagy et al. , , 2020 ] with a window size of 1024 as the base model , and fine-tune it for all systems and comparisons .", "Comments": [], "label": []}
{"id": 156493, "text": "2 ) DGEDT [ Tang et al. , , 2020 ] proposes a dual transformer structure based on dependency graph augmentation , which can simultaneously fuse representations of sequences and graphs .", "Comments": [], "label": []}
{"id": 157234, "text": "Transformer first converts the token sequence to d-dimensional embedding sequence E = [ e1 , e2 , ... , en ] ∈ R n×d through the embedding layer .", "Comments": [], "label": []}
{"id": 152007, "text": "As shown in Figure [ 7b , ] overall G-Transformer has a smooth curve of performance on the data scale from 1.25K to 160K .", "Comments": [], "label": []}
{"id": 157990, "text": "In this framework , we use a batch size of 4 , the maximum sequence length was set to 256 , decoder_max_length=3 , truncate_method= '' head '' , and teacher_forcing and predict_eos_token were set to default values .", "Comments": [], "label": []}
{"id": 157427, "text": "Extensive experiments conducted on the knowledge transfer from three single-turn QG datasets : MS MARCO , NewsQA , and SQuAD to three conversational QG datasets : CoQA , QuAC , and DoQA demonstrate the superior performance of our method .", "Comments": [], "label": []}
{"id": 153009, "text": "Specifically , we treat two input sequences X and Y¯ as one input sequence `` X < sep > Y¯ `` , where `` < sep > '' is a speck token for concatenation , and Y b as the output sequence , as shown in Figure [ 3 . ] Then we employ the Transformer model to accomplish this task as the conventional NMT task .", "Comments": [], "label": []}
{"id": 151890, "text": "Finally , we use a classifier on top of the entity representation for prediction .", "Comments": [], "label": []}
{"id": 157770, "text": "The last two ( diverse beam ) form candidates for relevance calibration . summarization corpus [ 6 ] for the PubMed and Clinical datsets , and we use a Longformer Encoder-Decoder [ Beltagy et al. , , 2020 ] trained on a more faithful , synthetic version of our clinical corpus from [ Adams et al. , 2022 ] .", "Comments": [], "label": []}
{"id": 159794, "text": "Our results indicate that , in general , LLMs should be the default approach to RE , especially given that one can train Flan-T5—which is dramatically smaller than GPT-3 , and publicly available—to achieve SOTA performance ( Figure [ 1 ] .", "Comments": [], "label": []}
{"id": 154527, "text": "The average is taken over the ROUGE-1 , ROUGE-2 , and ROUGE-L scores . 5.2 Few-Shot Fine-Tuning To explore the behavior of the few-shot fine-tuning , we compare the validation-set performance on Multi-News with varying number of training examples , from 10 to 2000 .", "Comments": [], "label": []}
{"id": 154566, "text": "In our experiments , we use javalang [ 3 ] to generate Java AST and reduce the vocabulary size to 50 , 336 , where 50 , 265 is the size of original RoBERTa vocabulary and 71 is the number of keywords in non-leaf nodes defined by javalang .", "Comments": [], "label": []}
{"id": 158315, "text": "Additionally , we design biomedical templates , and their details are in Appendix [ A . ] hard samples identified by the agent , we employ the adaptive contrastive learning method to extract valuable information from these samples .", "Comments": [], "label": []}
{"id": 155992, "text": "In the first case , we can see that the T5-model changes the name of the tropical storm from `` Debby '' to `` Maria '' , and it also changes the `` tropical storm '' to its hypernym `` hurricane '' , and all these changes contribute to a different expression Table 9 : Results of different strategies for choosing augmented data on DeBERTa ( xxlarge ) .", "Comments": [], "label": []}
{"id": 159956, "text": "Cascaded ( S2TT→TTS ) We combine a Conformer direct S2TT model and a Transformer TTS model .", "Comments": [], "label": []}
{"id": 160020, "text": "The value of CCC ranges from -1 ( perfect disagreement ) to 1 ( perfect agreement ) . https : //huggingface.co/microsoft/wavlm-base-plus Softplus ( x ) = ln ( 1 + exp ( x ) ) The values were manually selected from a small number of candidates .", "Comments": [], "label": []}
{"id": 152781, "text": "Specifically , we use the Adam optimizer and a batch size of 16 for NQ and 8 for WebQ , respectively .", "Comments": [], "label": []}
{"id": 158745, "text": "Jain et al . ( 2021 ) uses a convolutional encoder to encode the image and Transformer decoder to generate target translation .", "Comments": [], "label": []}
{"id": 158630, "text": "The DistilBERT we used for LEDGAR contains 6 transformer encoder layers each with 768 dimensions and 3072-dimensional feed-forward networks , resulting in 67M parameters .", "Comments": [], "label": []}
{"id": 154008, "text": "The model fed with codeswitch , BPE , and character embeddings ranked first and was significantly better than the result obtained by the model fed with BETO+BERT , BPE , and character embeddings .", "Comments": [], "label": []}
{"id": 155769, "text": "The estimated GPU hour per model in this paper is approximately 6 hours on average .", "Comments": [], "label": []}
{"id": 160123, "text": "A filtering heuristic can also be applied to remove obvious non-misleading claims from consideration ( e.g . filtering out claims supporting approved COVID-19 treatments in [ §4.1.3 ] .", "Comments": [], "label": []}
{"id": 160404, "text": "6.2 Evaluation of Text Simplification For evaluation , we use the following automatic metrics provided in the evaluation framework EASSE [ Alva-Manchego et al. , , 2019 ] : for simplification , SARI [ Xu et al. , , 2015 ] , for quality and semantic similarity to the target reference , BERTScore Precision ( BS-P ) is reported [ Zhang et al. , , 2019 ] , and BLEU for meaning preservation [ Papineni et al. , , 2002 ] , following the recommendations of [ Alva- ] [ Manchego et al. , 2021 ] regarding TS evaluation on English texts .", "Comments": [], "label": []}
{"id": 155803, "text": "We compare * latent- * GLAT with Transformer [ Vaswani et al. , , 2017 ] , NAT [ Gu et al. , 2018 ] , and GLAT [ Qian et al. , , 2021a ] models .", "Comments": [], "label": []}
{"id": 152718, "text": "PLATEB12-3 rnd means that we use random attention temperature of λ ∼ U [ 1.0 , 2.0 ] .", "Comments": [], "label": []}
{"id": 156633, "text": "Figure [ 6 ] shows the final COMET scores depending on the number of hypotheses .", "Comments": [], "label": []}
{"id": 157206, "text": "E Multitask Scale We test the effect of the amount of datasets we use for multitasking on the performance of the resulted model as a base model .", "Comments": [], "label": []}
{"id": 160362, "text": "Given sparse architecture of XM Transformer with GShard , all-to-English model shows +0.6 and -0.4 BLEU difference compared with Slavic-to-English model on EP/VP and FLEURS respectively , averaged over Slavic languages .", "Comments": [], "label": []}
{"id": 157341, "text": "Methods include gated neural networks [ Arevalo et al. , , 2017 ] to learn a linear combination of vector representations , projecting one modality 's matrix representation onto another and concatenating [ Kiela et al. , , 2020 ] , and using self-attention or cross-attention transformer layers to interact between modalities [ Kiela et al. , , 2020 ] [ Lu et al. , , 2019 ] .", "Comments": [], "label": []}
{"id": 154182, "text": "All our Transformer models contain L = 6 encoder layers and L = 6 decoder layers and all models are trained using THUMT [ Tan et al. , , 2020 ] framework .", "Comments": [], "label": []}
{"id": 158532, "text": "Another limitation is the current joint evaluation metrics , such as precision , recall , F1 scores , and Joint ROUGE scores , adopted as initial trials of evaluating of the `` summarize-then-explain '' joint setting .", "Comments": [], "label": []}
{"id": 153328, "text": "A.3.2 Effect of different dropout probabilities To further study the efficacy of our method in under-regularized scenarios , we compare the baseline transformer model with CipherDAug for the mosesdecoder/scripts/generic/multi-bleu.perl tensorflow/tensor2tensor/utils/get_ende_bleu.sh [ https : //github.com/takase/alone_ ] ( https : //github.com/takase/alone_seq2seq ) [ seq2seq ] ( https : //github.com/takase/alone_seq2seq ) dropout values of 0 ( no regularization ) , 0.1 , 0,2 and 0.3 in Table [ 11 . ]", "Comments": [], "label": []}
{"id": 153746, "text": "Demonstration examples are selected from the CSQA2 training set ; we use the annotated * Google featured snippet * as the knowledge .", "Comments": [], "label": []}
{"id": 159983, "text": "For En→Es , we use all samples from Europarl-ST [ Iranzo-Sánchez et al. , , 2020 ] and Must-C [ Di Gangi et al. , , 2019 ] and augment the training data by TEDLIUM3 [ Rousseau et al. , , 2012 ] , Librispeech [ Panayotov et al. , , 2015 ] , and Common Voice [ Ardila et al. , , 2020 ] , resulting in 3180hour source speech .", "Comments": [], "label": []}
{"id": 159008, "text": "For all our experiments , we used pre-established and published datasets , which do not pose any serious ethical concerns .", "Comments": [], "label": []}
{"id": 152650, "text": "This allows us to assess models ' speedups independently of their operating environment ( e.g. , CPU/GPU ) .", "Comments": [], "label": []}
{"id": 155841, "text": "We give a more detailed analysis of the training consumption in the Appendix [ E. ] Since PPT still converges a bit slower than FT , how to further accelerate the convergence of PT is worth studying in future work .", "Comments": [], "label": []}
{"id": 155824, "text": "We follow [ Lester et al. , 2021 ] to test PT with T5-XXL ( 11B parameters ) and use 100 tunable soft prompt tokens [ 1 ] .", "Comments": [], "label": []}
{"id": 151924, "text": "We took several measures To ensure diversity over the businesses , we employed a two-step sampling process : first sampled a business and then sampled a review for the business .", "Comments": [], "label": []}
{"id": 152364, "text": "For * * SN * * , we use its 'hate speech ' attribute ( `` attacks [ on ] a person or group on the basis of personal attributes or identities '' ) , which distinguishes between 'mild ' , 'bad ' , 'severe ' and 'no ' hate .", "Comments": [], "label": []}
{"id": 158508, "text": "Here , our definition of extractive oracles is further generalized than that in [ Mao et al. , 2022 ] , while extractive oracles in [ Mao et al. , 2022 ] only refer to the ROUGE-based automatically selected ones .", "Comments": [], "label": []}
{"id": 158974, "text": "( 2 ) Despite the strong performance achieved by SDDS across three dialogue summarization datasets , we use a pre-trained language model as the backbone of our proposed method , as a consequence , we can not go beyond the limitation of the maximum sequence length of the PLM for the dialog summarization scenario like meeting summarization so it remains a future challenge for dialog summarization in the extremely long format .", "Comments": [], "label": []}
{"id": 153915, "text": "As a result , GLM can significantly outperform T5 on NLU and seq2seq tasks with fewer parameters and data , as stated in Sections [ 3.2 ] and [ 3.3 . ] Comparison with UniLM [ Dong et al. , , 2019 ] .", "Comments": [], "label": []}
{"id": 155688, "text": "To train the LM , we use two objectives .", "Comments": [], "label": []}
{"id": 159211, "text": "B Omission Detection Models B.1 Implementation Details We use BERTbase and RoBERTabase as the backbone pre-trained encoder for the three frameworks .", "Comments": [], "label": []}
{"id": 158012, "text": "As a feature extractor , we employ a BiLSTM with 128 hidden units and set the number of hidden units for the reference layers to 256 .", "Comments": [], "label": []}
{"id": 152108, "text": "Finally we introduce a * Tracker * module to continuously track all the records with global memory , in which we utilize the global interdependency among records for multi-event extraction ( Sec [ 3.4 ] .", "Comments": [], "label": []}
{"id": 156164, "text": "The runtime for both methods are shown in Table [ 13 . ] These results were obtained by running the inference algorithm on NVIDIA ( c ) Depth 5 Figure 7 : Depth-wise comparison of ProofWriter ( `` Iter '' ) and FAIRR ( both trained on D0-3 dataset ) on limited inference budgets .", "Comments": [], "label": []}
{"id": 158427, "text": "Next , we use token-level features output by a pre-trained text encoder to initialize node representation according to the alignment information .", "Comments": [], "label": []}
{"id": 153706, "text": "We use a learning rate of 5e-5 for T5 .", "Comments": [], "label": []}
{"id": 160212, "text": "We use the default hyperparameters for HotpotQA from [ Xiong et al. , , 2021 ] in their codebase . [ 9 ] We use a maximum path length of 512 tokens , maximum question length of 64 , and answer length of 30 .", "Comments": [], "label": []}
{"id": 158026, "text": "We use * actual play * game transcript dataset from [ Callison- ] [ Burch et al. , 2022 ] scraped from Play-By-Post ( PBP ) , a web forum [ 4 ] where people play D & D by taking turns posting on the forum .", "Comments": [], "label": []}
{"id": 152795, "text": "To apply the activation boundary distillation to PLMs , we use classification embedding of the teacher and student as the distillation target .", "Comments": [], "label": []}
{"id": 160395, "text": "For dense XM Transformer , hyperparameters are the same as that for Slavic-to-English .", "Comments": [], "label": []}
{"id": 158258, "text": "We used pretrained models of Opus [ Tiede ] [ mann and Thottingal , 2020 ] for the former and MBart50 [ Tang et al. , , 2020 ] for the latter .", "Comments": [], "label": []}
{"id": 151750, "text": "The monolingual data we used is from newscrawl released by WMT2020 .", "Comments": [], "label": []}
{"id": 152520, "text": "Our overall approach is most similar to Sentence Transformers – we extend the pretraining of a transformer-based language model to produce useful sentence embeddings – but our proposed objective is self-supervised .", "Comments": [], "label": []}
{"id": 155383, "text": "Given a standard L-layer Transformer model and an input sequence X containing T tokens , the Transformer output H is calculated by where h l t is the hidden state of t-th token after the l-th layer , Self-Att ( · ) is the self-attention module , and FFN ( · ) is short for the feed-forward network .", "Comments": [], "label": []}
{"id": 151570, "text": "We used early stopping based on the BLEU score on the validation set at the end of every epoch ; the test scores reported are for a model trained for 63 epochs .", "Comments": [], "label": []}
{"id": 159487, "text": "Furthermore , the fusion module and pointer network consist of two and one layers of randomly initialized bidirectional Transformer blocks [ Vaswani et al. , , 2017 ] , respectively .", "Comments": [], "label": []}
{"id": 160341, "text": "In order to keep as much mined data as possible , we use VoxPopuli test set only when a language direction is not covered by EPST considering their domain similarity .", "Comments": [], "label": []}
{"id": 160006, "text": "We use the S2TT model in B3 for S2TT pre-training . t-mBART and u-mBART stand for text-based mBART and unit-based mBART , respectively .", "Comments": [], "label": []}
{"id": 155673, "text": "Weakly Supervised Baselines : ( 1 ) Snorkel [ Rat ] [ ner et al. , , 2017 ] is a classic WSL model .", "Comments": [], "label": []}
{"id": 152925, "text": "ST solutions , namely : LARGE-BPE , SMALL-BPE , and SMALL-CHAR ( see Appendix [ B ] for complete details about the models and training setups ) .", "Comments": [], "label": []}
{"id": 156984, "text": "In total , we implement 39 of the 47 pronoun features from eWAVE .", "Comments": [], "label": []}
{"id": 159093, "text": "And in the situation where contextual word embedding models are employed as background model , we use BERT ( Wolf et al. , 2020 ) with a CRF layer on top for the NER evaluation tasks and BERT with a token classification layer on top ( Wolf et al. , 2020 ) for POS tagging evaluation tasks .", "Comments": [], "label": []}
{"id": 156022, "text": "This concept separates our work from previous art that provide heuristic techniques for sequence length reduction , or approaches that require expensive training .", "Comments": [], "label": []}
{"id": 157537, "text": "GPT3 exhibits better OOD generalization than T5 with positional markers but it does not generalize well beyond 30 digits .", "Comments": [], "label": []}
{"id": 155546, "text": "All experiments for this paper requiring a GPU were run on the Canadian General Purpose Science Cluster ( GPSC ) in Dorval , Quebec .", "Comments": [], "label": []}
{"id": 155757, "text": "We use hyperparameters αc1 , αc2 , and αc to determine their trade-offs .", "Comments": [], "label": []}
{"id": 159228, "text": "We use the Adam optimizer to optimize our model , set the learning rate to 1e −2 , and train with a batch size of 16 .", "Comments": [], "label": []}
{"id": 154494, "text": "For the training of reading part , we adopt the same training setting except that the learning rate is 1e-4 for the base model and 5e-5 for the large model .", "Comments": [], "label": []}
{"id": 153867, "text": "Adapter [ Houlsby et al. , 2019 ] inserts MLP layers into each transformer layer .", "Comments": [], "label": []}
{"id": 153789, "text": "Abstract Several studies have reported the inability of Transformer models to generalize compositionally , a key type of generalization in many NLP tasks such as semantic parsing .", "Comments": [], "label": []}
{"id": 154990, "text": "We use fLM ( z , H ; θ ) to denote the Transformer network , where z is a word embedding , H is a sequence of past activations , and θ denotes the parameters of the Transformer network .", "Comments": [], "label": []}
{"id": 159402, "text": "Output ( Selfrationalization ) vs. without explanations ( Baseline ) on CoS-E and ECQA datasets . 3 Unified Structure While popular metrics like BLEU and ROUGE can evaluate text coherence and similarity , one critical aspect of explanations is how beneficial they can be .", "Comments": [], "label": []}
{"id": 153732, "text": "Our method leads to a larger improvement over the T5-11b baseline than self-talk ( by 1.89 % ) , showing that it is better at eliciting helpful knowledge from models .", "Comments": [], "label": []}
{"id": 156728, "text": "For the pretrained large models ( Figure [ 3 ] , context-aware models perform better than the context-agnostic on corpus-level metrics , especially COMET .", "Comments": [], "label": []}
{"id": 153191, "text": "Feature analysis using LightGBM : Using the best performing model , Table [ 5 ] shows the role of each feature set in the prediction task .", "Comments": [], "label": []}
{"id": 159098, "text": "WikiText-103 ( Merity et al. , 2017 ) , which is used in their published experimental setting , while AM was trained with the Wikipedia snapshot of 2019 , which is the original corpus of Word2Vec .", "Comments": [], "label": []}
{"id": 158469, "text": "We set B = 0.35 We use the all-MiniLM-L6-v2 model [ Reimers and ] [ Gurevych , 2019 ] for its good balance between the computational cost and the embedding quality .", "Comments": [], "label": []}
{"id": 151959, "text": "Experiments show that G-Transformer converges faster and more stably than Transformer on different settings , obtaining the state-of-the-art results under both non-pretraining and pre-training settings .", "Comments": [], "label": []}
{"id": 157055, "text": "A Details of Human Evaluation We use the template shown in Figure [ 2 ] to conduct the human evaluation .", "Comments": [], "label": []}
{"id": 158008, "text": "We use the weight matrix of the reference layer and the prototype set of the support contextual embedding to compute the transformation matrix .", "Comments": [], "label": []}
{"id": 153903, "text": "On a wide range of tasks across NLU , conditional and unconditional generation , GLM outperforms BERT , T5 , and GPT given the same model sizes and data , and achieves the best performance from a single pretrained model with 1.25× parameters of BERTLarge , demonstrating its generalizability to different downstream tasks .", "Comments": [], "label": []}
{"id": 154535, "text": "The best models on the validation set ( average ROUGE ) have then been finetuned with the reinforcement learning objectives in [ 2 ] and [ 4 ] .", "Comments": [], "label": []}
{"id": 157462, "text": "In fact , in our implementation , we find that using sophisticated image embedders or pre-training with natural images , such as ImageNet [ Russakovsky et al. , 2015 ] , do not improve the final downstream Table 1 : Entity-level precision , recall , and F1 score comparisons on four standard benchmarks .", "Comments": [], "label": []}
{"id": 159926, "text": "Apart from FNet and Linear Transformers , which are efficient Transformer alternatives , we do not attempt an exhaustive comparison to non-MLP-based efficient NLP models .", "Comments": [], "label": []}
{"id": 160309, "text": "2 Related work BERT [ Devlin et al. , , 2018 ] is a contextualized word representation model based on the concept of masked language model and pre-trained using bidirectional Transformers [ Vaswani et al. , , 2017 ] .", "Comments": [], "label": []}
{"id": 155663, "text": "Therefore , Transformer-based models perform worse than LSTM-based models , as shown in Table [ 7 ] and Table [ 13 . ] However , our DM-models still substantially reduce the perplexity compared with base models .", "Comments": [], "label": []}
{"id": 157851, "text": "We obtained the p-values as in Table [ 2 : ] • Compared to T5 : 4.32e-11 ( BERT-entailment all ) , 5.20e-98 , ( BERT-entailment 1 ) , 2.48e-34 ( BERT-entailment 2 ) . • Compared to CoHS-CQG : 7.62e-188 ( CC Score ) , 5.12e-119 ( Conv-Distinct 1 ) , 8.11e-173 ( Conv-Distinct 2 ) .", "Comments": [], "label": []}
{"id": 158493, "text": "We also apply unsupervised denoising training on T5 with highly-cited papers , which makes the PLM itself learn more academic knowledge .", "Comments": [], "label": []}
{"id": 157320, "text": "Finally , we train several transformers models ( BERT-base-cased , BERT-base-uncased , RoBERTa-base , and DistilBERT-base-cased ) with a sequence classification head on top at a batch size of 32 [ 10 ] for 40 epochs ( due to computational reasons ) for the architectural baselines and till convergence for the methodological baselines .", "Comments": [], "label": []}
{"id": 154874, "text": "Our proposed method significantly outperforms the Transformer [ Vaswani et al. , , 2017 ] baseline , demonstrating that our proposed phrase-level universal visual representation can be helpful to NMT .", "Comments": [], "label": []}
{"id": 159984, "text": "For Es→En , we use all samples from CoVoST2 , Europarl-ST , and mTEDx [ Elizabeth et al. , , 2021 ] , and augment the training data by Common Voice and multilingual Librispeech ( MLS ) [ Pratap et al. , , 2020 ] , resulting in 1404-hour source speech .", "Comments": [], "label": []}
{"id": 158875, "text": "To obtain the golden extraction function , we use a surrogate strategy and the surrogate extraction function is the fine-tuned encoder using demonstrations .", "Comments": [], "label": []}
{"id": 153273, "text": "The target-side language model is a Transformer decoder without the cross-attention modules , which is trained synchronously with the translation model .", "Comments": [], "label": []}
{"id": 154625, "text": "New sessions refer back to previous subjects , explore them in depth , or spark up conversation on new topics .", "Comments": [], "label": []}
{"id": 152519, "text": "The recently published Sentence Transformers [ Reimers and Gurevych , 2019 ] method fine-tunes pretrained , transformer-based language models like BERT [ Devlin et al. , , 2019 ] using labelled NLI datasets .", "Comments": [], "label": []}
{"id": 153102, "text": "Better ROUGE scores are bolded . input truncation .", "Comments": [], "label": []}
{"id": 151603, "text": "[ You et al. , 2020a ] focuses on computer vision tasks with convolutional networks such as VGG [ Simonyan and Zis ] [ serman , 2014 ] and ResNet [ He et al. , , 2016 ] .", "Comments": [], "label": []}
{"id": 158643, "text": "Different split methods are formulated in detail in Table [ 2 . ] As for EEG2text , We use ZuCo1.0 datasets [ Hol ] [ lenstein et al. , , 2018 ] , which comprises EEG recordings obtained from natural reading tasks , including both Normal Reading ( NR ) and Task-Specific Reading ( TSR ) .", "Comments": [], "label": []}
{"id": 157595, "text": "In this work , we use the following 16 relationship types , and the authors of this paper manually annotated templates to verbalize knowledge triple to natural language questions .", "Comments": [], "label": []}
{"id": 156702, "text": "We use to denote approximated values , explained in Appendix [ C. ] nificantly in instruction-tuned models .", "Comments": [], "label": []}
{"id": 151989, "text": "We use beam search and apply the maximum length constraint on each sentence .", "Comments": [], "label": []}
{"id": 155892, "text": "We set the maximum numbers of update steps to 500 , 700 , 2,400 , and 5,000 respectively for QSGen-Hier , QSGen-ChildQ , WIKIBIOSUM , [ https : //huggingface.co/allenai/ ] ( https : //huggingface.co/allenai/led-large-16384 ) [ led-large-16384 ] ( https : //huggingface.co/allenai/led-large-16384 ) and GOVREPORT .", "Comments": [], "label": []}
{"id": 155246, "text": "Given that we use BLEU to evaluate the generation quality , we only keep the word form with most example sentences for each definition tuple ( lemma , POS , definition ) in the validation/test set .", "Comments": [], "label": []}
{"id": 155378, "text": "Mixture of Experts ( MoE ) [ Jacobs et al. , , 1991 ] [ Jordan and Ja ] [ cobs , 1994 ] [ Shazeer et al. , , 2017 ] , in a much easier way , enables Transformers to scale up the number of parameters meanwhile introducing an affordable computational overhead .", "Comments": [], "label": []}
{"id": 155927, "text": "Abstract We propose a novel method to sparsify attention in the Transformer model by learning to select the most-informative token representations during the training process , thus focusing on the task-specific parts of an input .", "Comments": [], "label": []}
{"id": 153773, "text": "5.1 Molecule-level alignment We used centered kernel alignment ( CKA ) [ Ko ] [ rnblith et al. , , 2019 ] with RBF kernel to compare representations between different layers .", "Comments": [], "label": []}
{"id": 155249, "text": "For generation baselines and our models , we use AdamW [ Loshchilov and ] [ Hutter , 2019 ] with an initial learning rate of 1e − 5 to update parameters for four epochs and choose the checkpoints with the lowest validation loss .", "Comments": [], "label": []}
{"id": 159899, "text": "In Fig- Figure 3 : Relative improvement of HyperMixer over Transformer depending on what percentage of the training set is used .", "Comments": [], "label": []}
{"id": 158647, "text": "Such trend could be attributed to the inherent limitations of the transformer model in effectively learning long-term dependencies .", "Comments": [], "label": []}
{"id": 159066, "text": "For instance , * S-KPM+DA * has higher ROUGE scores than * Unsup-KPM+IC * , while * Unsup-KPM+IC * performs worse than * S-KPM+DA * according to both Soft-F1 and human evaluation .", "Comments": [], "label": []}
{"id": 155172, "text": "One reason for this is that the ROUGE-2 summaries favor exact keyword matches in selecting sentences , so the trained model simply learned to keyword matching in extreme cases .", "Comments": [], "label": []}
{"id": 153682, "text": "We use the Sentence-BERT package [ Reimers and Gurevych , 2019 ] . factually aligned with the context [ Massarelli et al. , , 2020 ] . 4.2 Classification For discriminative models , we use the following input sequence where [ CLS ] and [ SEP ] are model-specific special tokens .", "Comments": [], "label": []}
{"id": 159718, "text": "Our strategy is as follows : ( 1 ) randomly sample a batch of * valid prompts * ( in our experiments we use batch size 16 ) from T ∗ and apply them to the whole training set Xtrain ; ( 2 ) record the performance of each prompt word , i.e .", "Comments": [], "label": []}
{"id": 156991, "text": "4.3 Validating the Multi-VALUE Pipeline To validate our perturbation rules , we use the task from [ Ziems et al. , 2022 ] in which each annotator [ https : //calebziems.com/resources/ ] ( https : //calebziems.com/resources/value/dialect-quiz.html ) [ value/dialect-quiz.html ] ( https : //calebziems.com/resources/value/dialect-quiz.html ) Table 1 : Accuracy of 92 perturbation rules according to majority vote with at least 5 unique sentence instances .", "Comments": [], "label": []}
{"id": 153055, "text": "We use a simple reinitialization method to achieve the two-way knowledge transfer .", "Comments": [], "label": []}
{"id": 156342, "text": "C Supplemental Material : Fake News Article Detection For Fake News Article detection , we used the dataset released publicly by [ Nguyen et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 156440, "text": "Although the smaller iteration step can further accelerate the speed , the perplexity drops significantly . 6.2 Length Prediction We validate the different length prediction strategies on the WP dataset , as shown in Table [ 7 . ] We initialize the full mask sequence with ground truth length to inference .", "Comments": [], "label": []}
{"id": 155765, "text": "We used their publicly available implementation from < https : //github.com/juntaoy/dali-bridging > .", "Comments": [], "label": []}
{"id": 158352, "text": "For running the dynamic tree sampling algorithm , we use 32 GPUs to speed up the proof-finding procedure .", "Comments": [], "label": []}
{"id": 156330, "text": "For , YouTube , the embedding we used was the average of the number of views , dislikes , and comments for each video the source posted .", "Comments": [], "label": []}
{"id": 159238, "text": "All the datasets are available to use for research purposes , and for our work , we use all these datasets intended for their original purpose , i.e. , NER .", "Comments": [], "label": []}
{"id": 151462, "text": "In order to automate evaluation of consistency , we use a version of Alchemy with synthetically generated text .", "Comments": [], "label": []}
{"id": 154403, "text": "To make the quantization more accurate , we adopt a flexible variant of the original PACT , with different positive and negative clipping factors [ −αneg , αpos ] , where both αneg and αpos are initialized as 2.5 .", "Comments": [], "label": []}
{"id": 155307, "text": "For training , we utilize the Adam optimizer with learning rate of 10− and the batch size is 32 .", "Comments": [], "label": []}
{"id": 156216, "text": "Subsequently , [ Devlin et al. , 2019 ] proposed the Bidirectional Encoder Representations from Transformers ( BERT ) which uses bidirectional transformers [ Vaswani et al. , , 2017 ] to create context-dependent representations .", "Comments": [], "label": []}
{"id": 152730, "text": "Table 7 : ROUGE of teacher models with different attention temperature coefficient λ on test set of CNNDM . set is shown in Table [ 7 . ] Their results are all decent and close to each other ( at least for ROUGE-1 and ROUGE-L ) .", "Comments": [], "label": []}
{"id": 156016, "text": "1 Introduction Transformers [ Vaswani et al. , , 2017 ] have gradually become a key component for many state-of-theart natural language representation models .", "Comments": [], "label": []}
{"id": 156298, "text": "Note that for BERTScore , we used the F-1 score FBERT , as recommended by the authors [ Zhang et al. , 2019 ] .", "Comments": [], "label": []}
{"id": 157346, "text": "3.1 CLS fusion BERT and ViT models have a CLS hidden state at the same dimension , h ( i ) CLS , that resumes the information of the input at the end of the i transformer block .", "Comments": [], "label": []}
{"id": 158851, "text": "We use the train , validation , and test splits described in [ §4.3.1 . ] 5.1.2 Methods We evaluate methods based on fine-tuning and fewshot learning .", "Comments": [], "label": []}
{"id": 157044, "text": "We use the following metrics to evaluate the semantic similarity of paraphrases : • Semantic similarity measure by SimCSE : Given two paraphrase sentences , we use the supervised SimCSE model [ Gao et al. , , 2021 ] to get the sentence embeddings , and compute the cosine similarity between the two sentence embeddings as the semantic similarity .", "Comments": [], "label": []}
{"id": 158372, "text": "If you used existing packages ( e.g. , for preprocessing , for normalization , or for evaluation ) , did you report the implementation , model , and parameter settings used ( e.g. , NLTK , Spacy , ROUGE , etc . ) ?", "Comments": [], "label": []}
{"id": 152477, "text": "Following [ Keung et al. , 2019 ] , we use multilingual BERT as the feature encoder here and denote the encoder as mBERT-TLADV .", "Comments": [], "label": []}
{"id": 159572, "text": "Here , we directly utilize mBART-50 [ Tang et al. , , 2021 ] as the meta pre-trained model . mBART-50 is a multi-lingual BART [ Lewis et al. , 2020 ] with the transformer encoder-decoder architecture .", "Comments": [], "label": []}
{"id": 159682, "text": "Vince . ( note the word * events * does not appear anywhere in this text ) , the ASR repair model realizes that the actual utterance should 've been , * Delete the period after Vince . * Indeed , the T5 ASR repair model is able to make the appropriate correction to this utterance .", "Comments": [], "label": []}
{"id": 154601, "text": "Data Collection To build our publicly available dataset we employ crowdworkers .", "Comments": [], "label": []}
{"id": 159468, "text": "We use the architecture selection method only to select the architecture , and the selected architecture is trained from scratch ( not pruned ) in the upcoming experiments .", "Comments": [], "label": []}
{"id": 155272, "text": "To the best of our knowledge , we are the first to examine these . 3 Extracting Rationales 3.1 Post-hoc Explanations We employ a pre-trained BERT-base and fine-tune it on in-domain training data .", "Comments": [], "label": []}
{"id": 158917, "text": "We present results for this alternative approach in Appendix [ G. ] In order to make the scales of swtu and swtd compatible , we use a scaled s˜wtu value instead of swtu in Equation [ 7 . ]", "Comments": [], "label": []}
{"id": 155008, "text": "We use an MLP network to reparameterize a continuous prompt during training as suggested in [ Li and Liang , 2021 ] .", "Comments": [], "label": []}
{"id": 156227, "text": "We use the LS07 trial set for training the sentence similarity metric model ( for 4 epochs ) and for fine-tuning the parameters of our framework based on the * best * score .", "Comments": [], "label": []}
{"id": 160173, "text": "For instance , [ Ram et al. , 2022 ] proposed `` recurrent span retrieval '' to obtain * psuedo * question-document pairs in an unsupervised way for single-hop QA .", "Comments": [], "label": []}
{"id": 160232, "text": "We use standalone trigger and argument MI modules to create markers for downstream ED+MI and EAE+MI joint models , instead of using the MI module jointly trained in the ED+MI or EAE+MI models because the standalone one yields better performance .", "Comments": [], "label": []}
{"id": 156305, "text": "https : //dl.fbaipublicfiles.com/fairseq/models/bart.base.tar.gz 5 https : //huggingface.co/google https : //tac.nist.gov/ Table 1 : Scores are summary-level ( TAC ) and segment-level ( WMT ) Pearson correlations averaged over 2008 to 2011 for TAC ( pyramid score/responsiveness ) and over all source languages for WMT-2019 .", "Comments": [], "label": []}
{"id": 153188, "text": "We use this incremental approach as a way to test our intuition about the performativity of groups of features ( * i.e . * does adding a feature improve the performance of the model ) with regard to the task of classification .", "Comments": [], "label": []}
{"id": 154337, "text": "Specifically , we utilize the cloze-style query with a single [ Mask ] token as the model input .", "Comments": [], "label": []}
{"id": 155537, "text": "Similarly , the results of the listening test for our Fast-Speech2 voice built with 1 hour of data are not significantly different from our Tacotron2 voice built with 10 hours of data .", "Comments": [], "label": []}
{"id": 159567, "text": "Following recent CLS literature [ Ladhak et al. , , 2020 ] [ Perez-Beltrachini ] [ and Lapata , 2021 ] , we use mBART-50 [ Tang et al. , , 2021 ] as the summarization model , and train the model in the following four settings : Table 2 : Correct language rate ( % ) of the summaries generated by mBART ( MLS ) and mBART ( M2MS ) .", "Comments": [], "label": []}
{"id": 160293, "text": "5.2 Scatter Plot Analysis To visually analyze the distribution patterns and the relationship between human annotations and scores obtained from different metrics , we utilize scatter plots to show instances distributions and best fit lines to depict the trend .", "Comments": [], "label": []}
{"id": 159611, "text": "We use k-fold cross-validation .", "Comments": [], "label": []}
{"id": 152264, "text": "The dialog act `` complaint '' is mapped to the heuristic group `` complain system repetition '' ; '' closing '' is mapped to the disengaged intent `` request termination '' ; `` hold '' to `` hesitation '' ; '' other_answers '' to `` unsure answer '' ; `` back-channeling '' to `` back-channeling '' , and `` neg_answer '' to 'negative answer ' '' .", "Comments": [], "label": []}
{"id": 152352, "text": "Note that max length was set first to prevent incomplete termination and length penalty was determined based on the ROUGE scores on validation dataset .", "Comments": [], "label": []}
{"id": 153333, "text": "We use fencepost representation [ Cross and Huang , 2016 ] [ Stern et al. , , 2017 ] to encode the i-th boundary lying between x and xi+1 : then we represent span ( i , j ) as : Decoder .", "Comments": [], "label": []}
{"id": 152246, "text": "The core insight of our work is to reframe the training data annotation process as a process of denoising labels created by heuristic functions pre-defined .", "Comments": [], "label": []}
{"id": 157446, "text": "T5 : what should you be cautious of ?", "Comments": [], "label": []}
{"id": 155113, "text": "We utilize Z num ( t ) instead of Z txt , loc ( t ) as we extract global textual features for each time-step t − k in window K rather than S .", "Comments": [], "label": []}
{"id": 159162, "text": "For Gujarati Bing-API is the best performing , with IndicTrans and NLLB performances being very close .", "Comments": [], "label": []}
{"id": 152203, "text": "5.1 Experimental Setting We consider three transformer-based LMs of a different nature : two masked LMs , namely BERT [ De ] [ vlin et al. , , 2019 ] and RoBERTa [ Liu et al. , , 2019 ] , and GPT-2 , as a prominent example of an autoregressive language model .", "Comments": [], "label": []}
{"id": 154621, "text": "D Model Training Settings We use the openly available [ ParlAI ] ( https : //parl.ai/ ) framework for all training runs , as well as for evaluations , where metrics are measured using default settings .", "Comments": [], "label": []}
{"id": 154335, "text": "To mitigate such bias , we also create a hard query set for each relation by selecting a subset of their corresponding 1k queries using token and matching metrics ( i.e. , exact matching and ROUGE-L [ Lin ] [ and Och , 2004 ] ) .", "Comments": [], "label": []}
{"id": 157165, "text": "The posterior cognitive distribution PCS ( cs | Y ) and the emotional one PEC ( ec | Y ) are calculated as : We then optimize the KL divergence between the prior and posterior distributions during training : To further ensure the accuracy of discerned knowledge , similar to [ Bai et al. , 2021 ] , we employ the BOW loss to force the relevancy between cognitive / emotional knowledge and the target response .", "Comments": [], "label": []}
{"id": 151966, "text": "We use s-BLEU and d-BLEU [ Liu et al. , , 2020 ] as the * metrics * .", "Comments": [], "label": []}
{"id": 154377, "text": "In this work , we re-parameterize the clipping factor to make the quantizer adaptive to each module in the Transformer layers , and consider both weights outside and inside the clipping range when estimating the gradient of the clipping factor .", "Comments": [], "label": []}
{"id": 156365, "text": "F Implementing MUST with more user simulators To implement our MUST with more user simulators , we use * Simulated Agenda Dataset * to train four extra user simulators [ 8 ] .", "Comments": [], "label": []}
{"id": 159993, "text": "Unit-based mBART ( u-mBART ) We use a umBART model trained with En and Es unlabeled speech on VoxPopuli .", "Comments": [], "label": []}
{"id": 152670, "text": "Case-sensitive detokenized SACREBLEU [ Post , 2018 ] is reported on the tst-COMMON testset from MUST-C [ Gangi et al. , , 2019 ] .", "Comments": [], "label": []}
{"id": 151910, "text": "The performance degradation is less significant for RTHN , partly due to its use of the Transformer architecture for context modeling .", "Comments": [], "label": []}
{"id": 160222, "text": "Recent work formulates the EE task as text generation with transformer-based pre-trained language models that prompt the generative model to fill in synthetic [ Paolini et al. , , 2021 ] [ Huang et al. , , 2021 ] [ Lu et al. , , 2021 ] [ Li et al. , , 2021 ] or natural language templates [ Huang et al. , , 2022 ] [ Hsu et al. , , 2022 ] [ Ma et al. , , 2022 ] [ Ye et al. , , 2022 ] .", "Comments": [], "label": []}
{"id": 151821, "text": "Instead , we employ GS distribution [ Jang et al. , , 2016 ] as the variational posterior of z for efficient gradient propagation .", "Comments": [], "label": []}
{"id": 158950, "text": "We adopt KCC ( Kendall 's correlation coefficient [ Abdi , 2007 ] ) and NDCG ( normalized discounted cumulative gain [ Järvelin and Kekäläinen , 2002 ] ) as evaluation metrics for ranking tasks , and demonstrate the results in Table [ 13 . ]", "Comments": [], "label": []}
{"id": 152193, "text": "We use a batch size of 64 and perform grid search for the number of epochs n ∈ { 1 , . . . , 20 } and the learning rate l ∈ { 1 × 10− , 3 × 10− , 1 × 10− , 3 × 10−5 } ( selection criterion : F1 score ) .", "Comments": [], "label": []}
{"id": 157983, "text": "While some of these Reddit messages are likely conversational , this classification scheme is only a heuristic aimed at helping filter data .", "Comments": [], "label": []}
{"id": 154343, "text": "Since different PLMs use different tokenizers , we use char length of the query answers to split MedLAMA into different bins and test the probing performance over various answer lengths .", "Comments": [], "label": []}
{"id": 153694, "text": "`` Covid-19 vaccines may be * the worst threat we face * '' ) , we use a pre-trained BERT propaganda detector [ Da San Martino et al. , , 2019 ] which we denote here as ( Prop-BERT ) .", "Comments": [], "label": []}
{"id": 151831, "text": "In the experiments , we use the Adam optimizer [ Kingma and Ba , 2014 ] with learning rate 10− .", "Comments": [], "label": []}
{"id": 153270, "text": "For clarity , we use t to mark the 'token-level ' intermediate variables and s to mark the 'sentence-level ' ones in the following formulas .", "Comments": [], "label": []}
{"id": 155469, "text": "We use different self-attention masks to control the behavior of the model and use various tasks to pre-train the model , including masked language modeling , unidirectional language modeling , and denoising objective .", "Comments": [], "label": []}
{"id": 157512, "text": "We believe that overcoming the aforementioned limitations is of critical importance for the future application of Transformer-based LMs to reasoning-intensive tasks such as data format conversion and robotic process automation .", "Comments": [], "label": []}
{"id": 153291, "text": "We emphasize that decipherment itself is * not * the purpose of the present work : rather , we use ciphers simply to re-encode data while preserving its meaning .", "Comments": [], "label": []}
{"id": 160413, "text": "This method is totally dependent on the Table 13 : Statements of the manual evaluation aspects . used sentence transformer model , and the similarity threshold to set .", "Comments": [], "label": []}
{"id": 159951, "text": "3 Experimental setting 3.1 Data We use three datasets : Fisher Es→En [ Post et al. , , 2013 ] ( 170 hours ) , CVSS-C [ Jia et al. , , 2022c ] ( 547 hours ) , and mutli-domain En↔Es [ Popuri et al. , 2022 ] ( 20k hours for En→Es , 14k hours for Es→En ) corpora .", "Comments": [], "label": []}
{"id": 155657, "text": "B Impact of the Dependency Parser In our work , we use an off-the-shelf dependency parser to get the dependency parse trees for dependency modeling .", "Comments": [], "label": []}
{"id": 155245, "text": "https : //www.nltk.org/api/nltk.tokenize.html Figure 3 : The overview of the BERT-based definition/POS evaluation model .", "Comments": [], "label": []}
{"id": 151627, "text": "We use the above described adapter configuration in all of our experiments , since it is adopted in most prior work with few modifications .", "Comments": [], "label": []}
{"id": 160291, "text": "AMR nodes are iterated over and replaced by their ( near- ) synonyms in PropBank , or their synset in Word-Net [ Opitz et al. , , 2021 ] .", "Comments": [], "label": []}
{"id": 152028, "text": "In G-Transformer , we enable only the top K layers with combined attention .", "Comments": [], "label": []}
{"id": 157896, "text": "After fine-tuning T5 , we leverage it to incrementally turn the previously constructed dialogue flow F = [ f s 1 , f 2 , . . . , f nf ] into a synthetic dialogue U = [ u s 1 , u 2 , . . . , u nf ] .", "Comments": [], "label": []}
{"id": 154604, "text": "This brings challenges in open-domain dialogue modeling due to the large context size , e.g . an average of 1614 tokens as tokenized by the BlenderBot BPE dictionary [ Roller et al. , , 2020 ] , where the Transformer used in that work has a truncation length of 128 .", "Comments": [], "label": []}
{"id": 157954, "text": "Metrics Let ai , j be the testing accuracy on the i-th task after training on j-th task , the metrics for evaluating are , Baselines We use the following continual learning techniques as baselines : sentence prediction and task-id prediction ) for learning better generic and specific representation spaces .", "Comments": [], "label": []}
{"id": 154695, "text": "The transformer encoder operation is applied to these embedding vectors without any positional information .", "Comments": [], "label": []}
{"id": 156127, "text": "We use these distances to construct a * distance vector * v for a representation , where each element vi is the distance between the clusters of a pair of labels .", "Comments": [], "label": []}
{"id": 156009, "text": "The T5 model will fill the blanks in the masked sentences .", "Comments": [], "label": []}
{"id": 156815, "text": "Licenses : TSA is under Apache License 2.0 ; SC is under CC BY-NC-SA 3.0 ; CD and StockSen are under CC BY 4.0 ; and Numercay-600K , NC , and NAD are under CC BY-NC-SA 4.0 .", "Comments": [], "label": []}
{"id": 158003, "text": "Since the adapted prompt contains both quote and reply , we use an flexible truncation process to maximize the content that can still fit within the maximum input token sequence length ( 196 ) .", "Comments": [], "label": []}
{"id": 154943, "text": "We highlight however that the permuted document self-supervision task that we train on is independent of the dataset used and the task can be reproduced on any other corpus ; see also [ §4.3 . ] All other datasets we use are licensed freely for academic use .", "Comments": [], "label": []}
{"id": 160147, "text": "4.1 Cross-modal Regularization with Scheduled Sampling ( CRESS ) To bridge the modality gap during inference , we adopt scheduled sampling for both ST and MT to approximate the inference mode at training time .", "Comments": [], "label": []}
{"id": 154712, "text": "Assuming that POS tagging facilitates named entity recognition , this empirical result suggests that increasing the amount of POS tag information < https : //github.com/pytorch/xla/ > Table 3 : Performance results on the machine translated GLUE benchmark [ Wang et al. , , 2019 ] .", "Comments": [], "label": []}
{"id": 154475, "text": "The first stage reranks passages returned by the retriever , where we use the passage embeddings generated by DPR as the initial GNN node representation .", "Comments": [], "label": []}
{"id": 156214, "text": "SpreasheetCoder We implement Spreadsheet-Coder mainly following its paper including the BERT-based table context ( row/column ) encoder , two-stage decoder .", "Comments": [], "label": []}
{"id": 158452, "text": "For inference memory footprint , we use the same batch size for the original VL model and PuMer version and report the peak memory difference .", "Comments": [], "label": []}
{"id": 154049, "text": "T5 is fine-tuned with several supervised multi-task training objectives ( * e.g. , * machine translation , text summarization ) .", "Comments": [], "label": []}
{"id": 157144, "text": "We use the AdamW [ Loshchilov and Hutter , 2017 ] optimizer for downstream fine-tuning , and a batch size of 32 for BERT and RoBERTa , and a batch size of 2 for GPT-2 .", "Comments": [], "label": []}
{"id": 159282, "text": "YONO ( You Only Need One model ) [ Lee et al. , , 2021a ] is a representative model in this way , which integrates retriever , reranker , and generator models into a T5-large based singular transformer pipeline .", "Comments": [], "label": []}
{"id": 157109, "text": "However , we notice that on the IU X-RAY dataset , there is still a performance gap between our model and the best baseline ( i.e. , CMCA ) .", "Comments": [], "label": []}
{"id": 152126, "text": "To verifies the effectiveness of GIT to capture cross-sentence information , we first calculate the average number of sentences that the records involve for each document , and sort them in ascending order .", "Comments": [], "label": []}
{"id": 156744, "text": "LEXSYM We use automatic lexicon extraction to find semantic correspondence relations ( rϵ ) and types ( { rτ } ) as described in Appendix [ D. ] Next , we apply swap-based augmentation ( Eq . [ 3 ] .", "Comments": [], "label": []}
{"id": 155527, "text": "C Details of Masked Language Modeling Task C.1 Model configuration For the experiment with masked language modeling ( [ §5 ] , we set the number of layers of the Transformer encoders to 3 .", "Comments": [], "label": []}
{"id": 158882, "text": "For encoder-decoder models like T5 , we formulate in-context NER as a span corruption task and the model will generate the extraction task .", "Comments": [], "label": []}
{"id": 152095, "text": "We employ a generative classification method GDA , compared with the traditional MSP method , to make full use of data features and alleviate the problem . 6.2 Challenges Based on the above analysis , we summarize the current challenges faced by the NSD task : Function tokens .", "Comments": [], "label": []}
{"id": 151933, "text": "Specifically , for the i-th source token , we use h ∗ i as the query vector and h ∗ +1 , . . . , h ∗ + as the key vectors .", "Comments": [], "label": []}
{"id": 153018, "text": "We use the Adam Optimizer [ Kingma and Ba , 2015 ] with β = 0.9 and β = 0.98 to train our models .", "Comments": [], "label": []}
{"id": 159528, "text": "Experimental results on 10 NLP tasks show that our approach can consistently improve the performance of BERT , RoBERTa , BART , and T5 at different scales , and outperform several competitive baselines signifcantly .", "Comments": [], "label": []}
{"id": 158510, "text": "Multi-DYLE degenerates to DYLE when M = 1 using X ( 1 ) as a set of ROUGE-based extractive oracles .", "Comments": [], "label": []}
{"id": 158837, "text": "C Implementation Detail For binary classification of each intention category , for the transformer classifiers , we used a dropout layer ( with a rate of 0.5 ) , followed by a fully connected layer and a Sigmoid output layer .", "Comments": [], "label": []}
{"id": 156047, "text": "For each sequence-length configuration in F a Transformer with a token selection method is applied at both fine-tuning and inference .", "Comments": [], "label": []}
{"id": 157790, "text": "We adopt binary classifiers as decoders since they are more powerful than simple scoring functions .", "Comments": [], "label": []}
{"id": 159363, "text": "To generate y ′ , we use Nucleus Sampling [ Holtzman et al. , , 2020 ] with p = 0.96 , as this sampling method has shown advantages in * open-ended * generation [ Holtzman et al. , 2020 ] [ Zellers et al. , , 2019 ] .", "Comments": [], "label": []}
{"id": 155453, "text": "We used ERRANT to evaluate the accuracy of EB-GEC for each error type on the FCE-test . [ Table 3 ] shows three error types selected as having the most significant increase and decrease in accuracy for EB-GEC compared to the vanilla GEC .", "Comments": [], "label": []}
{"id": 155695, "text": "We use a peak learning rate 3e-4 and train for 40,000 steps .", "Comments": [], "label": []}
{"id": 157440, "text": "Generated Question with Knowledge Transferred from MS MARCO PEGASUS : when was bouvier born BART : when did jacqueline bouvier go to college T5 : when did jacqueline bouvier enter vogue SPARTA ( PEGASUS ) : when did bouvier go to college SPARTA ( BART ) : when did she go to vogue SPARTA ( T5 ) : when did she go to college Generated Question with Knowledge Transferred from NewsQA PEGASUS : what year did bouvier graduate from george washington university ?", "Comments": [], "label": []}
{"id": 153409, "text": "Within the same sampling setting , OBPE is better than corresponding BPE . 4.5 Ablation study We conducted experiments for different values of p that controls the amount of overlap in the generalized mean function ( Equation [ 5 ] ) .", "Comments": [], "label": []}
{"id": 155454, "text": "[ Svyatkovskiy et al. , 2020 ] proposes GPT-C that employs a left-to-right Transformer [ Vaswani et al. , 2017 ] to support generation tasks such as code completion , but the unidirectional framework is sub-optimal for understanding tasks .", "Comments": [], "label": []}
{"id": 160030, "text": "Then we input E into L stacked Transformer blocks , and each Transformer layer acts as follows : where H denotes the output of the l-th layer and H = E ; LN is layer normalization ; MHAttn is the multi-head attention mechanism ; FFN is a twolayer feed-forward network with ReLU as hidden activation function .", "Comments": [], "label": []}
{"id": 154505, "text": "When applying reinforcement learning to MDS , we contend that the reward should not simply be a ROUGE score against the reference summary , since this would dismiss key characteristics of the task such as inter-document information transfer .", "Comments": [], "label": []}
{"id": 157859, "text": "Following [ Tran Phu ] [ and Nguyen , 2021 ] , we employ 10-fold crossvalidation and only evaluate ECI performance for intra-sentence event pairs because the number of inter-sentence event pairs in Causal-TimeBank is quite small ( i.e. , only 18 pairs ) .", "Comments": [], "label": []}
{"id": 152653, "text": "In addition to these soft scores , we used the uniform-level threshold ( i.e. , 1/n ) to reach a binary score indicating tokens selected in each layer .", "Comments": [], "label": []}
{"id": 151629, "text": "We use a subset of GLUE tasks [ Wang et al. , , 2018 ] for our analysis .", "Comments": [], "label": []}
{"id": 153610, "text": "We use the span-level precision ( P ) , recall ( R ) and their F1 for evaluation , since OEI is essentially a span recognition task .", "Comments": [], "label": []}
{"id": 154213, "text": "https : // [ spacy.io ] ( https : //spacy.io/usage/v2-3 ) /usage/v2-3 .", "Comments": [], "label": []}
{"id": 151956, "text": "To gain more understanding , we make dedicated experiments on the influence of input length , data scale and model size for Transformer ( Section [ 3 ] , finding that a Transformer model can fail to converge when training with long sequences , small datasets , or big model size .", "Comments": [], "label": []}
{"id": 153163, "text": "For opinion extraction , we utilize a widely-used sentiment lexicon named Senti-WordNet [ Esuli and Sebastiani , 2006 ] to obtain the dictionary of opinion words .", "Comments": [], "label": []}
{"id": 154963, "text": "Following [ Scialom et al. , 2020 ] , we report the F1 ROUGE-L score of NLSSum with a full Python implemented ROUGE metric [ 3 ] , which calculates the overlap lexical units between extracted sentences and ground-truth .", "Comments": [], "label": []}
{"id": 156862, "text": "Bold fonts suggest the best results in the block . harder because translation knowledge may not be covered a lot during the pre-training process of T5 .", "Comments": [], "label": []}
{"id": 156270, "text": "3.2 Graph Neural Network We use the standard relational graph convolutional network ( RGCN ) [ Sejr Schlichtkrull et al. , , 2018 ] to obtain the graph representation of the context enriched with coreference information .", "Comments": [], "label": []}
{"id": 157293, "text": "Therefore , in this research , we use the style representations from the pre-trained classifier to compute the cosine similarity between the text ads to verify existing vendors and identify potential aliases and unknown vendors in an open-set environment [ Zhou et al. , , 2021a ] .", "Comments": [], "label": []}
{"id": 154626, "text": "CLIP has an imageonly encoder , which is either a ResNet-based architecture [ He et al. , , 2016 ] or a visual transformer [ Dosovitskiy et al. , , 2021 ] , and a text-only transformer .", "Comments": [], "label": []}
{"id": 154974, "text": "5.2 Human Evaluation The human evaluation is important for summarization tasks , since the ROUGE can only determine the textual representation overlapping .", "Comments": [], "label": []}
{"id": 159559, "text": "Thus in our work , we adopt manual prompts to stimulate model knowledge and help data-efficient model adaptation .", "Comments": [], "label": []}
{"id": 157912, "text": "By comparing ( BB-SynDG * w/o FF & UF * ) with BB-RS , we find that random sampling to obtain dialogue flow is less effective and even harms the performance ( BB-RS on WoW Seen under the KU setting ) , while our proposed heuristic sampling method works better .", "Comments": [], "label": []}
{"id": 156307, "text": "We used two approaches to compute the Pearson correlations : summary-level ( or segment-level ) and system-level .", "Comments": [], "label": []}
{"id": 158487, "text": "The lower right part is the idea verbalization module . above dictionary , are fed to our pretrained model T5 to verbalize an idea .", "Comments": [], "label": []}
{"id": 160225, "text": "We use T5-large [ Raffel et al. , , 2020 ] as the backbone text generation model for the two joint models .", "Comments": [], "label": []}
{"id": 157935, "text": "* * Roundtrip Alignment * * For assessing the quality of multilingual representations for a broad range of tail languages without human gold data , we adopt roundtrip evaluation [ Dufter et al. , , 2018 ] .", "Comments": [], "label": []}
{"id": 151790, "text": "The transformer encoder for language l predicts a masked token in a sentence in l as the monolingual loss .", "Comments": [], "label": []}
{"id": 154083, "text": "The Elasticsearch-based pre-filter process also helps manage the computational need .", "Comments": [], "label": []}
{"id": 153100, "text": "Experimental results show we outperform the state-of-the-art in all the ROUGE metrics , proving better capability to discriminate relevant information across many related documents and merge it consistently ( Fig .", "Comments": [], "label": []}
{"id": 156222, "text": "In order to have a fair comparison with the previous state-of-the-art models , for both datasets we used their processed versions as used in [ Melamud ] [ et al. , ] [ 2015b , 2016 ] .", "Comments": [], "label": []}
{"id": 159300, "text": "[ 2 ] In the fourth to sixth rows of Table [ 4 , ] it is clear to see 1 [ https : //huggingface.co/spaces/ ] ( https : //huggingface.co/spaces/evaluate-measurement/perplexity ) [ evaluate-measurement/perplexity ] ( https : //huggingface.co/spaces/evaluate-measurement/perplexity ) Note that different from [ Li et al. , 2022 ] which uses 2 as the base of the exponential function , we employ e as the base .", "Comments": [], "label": []}
{"id": 155964, "text": "All of the considerations assumed the use of dot-product attention except for LSH and Efficient Transformers .", "Comments": [], "label": []}
{"id": 155722, "text": "In our experiments , we confirm that the MFS significantly outperforms the standard softmax layer and alleviates the * softmax bottleneck * in the transformer-based LMs such as GPT-2 better than * mixture of softmax * ( MoS ) . 9 Acknowledgement We thank Michael Boratko , Jay Yoon Lee , Sabrina J .", "Comments": [], "label": []}
{"id": 151728, "text": "KATXGB is implemented using xgboost 0.9 with objective function * binary : logistic * .", "Comments": [], "label": []}
{"id": 155592, "text": "Table 4 : Results on the Summarization dataset : EDITCL improves ROUGE-F1 and SARI over EDITOR . marization on uncased text .", "Comments": [], "label": []}
{"id": 159268, "text": "To simplify following explanations , we use * h * , * t * , * e * and * p * to represent holder , target , expression , and polarity , respectively .", "Comments": [], "label": []}
{"id": 158545, "text": "Second , we investigate a graph-based semantic refinement transformer , which solves the limitation of insufficient semantic relationship information between utterances .", "Comments": [], "label": []}
{"id": 157200, "text": "As we used groups of datasets we report here the full list of datasets they contain .", "Comments": [], "label": []}
{"id": 158388, "text": "With respect to control strategy performance , we find that our content questions obtain the highest Rouge scores ( 42.4 Rouge-1 ) , outperforming keyword chains with only 25.0 Rouge-1 .", "Comments": [], "label": []}
{"id": 156436, "text": "The data noted with * represent our implementation . and lower perplexity , demonstrating the effectiveness of our model .", "Comments": [], "label": []}
{"id": 152824, "text": "We use exact match between strings to identify reference changes .", "Comments": [], "label": []}
{"id": 156094, "text": "Recent advances on pre-trained transformers have pushed this direction ; for example , [ Lewis et al. , 2020a ] jointly trained a generative QA model along with a text retrieval model , and [ Roberts et al. , 2020 ] explored an ambitious approach to directly generate an answer without any evidence documents .", "Comments": [], "label": []}
{"id": 155570, "text": "For baseline model parameters , we use the recommended set of parameters from the authors ' original papers .", "Comments": [], "label": []}
{"id": 155030, "text": "The model consists of 24 transformer layers , and the hidden size d of the model is set to 1,024 .", "Comments": [], "label": []}
{"id": 160060, "text": "We compare our implementation and reference SOTA scores in Appendix Table [ 7 . ] We use the RSOTA settings as the starting point to conduct the experiments .", "Comments": [], "label": []}
{"id": 156661, "text": "We use a DeBERTa-v2 [ He et al. , , 2020 ] model with SOTA performance on NLI as the teacher model .", "Comments": [], "label": []}
{"id": 158335, "text": "We show in Fig . [ 1 ] an example of a theorem proven in Lean [ de Moura et al. , , 2015 ] , a formal environment we use in this work .", "Comments": [], "label": []}
{"id": 158816, "text": "All three algorithms we employ that use this type of feedback ( reranking , reward-based learning , and DIRECTOR ) all show gains over the baseline without feedback , with improvements consistent across both BB2 and SeeKeR model variants .", "Comments": [], "label": []}
{"id": 159872, "text": "When using attention as token mixing the whole layer is equivalent to a Transformer encoder layer . low-resource regime ( D ) ; and efficient tuning for hyperparameters ( H ) .", "Comments": [], "label": []}
{"id": 154755, "text": "For the compressive transformer , both memories have size 1,024 .", "Comments": [], "label": []}
{"id": 158505, "text": "] ( https : //github.com/hkim-etri/ExplainMeetSum ) [ com/hkim-etri/ExplainMeetSum ] ( https : //github.com/hkim-etri/ExplainMeetSum ) By newly employing the evidence-based supervised extractor , the experimental results on the QM-Sum dataset show that the proposed Multi-DYLE outperforms DYLE with an increase of 3.13 in the ROUGE-1 score .", "Comments": [], "label": []}
{"id": 152327, "text": "In particular , we use ImageNet pretrained ResNet101 [ He et al. , , 2016 ] , which is widely used as a backbone network .", "Comments": [], "label": []}
{"id": 153027, "text": "As we can see , in the first setting with one constraint segment , BiTIIMT achieves modest speedup in decoding time compared with the Transformer baseline .", "Comments": [], "label": []}
{"id": 157805, "text": "We use AUC-PR to monitor the training of the classifiers .", "Comments": [], "label": []}
{"id": 155025, "text": "We train a separate Transformer encoder and an adapter network that directly maps a source sentence into a deep continuous prompt , leaving the mGPT model only serving as a decoder .", "Comments": [], "label": []}
{"id": 156834, "text": "5 Evidence Extraction Methods This section introduces several evidence extraction methods that we implemented within a convolutional neural network based model to establish baselines for code evidence extraction on MDACE .", "Comments": [], "label": []}
{"id": 151526, "text": "For a fair comparison between the two methods , we use only the per-sample loss in our primary ( single-model ) distillation methods .", "Comments": [], "label": []}
{"id": 156291, "text": "ROSE [ Conroy and Dang , 2008 ] is a linear combination model of different variants of ROUGE using canonical correlation .", "Comments": [], "label": []}
{"id": 152261, "text": "For heuristic groups 1 to 3 , if any segment contains a disengaged intent , the user response is auto-labeled as disengaged .", "Comments": [], "label": []}
{"id": 158673, "text": "StarE [ Galkin et al. , , 2020 ] leverages a message passing network , CompGCN [ Vashishth et al. , , 2020 ] , as an encoder to obtain the relation and entity embeddings , which are then fed into a transformer decoder to obtain the validity of facts .", "Comments": [], "label": []}
{"id": 154460, "text": "We present more details and metrics of data collection in Appendix [ C. ] 5.3 Results We use the hyper-parameters provided in the original papers .", "Comments": [], "label": []}
{"id": 151513, "text": "We use the standard task setting for XNER , where we take 100 % samples from the datasets as they come from various domains and sizes without any specific bias .", "Comments": [], "label": []}
{"id": 158052, "text": "In particular , we adopt a higher learning rate for the routers z to change quickly , and adopt a lower learning rate for the modular prompts p to change slowly and stably .", "Comments": [], "label": []}
{"id": 159579, "text": "The implementation details of the pre-training objectives , pre-training corpora and fine-tuning hyper-parameters are given in Appendix [ B . ] 5.3 Quantitative Results Table [ 4 ] shows the results on WikiLingua in terms of average ROUGE score ( RS ) and BERTSCORE ( BS ) .", "Comments": [], "label": []}
{"id": 154854, "text": "It contains about 102K pairs in total . 2.2 Latent-Variable Model For an image region r , we can obtain the visual features v with a pre-trained ResNet-101 Faster R-CNN [ He et al. , , 2016 ] [ Ren et al. , , 2015 ] , which contains rich visual information ( * e.g . * , color , size , shape , texture , and background ) .", "Comments": [], "label": []}
{"id": 153402, "text": "Here we used learning-rate 2e-5 and batch size 32 , with training duration as 16 epochs for NER , 8 epochs for POS and 3200 iterations for Text Classification and XNLI .", "Comments": [], "label": []}
{"id": 152151, "text": "Experiments on large-scale public dataset [ Zheng et al. , 2019 ] show GIT outperforms previous stateof-the-art by 2.8 F1 .", "Comments": [], "label": []}
{"id": 154960, "text": "The implementation of Tα ( · ) = σ ( g ( T 0 ( · ) ) ) is a two-layer transformer model T 0 ( · ) followed by a linear layer g ( · ) and a sigmoid function .", "Comments": [], "label": []}
{"id": 160318, "text": "A considerable amount of computational resources was used to conduct this study , since approximately 18,000 hours of GPU computation were used to create the 7 models presented here , as well as about 7,500 hours of GPU for debugging due to technical issues related to model configurations and poor performance , for a total of 25,500 hours .", "Comments": [], "label": []}
{"id": 158668, "text": "Algorithm 1 Normalizing Tail Events Input : A set of annotations A and relations R Output : A set of sentences in present tense F A `` ` 7 : end if `` ` B Implementation Details Rel-CSKGC We use RoBERTa-large containing 335M parameters as the base model .", "Comments": [], "label": []}
{"id": 154116, "text": "This could be because the hyperparameter sweep we used for our ParaBLEU models ( the same sweep as recommended by the authors of RoBERTalarge ) is suboptimal and a broader hyperparameter sweep may be required .", "Comments": [], "label": []}
{"id": 152829, "text": "We use a T5 [ Raffel et al. , , 2020 ] model fine-tuned for question-independent answer selection c → a on NQ , and select the top 15 candidates from beam search .", "Comments": [], "label": []}
{"id": 155484, "text": "For BigCloneBench dataset , we use the dataset provided by [ Lu et al. , 2021 ] , which includes 901,724/416,328/416,328 examples from 10 different functionalities for training/validation/testing .", "Comments": [], "label": []}
{"id": 158087, "text": "It means that the context S˜ is set to ˜ { w |w ∈ Pcontext } for all the three parts Pmask , Ptarget and Pcontext . 2.1.3 Pretraining Loss We adopt different loss functions for the masked part and the target part .", "Comments": [], "label": []}
{"id": 157916, "text": "We speculate that this may be due to that the scale of the LM we used limits the upper bound of the quality of the synthesized dialogues .", "Comments": [], "label": []}
{"id": 157890, "text": "Then , for each turn of a synthetic dialogue , at most one knowledge piece is sampled from K with heuristic constraints .", "Comments": [], "label": []}
{"id": 157775, "text": "Even though LongT5 has a maximum input sequence length of 16,384 , we chose 4,096 to match PRIMERA and because of GPU memory constraints .", "Comments": [], "label": []}
{"id": 154747, "text": "In a vanilla transformer , this is not feasible for long contexts due to the high memory requirements .", "Comments": [], "label": []}
{"id": 153227, "text": "B Implementation details We use `` bert-large-cased '' for PTB , `` bert-basechinese '' for CTB , and `` bert-multilingual-cased '' for UD , so the dimension of the input BERT embedding is 1024 , 768 , and 768 respectively .", "Comments": [], "label": []}
{"id": 158991, "text": "Specifically , we find that this T5-based rewriter trained to remove adversarial perturbations for the sentiment classification task on the SST-2 dataset , also removes adversarial perturbations for a news classification model ( on AGNews ) , increasing adversarial robustness by over 10 % .", "Comments": [], "label": []}
{"id": 158042, "text": "We use a keywordbased filter for both the dialogue and intent data before training our models .", "Comments": [], "label": []}
{"id": 154842, "text": "B.3 Network Embedding We use four datasets , referred to as Disease , Airport , Pubmed and Cora .", "Comments": [], "label": []}
{"id": 158414, "text": "References A Preparation of Evaluation Benchmarks For ID data , we use the train splits of the IMDB dataset on sentiment analysis [ Maas et al. , , 2011 ] , and the 20NewsGroups dataset on topic classification [ Lang , 1995 ] .", "Comments": [], "label": []}
{"id": 152694, "text": "However , the sequence-level knowledge of teacher mod- See the detailed example in Appendix [ E. ] We use cross attention because we can see how words in documents are selected during generation . els is not well utilized .", "Comments": [], "label": []}
{"id": 158176, "text": "Blue cells indicate agreement and red cells indicate disagreement towards the political proposition . center categories based on Allsides . [ 6 ] For social media , we use the left-leaning and right-leaning subreddit lists by [ Shen and Rose , 2021 ] and the PushShift API [ Baumgartner et al. , , 2020 ] .", "Comments": [], "label": []}
{"id": 151334, "text": "We use a shallow approximation of syntax , to ensure the availability of equivalent exemplars in the training data .", "Comments": [], "label": []}
{"id": 154924, "text": "[ 2 ] We use the corresponding words , contexts , and definitions in CWN for the definition generation task .", "Comments": [], "label": []}
{"id": 158405, "text": "We employ distance-based methods for zeroshot OOD detection , which measure the relative distances of samples in representation space .", "Comments": [], "label": []}
{"id": 157626, "text": "A.2 Training with ITR Configuration We use only TaPEx as a baseline for ITR at training time .", "Comments": [], "label": []}
{"id": 151965, "text": "We use the same settings of BART large model [ Lewis et al. , , 2020 ] , which involves 12 layers , 16 heads , 1024 dimension outputs , and 4096 dimension hidden vectors .", "Comments": [], "label": []}
{"id": 152909, "text": "We use Wikipedia articles as the monolingual data for pre-training and report the data statistics in Table [ 1 . ]", "Comments": [], "label": []}
{"id": 154767, "text": "Efficient transformers .", "Comments": [], "label": []}
{"id": 151614, "text": "We use the default training settings for pre-training and fine-tuning on both models .", "Comments": [], "label": []}
{"id": 158080, "text": "We observe that our system outperforms baselines by a large margin when the full knowledge Since the training scripts of Q-TOD is not released , we directly use its open-source checkpoint ( T5-Large ) and conduct inference with the full knowledge base .", "Comments": [], "label": []}
{"id": 153770, "text": "We used average recall at K ( R @ 1 and R @ 5 ) to measure the percentage of the ground truth that appears in the top K retrieved molecules .", "Comments": [], "label": []}
{"id": 160077, "text": "We use 100,000 sentences sampled from Wikipedia [ 4 ] as the corpus C [ 5 ] , and we use the following unsupervised sentence encoders for E1 , SimCSE [ Gao et al. , , 2021 ] , Prompt-BERT [ Jiang et al. , , 2022 ] , and SNCSE [ Wang et al. , , 2022 ] .", "Comments": [], "label": []}
{"id": 158721, "text": "In addition , to satisfy the requirements of the Stage II pretraining , we use the BLIP model [ Li et al. , , 2022b ] implemented by [ Li et al. , 2022a ] to generate the appropriate textual caption for each image .", "Comments": [], "label": []}
{"id": 152934, "text": "For en-es and en-fr , LARGE-BPE systems outperform the concurring SMALL-CHAR by ∼2 points only – a slim advantage compared to the large gap observed on BLEU score .", "Comments": [], "label": []}
{"id": 155806, "text": "We use inverse square root learning rate scheduling for WMT14 and a linear annealing learning rate from 3.0 × 10− to 1.0×10− in 250K steps for IWSLT14 .", "Comments": [], "label": []}
{"id": 155349, "text": "To perform word-level force alignment , we use Montreal Forced Aligner [ 2 ] toolkit , whose acoustic model is trained with LibriSpeech [ Panayotov et al. , , 2015 ] .", "Comments": [], "label": []}
{"id": 159365, "text": "Empirically , we set α = 1 and β = 0.01 . < https : //query.wikidata.org/ > < https : //stanfordnlp.github.io/stanza > Table 2 : Examples of the two generated propaganda techniques , as shown by texts in blue .", "Comments": [], "label": []}
{"id": 153627, "text": "Familiar readers may recognize this as the contrastive loss from SimCLR [ Chen et al. , , 2020 ] , for which we use random span sampling as augmentation .", "Comments": [], "label": []}
{"id": 154532, "text": "We speculate that this behavior is due to the larger scale of the coverage reward , as by providing more capacity to the network , we allow the control variate to increasingly correlate with the multi-document coverage reward rather than the ROUGE reward .", "Comments": [], "label": []}
{"id": 153275, "text": "We use Adam optimizer [ Kingma and Ba , 2014 ] with 4000 warmup steps to optimize models .", "Comments": [], "label": []}
{"id": 157103, "text": "For MIMIC-CXR , the training epoch of < https : //github.com/tylin/coco-caption > Table 2 : Ablation results of our model and its variants , where ORGAN * w/o * Plan is the standard Transformer model . the planner/generator is set to 3/5 , and β is set to 5 .", "Comments": [], "label": []}
{"id": 154118, "text": "We extract the visual feature v for an image with the ResNet , and append the question text with all the answers in the answer set as a text sequence .", "Comments": [], "label": []}
{"id": 158690, "text": "The offline model is made of 12 Conformer encoder layers [ Gulati et al. , , 2020 ] and 6 Transformer decoder layers ( dmax = 6 ) with a total of ∼115M parameters .", "Comments": [], "label": []}
{"id": 159101, "text": "In the tasks with Word2Vec as the background model , we train GRM with five learning rates { 5e−3 , 3e−3 , 1e−3 , 8e−4 , 5e−4 } and select the best one to report results .", "Comments": [], "label": []}
{"id": 157222, "text": "To keep the comparison relatively fair , we used only an LSTM encoder and randomly initialized token embeddings for this experiment .", "Comments": [], "label": []}
{"id": 153287, "text": "Although we introduce an additional LM to calculate the CBMI values , it only brings a slight increase of model parameters and GPU calculation cost during model training .", "Comments": [], "label": []}
{"id": 153717, "text": "We do inference with the zero-shot and finetuned T5 models .", "Comments": [], "label": []}
{"id": 156091, "text": "* indicates that the result is taken directly from the original paper [ Ferguson et al. , , 2020 ] ( see their Table-3 ) , while * * indicates that we obtain the result of FID with our implementation .", "Comments": [], "label": []}
{"id": 157844, "text": "This heuristic may not be optimal for the model to gather necessary information from history to generate conversational questions in the next turns , as discussed by [ Do et al. , 2022 ] .", "Comments": [], "label": []}
{"id": 157651, "text": "Instead of extracting silver labels , We use the gold Instruction State ( i.e. , `` tracker_completed_step '' in CookDial ) and User Intent information from the CookDial dataset .", "Comments": [], "label": []}
{"id": 159596, "text": "Specifically , in the case of ROBERTA , wordpieces are masked during the pre-training process , while for T5 , spans are masked .", "Comments": [], "label": []}
{"id": 155807, "text": "We use tokenized and cased BLEU scores [ Papineni et al. , , 2002 ] [ 7 ] to evaluate the generation quality of MT and PG tasks .", "Comments": [], "label": []}
{"id": 155744, "text": "The templates we use are listed in [ Table 12 . ] For the * family * category , our templates assume the words are not pronouns , so we exclude the set of four words that include * he * or * she * .", "Comments": [], "label": []}
{"id": 151385, "text": "5.1 Experimental Settings We use four mainstream Chinese NER benchmarking datasets : Weibo [ Peng and Dredze , 2015 ] [ He and Sun , 2016 ] , Resume [ Zhang and Yang , 2018 ] , MSRA [ Levow , 2006 ] , and Ontonotes 4.0 [ Weischedel and Consortium , 2013 ] .", "Comments": [], "label": []}
{"id": 155474, "text": "We use the smoothed BLEU-4 [ Lin and Och , 2004 ] as the evaluation metric and report overall score of six PLs , including Ruby , JavaScript , Go , Python , Java , and PHP .", "Comments": [], "label": []}
{"id": 155128, "text": "In this paper , we adopt the risk aversion formulation [ Fabozzi et al. , , 2007 ] of the mean-variance risk minimization model by [ Markowitz , 1952 ] , which models both portfolio return and risk .", "Comments": [], "label": []}
{"id": 152759, "text": "Following previous work [ Karpukhin et al. , 2020 ] [ Oguz et al. , , 2020 ] , we use the uncased BERT-base [ Devlin et al. , , 2019 ] model as the encoder , where the [ CLS ] token representation is used as the document/question vector .", "Comments": [], "label": []}
{"id": 153912, "text": "T5 proposes a similar blank infilling objective to pretrain an encoder-decoder Transformer .", "Comments": [], "label": []}
{"id": 154577, "text": "For fair comparison , we use the smoothed BLEU-4 score [ Lin and Och , 2004 ] as in CodeXGLUE .", "Comments": [], "label": []}
{"id": 152695, "text": "3 Summarization Distillation 3.1 Transformer based abstractive summarization Abstractive summarization aims to rewrite a document into its shorter form ( i.e. , summary ) , which is a typical Seq2Seq learning problem .", "Comments": [], "label": []}
{"id": 153557, "text": "In contrast , T5 can produce more granular information at the city level , and maintain a relatively stable score across domains ( 70-81 % EM-city ) .", "Comments": [], "label": []}
{"id": 160349, "text": "Table 6 : BLEU scores of bilingual S2S models on FLEURS sets . ( 2 ) XM Transformer .", "Comments": [], "label": []}
{"id": 157388, "text": "Joe Davison , Sam Shleifer , Patrick von Platen , Clara Ma , Yacine Jernite , Julien Plu , Canwen Xu , Teven Le Scao , Sylvain Gugger , Mariama Drame , Quentin Lhoest , and Alexander Rush .", "Comments": [], "label": []}
{"id": 153127, "text": "Also for all datasets , we only train on the train-sets given and we use valid-sets to do early stopping , whereas some works ( like AUTSL ) train on combination of train-set and val-set to report the final test accuracy .", "Comments": [], "label": []}
{"id": 159988, "text": "We used the same number of samples for all direct S2ST models for a fair comparison .", "Comments": [], "label": []}
{"id": 152578, "text": "We use the combination of EuroParl-ST [ Iranzo-Sanchez et al . ] ´ , [ 2020 ] and CoVoST 2 [ Wang et al. , , 2020b ] for both ASR and ST labeled data in 3 languages ( directions ) .", "Comments": [], "label": []}
{"id": 152086, "text": "Therefore , we adopt Remove as the main strategy in this paper .", "Comments": [], "label": []}
{"id": 159485, "text": "In the second stage , we use a denoising autoencoder ( DAE ) loss to train another encoderdecoder model for reconstructing x : This stage is unrelated to author styles , and helps achieve better content preservation .", "Comments": [], "label": []}
{"id": 154886, "text": "Instead of using LSTM layers , GPT is based on 12 layers of Transformer decoders with each decoder layer composed of a 768-dimensional feed-forward layer and 12 self-attention heads .", "Comments": [], "label": []}
{"id": 151809, "text": "Concerning the respective performances on MLDoc and XSR , we see that lightweight model with 2 transformer layers obtains the peak performance on MLdoc , and the performances decrease when we add more layers .", "Comments": [], "label": []}
{"id": 158438, "text": "We implement a simple node-to-text aligner and compress the obtained AMR graph as described in Sec [ -B ] for AMRBART .", "Comments": [], "label": []}
{"id": 158232, "text": "For main results we use τ = 0.5 for GPT-2 and τ = 1.0 for OPT .", "Comments": [], "label": []}
{"id": 158029, "text": "In practice , we train a sequence-to-sequence model T5 [ Raffel et al. , 2020 ] on 45k mined intents for our training and valid data .", "Comments": [], "label": []}
{"id": 157180, "text": "We first minimize LBOW for pretraining knowledge discernment mechanisms , and then minimize L for training overall model . 4.2 Automatic Evaluation In the model 's generation evaluation , we adopt the widely used Perplexity ( PPL ) and Distinct-1/2 ( Dist-1/2 ) [ Li et al. , , 2016 ] .", "Comments": [], "label": []}
{"id": 159251, "text": "[ 11 ] Further fine-tuning with soft labels To compensate for deficiencies in such assertive binary pseudo-labels , we use soft labeling to make the Here , we specifically select * contradiction * pairs in e-SNLI as this relationship is closer to our goal than the other two .", "Comments": [], "label": []}
{"id": 154560, "text": "Firstly , compared See more discusses in Appendix [ A.7 . ] https : //github.com/huggingface/transformers Table 1 : Results of OOD classificaion with different IND classes rate ( 25 % , 50 % and 75 % ) on BANKING and StackOverflow .", "Comments": [], "label": []}
{"id": 152338, "text": "For extractive models , we used some simple baseline models [ Brazinskas and Titov ] ˇ , [ 2020 ] .", "Comments": [], "label": []}
{"id": 160122, "text": "In this task , we use * trendiness * ( e.g . defined with Fisher 's Exact Test in [ §4.1.3 ] as a factor in ranking claims , where a more popular or widely discussed claim will have a greater trendiness .", "Comments": [], "label": []}
{"id": 154033, "text": "TAB-T5 : Machinery and equipment was the most valuable commodity for Singapore in 2019 , with an import value of 236.8 billion Singapore dollars .", "Comments": [], "label": []}
{"id": 151426, "text": "For the word vector-based metrics , we use 300-dimensional GloVe vectors [ Pennington et al. , , 2014 ] pretrained on 6B tokens from Wikipedia 2014 and the Gigaword 5 corpus [ Parker et al. , , 2011 ] .", "Comments": [], "label": []}
{"id": 156250, "text": "Model generates * novel * knowledge We find a significant portion of * novel * knowledge generated from the COMET and TBS models that is not present in the training data .", "Comments": [], "label": []}
{"id": 159071, "text": "In this work , we use the dataset ArgKP-2021 , which contains arguments obtained by crowdsourcing on 31 topics and key points written by experts [ Friedman et al. , , 2021 ] . 27k samples are present in the form of ⟨ argument , key point , label ⟩ triples , and are grouped by positive or negative stance .", "Comments": [], "label": []}
{"id": 153973, "text": "We use the context-response pairs from the WoW dataset for training the model .", "Comments": [], "label": []}
{"id": 157251, "text": "All in all , we used 8700 GPU hours for pre-training and 1100 GPU hours for fine-tuning all models . 3.2 Empirical Findings We summarize the GLUE test set results in Table [ 1 , ] based on which we remark on four observations .", "Comments": [], "label": []}
{"id": 159184, "text": "For each dialogue , we use different abstractive models to generate diverse candidates and propose a reference-based strategy to automatically label omissions for these candidates .", "Comments": [], "label": []}
{"id": 154647, "text": "We then extract relations by looking for dependency paths between the heads of noun chunks that contain the following keywords : We extract superlative relations by looking for dependency paths off the head of a noun chunk containing the following keywords : D Description of ALBEF The ALBEF model has an image-only transformer and a text-only transformer like CLIP but also has a multi-modal transformer that operates on the outputs of these two transformers .", "Comments": [], "label": []}
{"id": 154456, "text": "We used three models : DeepComHybrid model from [ Hu et al . ] [ 2018a , 2020 ] , Transformer model and Seq2Seq baseline from [ Ahmad et al. , 2020 ] .", "Comments": [], "label": []}
{"id": 158366, "text": "Additionally , we want to use the defined information gain measure as a standalone feature of data , so we use a different LM to compute the cross entropy than the LM on which we perform ICL .", "Comments": [], "label": []}
{"id": 154036, "text": "Moving beyond template-based summaries , [ Obeid and Hoque , 2020 ] adapted a transformer-based model on a dataset of 8,305 charts , while [ Spreafico and Carenini , 2020 ] applied an LSTM based encoder-decoder model on a dataset of 306 chart summaries .", "Comments": [], "label": []}
{"id": 158204, "text": "Extensive ASR-U experiments on synthetic languages with three classes of transition graphs provide strong empirical evidence for our theory ( code available at [ cactuswith ] ( https : //github.com/cactuswiththoughts/UnsupASRTheory.git ) [ thoughts/UnsupASRTheory.git ] ( https : //github.com/cactuswiththoughts/UnsupASRTheory.git ) .", "Comments": [], "label": []}
{"id": 159341, "text": "Dynamic Conv [ Wu et al. , , 2019 ] and Lite Transformer [ Wu et al. , , 2020 ] modify the MHA arch to reduce parameters .", "Comments": [], "label": []}
{"id": 156647, "text": "6 Conclusions We start by asking how far we can go at detecting and mitigating hallucinations if we use nothing but the translation model itself .", "Comments": [], "label": []}
{"id": 158019, "text": "This reconstruction follows [ An et al. , 2023 ] and is similar to the conversion from Lambda calculus to FunQL in Geo domain [ Zelle and Mooney , 1996 ] [ Kate et al. , , 2005 ] [ Zettlemoyer and Collins , 2012 ] .", "Comments": [], "label": []}
{"id": 151553, "text": "We use a different position embedding for the image source , and do not perform the graph propagation here ( as we do not have * introduced * or * altered * annotations ) ; this separate embedding captures the inductive bias that the edited is more important than the source .", "Comments": [], "label": []}
{"id": 151497, "text": "The loss function used for all models is Pytorch 's BCELosswithLogits .", "Comments": [], "label": []}
{"id": 158015, "text": "We adopt MLADA as the basic model , which leads to state-of-the-art performance .", "Comments": [], "label": []}
{"id": 154543, "text": "[ https : //github.com/PyTorchLightning/ ] ( https : //github.com/PyTorchLightning/pytorch-lightning ) [ pytorch-lightning ] ( https : //github.com/PyTorchLightning/pytorch-lightning ) Table 7 : Average ROUGE and coverage scores over the Multi-News validation set for different values of the reward mixing coefficient , β .", "Comments": [], "label": []}
{"id": 154487, "text": "For the reading module , same as [ Izacard and Grave , 2021 ] , we initialize it with the pretrained T5-base and T5-large models [ Raffel et al. , , 2019 ] , and we name the former one as KG-FiD ( base ) and the latter one as KG-FiD ( large ) .", "Comments": [], "label": []}
{"id": 152805, "text": "Note that we used the RoBERTa-base model in this section because of the training stability .", "Comments": [], "label": []}
{"id": 159171, "text": "Labeling functions are built using previously trained keywords , regular expressions and NER models .", "Comments": [], "label": []}
{"id": 157567, "text": "Sen./Sec . refers to the number of sentences that could be processed per second , which we use to compare the efficiency of different models .", "Comments": [], "label": []}
{"id": 158752, "text": "In order to achieve this goal , we regularize the output predictions for the visual and textual input by minimizing the Jensen-Shannon Divergence ( JSD ) between the two output distributions as follows : mathbf { p } ( mathbf { y } _n | mathbf { y } _ { Due to the lack of sufficient image translation training data , we use a multi-stage training strategy to transfer knowledge from other tasks ( e.g. , MT ) that have a large amount of training data .", "Comments": [], "label": []}
{"id": 158664, "text": "Dense-ATOMIC has the potential of providing contextual information for Commonsense Reasoning In order to further validate the effectiveness of multi-hop paths in Dense-ATOMIC , we utilize BART [ Lewis et al. , , 2020 ] to perform generative Commonsense Reasoning with or without multi-hop paths .", "Comments": [], "label": []}
{"id": 156394, "text": "They also proposed `` adaptive lambda '' , which dynamically computes the weights of the lambda of linear interpolation in Equation [ 2 ] from the distance between the query and the nearest neighbor [ https : //github.com/elastic/ ] ( https : //github.com/elastic/elasticsearch ) [ elasticsearch ] ( https : //github.com/elastic/elasticsearch ) ( a ) Translation speed for different batch sizes .", "Comments": [], "label": []}
{"id": 157280, "text": "Therefore , we coarsely tune the hyperparameters to maximize and stabilize the performance of the Ours ( K=1 ) baseline under the memory and computational time constraints in our GPUs .", "Comments": [], "label": []}
{"id": 154413, "text": "We adopt the n-stream self-attention proposed in ProphetNet [ Qi et al. , , 2020 ] in our decoder .", "Comments": [], "label": []}
{"id": 154378, "text": "The embedding layer and all weights in the Transformer layers are quantized with the proposed module-dependent dynamic scaling .", "Comments": [], "label": []}
{"id": 160133, "text": "We use the best-performing model from [ Soares et al. , 2019 ] , [ ENTITY MARKERS - ENTITY START ] , which uses special tokens [ Cstart ] , [ Cend ] to mark treatment span in a sentence .", "Comments": [], "label": []}
{"id": 152911, "text": "* * Finetuning & Test Data : * * We use the news commentary data from the English-Russian translation task in WMT18 [ Specia et al. , , 2018 ] for finetuning and evaluate the performance on the WMT18 test data from the news domain .", "Comments": [], "label": []}
{"id": 158694, "text": "In detail : Cross Attention Augmented Transformer ( CAAT ) – the state-of-the-art architecture for SimulST [ Liu et al. , , 2021b ] , winner of the IWSLT 2021 SimulST task [ Anastasopoulos et al. , , 2021 ] .", "Comments": [], "label": []}
{"id": 160180, "text": "To workaround that , we utilize * demonstration ensembling * , where different in-context demonstrations are used to compute scores for a given path , and the scores are combined by a mean or max operation .", "Comments": [], "label": []}
{"id": 159291, "text": "Finally , given the trained DALM , we employ it to generate many labeled target-domain data in an autoregressive manner with a probability-based generation strategy .", "Comments": [], "label": []}
{"id": 158878, "text": "We employ the T5-v1.1-large [ Raffel et al. , , 2020 ] model as the initial model for MetaNER and further pre-train 500k steps with learning rate=5e-5 and warm-up steps=10k .", "Comments": [], "label": []}
{"id": 156858, "text": "PETL Modules and PROPETL We use the Pfeiffer adapter [ Pfeiffer et al. , , 2021 ] as our adapter module and set the bottleneck dimension as 64 by default .", "Comments": [], "label": []}
{"id": 156828, "text": "Label : absolute NAD : [ Chen et al. , , 2019a ] target numbers and cash tags are indicated by `` < > '' and `` | | '' correspondingly : FSRL : [ Lamm et al. , , 2018a ] we use different colors to denote different semantic roles : purple for WHOLE , red for THEME , blue for MANNER , forestgreen for VALUE , orange for TIME , goldenrod for QUANT , pink for AGENT , cyan for SOURCE , and sepia for CAUSE .", "Comments": [], "label": []}
{"id": 156637, "text": "The only internal method ALTI performs better than COMET-QE for fully detached hallucinations , but is inferior when looking at other translations : it Table 2 : Average COMET scores ( ↑ ) after reranking MC dropout hypotheses by various methods .", "Comments": [], "label": []}
{"id": 157715, "text": "It then uses constrained decoding like ReTraCk but using T5 [ Raffel et al. , , 2020b ] .", "Comments": [], "label": []}
{"id": 153010, "text": "In addition , because the augmented data is used for bilingual text infilling task and the other data is used for translation task , our training method resembles an instance of multi-task learning ( MTL ) framework [ Caruana , 1997 ] [ Dong et al. , , 2015 ] [ Liu et al. , , 2016 ] [ Wang et al. , 2020b ] , where both tasks are modeled by the same Transformer architecture .", "Comments": [], "label": []}
{"id": 160174, "text": "We also utilize * demonstration ensembling * to leverage more examples than what can fit into the context of transformer LLMs by combining reranking probabilities computed with different demonstrations .", "Comments": [], "label": []}
{"id": 152020, "text": "The results are shown in Table [ 3 . ] We take the G-Transformer fine-tuned on the sentence-level Transformer as our starting point .", "Comments": [], "label": []}
{"id": 151955, "text": "Experiments show that G-Transformer converges faster and more stably than Transformer , achieving new state-of-the-art BLEU scores for both nonpretraining and pre-training settings on three benchmark datasets .", "Comments": [], "label": []}
{"id": 153683, "text": "For T5 we take the loss with respect only to the output . 4.4 Masked Fine-Tuning To improve generalization of MRF models , we use an additional masked fine-tuning step .", "Comments": [], "label": []}
{"id": 157316, "text": "We use Adam optimizer with β1 = 0.9 , β2 = 0.999 , L2 weight decay of 0.01 , and a learning rate of 0.001 with warm-up over the first 500 steps , and a linear decay .", "Comments": [], "label": []}
{"id": 156838, "text": "We used BCE for the tagging loss , and added it to the label loss through a weight term : 5.2.4 CNN Tagging Layer We extended the linear tagging layer by adding a CNN layer as another method for evidence extraction .", "Comments": [], "label": []}
{"id": 157674, "text": "For example , [ Jiao et al. , 2019 ] and [ Zhu et al. , 2020 ] encode each utterance at first , and then leverage LSTMs and Transformers to aggregate the utterances , while [ Shen et al. , 2021b ] use memory caches to encode utterances sequentially .", "Comments": [], "label": []}
{"id": 152875, "text": "On the zero-shot cross-lingual abstractive summarization task , SixT+ improves mBART-ft by 12.3 average ROUGE-L across 5 zero-shot directions .", "Comments": [], "label": []}
{"id": 154750, "text": "We consider the transformer-XL [ 6 ] [ Dai et al. , , 2019 ] and the compressive transformer [ 7 ] [ Rae et al. , , 2019 ] as baselines .", "Comments": [], "label": []}
{"id": 154204, "text": "Again , we utilize fin-bert with and without our numeric pseudo-tokens , whose representations are learned during fine-tuning . sec-bert : We also release our own family of bert models .", "Comments": [], "label": []}
{"id": 158090, "text": "Similar distillation loss is applied to the decoder . mathbf { D } = begin { array } { c } dots end { array } tag { 11 } where y denotes the t-th position of the target sentence , and we use H d to represent the hidden states of an intermediate layer l of the decoder .", "Comments": [], "label": []}
{"id": 158725, "text": "Stage I : Image-Text Matching In stage I , similar to ViLT [ Kim et al. , , 2021 ] , we use non-dialogue multi-modal data D to learn the fundamental intermodal alignment , and this stage involves only three experts , including the CAPTION expert , IMAGE expert and GROUNDING expert .", "Comments": [], "label": []}
{"id": 154390, "text": "ROUGE 1 , 2 , L are used to evaluate the performance of this task .", "Comments": [], "label": []}
{"id": 159511, "text": "Such a way is time-consuming and may also involve noisy images.Inspired by recent works that show the effective visual-language alignment in VL-PTMs [ Rad- ] < https : //spacy.io/ > [ ford et al. , , 2021 ] [ Li et al. , , 2021 ] , we utilize the visually-aligned text encoders to generate the visual augmentation representations of VH-words .", "Comments": [], "label": []}
{"id": 159795, "text": "However , we adopt a much simpler scheme than prior work : We linearize inputs with a single relation type ( e.g .", "Comments": [], "label": []}
{"id": 153454, "text": "We use their available results instead . truth and the original dialogue in the three aspects .", "Comments": [], "label": []}
{"id": 158115, "text": "For classification tasks where the answer is not a single-word binary classification , we also report ROUGE-L scores following [ Mishra et al. , 2022 ] , which treats all tasks as text generation problems .", "Comments": [], "label": []}
{"id": 153057, "text": "For WMT17 En-Tr , we use * newstest2016 * as the validation set and * newstest2017 * as the test set .", "Comments": [], "label": []}
{"id": 151949, "text": "In specific , we use the union set of the forward and backward alignments as the selflabeled alignments so that all tokens are aligned at least once .", "Comments": [], "label": []}
{"id": 158249, "text": "While this work was under submission and anonymity period , [ Leviathan et al. , 2022 ] , [ Chen et al. , 2023 ] and [ Kim et al. , 2023 ] concurrently proposed decoding approaches that speed up inference of a large transformer model by using another smaller model to draft tokens .", "Comments": [], "label": []}
{"id": 157525, "text": "With the tutor ( right ) , the LM or just a transformer ( left ) generates an action sequence that simulates how humans do arithmetic addition .", "Comments": [], "label": []}
{"id": 153499, "text": "FSAT is a brand-new perspective of efficient Transformer , i.e . learning the sparse structure of an attention ma- 234 trix in end-to-end fashion .", "Comments": [], "label": []}
{"id": 158843, "text": "D Question Selection The Conversation Gone Awry dataset , which we used to extract our questions , includes 2094 conversations that start and remain civil and 2094 conversations that start civil but end with a personal attack .", "Comments": [], "label": []}
{"id": 154262, "text": "Ethical Considerations The proposed approach takes steps towards a novel paradigm that might partially mitigate the need for energy-intensive GPU training – potentially leading to positive environmental impact down the line .", "Comments": [], "label": []}
{"id": 158296, "text": "For CTC , we use 1-reference only as the ground truth target and average the scores based on embedding-based CTC ( CTC-E ) , discriminator-based CTC ( CTC-D ) , and regressor-based CTC ( CTC-R ) w.r.t . the two aspects of evaluation : `` Consistency '' and `` Relevance '' .", "Comments": [], "label": []}
{"id": 154653, "text": "For ALBEF ITC , we use the final layer of the visual transformer for GradCAM .", "Comments": [], "label": []}
{"id": 155866, "text": "We first apply HIBRIDS to Transformer encoder self-attention computation , which is called HIBRIDS-ENC .", "Comments": [], "label": []}
{"id": 157354, "text": "The hCLSI− and hCLST− hidden states are not used for the input of the next self-attention block ; instead , they are replaced by the hCLSimg and hCLStxt versions from the transformer of the other modality .", "Comments": [], "label": []}
{"id": 158374, "text": "4 Experimental Setup We describe the experimental setup that we use to study SOCRATIC pretraining along with empirical studies justifying our design decisions .", "Comments": [], "label": []}
{"id": 151957, "text": "Given the above observations , we investigate a novel extension of Transformer , by restricting selfattention and target-to-source attention to a local context using a guidance mechanism .", "Comments": [], "label": []}
{"id": 159233, "text": "For all our low-resource experiments , we use attention maps from a 24-layer XLMRoBERTa-large fine-tuned on the low-resource gold dataset for that particular setting .", "Comments": [], "label": []}
{"id": 152449, "text": "We use Macro-F1 for corpus-level evaluation . † indicates significance at 95 % confidence , and best implemented model in each metric is bolded .", "Comments": [], "label": []}
{"id": 152237, "text": "BERT models : We used the 90 % train split of the corpus ( with the original tokenization ) to train two BERT models , with 6 and 12 layers : BERT-small ( 6 layers ) : This model has been trained from scratch using a vocabulary of 52,000 ( sub- ) words and a batch size of 208 .", "Comments": [], "label": []}
{"id": 153753, "text": "FragNet [ Shrivastava and Kell , 2021 ] used encoder-decoder Transformers to reconstruct SMILES and enforced extra supervision to the latent space with augmented SMILES and contrastive learning .", "Comments": [], "label": []}
{"id": 160200, "text": "Strikingly , in the no instruction case , T5-Large performs * worse * than T5-Base in terms of AR @ 2 , showing that scaling does not consistently help recall when no instructions are used .", "Comments": [], "label": []}
{"id": 156286, "text": "Indeed , historical n-gram matching metrics such as ROUGE [ Lin , 2004 ] for summarization , BLEU [ Papineni et al. , , 2002 ] and METEOR [ Banerjee and ] [ Lavie , 2005 ] for translation , while affordable , are not very reliable , as they are based on surface-form matching only , i.e. , lexical similarity , and have thus no sense of semantic similarity .", "Comments": [], "label": []}
{"id": 158934, "text": "Table 4 : Performance of COMET-QE ( wmt20-comet-qe-da ) and CometKiwi ( wmt22-cometkiwi-da ) on the on-the-fly detection scenario .", "Comments": [], "label": []}
{"id": 154206, "text": "These replacements also happen when pretraining word2vec subword embeddings ; hence , an embedding is obtained for each pseudo-token .", "Comments": [], "label": []}
{"id": 159493, "text": "And we report recall ( BS-R ) , precision ( BS-P ) and F1 score ( BS-F1 ) for BS . ( 3 ) Overall : We use the geometric mean of a-ACC and BLEU/BS-F1 score ( BL-Overall/BS-Overall ) to assess the overall performance of models [ Kr ] [ ishna et al. , , 2020 ] [ Lee et al. , , 2021 ] .", "Comments": [], "label": []}
{"id": 155618, "text": "We use the cosine similarity to calculate the similarity between Cˆ and C~ , and obtain Sclass .", "Comments": [], "label": []}
{"id": 152641, "text": "2 Background 2.1 Self-attention Weights Self-attention is a core component of the Transformers [ Vaswani et al. , , 2017 ] which looks for the relation between different positions of a single sequence of token representations ( x1 , ... , xn ) to build contextualized representations .", "Comments": [], "label": []}
{"id": 156452, "text": "Thereafter , we used backpropagation of multiple losses to train the model , using a corpus of CCG derivations and dependency structures as the basis for losses .", "Comments": [], "label": []}
{"id": 159128, "text": "COMET-metric variants have the highest overall correlations for all the languages . 5.2 System-level Evaluation Table [ 2 ] shows the Pearson and Kendall-tau correlations at the system-level following [ Louis and ] [ Nenkova , 2013 ] .", "Comments": [], "label": []}
{"id": 153664, "text": "A.2 Implementation The batch size is set to 64 for stage 1 and 128 for stage 2 in all experiments to fully utilize the GPU memory .", "Comments": [], "label": []}
{"id": 159811, "text": "As a potential mechanism to improve the performance of Flan-T5 for RE , we propose enriching the supervision used to fine-tune the model with * chain- * Figure 3 : We propose fine-tuning Flan-T5 ( large ) for relation extraction ( RE ) using standard supervision * and Chain-of-Thought ( CoT ) reasoning * elicited from GPT-3 for RE .", "Comments": [], "label": []}
{"id": 157776, "text": "As noisy proxies for human judgments [ Peyrard and Gurevych , 2018 ] , subtle differences in relevance metrics ( e.g , ROUGE and BERTScore ) might not be meaningful .", "Comments": [], "label": []}
{"id": 159104, "text": "The word vocabulary size of the background Word2Vec model is 397,585 .", "Comments": [], "label": []}
{"id": 159394, "text": "We use a 6-layer Transformer for both the text encoder and the crossmodal fusion network .", "Comments": [], "label": []}
{"id": 158248, "text": "Further orthogonal approaches use specialized hardware ( TPU ) with low-precision calculations [ Wu et al. , , 2016 ] or software optimizations [ Kim et al. , , 2019 ] .", "Comments": [], "label": []}
{"id": 153022, "text": "We employ BLEU [ Pa ] [ pineni et al. , , 2002 ] to measure translation quality , and human editing cost to measure efficiency , which is calculated as the edit distance by counting deletions on word level and insertions on char level .", "Comments": [], "label": []}
{"id": 152616, "text": "To measure this uniformly across datasets , we draw on Item Response Theory and evaluate 29 datasets using predictions from 18 pretrained Transformer models on individual test examples .", "Comments": [], "label": []}
{"id": 153649, "text": "After training , we employ a simple non-parametric clustering algorithm to obtain clustering results .", "Comments": [], "label": []}
{"id": 156261, "text": "B.2 Evaluation Detail We present the MTurk interface we use for response quality and knowledge quality evaluation in Figures [ 7 , ] [ 8 , ] and [ 9 ] including instructions and examples .", "Comments": [], "label": []}
{"id": 160224, "text": "We use the 2020 version of MACCROBAT .", "Comments": [], "label": []}
{"id": 154056, "text": "They compared the summaries based on three criteria : ( * i * ) Factual correctness : Which summary is more factually Table 5 : Human evaluation results for comparing between the outputs of TAB-T5 , OCR-T5 and the gold summary .", "Comments": [], "label": []}
{"id": 158267, "text": "Other methods require further elaborate techniques like profound architectural changes , additional losses to force parallel translation and sequence-level distillation from large autoregressive transformers [ Gu ] [ and Kong , 2021 ] .", "Comments": [], "label": []}
{"id": 156183, "text": "BART [ Lewis ] * et al . * , [ 2019 ] A pre-trained denoising autoencoder with standard Transformer-based architecture and shows effectiveness in NLG .", "Comments": [], "label": []}
{"id": 155212, "text": "We use the publicly available checkpoints to initialise the pre-trained models .", "Comments": [], "label": []}
{"id": 156717, "text": "5.1 Trained Models We train a sentence-level and document-level concatenation-based small transformer ( base ) for every target language .", "Comments": [], "label": []}
{"id": 158969, "text": "For evaluation metrics , following standard practice in summarization [ Zhang et al. , , 2020a ] [ Cheng et al. , , 2023b ] , we adopt ROUGE ( R-1/2/L ) [ Lin , 2004 ] , BERTScore [ Zhang et al. , , 2020b ] , BARTScore [ Yuan et al. , , 2021 ] and Mover-Score [ Zhao et al. , , 2019 ] .", "Comments": [], "label": []}
{"id": 159995, "text": "Speech encoder We used a 16-layer Conformer encoder stacked on 2-dimensional convolution blocks when training models from scratch .", "Comments": [], "label": []}
{"id": 159997, "text": "When pre-training the encoder with wav2vec2.0 and w2v-BERT , we used a 24-layer Conformer encoder and stacked a one-layer length adaptor [ Li et al. , , 2021 ] on it .", "Comments": [], "label": []}
{"id": 158930, "text": "D Computational runtime of our detectors Our detectors do not require access to a GPU machine .", "Comments": [], "label": []}
{"id": 157060, "text": "MITQA then uses an early interaction ( cross attention ) Transformer network to score retrieval units .", "Comments": [], "label": []}
{"id": 152627, "text": "Cust . denotes cases when we use our own custom split .", "Comments": [], "label": []}
{"id": 154540, "text": "A.3 Scale of the Coverage Reward The multi-document reward is used in [ 8 ] in convex combination with the ROUGE-L F1 score , and an appropriate value of the mixing coefficient , β , needs to be explored .", "Comments": [], "label": []}
{"id": 158154, "text": "To this end , we present a plug-and-play and lightweight method named graph-induced fine-tuning ( GIFT ) which can adapt various Transformer-based pre-trained language models ( PLMs ) for universal MPC understanding .", "Comments": [], "label": []}
{"id": 152068, "text": "Following the Transformer-Base setting in [ Vaswani et al. , 2017 ] , we set the hidden size to 512 and the encoder/decoder layers to 6 .", "Comments": [], "label": []}
{"id": 162248, "text": "Table 1 : Details of the datasets 4.2 Experiment Settings We implement our approach by using TensorFlow [ 1 ] , and run experiments on a workstation with Intel Xeon 2.1GHz CPU , an NVIDIA Tesla P100 GPU and 64 GB memory .", "label": [[93, 103, "Software-Entity"], [154, 175, "Hardware-device"], [181, 202, "Hardware-device"], [207, 219, "Device-Memory"]], "Comments": []}
{"id": 162260, "text": "Given the input ( X , X+ , X− ) , the training objective is to minimize − X ( X , X+ , X− ) ∈X E X , X+ - fq ( X ) · fk ( X + ) − log Z Z = exp ( fq ( X ) · fk ( X + ) ) +X X− exp ( fq ( X ) · fk ( X ) ) X = { ( x context , x target , xˆ target ) , ( x target , x context , xˆ context ) } [ Appendix D ] provides the PyTorch-like pseudocode of MoCo for our proposed span pair task .", "label": [[317, 324, "Software-Entity"]], "Comments": []}
{"id": 162262, "text": "It takes about 22 hours for 10 , 000 steps using 1 Nvidia Tesla T4 16GB GPU . 3.4 Fine-tuning for Entity Relation Extraction We define the entity relation extraction task as [ Sun et al. , 2019a ] [ 6 ] .", "label": [[49, 50, "Device-Count"], [51, 66, "Hardware-device"], [67, 75, "Device-Memory"]], "Comments": []}
{"id": 162271, "text": "D Pseudocode ( PyTorch-like ) [ Figure 6 ] shows the PyTorch-like pseudocode of our span pair contrastive learning .", "label": [[15, 22, "Software-Entity"], [53, 60, "Software-Entity"]], "Comments": []}
{"id": 162272, "text": "`` ` f_m : momentum span pair encoder f_e : span pair encoder span_pair_queue : dictionary as a quue of span pair representation ( CxK ) context_queue : dictionary as a quue of context representation ( CxK ) m : momentum t : temperature f_m.params = f_e.params initialize for x in loader : load a minibatch x with N samples x_span_pair = mask ( x ) mask context part x_context = mask ( x ) mask span pair span_pair_q = f_e.forward ( x_span_pair ) span pair queries : NxC context_q = f_e.forward ( x_context ) context queries : NxC span_pair_k = f_m.forward ( x_span_pair ) span pair keys : NxC context_k = f_m.forward ( x_span_pair ) context keys : NxC span_pair_k = span_pair_k.detach ( ) no gradient to span pair keys context_k = context_k.detach ( ) no gradient to context keys span pair positive logits : Nx1 l_span_pair_pos = bmm ( span_pair_q.view ( N , 1 , C ) , context_k.view ( N , C , 1 ) ) context positive logits : Nx1 l_context_pos = bmm ( context_q.view ( N , 1 , C ) , span_pair_k.view ( N , C , 1 ) ) span pair negative logits : NxK l_span_pair_neg = mm ( span_pair_q.view ( N , C ) , context_queue.view ( C , K ) ) context negative logits : NxK l_context_neg = mm ( context_q.view ( N , C ) , span_pair_queue.view ( C , K ) ) span pair logits : Nx ( 1+K ) span_pair_logits = cat ( [ l_span_pair_pos , l_span_pair_neg ] , dim=1 ) context logits : Nx ( 1+K ) context_logits = cat ( [ l_context_pos , l_context_neg ] , dim=1 ) contrastive loss labels = zeros ( N ) positives are the 0-th span_pair_loss = CrossEntropyLoss ( span_pair_logits / t , labels ) context_loss = CrossEntropyLoss ( context_logits / t , labels ) loss = span_pair_loss + context_loss gradient back-propagation : span pair encoder f_e loss.backward ( ) update ( f_e.params ) momentum update : key network f_m.params = m * f_m.params + ( 1 - m ) * f_e.params update dictionary enqueue the current minibatch enqueue ( span_pair_queue , span_pair_k ) enqueue ( context_queue , context_k ) dequeue the earliest minibatch dequeue ( span_pair_queue ) dequeue ( context_queue ) `` ` Figure 6 : Pseudocode of Contrastive Span Pair Objective ( CSPO ) in a PyTorch-like style .", "label": [[2133, 2140, "Software-Entity"]], "Comments": []}
{"id": 162277, "text": "We focus on centroid based k-means ( KM ) , Spherical k-means ( SK ) , and k-medoids ( KD ) for hard clustering , and von Mises-Fisher Models ( VMFM ) and Gaussian Mixture Models ( GMM ) for soft clustering ; as pre-trained embeddings we consider word2vec [ Mikolov et al. , , 2013 ] , GloVe [ Pennington et al. , , 2014 ] , FastText [ Bo ] [ janowski et al. , , 2017 ] , Spherical [ Meng et al. , , 2019 ] , ELMo [ Peters et al. , , 2018 ] , and BERT [ De ] [ vlin et al. , , 2018 ] .", "label": [[325, 333, "Software-Entity"]], "Comments": []}
{"id": 162281, "text": "4.1 Cost of obtaining Embeddings For readily available pretrained word embeddings such as word2vec , FastText , GloVe and Spherical , the embeddings can be considered as 'given ' as the practioner does not need to generate these embeddings from scratch .", "label": [[101, 109, "Software-Entity"]], "Comments": []}
{"id": 162291, "text": "6.1 Runtime Running LDA with MALLET [ McCallum , 2002 ] takes a minute , but performs no better than KM r , which takes little more than 10 seconds on CPU using sklearn [ Pedregosa et al. , , 2011 ] , and 3-4 seconds using a simple implementation using JAX [ Bradbury et al. , , 2018 ] on GPU .", "label": [[161, 168, "Software-Entity"], [253, 256, "Software-Entity"]], "Comments": []}
{"id": 162296, "text": "Fast-Text and GloVe can achieve similar performance to BERT on 20NG but are slightly inferior on Reuters .", "label": [[0, 9, "Software-Entity"]], "Comments": []}
{"id": 162300, "text": "TF Word2Vec 0.19 0.17 0.20 FastText 0.25 0.25 0.25 GloVe 0.23 0.21 0.23 BERT 0.25 0.24 0.25 TF-IDF TF-DF unlike LDA , which uses the highest posterior probability allowing duplicate words to appear in duplicate topics , using a hard clustering algorithm for assignment mean that each word is assigned to one topic only .", "label": [[27, 35, "Software-Entity"]], "Comments": []}
{"id": 162332, "text": "In our experiments , one epoch of RL-MMR takes 0.87 to 0.91s on a GTX 1080 GPU with less than 1.2 GB memory usage .", "label": [[66, 78, "Hardware-device"], [94, 107, "Device-Memory"]], "Comments": []}
{"id": 162376, "text": "E.2 Computing Infrastructure and Runtime For the full hyperparameter sweep , we used an Amazon Web Services ParallelCluster [ https : // ] ( https : //github.com/aws/aws-parallelcluster ) [ github.com/aws/aws-parallelcluster ] ( https : //github.com/aws/aws-parallelcluster ) with 40 nodes of g4dn.xlarge instances ( consisting of Nvidia T4 GPUs with 16 GB RAM ) , which ran for about 5 days .", "label": [[88, 107, "Cloud-Platform"], [281, 289, "Device-Count"], [293, 304, "Hardware-device"], [338, 345, "Hardware-device"], [351, 360, "Software-Entity"]], "Comments": []}
{"id": 162377, "text": "For initial experimentation , we used a SLURM cluster with a mix of consumer-grade Nvidia GPUs ( e.g. , 1080 , 2080 ) .", "label": [[40, 45, "Software-Entity"], [83, 117, "Hardware-device"]], "Comments": []}
{"id": 162387, "text": "We employ the Rank-BM25 tool , [ 6 ] based on [ Trotman et al. , 2014 ] .", "label": [[14, 23, "Software-Entity"]], "Comments": []}
{"id": 162395, "text": "All experiments are conducted on NVIDIA Tesla V110 GPUs . 3.3 Discussion First , there is the existential question about candidate selection : why not simply train the final ranking algorithm with random negative samples instead of the token-based first step ?", "label": [[33, 55, "Hardware-device"]], "Comments": []}
{"id": 162407, "text": "Retrieval metrics include Precision , Recall and F1 at positions 1 , 3 and 5 , of which F1 @ 3 is considered the main metric . 4.3 Implementation and Training Details The model was implemented in Pytorch [ Paszke et al. , 2019 ] using the huggingface reimplementation of BERT [ Wolf et al. , , 2019 ] .", "label": [[196, 203, "Software-Entity"], [239, 250, "Software-Entity"]], "Comments": []}
{"id": 162408, "text": "The model contains approximately 120M trainable parameters and was trained on a single GeForce GTX 1080 Ti GPU for around 12 hours to achieve best performance .", "label": [[80, 86, "Device-Count"], [87, 110, "Hardware-device"]], "Comments": []}
{"id": 162409, "text": "Even though we could use post hoc ( interpret predictions given an arbitrary model ) methods such as LIME [ Ribeiro et al. , , 2016 ] , SHAP [ Lundberg and ] [ Lee , 2017 ] , and L2X [ Chen et al. , , 2018 ] to interpret these black-box models , these interpretation methods are designed to detect the contributions only from unimodal features but not bimodal or trimodal explanatory features .", "label": [[101, 105, "Software-Entity"], [136, 140, "Software-Entity"]], "Comments": []}
{"id": 162418, "text": "4.3 Results and Discussions We trained our model on 1 RTX 2080 GPU .", "label": [[52, 53, "Device-Count"], [54, 66, "Hardware-device"]], "Comments": []}
{"id": 162423, "text": "We use an automatic speech recognition ( ASR ) system [ Google-Speech-V2 ] to generate audio transcripts ( word error rate > 30 % ) to replace the ground-truth transcripts provided by the How2 dataset .", "label": [[56, 72, "Software-Entity"]], "Comments": []}
{"id": 162432, "text": "The training of the proposed models are conducted on { 1 , 2 } GeForce RTX 2080 Ti GPUs for 50 epochs with a batch size of { 4 , 16 } .", "label": [[55, 56, "Device-Count"], [59, 60, "Device-Count"], [63, 87, "Hardware-device"]], "Comments": []}
{"id": 162445, "text": "A.2 Evalution Metrics We use the nmtpytorch evaluation library [ https : ] ( https : //github.com/lium-lst/nmtpytorch ) [ //github.com/lium-lst/nmtpytorch ] ( https : //github.com/lium-lst/nmtpytorch ) suggested by the How2 Challenge , which includes BLEU ( 1 , 2 , 3 , 4 ) , ROUGE-L , METEOR , and CIDEr evaluation metrics .", "label": [[33, 43, "Software-Entity"]], "Comments": []}
{"id": 162446, "text": "In addition , we also use a ROUGE evaluation library [ https : //github.com/ ] ( https : //github.com/neural-dialogue-metrics/rouge ) [ neural-dialogue-metrics/rouge ] ( https : //github.com/neural-dialogue-metrics/rouge ) , which supports the evaluation of ROUGE series metrics ( ROUGE-N , ROUGE-L and ROUGE-W ) .", "label": [[28, 33, "Software-Entity"]], "Comments": []}
{"id": 162448, "text": "The whole network is trained with the Adam optimizer with a learning rate of 0.001 on GTX Titan X GPUs for 60 epochs .", "label": [[86, 102, "Hardware-device"]], "Comments": []}
{"id": 162452, "text": "The model converges after 100 epochs on GTX Titan X GPUs .", "label": [[40, 56, "Hardware-device"]], "Comments": []}
{"id": 162465, "text": "For the response selection task , we implemented our experiments based on ParlAI [ 4 ] .", "label": [[74, 80, "Software-Entity"]], "Comments": []}
{"id": 162467, "text": "Our experiments were carried out on 1 to 4 Nvidia Telsa V100 32G GPU cards .", "label": [[36, 42, "Device-Count"], [43, 60, "Hardware-device"], [61, 64, "Device-Memory"]], "Comments": []}
{"id": 162476, "text": "Figure 3 : The number of uncoordinated slots of our joint model ( One Pass ) , joint model with CRF ( One Pass+CRF ) and SlotRefine ( Two Pass ) during training . is conducted with a single Tesla P40 GPU .", "label": [[183, 189, "Device-Count"], [190, 203, "Hardware-device"]], "Comments": []}
{"id": 162485, "text": "For toxicity classification tasks , we run the model on a GeForce GTX 1080 Ti GPU for 2 epochs , which takes about 3 hours to finish the fine-tuning procedure .", "label": [[58, 81, "Hardware-device"]], "Comments": []}
{"id": 162490, "text": "First , we append part-of-speech tags ( POS ) to each caption using Stanza [ Qi et al. , , 2020 ] .", "label": [[68, 74, "Software-Entity"]], "Comments": []}
{"id": 162507, "text": "D Infrastructures and Statistics We use PyTorch 1.0.0 [ Paszke et al. , , 2019 ] with CUDA toolkit version 10.1 .", "label": [[40, 53, "Software-Entity"], [86, 90, "Software-Entity"]], "Comments": []}
{"id": 162508, "text": "We train our models with NVIDIA 2080 Ti GPUs .", "label": [[25, 44, "Hardware-device"]], "Comments": []}
{"id": 162517, "text": "The average runtime is 198 ± 3.6 minutes on an Intel Xeon Gold 6134 ( 3.20GHz ) CPU with 512 GB RAM .", "label": [[47, 83, "Hardware-device"], [89, 99, "Device-Memory"]], "Comments": []}
{"id": 162549, "text": "How2R Amazon Mechanical Turk ( AMT ) is used to collect annotations on HowTo100M videos .", "label": [[6, 36, "Cloud-Platform"]], "Comments": []}
{"id": 162569, "text": "Our models are implemented based on Py-Torch ( Paszke et al. , 2017 ) .", "label": [[36, 44, "Software-Entity"]], "Comments": []}
{"id": 162570, "text": "To speed up training , we use Nvidia Apex for mixed precision training .", "label": [[30, 41, "Hardware-device"]], "Comments": []}
{"id": 162572, "text": "All pre-training experiments are run on Nvidia V100 GPUs ( 32GB VRAM ; NVLink connection ) .", "label": [[40, 56, "Hardware-device"], [59, 68, "Device-Memory"]], "Comments": []}
{"id": 162574, "text": "The best pre-trained model is trained on 16 V100 GPUs for about 3 weeks .", "label": [[41, 43, "Device-Count"], [44, 53, "Hardware-device"]], "Comments": []}
{"id": 162575, "text": "Finetuning experiments are implemented on the same hardware or Titan RTX GPUs ( 24GB VRAM ) with AdamW optimizer but different learning rates .", "label": [[63, 77, "Hardware-device"], [80, 89, "Device-Memory"]], "Comments": []}
{"id": 162586, "text": "During the vokenization process of English Wikipedia and Wiki103 , we use the faiss [ Johnson et al. , 2019 ] library to speed up the nearest neighbor search .", "label": [[78, 83, "Software-Entity"]], "Comments": []}
{"id": 162589, "text": "During the vokenization process , we use the faiss [ Johnson et al. , , 2019 ] library to speed up the nearest neighbor search .", "label": [[45, 50, "Software-Entity"]], "Comments": []}
{"id": 162590, "text": "The vokenization runs at a speed of 100K tokens / second with 4 Titan V100 GPU .", "label": [[62, 63, "Device-Count"], [64, 78, "Hardware-device"]], "Comments": []}
{"id": 162593, "text": "The whole framework is built on Py-Torch [ Paszke et al. , , 2019 ] .", "label": [[32, 40, "Software-Entity"]], "Comments": []}
{"id": 162594, "text": "The implementations of BERT [ Devlin et al. , , 2019 ] and RoBERTa [ Liu et al. , 2019 ] are borrowed from PyTorch Transformers [ Wolf et al. , , 2019 ] [ 11 ] .", "label": [[107, 114, "Software-Entity"]], "Comments": []}
{"id": 162595, "text": "All evaluation code is from the PyTorch Transformers as well .", "label": [[32, 39, "Software-Entity"]], "Comments": []}
{"id": 162601, "text": "For pretraining the LMs , we adopt the Fairseq [ 4 ] toolkit [ Ott et al. , 2019 ] and the basic transformer decoder LM architecture [ Vaswani et al. , , 2017 ] [ 5 ] .", "label": [[39, 46, "Software-Entity"]], "Comments": []}
{"id": 162603, "text": "We run all methods in a single 2080Ti GPU . 4.2 Compared Methods We choose two kinds of baselines .", "label": [[24, 30, "Device-Count"], [31, 41, "Hardware-device"]], "Comments": []}
{"id": 162606, "text": "Since the Fairseq toolkit has already integrated the transformer architecture , we directly use it for convenience .", "label": [[10, 17, "Software-Entity"]], "Comments": []}
{"id": 162619, "text": "In our experiments , we use hugginface 's implementation [ Wolf et al. , , 2019 ] of BERT ( base version ) and initialize parameters of the BERT encoding layer with pretrained clinical BERT [ Alsentzer et al. , 2019 ] models .", "label": [[28, 41, "Software-Entity"]], "Comments": []}
{"id": 162621, "text": "All experiments are run with an NVIDIA GeForce RTX 2080 Ti .", "label": [[32, 58, "Hardware-device"]], "Comments": []}
{"id": 162633, "text": "Structured SANs [ Zhu et al. , 2019 ] and [ Cai and ] [ Lam , 2020 ] extend SAN s by incorporating the relation ruv between node u and node v in the Our implementation is based on MXNET [ Chen et al. , , 2015 ] and the Sockeye toolkit [ Felix et al. , , 2017 ] .", "label": [[219, 226, "Software-Entity"]], "Comments": []}
{"id": 162640, "text": "Implementations are based on MXNet [ Chen et al. , , 2015 ] and the Sockeye neural machine translation toolkit [ Felix et al. , , 2017 ] .", "label": [[68, 75, "Software-Entity"]], "Comments": []}
{"id": 162641, "text": "Results on speed are based on beam size 10 , batch size 30 on an NVIDIA RTX 1080 GPU . observe that the mechanism helps to alleviate oscillation when training the weight tied model .", "label": [[65, 84, "Hardware-device"]], "Comments": []}
{"id": 162652, "text": "Training and testing are performed on a GeForce GTX 1080 Ti GPU ( 11GB ) .", "label": [[40, 63, "Hardware-device"], [66, 70, "Device-Memory"]], "Comments": []}
{"id": 162658, "text": "Training and testing are performed on a GeForce GTX 1080 Ti GPU ( 11GB ) .", "label": [[40, 63, "Hardware-device"], [66, 70, "Device-Memory"]], "Comments": []}
{"id": 162681, "text": "Hardware and Implements Our models are trained on a single CPU ( Intel i7-5960X ) and an nVidia 1080 Ti GPU , in terms of an implementation using Pytorch 1.0 [ 3 ] .", "label": [[52, 58, "Device-Count"], [65, 79, "Hardware-device"], [89, 107, "Hardware-device"], [146, 157, "Software-Entity"]], "Comments": []}
{"id": 162719, "text": "On CNNDM and XSum , we use the pre-trained BART [ 12 ] and PEGASUS [ 13 ] models from the * Transformers * [ Wolf et al. , , 2020 ] library as the base abstractive models for candidate summary generation and model finetuning respectively .", "label": [[92, 104, "Software-Entity"]], "Comments": []}
{"id": 162720, "text": "We use 4 NVIDIA RTX 3090 GPUs for the model training , and the average running time for one epoch is around 20 hours .", "label": [[7, 8, "Device-Count"], [9, 29, "Hardware-device"]], "Comments": []}
{"id": 162742, "text": "Training Our code is implemented on Tensorflow 2.6 [ Abadi et al. , , 2016 ] with 4 NVIDIA TI-TAN Xp 12G GPU .", "label": [[36, 50, "Hardware-device"], [82, 83, "Device-Count"], [84, 108, "Hardware-device"]], "Comments": []}
{"id": 162746, "text": "We use the Moses tokenizer developed by [ Koehn et al. , , 2007 ] for tokenization and use fastBPE to learn shared 32K BPE [ Sennrich et al. , , 2016 ] for a language pair .", "label": [[11, 16, "Software-Entity"]], "Comments": []}
{"id": 162747, "text": "We use * sacreBleu * [ Post , 2018 ] with standard settings [ 6 ] to evaluate the quality of translation .", "label": [[9, 18, "Software-Entity"]], "Comments": []}
{"id": 162766, "text": "For each video , we choose the first English caption that contains at least one noun phrase as detected by spaCy [ 2 ] [ Honnibal et al. , , 2020 ] , and randomly blank one of these noun phrases to generate an instance .", "label": [[107, 112, "Software-Entity"]], "Comments": []}
{"id": 162767, "text": "We use Amazon Mechanical Turk ( AMT ) for the annotation .", "label": [[7, 37, "Cloud-Platform"]], "Comments": []}
{"id": 162768, "text": "Figure [ 2 ] shows the annotation interface We used the model en_core_web_trf from spaCy v3 .", "label": [[83, 91, "Software-Entity"]], "Comments": []}
{"id": 162769, "text": "We show the mean values of the agreement metrics percaption and per-answer ( recall there are multiple answers per caption , so in the former case we first average among the answers within the caption and Figure 3 : The 2D t-SNE [ Van der Maaten and Hin ] [ ton , 2008 ] representation of the clustering of the top 100 most frequent answers provided for the blanks .", "label": [[223, 228, "Software-Entity"]], "Comments": []}
{"id": 162795, "text": "B Experiments and Results B.1 More Implementation Details We use the T5 model from the HuggingFace Transformers library [ Wolf et al. , , 2020 ] .", "label": [[87, 98, "Software-Entity"]], "Comments": []}
{"id": 162796, "text": "We train the model with Adam [ Kingma and Ba , 2014 ] on a V100-16Gb with a batch size of 64 for 10 epochs ( 4,000 steps ) using a learning rate of 1e-4 with a warm-up of one epoch and a linear decay .", "label": [[59, 63, "Hardware-device"], [64, 68, "Hardware-device"]], "Comments": []}
{"id": 162799, "text": "For each example , we choose the first beam that is a noun phrase , as detected by spaCy [ Honnibal et al. , , 2020 ] , or the first one if none .", "label": [[83, 88, "Software-Entity"]], "Comments": []}
{"id": 162806, "text": "In total , 28,415 distinct MeSH terms are covered in the training dataset . 4.2 Implementation Details We implement our model in PyTorch [ Paszke et al. , 2019 ] .", "label": [[129, 136, "Software-Entity"]], "Comments": []}
{"id": 162810, "text": "We use FAISS [ Johnson et al. , , 2019 ] to find similar documents for each citation among the training set , and the whole process takes 10 hours .", "label": [[7, 12, "Software-Entity"]], "Comments": []}
{"id": 162812, "text": "The model trained for 50 hours on a single NVIDIA V100 GPU .", "label": [[36, 42, "Device-Count"], [43, 58, "Hardware-device"]], "Comments": []}
{"id": 162816, "text": "To perform this top-k search , we use efficient similarity search libraries such as FAISS [ Johnson et al. , , 2017 ] .", "label": [[84, 89, "Software-Entity"]], "Comments": []}
{"id": 162817, "text": "The first is the SP model [ Wieting et al. , ] [ 2019 , 2021 ] , which encodes a sentence as the average of the sub-word unit embeddings generated by SentencePiece [ Kudo and ] [ Richardson , 2018 ] .", "label": [[150, 163, "Software-Entity"]], "Comments": []}
{"id": 162818, "text": "For comparison , we additionally experiment with search engines as Mb , specifically Elasticsearch with the standard BM25 weighting metric [ Robertson and Zaragoza , 2009 ] .", "label": [[85, 98, "Software-Entity"]], "Comments": []}
{"id": 162819, "text": "We complement our evaluation with crowdsourced human judgments via Amazon Mechanical Turk ( MTurk ) .", "label": [[67, 99, "Cloud-Platform"]], "Comments": []}
{"id": 162821, "text": "For the search engine , we use Elasticsearch with the standard BM25 metric [ Robertson ] [ and Zaragoza , 2009 ] . [ 9 ] We denote the relevance score calculated by BM25 between the query q and a textually represented video v as Rel ( q , v ) .", "label": [[31, 44, "Software-Entity"]], "Comments": []}
{"id": 162823, "text": "We use spaCy [ Hon ] [ nibal et al. , , 2020 ] to extract and lemmatize the verb in each step and rank the verbs by their frequency in each cluster .", "label": [[7, 12, "Software-Entity"]], "Comments": []}
{"id": 162824, "text": "A Crowdsourcing Details As discussed in [ section 5 , ] we use Amazon Mechanical Turk ( mTurk ) to collect human judgements of linked wikiHow articles .", "label": [[63, 95, "Cloud-Platform"]], "Comments": []}
{"id": 162825, "text": "Encoding all steps and goals in wikiHow took around two hours on a 2080Ti ( 12GB ) GPU .", "label": [[67, 73, "Hardware-device"], [76, 80, "Device-Memory"]], "Comments": []}
{"id": 162826, "text": "For SBERT , the encoding took around an hour on a v100 GPU ( 32GB ) .", "label": [[50, 58, "Hardware-device"], [61, 65, "Device-Memory"]], "Comments": []}
{"id": 162827, "text": "Reranking We used the transformers library [ Wolf et al. , , 2020 ] for re-ranking .", "label": [[22, 34, "Software-Entity"]], "Comments": []}
{"id": 162829, "text": "Finetuning took around two hours on a 2080Ti ( 12GB ) GPU for BERT and eight hours on a v100 GPU ( 32GB ) for DEBERTA .", "label": [[38, 44, "Hardware-device"], [47, 51, "Device-Memory"], [88, 96, "Hardware-device"], [99, 103, "Device-Memory"]], "Comments": []}
{"id": 162830, "text": "We used the default hyperparameters provided by the transformers library .", "label": [[52, 64, "Software-Entity"]], "Comments": []}
{"id": 162846, "text": "The baseline model is trained for 4 hours on 4 V100 GPUs ; and it takes an additional 1 hour to train the proposed framework .", "label": [[45, 46, "Device-Count"], [47, 56, "Hardware-device"]], "Comments": []}
{"id": 162847, "text": "The second round training fine-tunes the TSM video encoder ( which is frozen in the first round training ) on S-MiT jointly with the rest of the components , which takes 2 days on 8 Titan RTX GPUs .", "label": [[180, 181, "Device-Count"], [182, 196, "Hardware-device"]], "Comments": []}
{"id": 162848, "text": "The baseline model is trained for 36 hours on 1 V100 GPU ; and it takes an additional 4 hours to train the proposed framework .", "label": [[46, 47, "Device-Count"], [48, 56, "Hardware-device"]], "Comments": []}
{"id": 162850, "text": "The baseline model is trained for 12 hours on 8 2080Ti GPUs ; and it takes an additional 6 hours to train the proposed framework .", "label": [[46, 47, "Device-Count"], [48, 59, "Hardware-device"]], "Comments": []}
{"id": 162857, "text": "( a ) With Cross-modal Code Matching ( b ) Without Cross-modal Code Matching Figure 6 : T-SNE visualization of the codebook with and without the proposed Cross-Modal Code Matching Objective .", "label": [[88, 93, "Software-Entity"]], "Comments": []}
{"id": 162863, "text": "Our event representation model is implemented using the Texar-PyTorch package [ Hu et al. , , 2019 ] .", "label": [[56, 69, "Software-Entity"]], "Comments": []}
{"id": 162874, "text": "To speed up inference , we precompute the answer embeddings and use FAISS [ Johnson et al. , , 2019 ] for similarity scoring . 3.1 Top-k Recall of Our QA Model To evaluate our bi-encoder , we compute its top-k recall on the question-answer pairs from the NYT test set .", "label": [[68, 73, "Software-Entity"]], "Comments": []}
{"id": 162886, "text": "Our model training is based on Transformers library [ 5 ] .", "label": [[31, 43, "Software-Entity"]], "Comments": []}
{"id": 162887, "text": "All our experiments are based on 8-GPU machines .", "label": [[33, 38, "Device-Count"]], "Comments": []}
{"id": 162896, "text": "The evaluation metric is based on SacreBLEU .", "label": [[34, 43, "Software-Entity"]], "Comments": []}
{"id": 162911, "text": "All models are implemented using FAIRSEQ S2T [ Ott et al. , , 2019 ] [ Wang et al. , , 2020b ] [ 3 ] .", "label": [[33, 40, "Software-Entity"]], "Comments": []}
{"id": 162912, "text": "ASR We train the transformer-based Spanish ASR system with the default hyper-parameters and s2t_transformer_s architecture in FAIRSEQ S2T [ Wang et al. , , 2020b ] .", "label": [[126, 133, "Software-Entity"]], "Comments": []}
{"id": 162913, "text": "MT As the input to the MT model is characters , we follow the default gru_transformer setup in FAIRSEQ [ Ott et al. , , 2019 ] to prepend a bidirectional recurrent layer with gated recurrent units ( GRU ) to the transformer encoder to incorporate a larger context [ Wang et al. , , 2020a ] .", "label": [[95, 102, "Software-Entity"]], "Comments": []}
{"id": 162922, "text": "As the ASR output is in lowercase and without punctuation except apostrophes , we normalize the reference text before computing BLEU using SACREBLEU [ Post , 2018 ] [ 6 ] .", "label": [[139, 148, "Software-Entity"]], "Comments": []}
{"id": 162932, "text": "4.6 System benchmark In addition to evaluating the quality of the system output , we examine the efficiency of the models during inference by benchmarking the runtime , total number of floating point operations ( FLOPs ) and max memory on an Intel® Xeon® Gold 6230 CPU .", "label": [[242, 268, "Hardware-device"]], "Comments": []}
{"id": 162936, "text": "C Significance test Table [ 6 ] shows the * p * -values from paired significance tests between the nine systems ( ID 2-10 ) in Table [ 2 . ] We conduct the tests with the paired bootstrap resampling method supported in the SACREBLEU tool [ Post , 2018 ] .", "label": [[223, 232, "Software-Entity"]], "Comments": []}
{"id": 162943, "text": "To evaluate the faithfulness of generated summaries , we use automatic faithfulness evaluation tools FEQA [ Durmus et al. , , 2020 ] and DAE [ Goyal and Durrett , 2020 ] [ 3 ] .", "label": [[101, 105, "Software-Entity"], [137, 140, "Software-Entity"]], "Comments": []}
{"id": 162953, "text": "Language Model Hyperparameters All language models used in this paper are based on the Transformer encoder-decoder architecture from the Fairseq library [ Ott et al. , , 2019 ] that is written in PyTorch [ Paszke et al. , , 2017 ] .", "label": [[137, 144, "Software-Entity"], [196, 203, "Software-Entity"]], "Comments": []}
{"id": 162955, "text": "These hyperparameter values are based on the recommended values from the fairseq [ Ott et al. , , 2019 ] library All experiments are conducted on 4 Tesla V100 GPUs with 32GB of memory .", "label": [[73, 80, "Software-Entity"], [146, 147, "Device-Count"], [148, 163, "Hardware-device"], [169, 173, "Device-Memory"]], "Comments": []}
{"id": 162974, "text": "Our system includes a single Nvidia GeForce RTX 2080 GPU and a single Intel i9 core .", "label": [[22, 28, "Device-Count"], [29, 56, "Hardware-device"], [63, 69, "Device-Count"], [70, 83, "Hardware-device"]], "Comments": []}
{"id": 162988, "text": "More runtime and parameter details are provided in Appendix [ B . ] All the neural models are trained on GeForce GTX TitanX GPUs . 4.4 Evaluation metrics We use the precision , recall , F1 score , and Jaccard score as our evaluation metrics [ Manning et al. , , 2008 ] .", "label": [[105, 128, "Hardware-device"]], "Comments": []}
{"id": 163000, "text": "It is based on AllenNLP [ Gardner et al. , , 2017 ] and HuggingFace Transformers [ Wolf et al. , , 2019 ] , and its source code is freely available .", "label": [[15, 23, "Software-Entity"], [56, 67, "Software-Entity"]], "Comments": []}
{"id": 163006, "text": "[ 12 ] This was chosen because the out-off-the-box AllenNLP tokenizer was too slow , and HuggingFace Transformers ' tokenizers did not provide a BPE-to-words mapping .", "label": [[51, 59, "Software-Entity"], [89, 100, "Software-Entity"]], "Comments": []}
{"id": 163007, "text": "Our work is fully implemented with Transformers from the HuggingFace Transformers library .", "label": [[57, 68, "Software-Entity"]], "Comments": []}
{"id": 163008, "text": "In particular , we moved to the recently released fast tokenizers from HuggingFace .", "label": [[71, 82, "Software-Entity"]], "Comments": []}
{"id": 163013, "text": "Inference time for NVIDIA Tesla P100 on BEA-2019 dev , single models , batch size=128 , averaged over 5 inferences .", "label": [[19, 36, "Hardware-device"]], "Comments": []}
{"id": 163015, "text": "For text pre-processing , we use the tokenizer from Spacy . [ 13 ] As a teacher , we use the ensemble of the sequence taggers containing Large encoders with a 5K vocabulary : DeBERTa ( L ) + RoBERTa ( L ) + XLNet ( L ) ( Table [ 7 ] .", "label": [[52, 57, "Software-Entity"]], "Comments": []}
{"id": 163021, "text": "The process is illustrated in Figure [ 2 ; ] corpus statistics are included in Appendix [ A . ] 4.1 Data Source For building the WIKIFLUENT corpus , we extracted 934k first paragraphs of articles from a Wikipedia dump using WikiExtractor [ Attardi , 2015 ] .", "label": [[224, 237, "Software-Entity"]], "Comments": []}
{"id": 163023, "text": "4.2 Split-and-Rephrase To generate a set of simple sentences , we divide each paragraph into sentences using NLTK [ Bird , 2006 ] and apply a * split-and-rephrase * model on each sentence .", "label": [[109, 113, "Software-Entity"]], "Comments": []}
{"id": 163036, "text": "C Experimental Setup We implemented the models for split-and-rephrase , aggregation , and paragraph compression in Py-Torch Lightning [ Paszke et al. , , 2019 ] , using the PyTorch [ Falcon et al. , , 2019 ] version of the BART and RoBERTa models from the Huggingface library [ Wolf et al. , , 2019 ] .", "label": [[115, 133, "Software-Entity"], [173, 180, "Software-Entity"], [256, 267, "Software-Entity"]], "Comments": []}
{"id": 163038, "text": "We train the models for 1 epoch on a single GeForce RTX 3090 GPU with 24 GB RAM .", "label": [[37, 43, "Device-Count"], [44, 64, "Hardware-device"], [70, 79, "Device-Memory"]], "Comments": []}
{"id": 163041, "text": "Narrative collection : In the second step , we use Amazon Mechanical Turk to collect a diverse set of narratives corresponding to each proverb .", "label": [[51, 73, "Cloud-Platform"]], "Comments": []}
{"id": 163057, "text": "We performed this study using the Amazon Mechanical Turk platform .", "label": [[34, 56, "Cloud-Platform"]], "Comments": []}
{"id": 163065, "text": "F.3 Obtaining keywords for narrative generation We consider the named entities and verbs present in a narrative ( extracted using spacy [ Honnibal ] [ et al . ] )", "label": [[130, 135, "Software-Entity"]], "Comments": []}
{"id": 163066, "text": "F.5 Hyper-parameter settings For all the transformer based models we use the implementation of HuggingFace library [ Wolf et al. , , 2019 ] .", "label": [[95, 106, "Software-Entity"]], "Comments": []}
{"id": 163067, "text": "All the model based hyper-parameters are thus kept default to the settings in the Hugging-Face library .", "label": [[82, 94, "Software-Entity"]], "Comments": []}
{"id": 163073, "text": "F.6 Software and hardware specifications All the models are coded using Pytorch 1.4.0 [ 8 ] [ Paszke et al. , , 2019 ] and related libraries like numpy [ Oliphant , 2006 ] , scipy [ Virtanen et al. , , 2020 ] etc .", "label": [[72, 85, "Software-Entity"], [146, 151, "Software-Entity"], [174, 179, "Software-Entity"]], "Comments": []}
{"id": 163074, "text": "We run all experiments on GeForce RTX 2080 GPU of size 12 GB .", "label": [[26, 46, "Hardware-device"], [55, 60, "Device-Memory"]], "Comments": []}
{"id": 163080, "text": "We used Python 's datetime library to align Twitter time values ( UTC ) with the financial signal ( EST , New York Stock Exchange ) [ 8 ] Note that price variations in 30-minutes intervals are considerably more granular than the financial signal used in NLP work , which is mostly limited to daily data [ Sawhney et al. , , 2020a ] .", "label": [[8, 26, "Software-Entity"]], "Comments": []}
{"id": 163091, "text": "We use sklearn 's implementation [ 17 ] of accuracy and macro-averaged precision , recall and F scores [ Pedregosa et al. , , 2011 ] .", "label": [[7, 17, "Software-Entity"]], "Comments": []}
{"id": 163092, "text": "Models were trained on Google Colab 's GPU .", "label": [[23, 42, "Hardware-device"]], "Comments": []}
{"id": 163109, "text": "To further examine this , we visualize the MultiATIS++ test set in Figure [ 3 ] and observe * less discriminable * encodings Figure 3 : t-SNE comparison using mBART50 and ZX-PARSE encoders ( MultiATIS++ test set ) .", "label": [[136, 141, "Software-Entity"]], "Comments": []}
{"id": 163110, "text": "For natural language , all models use byte-level BPE tokenization [ Wang et al. , , 2020b ] and logical forms are tokenized using whitespace .", "label": [[38, 52, "Software-Entity"]], "Comments": []}
{"id": 163113, "text": "Reproducibility All models were implemented using AllenNLP [ Gardner et al. , , 2018 ] and Py-Torch [ Paszke et al. , , 2019 ] , using pre-trained models from HuggingFace [ Wolf et al. , , 2019 ] .", "label": [[50, 58, "Software-Entity"], [91, 99, "Software-Entity"], [159, 170, "Software-Entity"]], "Comments": []}
{"id": 163114, "text": "Each model is trained on 1 NVIDIA RTX3090 GPU in a cluster configuration , with no model requiring over 24 hours to complete training .", "label": [[25, 26, "Device-Count"], [27, 45, "Hardware-device"]], "Comments": []}
{"id": 163130, "text": "Table 4 : Machine translation test SacreBLEU .", "label": [[35, 44, "Software-Entity"]], "Comments": []}
{"id": 163133, "text": "We evaluate with SacreBLEU [ Post , 2018 ] .", "label": [[17, 26, "Software-Entity"]], "Comments": []}
{"id": 163137, "text": "Inference speed is measured on the same V100 GPU .", "label": [[40, 48, "Hardware-device"]], "Comments": []}
{"id": 163144, "text": "The hyperparameters are summarized in Table [ 10 . ] All models are trained on 4 A100 GPUs .", "label": [[79, 80, "Device-Count"], [81, 90, "Hardware-device"]], "Comments": []}
{"id": 163146, "text": "The tokenization is also the same as [ Miculicich et al. , 2018 ] : we tokenize and truecase Spanish and English with Moses [ Koehn et al. , , 2007 ] and run byte-pair encoding with 30k splits , shared between the two languages .", "label": [[118, 123, "Software-Entity"]], "Comments": []}
{"id": 163150, "text": "Other hyperparameters are summarized in Table [ 11 . ] All models are trained on 4 RTX 2080 Ti GPUs .", "label": [[81, 82, "Device-Count"], [83, 99, "Hardware-device"]], "Comments": []}
{"id": 163158, "text": "Finally , we applied sentence tokenization using spaCy 's dependency parser-based sentence segmenter [ 5 ] to standardize the size of the windows in our dataset .", "label": [[49, 57, "Software-Entity"]], "Comments": []}
{"id": 163159, "text": "When quoted material is being employed as < https : //spacy.io/ > , the default segmenter in spaCy is modified to use ellipses , colons , and semicolons as custom sentence boundaries , based on the observation that literary scholars often only quote part of what would typically be defined as a sentence .", "label": [[93, 98, "Software-Entity"]], "Comments": []}
{"id": 163164, "text": "McKee * , * voice * , * call * , and * back * appear in both places : [ https : //github.com/jwieting/ ] ( https : //github.com/jwieting/beyond-bleu ) [ beyond-bleu ] ( https : //github.com/jwieting/beyond-bleu ) We set |B| = 100 , and train all models for 10 epochs on a single RTX8000 GPU with an initial learning rate of 1e-5 using the Adam optimizer [ Kingma and Ba , 2015 ] , early stopping on validation loss .", "label": [[272, 278, "Device-Count"], [279, 290, "Hardware-device"]], "Comments": []}
{"id": 163165, "text": "Our implementation uses the Hugging-Face transformers library [ Wolf et al. , , 2020 ] .", "label": [[28, 40, "Software-Entity"]], "Comments": []}
{"id": 163166, "text": "Specifically , we feed the preprocessed text through spaCy 's [ 18 ] dependency parser-based sentence segmenter on the cleaned text .", "label": [[53, 61, "Software-Entity"]], "Comments": []}
{"id": 163167, "text": "The default segmenter in spaCy is modified to use ellipses , colons , and semicolons as custom sentence boundaries , based on the observation that literary scholars often only quote part of what would typically be defined as a sentence ( Table [ A2 ] .", "label": [[25, 30, "Software-Entity"]], "Comments": []}
{"id": 163176, "text": "All experiments in the paper are ran on NVIDIA Tesla V100 GPU and 2.2 GHz Intel Xeon Silver 4114 CPU processor .", "label": [[40, 61, "Hardware-device"], [66, 100, "Hardware-device"]], "Comments": []}
{"id": 163182, "text": "All chatbots are running and evaluated on a Intel ( R ) Xeon ( R ) CPU E5-2678 v3 @ 2.50GHz with NVIDIA GeForce RTX 2080 and a 12GB RAM .", "label": [[44, 91, "Hardware-device"], [97, 120, "Hardware-device"], [127, 135, "Device-Memory"]], "Comments": []}
{"id": 163196, "text": "For semantic parsing , we use pytorch neural symbolic machine [ Liang et al. , ] [ 2017 , 2018 ] [ Yin et al. , , 2020 ] as our baseline and improve it with the operation-oriented tree .", "label": [[30, 37, "Software-Entity"]], "Comments": []}
{"id": 163198, "text": "All experiments were conducted on a workstation with 128 GB of RAM and 2 RTX 3090 GPUs .", "label": [[53, 66, "Device-Memory"], [71, 72, "Device-Count"], [73, 86, "Hardware-device"]], "Comments": []}
{"id": 163204, "text": "The run time of the fine-tuning step of the inter-training task takes five and a half minutes for the largest train set ( 15K instances ) on a Tesla V100-PCIE-16GB GPU .", "label": [[143, 158, "Hardware-device"], [159, 163, "Device-Memory"]], "Comments": []}
{"id": 163207, "text": "In total , this would equate to about 60 hours on a single Tesla V100- PCIE-16GB GPU . 4 Results Table [ 2 ] depicts the results over all datasets , focusing on the practical use case of a budget of 64 Setting the number of clusters to be equal to the number of classes resulted in inferior accuracy .", "label": [[52, 58, "Device-Count"], [59, 75, "Hardware-device"], [76, 80, "Device-Memory"]], "Comments": []}
{"id": 163209, "text": "Figure [ 3 ] depicts t-SNE [ van der Maaten and Hinton , 2008 ] 2D visualizations of the output embeddings over the full train set of several datasets , comparing the [ CLS ] embeddings before and after inter-training .", "label": [[21, 26, "Software-Entity"]], "Comments": []}
{"id": 163210, "text": "Moreover , and perhaps not surprisingly , the apparent visual separation resulting from inter-training is aligned with the performance gain obtained later on in the fine-tuning phase Figure 3 : t-SNE visualizations of model embeddings over the train set , using BERT ( top ) vs .", "label": [[194, 199, "Software-Entity"]], "Comments": []}
{"id": 163213, "text": "A Datasets Links for downloading the datasets : `` ` 20 newsgroups : http : //qwone.com/ ~jason/20Newsgroups/ We used the version provided by scikit : https : //scikit-learn.org/0 .", "label": [[142, 148, "Software-Entity"]], "Comments": []}
{"id": 163220, "text": "Both probing methods are implemented using PyTorch [ Paszke et al. , , 2019 ] and use mBERT as implemented in the Transformers library [ Wolf et al. , 2020 ] .", "label": [[43, 50, "Software-Entity"], [114, 134, "Software-Entity"]], "Comments": []}
{"id": 163229, "text": "Both it and DEPPROBE ( this work ) are implemented in Py-Torch v1.9.0 [ Paszke et al. , , 2019 ] and use mBERT ( bert-base-multilingual-cased ) from the Transformers library v4.8.2 [ Wolf et al. , , 2020 ] .", "label": [[54, 69, "Software-Entity"], [153, 165, "Software-Entity"]], "Comments": []}
{"id": 163231, "text": "For our analyses ( Section [ 5 ] , we use numpy v1.21.0 [ Harris et al. , , 2020 ] , SciPy v1.7.0 [ Virtanen et al. , , 2020 ] and Matplotlib v3.4.3 [ Hunter , 2007 ] .", "label": [[42, 55, "Software-Entity"], [85, 97, "Software-Entity"], [131, 148, "Software-Entity"]], "Comments": []}
{"id": 163232, "text": "Training Details Each model is trained on an NVIDIA A100 GPU with 40GBs of VRAM and an AMD Epyc 7662 CPU .", "label": [[45, 60, "Hardware-device"], [66, 79, "Device-Memory"], [87, 104, "Hardware-device"]], "Comments": []}
{"id": 163242, "text": "4 Experiments 4.1 Implementation Details We implement our model using the PyTorch library and use the Stanford Stanza library [ 2 ] for sentence tokenization .", "label": [[74, 81, "Software-Entity"], [111, 117, "Software-Entity"]], "Comments": []}
{"id": 163244, "text": "To compare baselines within the same framework , we re-implement all of them in PyTorch .", "label": [[80, 87, "Software-Entity"]], "Comments": []}
{"id": 163255, "text": "We use 23GB GPU memory a NVidia P40 on ADC and AES and 46GB GPU memory of two NVidia P40s for each run on AWQ .", "label": [[7, 11, "Device-Memory"], [25, 35, "Hardware-device"], [55, 59, "Device-Memory"], [74, 77, "Device-Count"], [78, 89, "Hardware-device"]], "Comments": []}
{"id": 163256, "text": "We split a text at the sentence level by Stanford Stanza library , and tokenize them by the XLNet tokenizer .", "label": [[50, 56, "Software-Entity"]], "Comments": []}
{"id": 163258, "text": "The platform offers 60 free GPU hours from Aliyun [ 6 ] to help researchers develop and train their models . 3.7 Distribution and Maintenance Our CBLUE benchmark was released online on April 1 , 2021 .", "label": [[43, 49, "Cloud-Platform"]], "Comments": []}
{"id": 163259, "text": "We will continue to maintain the benchmark by adding new tasks . 3.8 Reproducibility To make it easier to use the CBLUE benchmark , we also offer a toolkit implemented in PyTorch [ Paszke et al. , , 2019 ] for reproducibility .", "label": [[171, 178, "Software-Entity"]], "Comments": []}
{"id": 163260, "text": "Models We evaluate CBLUE on the following public available Chinese pre-trained models : We implement all baselines with PyTorch [ Paszke et al. , , 2019 ] .", "label": [[120, 127, "Software-Entity"]], "Comments": []}
{"id": 163261, "text": "We utilize Pytorch to conduct experiments , and all running hyper-parameters are shown in the following Tables .", "label": [[11, 18, "Software-Entity"]], "Comments": []}
{"id": 163267, "text": "Since the dynamic programming formulas are differentiable , the entire model can be trained by backpropagation in an end-to-end manner with auto-differentiation tools ( such as PyTorch ) .", "label": [[177, 184, "Software-Entity"]], "Comments": []}
{"id": 163275, "text": "Our experiments were conducted on an i9-9940X CPU and an RTX6000 graphic card .", "label": [[37, 49, "Hardware-device"], [57, 64, "Hardware-device"]], "Comments": []}
{"id": 163277, "text": "Inf.Time : Average inference time in seconds for one sample on an i9-9940X CPU and a RTX6000 GPU .", "label": [[66, 78, "Hardware-device"], [85, 96, "Hardware-device"]], "Comments": []}
{"id": 163300, "text": "Inf.Time : Average inference time in seconds for one sample on an i9-9940X CPU and a RTX6000 GPU .", "label": [[66, 78, "Hardware-device"], [85, 96, "Hardware-device"]], "Comments": []}
{"id": 163309, "text": "All experiments are performed on 8 V100 GPUs .", "label": [[33, 34, "Device-Count"], [35, 44, "Hardware-device"]], "Comments": []}
{"id": 163333, "text": "We use the Stanford segmenter [ Tseng et al. , , 2005 ] for Chinese word segmentation and apply the script tokenizer.pl of Moses [ Koehn et al. , , 2007 ] for English , German and French tokenization .", "label": [[11, 29, "Software-Entity"], [123, 128, "Software-Entity"]], "Comments": []}
{"id": 163368, "text": "Training CCM on the CRD dataset takes about a week on one Titan X GPU .", "label": [[54, 57, "Device-Count"], [58, 69, "Hardware-device"]], "Comments": []}
{"id": 163390, "text": "In addition , given a singing recording , 1 ) to obtain its content vectors , we train an Automatic Speech Recognition ( ASR ) model based on Conformer [ Gulati et al. , , 2020 ] with both speech and singing data , and extract the hidden states from the ASR encoder ( viewed as the content encoder ) output as the linguistic content information , which are also called phonetic posterior-grams ( PPG ) ; 2 ) to obtain the vocal timbre , we leverage the open-source API resemblyzer [ 8 ] as the timbre encoder , which is a deep learning model designed for speaker verification [ Wan et al. , , 2018 ] , to extract the identity information of a singer .", "label": [[142, 151, "Software-Entity"], [469, 480, "Software-Entity"]], "Comments": []}
{"id": 163392, "text": "Implementation Details We train the Neural Singing Beautifier on a single 32G Nividia V100 GPU with the batch size of 64 sentences for both 100k steps in Stage 1 and Stage 2 respectively .", "label": [[67, 73, "Cloud-Platform"], [74, 77, "Device-Memory"], [78, 94, "Hardware-device"]], "Comments": []}
{"id": 163400, "text": "We train NSVB on a single V100 32G GPU for almost 22 hours to finish two-stage training .", "label": [[19, 25, "Device-Count"], [26, 30, "Hardware-device"], [31, 34, "Device-Memory"]], "Comments": []}
{"id": 163429, "text": "At the token level , we only mask the most informative tokens which can be decided by the POS tags given by SpaCy [ 5 ] as it is meaningless to mask some words such as prepositions `` to '' and `` in '' .", "label": [[108, 113, "Software-Entity"]], "Comments": []}
{"id": 163432, "text": "Dialogue history augmentation We use the * transformer_wmt_en_de * Transformer model in Fairseq [ 7 ] as the translation model .", "label": [[88, 95, "Software-Entity"]], "Comments": []}
{"id": 163439, "text": "All experiments are implemented via PyTorch on 32GB NVIDIA V100 GPUs .", "label": [[36, 43, "Software-Entity"], [47, 51, "Device-Memory"], [52, 68, "Hardware-device"]], "Comments": []}
{"id": 163465, "text": "For the former , we follow the common practice in COMET [ 3 ] [ Rei et al. , , 2020 ] to collect and preprocess the dataset .", "label": [[50, 55, "Software-Entity"]], "Comments": []}
{"id": 163467, "text": "Model Setting We implement our approach upon COMET [ Rei et al. , , 2020 ] repository and follow their work to choose XLM-R [ Conneau et al. , , 2020 ] as the PLM .", "label": [[45, 50, "Software-Entity"]], "Comments": []}
{"id": 163472, "text": "B Reproducibility All the models reported in this paper were finetuned on a single Nvidia V100 ( 32GB ) GPU .", "label": [[76, 82, "Device-Count"], [83, 94, "Hardware-device"], [97, 101, "Device-Memory"]], "Comments": []}
{"id": 163473, "text": "Specifically for UniTE-UP and UniTE-MUP , the pretraining is arranged on 4 Nvidia V100 ( 32GB ) GPUs .", "label": [[73, 74, "Device-Count"], [75, 86, "Hardware-device"], [89, 93, "Device-Memory"]], "Comments": []}
{"id": 163474, "text": "Our framework is built upon COMET repository [ Rei et al. , , 2020 ] .", "label": [[28, 33, "Software-Entity"]], "Comments": []}
{"id": 163476, "text": "[ 2 ] In the generation process , we take transformer-big [ Vaswani et al. , , 2017 ] as the configuration for m , and m is trained with the self-constructed examples mentioned in Section [ 3.2 ] on eight V100 GPU cards .", "label": [[199, 204, "Device-Count"], [205, 213, "Hardware-device"]], "Comments": []}
{"id": 163478, "text": "All models are implemented based on the open-source toolkit fairseq [ Ott et al. , , 2019 ] and trained on the machine with eight V100 GPU cards . [ 4 ] All bilingual models are trained for 300,000 steps and multi-lingual models are trained for 500,000 steps .", "label": [[60, 67, "Software-Entity"], [124, 129, "Device-Count"], [130, 138, "Hardware-device"]], "Comments": []}
{"id": 163481, "text": "We use BLEU scores [ Papineni et al. , , 2002 ] to measure the model performance and all BLEU scores are calculated with sacreBLEU [ Post , 2018 ] .", "label": [[121, 130, "Software-Entity"]], "Comments": []}
{"id": 163482, "text": "[ 5 ] Intel ( R ) Xeon ( R ) Platinum 8255C CPU @ 2.50GHz Detained training process for m can be found in the Appendix [ A.1 . ]", "label": [[6, 57, "Hardware-device"]], "Comments": []}
{"id": 163489, "text": "We use a vocabulary of 64,000 sub-word units for all of the multi-lingual models , which is tokenized from the combination of all the training corpus with SentencePiece .", "label": [[155, 168, "Software-Entity"]], "Comments": []}
{"id": 163495, "text": "The model is trained on 8 V100 GPU cards , with learning rate , max-token , and update-freq set as 0.01 , 8192 , and 10 respectively .", "label": [[24, 25, "Device-Count"], [26, 34, "Hardware-device"]], "Comments": []}
{"id": 163507, "text": "We train the model on one machine with an NVIDIA RTX 3090 ( GPU ) and Intel Core i9 10900K ( CPU ) .", "label": [[42, 57, "Hardware-device"], [70, 90, "Hardware-device"]], "Comments": []}
{"id": 163518, "text": "Training and Inference We train the model on 1 NVIDIA 2080Ti GPU , with batch size of 48 sentences .", "label": [[45, 46, "Device-Count"], [47, 64, "Hardware-device"]], "Comments": []}
{"id": 163555, "text": "Query scoring and retrieval is done with Elasticsearch . [ 3 ] These abstracts are concatenated together and form the input along with the term question for our models ( [ §4 ] .", "label": [[41, 54, "Software-Entity"]], "Comments": []}
{"id": 163559, "text": "We use AllenNLP 's implementation of BiDAF trained on SQuAD .", "label": [[7, 18, "Software-Entity"]], "Comments": []}
{"id": 163570, "text": "We recruited participants on Amazon Mechanical Turk .", "label": [[29, 51, "Cloud-Platform"]], "Comments": []}
{"id": 163593, "text": "We both report the tokenized BLEU and SacreBLEU scores for comparison with previous work . 3.3 Efficiency Discussion ODE Transformer is efficient to use .", "label": [[38, 47, "Software-Entity"]], "Comments": []}
{"id": 163597, "text": "For the WMT'14 English-French ( En-Fr ) task , we used the dataset provided within Fairseq , i.e. , 36M training sentence pairs from WMT'14 . * newstest2012+newstest2013 * was the validation data and * newstest2014 * was the test data .", "label": [[83, 90, "Software-Entity"]], "Comments": []}
{"id": 163625, "text": "For the En-De and En-Fr tasks , the datasets used in this work could be found in Fairseq .", "label": [[81, 88, "Software-Entity"]], "Comments": []}
{"id": 163628, "text": "All experiments were trained on 8 GPUs with 4 , 096 tokens on each GPU .", "label": [[32, 38, "Device-Count"]], "Comments": []}
{"id": 163632, "text": "Both tokenized BLEU and SacreBLEU [ 11 ] scores were reported on the En-De and En-Fr tasks .", "label": [[24, 33, "Software-Entity"]], "Comments": []}
{"id": 163647, "text": "125M parameters ) that the task organizers fine-tuned on the source domain data sets via the Huggingface Transformers library v3.5.1 [ Wolf et al. , , 2020 ] .", "label": [[93, 104, "Software-Entity"]], "Comments": []}
{"id": 163653, "text": "For the time recognition data , we split it into sentences using the English sentencizer from Spacy v2.3.2 [ Honnibal et al. , 2020 ] .", "label": [[94, 106, "Software-Entity"]], "Comments": []}
{"id": 163654, "text": "All hyperparameters are given in appendix [ A.1 . ] All experiments are run on a single Nvidia P100 GPU .", "label": [[81, 87, "Device-Count"], [88, 103, "Hardware-device"]], "Comments": []}
{"id": 163666, "text": "We evaluate faithfulness in terms of the ability of Figure 4 : Histogram and t-SNE [ Van der Maaten and ] [ Hinton , 2008 ] visualizations of the modality , negation , and content spaces learned by the INF+ADV+MIN model . the model to preserve the negation , uncertainty , and content of the input .", "label": [[77, 82, "Software-Entity"]], "Comments": []}
{"id": 163672, "text": "We report case-sensitive de-tokenized BLEU using sacreBLEU [ Post , 2018 ] .", "label": [[49, 58, "Software-Entity"]], "Comments": []}
{"id": 163704, "text": "We run our experiments on NVIDIA Tesla V100 GPUs .", "label": [[26, 48, "Hardware-device"]], "Comments": []}
{"id": 163736, "text": "All experiments were conducted using a single GPU card .", "label": [[39, 49, "Device-Count"]], "Comments": []}
{"id": 163741, "text": "CamemBERTbase uses the RoBERTa architecture with SentencePiece tokenizer and optimized whole-word masking and was trained on the 138GB French part of OSCAR [ Ortiz Suárez et al. , , 2019 ] , which is built from CommonCrawl .", "label": [[49, 62, "Software-Entity"], [211, 222, "Software-Entity"]], "Comments": []}
{"id": 163743, "text": "FrALBERT uses the AlBERT architecture , which corresponds to the BERT architecture with sentence order prediction training objective , SentencePiece tokenizer and parameter sharing to reduce the size of the model , it was trained on a 4GB Wikipedia dump .", "label": [[135, 148, "Software-Entity"]], "Comments": []}
{"id": 163749, "text": "For this purpose , multiple researchers enhanced NLP datasets with textual explanations . [ Camburu et al. , 2018 ] extended the Stanford Natural Language Inference dataset ( SNLI ) [ Bow ] [ man et al. , , 2015 ] using Amazon Mechanical Turk .", "label": [[220, 242, "Cloud-Platform"]], "Comments": []}
{"id": 163750, "text": "The expanded dataset is called * e-SNLI * and contains textual , human-generated explanations for each of SNLI 's entailment relation pairs . [ Ra ] [ jani et al. , 2019 ] , also using Amazon Mechanical Turk , expanded COMMONSENSEQA [ Talmor et al. , 2019 ] .", "label": [[185, 207, "Cloud-Platform"]], "Comments": []}
{"id": 163751, "text": "We used NLTK 's word_tokenize [ 5 ] to obtain the tokens , so their count Table 3 : Distribution of gold standard scores .", "label": [[8, 15, "Software-Entity"]], "Comments": []}
{"id": 163755, "text": "4.1 Experimental Settings As baselines , we utilize HuggingFace 's implementation of the T5-base and mT5-base models [ Wolf et al. , 2020 ] .", "label": [[52, 66, "Software-Entity"]], "Comments": []}
{"id": 163758, "text": "Each model trained for approximately 1-5 hours on 2 Nvidia RTX 2080 Ti cards with 11 GB of RAM .", "label": [[50, 51, "Device-Count"], [52, 70, "Hardware-device"], [82, 94, "Device-Memory"]], "Comments": []}
{"id": 163772, "text": "We performed the training using the Hugging-Face Transformers library [ Wolf et al. , , 2020 ] and PyTorch [ Paszke et al. , , 2019 ] , learning rate of 1e-5 , and the Adam optimizer [ Kingma and Ba , 2015 ] .", "label": [[36, 48, "Software-Entity"], [99, 106, "Software-Entity"]], "Comments": []}
{"id": 163776, "text": "We performed the training using the HuggingFace Transformers library and PyTorch , using the the pretrained T5-large model ( 770M parameters ) .", "label": [[36, 47, "Software-Entity"], [73, 80, "Software-Entity"]], "Comments": []}
{"id": 163777, "text": "For model fine-tuning , a single RTX 3090 was used .", "label": [[26, 32, "Device-Count"], [33, 41, "Hardware-device"]], "Comments": []}
{"id": 163792, "text": "Each GPU 's batch contains up to 3072 ( u , d , lf ) segments and we used 8 ( 16 ) GPUs to train base ( large ) MS-TLM .", "label": [[74, 75, "Device-Count"], [78, 80, "Device-Memory"]], "Comments": []}
{"id": 163794, "text": "All participants were recruited using the Amazon Mechanical Turk platform .", "label": [[42, 64, "Cloud-Platform"]], "Comments": []}
{"id": 163796, "text": "Since the input sequence will be tokenized into subwords in the contextualized encoder , we tokenize sequences in word-level with * nltk * tokenizer [ Bird et al. , , 2009 ] additionally and implement * POS-Enhanced Encoder * , where each subword in a complete word will share the same POS tag .", "label": [[132, 136, "Software-Entity"]], "Comments": []}
{"id": 163801, "text": "3 Experiments 3.1 Setup & Dataset The experiments are run on 8 NVIDIA Tesla P40 GPUs and the implementation of * POI-Net * is based on the Pytorch implementation of ALBERT [ Paszke et al. , , 2019 ] .", "label": [[61, 62, "Device-Count"], [63, 84, "Hardware-device"], [139, 146, "Software-Entity"]], "Comments": []}
{"id": 163803, "text": "To measure the anti-interference of * POS Embedding * , we randomly modify part of POS tags from * nltk * POS tagger to error tags , and the results on SQuAD 1.1 development set are shown in Table [ 6 . ] Table 6 : Results of robustness research of POS Embedding on dev sets from SQuAD 1.1 .", "label": [[99, 103, "Software-Entity"]], "Comments": []}
{"id": 163805, "text": "A Part-Of-Speech Tags List In this appendix , we list all 39 POS tags ( including POS tags from * nltk * POS tagger and defined by us ) in Table [ 9 . ]", "label": [[98, 102, "Software-Entity"]], "Comments": []}
{"id": 163812, "text": "In our implementation , ] we apply [ SpaCy ( Honnibal and M ] ontani , 2017 ) and NLTK ( Loper and Bird , 2002 ) for text manipulation .", "label": [[37, 42, "Software-Entity"], [82, 86, "Software-Entity"]], "Comments": []}
{"id": 163813, "text": "We run ValCAT on Intel Xeon E5-2690 2.6GHz Processor with V100 GPU .", "label": [[17, 52, "Hardware-device"], [58, 66, "Hardware-device"]], "Comments": []}
{"id": 163828, "text": "All settings are trained on 4 Nvidia V100 GPUs with 16k tokens in a batch .", "label": [[28, 29, "Device-Count"], [30, 46, "Hardware-device"]], "Comments": []}
{"id": 163830, "text": "To select the sentence pairs with different word orders , we use spaCy [ Honnibal et al. , , 2020 ] to parse the dependency of Russian sentences .", "label": [[65, 70, "Software-Entity"]], "Comments": []}
{"id": 163841, "text": "Using the NLPAug library [ Ma , 2019 ] we substitute up to 10 % of the words in each sentence with their synonyms found in WordNet [ Feinerer and Hornik , 2020 ] and present the results in Table [ 4 . ]", "label": [[10, 16, "Software-Entity"]], "Comments": []}
{"id": 163862, "text": "Crowdworkers on Amazon Mechanical Turk [ 8 ] were shown the < https : //www.mturk.com > Table 5 : Ablation ROUGE F1 scores on TAC 2011 and DUC 2004 .", "label": [[16, 38, "Cloud-Platform"]], "Comments": []}
{"id": 163885, "text": "We implement our models [ 1 ] in the Marian toolkit [ Junczys-Dowmunt et al. , , 2018 ] .", "label": [[37, 43, "Software-Entity"]], "Comments": []}
{"id": 163886, "text": "For the CTC loss computation , we use the warp-ctc library [ Amodei et al. , 2016 ] . 4.1 Teacher Models For training our baseline autoregressive models , we closely follow the approach of [ Chen et al. , 2021 ] .", "label": [[42, 50, "Software-Entity"]], "Comments": []}
{"id": 163896, "text": "We run the training of each model for three weeks on four Nvidia Pascal P100 GPUs . 5 Results In this section , we try to view the results of the NAR and efficiency research in a shared perspective .", "label": [[53, 57, "Device-Count"], [58, 81, "Hardware-device"]], "Comments": []}
{"id": 163897, "text": "We use Sacrebleu [ Post , 2018 ] as the implementation of the BLEU score metric .", "label": [[7, 16, "Software-Entity"]], "Comments": []}
{"id": 163898, "text": "To bring the evaluation Table 2 : The BLEU scores of the NAR models on the WMT 14 test set up to date with the current state-of-the-art translation systems , we also evaluate our models using COMET [ Rei et al. , , 2020 ] [ 5 ] and BLEU [ 6 ] scores on the recent WMT 21 test set .", "label": [[192, 197, "Software-Entity"]], "Comments": []}
{"id": 163907, "text": "For the GPU decoding measurements , we use a single Nvidia Ampere A100 GPU .", "label": [[45, 51, "Device-Count"], [52, 74, "Hardware-device"]], "Comments": []}
{"id": 163908, "text": "The CPU evaluation was performed on a 36-core CPU Intel Xeon Gold 6354 server from Oracle cloud .", "label": [[38, 49, "Device-Count"], [50, 70, "Hardware-device"], [83, 95, "Cloud-Platform"]], "Comments": []}
{"id": 163909, "text": "We measure the overall wall time to translate the Table 4 : The comparison of the decoding time of various NAR models for a single sentence in a batch on a P100 GPU .", "label": [[156, 164, "Hardware-device"]], "Comments": []}
{"id": 163914, "text": "We also ran the evaluation on an Nvidia Pascal P100 GPU , which showed that when the batch size is large enough , autoregressive models eventually match the speed of non-autoregressive models .", "label": [[33, 55, "Hardware-device"]], "Comments": []}
{"id": 163920, "text": "On Figure 2 : The decoding times to translate the efficiency task test set using various batch size settings , computed on a single Nvidia Ampere A100 GPU , i.e . the GPU type used for evaluation in the efficiency task .", "label": [[125, 131, "Device-Count"], [132, 154, "Hardware-device"]], "Comments": []}
{"id": 163921, "text": "Figure 3 : The decoding times to translate the efficiency task test set using various batch size settings , computed on a single Nvidia Pascal P100 GPU .", "label": [[122, 128, "Device-Count"], [129, 151, "Hardware-device"]], "Comments": []}
{"id": 163933, "text": "All experiments were run on a single NVIDIA GeForce RTX 3090 GPU ; training one BAD-X BA or multilingal LA for 50,000 iterations took around 24 hours ( MAD-X LA for 25,000 steps took around 12 hours ) .", "label": [[30, 36, "Device-Count"], [37, 64, "Hardware-device"]], "Comments": []}
{"id": 163940, "text": "Finally , TIE is trained and evaluated on four Nvidia A10 Graphics Cards with batch size 32 for two epochs .", "label": [[42, 46, "Device-Count"], [47, 57, "Hardware-device"]], "Comments": []}
{"id": 163964, "text": "Repositories The RSTGen models were extended from pretrained models in Huggingface 's Transformers repository [ Wolf et al. , , 2019 ] .", "label": [[71, 85, "Software-Entity"]], "Comments": []}
{"id": 163965, "text": "We used Pytorch-Lightning [ Falcon et al. , , 2019 ] for all our training scripts .", "label": [[8, 25, "Software-Entity"]], "Comments": []}
{"id": 163966, "text": "Hardware For fine-tuning the RSTGen model on the RST Annotated Dateset , we used 2 GeForce RTX 3090 ( 24GB ) .", "label": [[81, 82, "Device-Count"], [83, 99, "Hardware-device"], [102, 106, "Device-Memory"]], "Comments": []}
{"id": 163967, "text": "For the Argument Generation Tasks and the Story Generation Task , the RSTGen variants and RST Neural Sampler are fine-tuned using 1 GeForce RTX 3090 ( 24GB ) .", "label": [[130, 131, "Device-Count"], [132, 148, "Hardware-device"], [151, 155, "Device-Memory"]], "Comments": []}
{"id": 163983, "text": "The models are trained on one Tesla V100 DGXS with 32GB memory .", "label": [[26, 29, "Device-Count"], [30, 45, "Hardware-device"], [51, 62, "Device-Memory"]], "Comments": []}
{"id": 163985, "text": "We use fairseq [ 2 ] [ Ott et al. , , 2019 ] to conduct the experiments .", "label": [[7, 14, "Software-Entity"]], "Comments": []}
{"id": 163986, "text": "A.2.2 Learning from Weak Supervisions : Style Transfer We use the Adam optimizer with learning rate − , the batch size is 128 and the model is trained on one Tesla V100 DGXS 32GB .", "label": [[154, 157, "Device-Count"], [158, 168, "Hardware-device"], [174, 178, "Device-Memory"]], "Comments": []}
{"id": 163989, "text": "Fairseq [ Ott et al. , , 2019 ] is adopted to conduct the experiments .", "label": [[0, 7, "Software-Entity"]], "Comments": []}
{"id": 163991, "text": "A.3.2 Comparison with Loss Truncation The Loss Truncation ( LT [ Kang and Hashimoto , 2020 ] ) , method adaptively removes high log loss Fairseq ( -py ) is MIT-licensed . examples as a way to optimize for distinguishability .", "label": [[137, 144, "Software-Entity"]], "Comments": []}
{"id": 163995, "text": "The models are tested on one Tesla V100 DGXS with 32 GB memory , the batch size is 128 , max number of tokens is 6000 and update frequency is 4 .", "label": [[25, 28, "Device-Count"], [29, 39, "Hardware-device"], [50, 62, "Device-Memory"]], "Comments": []}
{"id": 164008, "text": "We train both the left-context-only and the left-and-rightcontext models on a single RTX-8000 GPU for 10 epochs , using the Adam optimizer [ Kingma and Ba , 2015 ] with learning rate initialized at 1e ´ 5 for 10 epochs with early stopping .", "label": [[78, 84, "Device-Count"], [85, 97, "Hardware-device"]], "Comments": []}
{"id": 164009, "text": "We perform Books3 pretraining for both left-contextonly and left-and-right-context models on a single RTX-8000 GPU for 5 epochs using Adam with learning rate initialized at 1e ´ 5 .", "label": [[95, 101, "Device-Count"], [102, 114, "Hardware-device"]], "Comments": []}
{"id": 164039, "text": "For this ablation study , pre-training is only performed via KD with the pre-trained BERTBASE as Figure 4 : T-SNE visualization of the output of the middle Transformer layer of the fine-tuned models on SST-2 dev .", "label": [[108, 113, "Software-Entity"]], "Comments": []}
{"id": 164040, "text": "We also used t-SNE to visualize the output of the FFN of the middle layer ( layer 6 ) of the finetuned KroneckerBERT with and without KD in comparison with the fine-tuned teacher , on SST-2 dev .", "label": [[13, 18, "Software-Entity"]], "Comments": []}
{"id": 164045, "text": "The training was performed on V100 GPU and the average latency for training of KroneckerBERT for a batch size of 64 was 32ms .", "label": [[30, 38, "Hardware-device"]], "Comments": []}
{"id": 164062, "text": "The model is based on Huggingface Transformers [ Wolf et al. , , 2020 ] .", "label": [[22, 33, "Software-Entity"]], "Comments": []}
{"id": 164067, "text": "This fine-tuning took approximately 6 hours using 1 NVIDIA V100 .", "label": [[50, 51, "Device-Count"], [52, 63, "Hardware-device"]], "Comments": []}
{"id": 164070, "text": "This took approximately a week using 4 NVIDIA V100 .", "label": [[37, 38, "Device-Count"], [39, 50, "Hardware-device"]], "Comments": []}
{"id": 164073, "text": "It took approximately 2 hours to train a classifier on 10 % of the train data using 1 NVIDIA TITAN RTX .", "label": [[84, 85, "Device-Count"], [86, 102, "Hardware-device"]], "Comments": []}
{"id": 164076, "text": "This took about 5 hours using 1 NVIDIA V100 .", "label": [[30, 31, "Device-Count"], [32, 43, "Hardware-device"]], "Comments": []}
{"id": 164077, "text": "All models were run on a single NVIDIA A100 using cuda 11.1 and cudnn 8.0.5. https : //aihub.or.kr/aihub-data/natural-language/about Table 11 : Average inference latency of proposed model architectures .", "label": [[32, 43, "Hardware-device"], [50, 59, "Software-Entity"], [64, 75, "Software-Entity"]], "Comments": []}
{"id": 164091, "text": "For shallow models , we optimize them by feeding gradients of either https : //github.com/kyzhouhzau/BERT-NER loss function ( see Appendix [ B ] for their derivation ) into a L-BFGS optimizer [ Liu and Nocedal , 1989 ] in Scikit-Learn .", "label": [[222, 234, "Software-Entity"]], "Comments": []}
{"id": 164092, "text": "For deep models ( Bi-LSTM and BERT ) , we rely on TensorFlow 's automatic differentiation and Adam gradient descent optimizer [ Kingma and Ba , 2014 ] because manually deriving gradients for deep models is infeasible .", "label": [[50, 63, "Software-Entity"]], "Comments": []}
{"id": 164093, "text": "Adam in TensorFlow ) to find high-quality local minima than second-order methods like L-BFGS .", "label": [[8, 18, "Software-Entity"]], "Comments": []}
{"id": 164094, "text": "References A Implementation Details A.1 Software and Hardware Environment All the deep learning models are implemented in Tensorflow 1.12.0 environment .", "label": [[122, 132, "Software-Entity"]], "Comments": []}
{"id": 164095, "text": "Softmax regression ( or multinomial logistic regression ) model is from Scikit-Learn package in version 0.23.2 .", "label": [[72, 84, "Software-Entity"]], "Comments": []}
{"id": 164096, "text": "The CRF model is implemented by the package sklearn-crfsuite in version of 0.3.6 .", "label": [[44, 60, "Software-Entity"]], "Comments": []}
{"id": 164097, "text": "Data resampling and CRF training/evaluation were performed on 2.60 GHz Intel CPUs and 8GB RAM .", "label": [[61, 81, "Hardware-device"], [86, 93, "Device-Memory"]], "Comments": []}
{"id": 164098, "text": "Bi-LSTM and BERT training/evaluation were performed on GPUs ( GeForce GTX1080 8GB and Tesla V100 16GB ) .", "label": [[62, 77, "Hardware-device"], [78, 81, "Device-Memory"], [86, 96, "Hardware-device"], [97, 101, "Device-Memory"]], "Comments": []}
{"id": 164099, "text": "For details of them , please see documents of sklearn , crfsuite and BERT-NER .", "label": [[46, 53, "Software-Entity"], [56, 64, "Software-Entity"]], "Comments": []}
{"id": 164102, "text": "B Derivation of Loss Function Gradients for Softmax Regression When using the shallow model with softmax output layer and focal/Dice loss functions , we optimize the model parameters by the quasi-Newton method L-BFGS provided by Python Scikit-Learn .", "label": [[236, 248, "Software-Entity"]], "Comments": []}
{"id": 164104, "text": "Projections to low dimensional spaces , e.g. , by t-SNE and PCA , lose some information , and it is difficult to control and interpret the aspects that these projections preserve .", "label": [[50, 55, "Software-Entity"]], "Comments": []}
{"id": 164109, "text": "We used a Linux server with Intel Xeon E7-4830 v4 CPUs in the experiments . 4.1 Computing Embeddings We used 300-dimensional GloVe embedding with the first 40 000 words as the input embeddings { xv } .", "label": [[28, 54, "Hardware-device"]], "Comments": []}
{"id": 164111, "text": "RandProj , which led to 99 trials in the first comparison . 4.3 Assesment via Crowdsourcing We conducted a user study at Amazon Mechanical Turk to confirm the effectiveness of WORDTOUR .", "label": [[121, 143, "Cloud-Platform"]], "Comments": []}
{"id": 164118, "text": "FastText.zip [ Joulin et al. , , 2016 ] quantizes and prunes word embeddings for memory-efficient text classification .", "label": [[0, 8, "Software-Entity"]], "Comments": []}
{"id": 164119, "text": "Although Fast-Text.zip saves considerable memory consumption without harming downstream tasks , it prunes words that are irrelevant to text classification , whereas we aim to retain the original vocabulary in this work .", "label": [[9, 18, "Software-Entity"]], "Comments": []}
{"id": 164121, "text": "One is model agnostic approaches including Shapley regression values [ Shapley , 1953 ] and LIME [ Ribeiro et al. , , 2016 ] .", "label": [[92, 96, "Software-Entity"]], "Comments": []}
{"id": 164128, "text": "During inference time , we apply the hidden layer encoder Hencoder to all the input datasets and index , then using FAISS [ 1 ] [ Johnson et al. , , 2017 ] offline .", "label": [[116, 121, "Software-Entity"]], "Comments": []}
{"id": 164129, "text": "FAISS is an efficient , open-source library for similarity search and clustering on dense vectors , which can be applied to large-scale vectors .", "label": [[0, 5, "Software-Entity"]], "Comments": []}
{"id": 164133, "text": "Specifically , as in the application of SHAP , the feature attribution of * SHAP-Deep * as a variant of * DeepLIFT * is defined as : 5 Experiments In this section , we compare the proposed method to the state-of-the-art feature attribution methods under different use cases .", "label": [[40, 44, "Software-Entity"], [76, 85, "Software-Entity"], [106, 114, "Software-Entity"]], "Comments": []}
{"id": 164141, "text": "For all 9 , 645 sentences in SST , Amazon Mechanical Turk labeled the sentiment for words/phrases/sentences yielded from the Stanford Parser [ Manning et al. , , 2014 ] on a scale between 1 and 25 .", "label": [[35, 57, "Cloud-Platform"], [125, 140, "Software-Entity"]], "Comments": []}
{"id": 164146, "text": "References Appendix : A Model Implementation Detail All experiments are conducted with eight NVIDIA Tesla V100 GPUs with 2.5 GHz ( base ) and 3.1 GHz ( sustained all-core turbo ) Intel Xeon 8175M processors .", "label": [[87, 92, "Device-Count"], [93, 115, "Hardware-device"], [121, 128, "Device-Memory"], [142, 149, "Device-Memory"], [179, 195, "Hardware-device"]], "Comments": []}
{"id": 164164, "text": "For all experiments , we use Ubuntu 18.04 with one 40GB A100 Nvidia GPU , 1 TB RAM and 16 TB hard disk space .", "label": [[51, 55, "Device-Memory"], [56, 71, "Hardware-device"], [74, 83, "Device-Memory"], [87, 92, "Device-Memory"]], "Comments": []}
{"id": 164207, "text": "B.3 Implementation Details of Evidentiality Labeling Model We use PyTorch [ Paszke et al. , , 2019 ] via Hugging-Face transformers RoBERTA [ Liu et al. , , 2019 ] implementation . [ 10 ] We tune our model from RoBERTa-base .", "label": [[66, 73, "Software-Entity"], [105, 117, "Software-Entity"]], "Comments": []}
{"id": 164232, "text": "We note that the difference between STL-PLM and MTL is whether to use one synthetic QA dataset or multiple synthetic QA datasets for its training . 4.1.3 Implementations We employ RoBERTa-L [ Liu et al. , , 2019b ] from Hugging Face 's transformers toolkit for all experiments .", "label": [[220, 235, "Software-Entity"]], "Comments": []}
{"id": 164236, "text": "In this figure , every sample is mapped into a 1024-dimensional feature space through RoBERTa-L model and projected back into a twodimensional plane by t-SNE .", "label": [[152, 157, "Software-Entity"]], "Comments": []}
{"id": 164237, "text": "This implies that * KG-C adapter * Figure 3 : t-SNE visualization of the hidden representation from ( a ) PLM and ( b ) * KG-C adapter * .", "label": [[46, 51, "Software-Entity"]], "Comments": []}
{"id": 164242, "text": "The experiments are conducted split across NVIDIA GeForce 3090 and NVIDIA RTX A5000 .", "label": [[43, 62, "Hardware-device"], [67, 83, "Hardware-device"]], "Comments": []}
{"id": 164265, "text": "B Details and Lessons from Experimenting with GPT-3 's API B.1 Choice of Model We use the davinci model provided by OpenAI LP 's API , which corresponds to [ 6 ] the 175 billion parameter model reported in [ Brown et al. , 2020 ] .", "label": [[116, 122, "Software-Entity"]], "Comments": []}
{"id": 164266, "text": "Fine-Tuning As mentioned in [ Section 3 , ] we use priming ( a.k.a . in-context learning ) in lieu of fine-tuning because , at the time of writing , OpenAI 's fine-tuning API is limited to 10 runs per month .", "label": [[149, 158, "Software-Entity"]], "Comments": []}
{"id": 164270, "text": "Note that T5 and T0 are trained with the Adafactor optimizer [ Shazeer and Stern , 2018 ] in Mesh TensorFlow .", "label": [[93, 108, "Software-Entity"]], "Comments": []}
{"id": 164271, "text": "Our implementation is in PyTorch , and we find that fine-tuning T5 with PyTorch 's implementation of Adafactor yields substantially worse results than the usual choice of the AdamW optimizer .", "label": [[25, 32, "Software-Entity"], [72, 82, "Software-Entity"]], "Comments": []}
{"id": 164272, "text": "We corresponded with [ Raffel et al. , 2020 ] , who advised us that it might be due to the fact that PyTorch does not have the same learning rate scheduler implementation as TensorFlow 's Adafactor does .", "label": [[101, 108, "Software-Entity"], [174, 187, "Software-Entity"]], "Comments": []}
{"id": 164274, "text": "D Compute Used Each ALBERT 235M model is trained on a single Nvidia RTX3090 .", "label": [[54, 60, "Device-Count"], [61, 75, "Hardware-device"]], "Comments": []}
{"id": 164276, "text": "[ https : //github.com/bigscience-workshop/t-zero/tree/ ] ( https : //github.com/bigscience-workshop/t-zero/tree/master/examples ) [ master/examples ] ( https : //github.com/bigscience-workshop/t-zero/tree/master/examples ) Each T5 LMA 770M model is trained on a single A6000 .", "label": [[263, 269, "Device-Count"], [270, 275, "Hardware-device"]], "Comments": []}
{"id": 164279, "text": "The 11B models are each trained on eight V100s ( each with 32GB of memory ) .", "label": [[35, 40, "Device-Count"], [41, 46, "Hardware-device"], [59, 73, "Device-Memory"]], "Comments": []}
{"id": 164308, "text": "( e ) BM25 system based on lexical matching from Elasticsearch [ 10 ] .", "label": [[49, 62, "Software-Entity"]], "Comments": []}
{"id": 164311, "text": "Three retrievers for the first-phrase retrieval are tested : BM25 from Elasticsearch , the zero-shot MS MARCO retriever and the GPL retriever enhanced by TSDAE pre-training .", "label": [[71, 84, "Software-Entity"]], "Comments": []}
{"id": 164313, "text": "For reference , training a distilbert-base model for 100k steps takes about 9.6 hours on a single V100 GPU . 6.2 Influence of Corpus Size We next analyze the influence of different corpus sizes .", "label": [[91, 97, "Device-Count"], [98, 106, "Hardware-device"]], "Comments": []}
{"id": 164332, "text": "We use CountVectorizer with default whitespace tokenization in sklearn [ Pedregosa et al. , 2011 ] to perform this task .", "label": [[63, 70, "Software-Entity"]], "Comments": []}
{"id": 164339, "text": "For simplicity , we use BPE tokenizer and re-use the embedding table from RoBERTa-Large for our student Bi-LSTM and CNN model .", "label": [[24, 27, "Software-Entity"]], "Comments": []}
{"id": 164346, "text": "This is realistic nowadays with the growth of public model hubs such as TensorFlow Hub [ 5 ] and Hugging Face Models [ 6 ] .", "label": [[72, 82, "Software-Entity"], [97, 109, "Software-Entity"]], "Comments": []}
{"id": 164359, "text": "We thank huggingface dataset team [ Lhoest et al. , , 2021 ] for providing easy access to these datasets .", "label": [[9, 20, "Software-Entity"]], "Comments": []}
{"id": 164360, "text": "A.2 Implementation Details N-gram pre-processing are implemented with scikit-learn [ Pedregosa et al. , , 2011 ] .", "label": [[70, 82, "Software-Entity"]], "Comments": []}
{"id": 164361, "text": "DistilBERT [ Sanh et al. , , 2019 ] and MobileBERT baselines are implemented in huggingface transformers [ Wolf et al. , , 2020 ] .", "label": [[80, 91, "Software-Entity"]], "Comments": []}
{"id": 164362, "text": "RoBERTa-Large , BiLSTM , CNN , and DAN experiments are implemented with fairseq [ Ott et al. , , 2019 ] .", "label": [[72, 79, "Software-Entity"]], "Comments": []}
{"id": 164364, "text": "We train DAN models on either A100 40GB PCIe or Quadro RTX 8000 depending on availability .", "label": [[30, 34, "Hardware-device"], [35, 39, "Device-Memory"], [48, 63, "Hardware-device"]], "Comments": []}
{"id": 164365, "text": "GPU inference is performed with one Quadro RTX 8000 GPU , and CPU inference is performed with 56 Intel Xeon CPU E5-2690 v4 CPUs .", "label": [[32, 35, "Device-Count"], [36, 55, "Hardware-device"], [94, 96, "Device-Count"], [97, 127, "Hardware-device"]], "Comments": []}
{"id": 164368, "text": "Parallel Training We try further scaling up the student model by splitting the gigantic embedding table to different GPUs and enable parallel training , as implemented in Megatron-LM [ Shoeybi et al. , , 2019 ] .", "label": [[171, 179, "Software-Entity"]], "Comments": []}
{"id": 164372, "text": "For reference , in Table [ 9 ] we report the tokenization time on the 25,000 training instances in the IMDB dataset with ( 1 ) n-gram tokenization ( used by DAN , implemented with scikit-learn ) ; ( 2 ) BPE tokenization ( used by RoBERTa/DistilRoBERTa , implemented with fairseq ) ; ( 3 ) WordPiece tokenization ( used by DistilBERT , implemented with huggingface transformers ) .", "label": [[180, 192, "Software-Entity"], [271, 278, "Software-Entity"], [352, 363, "Software-Entity"]], "Comments": []}
{"id": 164374, "text": "Secondly , DAN models still have better tokenization speed than transformer models that use BPE/WordPiece tokenization .", "label": [[92, 105, "Software-Entity"]], "Comments": []}
{"id": 164376, "text": "For example , the user could implement a DAN model with BPE tokenzation .", "label": [[56, 59, "Software-Entity"]], "Comments": []}
{"id": 164386, "text": "We run our experiments on one Tesla A6000 GPU and carry out five trials with different seeds to report the mean and one standard error .", "label": [[26, 29, "Device-Count"], [30, 45, "Hardware-device"]], "Comments": []}
{"id": 164387, "text": "Based on Huggingface [ Wolf et al. , , 2019 ] , we apply cased BERT-base [ Devlin et al. , , 2019 ] and RoBERTalarge [ Liu et al. , , 2019 ] for DocRED and cased SciBERT [ Beltagy et al. , , 2019 ] for CDR and GDA .", "label": [[9, 20, "Software-Entity"]], "Comments": []}
{"id": 164391, "text": "If given a plain document , we shall utilize existing tools ( e.g. , spaCy ) to get noisy annotations and apply our method afterward .", "label": [[69, 74, "Software-Entity"]], "Comments": []}
{"id": 164398, "text": "First , we run the spaCy tagger [ Honni ] [ bal et al. , , 2020 ] [ 3 ] to assign POS tags .", "label": [[19, 24, "Software-Entity"]], "Comments": []}
{"id": 164399, "text": "[ 4 ] Finally , we apply the spaCy dependency parser to the texts with the corrected POS tags and identified main verbs and arguments .", "label": [[29, 34, "Software-Entity"]], "Comments": []}
{"id": 164400, "text": "List names can be indicative of task intents : For example , a to-do text ( * description * = `` avocados '' , [ Short for Latent Intent Task Embedding ] The code is available at [ github.com/microsoft/Intent ] ( https : //github.com/microsoft/Intent-based-Task-Representation-Learning ) [ based-Task-Representation-Learning ] ( https : //github.com/microsoft/Intent-based-Task-Representation-Learning ) We use the English model en_core_web_lg v3.0.0 We extracted the first 100 files from DepCC and re-tagged the sentences using spaCy .", "label": [[529, 534, "Software-Entity"]], "Comments": []}
{"id": 164408, "text": "* Data collection : We leverage COMET [ Hwang et al. , 2021 ] , a BART model [ Lewis et al. , , 2020 ] fine-tuned on ATOMIC 20 , to collect weak supervision signals about to-do tasks ' prerequisites and goals .", "label": [[32, 37, "Software-Entity"]], "Comments": []}
{"id": 164421, "text": "We use a logistic regression classifier implemented in scikit-learn [ Pe ] [ dregosa et al. , , 2011 ] , with or without a penalty term .", "label": [[55, 67, "Software-Entity"]], "Comments": []}
{"id": 164423, "text": "4.2.1 Implementation Details We implemented our MTL framework using Py-Torch v1.10.0 [ Paszke et al. , , 2019 ] and ran experiments on NVIDIA GeForce GTX TITAN X and RTX A6000 ( for BERTlarge ) .", "label": [[68, 84, "Software-Entity"], [135, 161, "Hardware-device"], [166, 175, "Hardware-device"]], "Comments": []}
{"id": 164424, "text": "We use uncased BERTbase , uncased BERTlarge , and cased RoBERTabase , in the transformers library v4.6.1 [ Wolf et al. , , 2020 ] with the default parameters for dimensions , activation functions , and dropout .", "label": [[77, 89, "Software-Entity"]], "Comments": []}
{"id": 164434, "text": "Table 13 : Result of in-dataset fine-tuning . ous tasks and is considered to be the best-quality general-purpose encoder [ Reimers , 2021 ] . word2vec and fastText : Unlike the other baseline encoders , word2vec [ Mikolov et al. , , 2013 ] and fastText [ Bojanowski et al. , , 2017 ] do not contextualize embeddings .", "label": [[155, 163, "Software-Entity"], [244, 252, "Software-Entity"]], "Comments": []}
{"id": 164436, "text": "For fastText , we use a 300D model trained on Common-Crawl 2M .", "label": [[4, 12, "Software-Entity"]], "Comments": []}
{"id": 164438, "text": "Sentence-MPNet is trained with a huge amount of additional training data but still under-performs LITE . word2vec and fastText performed similarly and outperform vanilla BERT and RoBERTa on UIT and ID .", "label": [[118, 126, "Software-Entity"]], "Comments": []}
{"id": 164440, "text": "Transformers : We used Huggingface 's transformers library [ Wolf et al. , , 2020 ] to run pre-trained Transformer models .", "label": [[23, 37, "Software-Entity"]], "Comments": []}
{"id": 164441, "text": "Sentence Transformers : We use the Sentence-BERT library [ Reimers and Gurevych , 2019 ] [ 13 ] to run pre-trained sentence encoders .", "label": [[35, 48, "Software-Entity"]], "Comments": []}
{"id": 164456, "text": "A Dataset statistics For test sets : For training sets , the numbers of pairs of documents and reference summaries in the train split are : For each dataset , we use the entire ( except for Big-Patent , 10 % due to its huge size ) train split in Google Tensorflow Datasets for training .", "label": [[253, 263, "Software-Entity"]], "Comments": []}
{"id": 164457, "text": "B Computational environment and cost All experiments were carried out on one RTX3090 GPU installed on a desktop computer .", "label": [[73, 76, "Device-Count"], [77, 88, "Hardware-device"]], "Comments": []}
{"id": 164458, "text": "The research community replied to the limitations of large massively multilingual models by developing language-specific monolingual language Table 1 : Monolingual models from the transformers library [ Wolf et al. , , 2020 ] covering all the ( non-English ) languages of the XL-WSD dataset [ Pasini et al. , , 2021 ] .", "label": [[180, 192, "Software-Entity"]], "Comments": []}
{"id": 164461, "text": "We used the SPAMS library [ Mairal et al. , , 2009 ] for calculating D and α .", "label": [[12, 17, "Software-Entity"]], "Comments": []}
{"id": 164463, "text": "4 Experimental results All the neural language models that we relied on during our experiments were obtained from the transformers library [ Wolf et al. , , 2020 ] .", "label": [[118, 130, "Software-Entity"]], "Comments": []}
{"id": 164464, "text": "We used four NVIDIA Titan 2080 GPUs for our experiments .", "label": [[8, 12, "Device-Count"], [13, 35, "Hardware-device"]], "Comments": []}
{"id": 164472, "text": "In addition to the Tatoeba corpus , we used the word2word library [ Choe et al. , , 2020 ] containing word translation pairs between more than 3,500 language pairs .", "label": [[48, 57, "Software-Entity"]], "Comments": []}
{"id": 164479, "text": "1 Introduction and Motivation The dominant class of models in NLP ( pre-trained transformer models ; [ Brown et al. , , 2020 ] [ Devlin et al. , 2019 ] [ Bommasani et al. , , 2021 ] use tokenization schemes , like BPE or WordPiece tokenization [ Sennrich et al. , , 2016 ] [ Schuster and Nakajima , 2012 ] [ Kudo and Richardson , 2018 ] , that break text into word pieces .", "label": [[214, 217, "Software-Entity"], [221, 230, "Software-Entity"]], "Comments": []}
{"id": 164486, "text": "Our first model ( the SpaCy model ) uses the SpaCy library [ Honnibal and Montani , 2017 ] to obtain distributions over features for each token in the vocabulary : Fine-Grained Part of Speech Table 3 : The best and worst performing characters from Experiment 2 on the SpaCy syntactic baseline , the GPT-J syntactic baseline , and the Control .", "label": [[45, 50, "Software-Entity"], [268, 273, "Software-Entity"]], "Comments": []}
{"id": 164488, "text": "Because SpaCy is built to operate over words , not tokens , we also construct custom syntactic baselines that can tag subwords , as opposed to tokens .", "label": [[8, 13, "Software-Entity"]], "Comments": []}
{"id": 164489, "text": "The method is the same as in Experiment 1 , where the goal is to predict the presence or absence of a character α in a token , except that instead of using the token 's model embeddings as input , we instead use syntactic feature vectors ( obtained either from SpaCy or a custom tagger ) as input .", "label": [[261, 266, "Software-Entity"]], "Comments": []}
{"id": 164490, "text": "Syntactic baselines The SpaCy model has 3 features for each token : NER , PoS , and Coarse-Grained PoS tags .", "label": [[24, 29, "Software-Entity"]], "Comments": []}
{"id": 164491, "text": "The custom syntactic tagger , which is intended to solve the problem that SpaCy tags words and not subword tokens , takes a ( subword ) token 's model embedding as input and outputs a vector of probabilities over part of speech and named entity categories .", "label": [[74, 79, "Software-Entity"]], "Comments": []}
{"id": 164492, "text": "Unlike the SpaCy tagger , our custom GPT-J-Tagger outputs a probability distribution over categories .", "label": [[11, 16, "Software-Entity"]], "Comments": []}
{"id": 164495, "text": "Using the syntactic baselines leads to substantially improved performance over control tasks , and the GPT-J-Tagger does better than the SpaCy tagger .", "label": [[137, 142, "Software-Entity"]], "Comments": []}
{"id": 164500, "text": "Appendix B Probing for Character Information We use off-the-shelf APIs for lemmatization and WordNet from NLTK ( Apache License 2.0 ; [ Bird et al. , 2009 ] .", "label": [[106, 110, "Software-Entity"]], "Comments": []}
{"id": 164501, "text": "Our implementation uses PyTorch ( BSD License ; [ Paszke et al. , , 2019 ] , HuggingFace ( Apache License 2.0 ; [ Wolf et al. , , 2019 ] and custom APIs for GPT-J 's embedding .", "label": [[24, 31, "Software-Entity"], [77, 88, "Software-Entity"]], "Comments": []}
{"id": 164502, "text": "B.1 PLMs considered Details of the PLMs used along with their modelcard on Huggingface : Table 6 : Results for the main probing experiment , across models .", "label": [[75, 86, "Software-Entity"]], "Comments": []}
{"id": 164505, "text": "They were run on a mix of NVidia Tesla K80 , GTX 1080 Ti , P100 , V100 GPUs with Dell R740 and Intel Xeon CPUs .", "label": [[26, 42, "Hardware-device"], [45, 56, "Hardware-device"], [59, 63, "Hardware-device"], [66, 75, "Hardware-device"], [81, 90, "Hardware-device"], [95, 110, "Hardware-device"]], "Comments": []}
{"id": 164506, "text": "Its model card in Huggingface is Table 9 : Experiment 1 hyperparameters .", "label": [[18, 29, "Software-Entity"]], "Comments": []}
{"id": 164507, "text": "These experiments take less than 20 minutes for each run requiring less than 12 GB of GPU memory and were run on a mix of NVidia Tesla K80 , GTX 1080 Ti , P100 , V100 GPUs with Dell R740 and Intel Xeon CPUs .", "label": [[122, 138, "Hardware-device"], [141, 152, "Hardware-device"], [155, 159, "Hardware-device"], [162, 171, "Hardware-device"], [177, 186, "Hardware-device"], [191, 206, "Hardware-device"]], "Comments": []}
{"id": 164508, "text": "Appendix D Syntax Baseline for Character information D.1 Custom syntax taggers First we consider an off-the-shelf SpaCy model with 3 features for each token : NER , PoS , and Coarse-Grained PoS tags .", "label": [[114, 119, "Software-Entity"]], "Comments": []}
{"id": 164509, "text": "The SpaCy tagger is not perfectly suited to our task since it operates at the word level , whereas we are concerned with obtaining a subword token 's embeddings .", "label": [[4, 9, "Software-Entity"]], "Comments": []}
{"id": 164510, "text": "Unlike the SpaCy tagger , our custom GPT-J-Tagger outputs a probability distribution over categories so we can use this distribution over labels as the vector of interest , rather than a one-hot vector .", "label": [[11, 16, "Software-Entity"]], "Comments": []}
{"id": 164511, "text": "When extracting syntactic features for this model , we first do the same pre-processing of removing the special preceding whitespace of GPT 's tokens as SpaCy before input into the BERT model .", "label": [[153, 158, "Software-Entity"]], "Comments": []}
{"id": 164513, "text": "D.2 Results and Hyperparameters We use off-the-shelf APIs for lemmatization and WordNet from NLTK .", "label": [[93, 97, "Software-Entity"]], "Comments": []}
{"id": 164514, "text": "Our implementation uses PyTorch [ Paszke et al. , , 2019 ] , HuggingFace [ Wolf et al. , 2019 ] and custom APIs ( now released ) for GPT-J 's embedding .", "label": [[24, 31, "Software-Entity"], [61, 72, "Software-Entity"]], "Comments": []}
{"id": 164515, "text": "Our BERT-Model is initialized with 'bert-base-cased ' from Huggingface with default values of hyperparameters .", "label": [[59, 70, "Software-Entity"]], "Comments": []}
{"id": 164516, "text": "Our implementation was done using PyTorch and optimized via Adam with betas of 0.9 & 0.999 and epsilon of 1e-08 without weight decay over the standard Cross Entropy loss .", "label": [[34, 41, "Software-Entity"]], "Comments": []}
{"id": 164518, "text": "The best learning rates for SpaCy , BERT-sentence , BERT-token , GPT-J and Control were found to be 1e-3 , 1e-3 , 3e-3 , 1e-4 , 1e-2 , respectively .", "label": [[28, 33, "Software-Entity"]], "Comments": []}
{"id": 164519, "text": "Our Table 15 : Standard Deviation of POS/NER labels implementation is done using PyTorch and Huggingface .", "label": [[81, 88, "Software-Entity"], [93, 104, "Software-Entity"]], "Comments": []}
{"id": 164522, "text": "The user answers are then presented , where each answer has been automatically segmented into individual sentences using SpaCy [ Honnibal et al. , , 2020 ] .", "label": [[121, 126, "Software-Entity"]], "Comments": []}
{"id": 164524, "text": "Accordingly , we use the sentence-transformers library [ Reimers and ] [ Gurevych , 2019a ] to perform clustering .", "label": [[25, 54, "Software-Entity"]], "Comments": []}
{"id": 164544, "text": "Environmental Cost The experiments described in the paper make use of V100 GPUs .", "label": [[70, 79, "Hardware-device"]], "Comments": []}
{"id": 164582, "text": "However , when we fine-tune for WikiQA on one A100-GPU , we only observe latency increasing from 71s→81s ( only 14.1 % increase ) .", "label": [[42, 45, "Device-Count"], [46, 54, "Hardware-device"]], "Comments": []}
{"id": 164595, "text": "We implemented our code based on HuggingFace 's Transformers library [ Wolf et al. , , 2020 ] .", "label": [[33, 47, "Software-Entity"]], "Comments": []}
{"id": 164604, "text": "Empirically , when we fine-tune for WikiQA on one A100-GPU , we only observe latency increasing from 71s→81s ( increase of only 14.1 % ) .", "label": [[46, 49, "Device-Count"], [50, 58, "Hardware-device"]], "Comments": []}
{"id": 164608, "text": "We use the Huggingface [ Wolf et al. , , 2020 ] XLM-Roberta base pre-trained model .", "label": [[11, 22, "Software-Entity"]], "Comments": []}
{"id": 164609, "text": "The various task specific heads are linear classifiers on the encoder output per the Huggingface implementation .", "label": [[85, 96, "Software-Entity"]], "Comments": []}
{"id": 164610, "text": "These choices was made based on the standard evaluation for the task from the Huggingface metrics module .", "label": [[78, 89, "Software-Entity"]], "Comments": []}
{"id": 164622, "text": "SEMSIM computes the cosine similarity between the average of word embeddings ( spaCy : [ Honnibal and Montani , 2021 ] of e out t and that of q .", "label": [[79, 84, "Software-Entity"]], "Comments": []}
{"id": 164642, "text": "We used four NVIDIA V100 GPUs for all experiments .", "label": [[8, 12, "Device-Count"], [13, 29, "Hardware-device"]], "Comments": []}
{"id": 164643, "text": "To train/test the models , we use five datasets of * WMT * , [ 2 ] * OpenSubtitle * or * OS * [ Lison and Tiedemann , 2016 ] , * PHP * , * Ubuntu * or * UB * , and * TED * [ Tiedemann , 2012 ] where all corpora are * normalized * and * tokenized * with the scripts provided by Moses .", "label": [[277, 282, "Software-Entity"]], "Comments": []}
{"id": 164663, "text": "We only report correlation analysis for this dataset as it was not a part of SummaC . 4.2 Experiment Setup Metric Implementation Metrics were applied directly from the original GitHub repository or by using the SacreRouge Library [ Deutsch and Roth , 2020 ] , which was also used in correlation analysis .", "label": [[211, 221, "Software-Entity"]], "Comments": []}
{"id": 164664, "text": "The learned metrics make use of code released from [ Laban et al. , 2021 ] for training , and all models are implemented in PyTorch [ Li et al. , , 2020 ] and in the Transformers library [ Wolf et al. , , 2019 ] .", "label": [[124, 131, "Software-Entity"], [166, 186, "Software-Entity"]], "Comments": []}
{"id": 164673, "text": "Environmental Cost The experiments described in the paper primarily make use of A100 GPUs .", "label": [[80, 89, "Hardware-device"]], "Comments": []}
{"id": 164691, "text": "Our code was implemented mainly using the Python libraries Pytorch [ Paszke et al. , , 2019 ] , Transformers [ Wolf et al. , , 2020 ] , Sklearn [ Pedregosa et al. , , 2011 ] , and the experiments were logged using Wandb [ Biewald , 2020 ] .", "label": [[59, 66, "Software-Entity"], [96, 108, "Software-Entity"], [136, 143, "Software-Entity"], [214, 219, "Software-Entity"]], "Comments": []}
{"id": 164692, "text": "Finetuning took 7 hours on a GeForce RTX 2080 Ti GPU .", "label": [[29, 52, "Hardware-device"]], "Comments": []}
{"id": 164693, "text": "Bias in Bios contains almost 400k biographies , and we obtain validation ( 10 % ) and test set ( 25 % ) by splitting with Scikit-learn 's [ Pedregosa et al. , 2011 ] test_train_split with our random seeds .", "label": [[122, 137, "Software-Entity"]], "Comments": []}
{"id": 164695, "text": "Fine-tuning took 3 hours on an NVIDIA RTX A6000 GPU .", "label": [[31, 51, "Hardware-device"]], "Comments": []}
{"id": 164699, "text": "We calculate Independence and Separation via Kullback–Leibler ( KL ) divergence , using the AllenNLP implementation ( [ https : //github.com/ ] ( https : //github.com/allenai/allennlp ) [ allenai/allennlp ] ( https : //github.com/allenai/allennlp ) ) .", "label": [[92, 100, "Software-Entity"]], "Comments": []}
{"id": 164708, "text": "B Experimental Settings The systems are implemented with PyTorch .", "label": [[57, 64, "Software-Entity"]], "Comments": []}
{"id": 164712, "text": "Our models are trained on Nvidia Tesla V100 GPUs with 32GB memory .", "label": [[26, 48, "Hardware-device"], [54, 65, "Device-Memory"]], "Comments": []}
{"id": 164717, "text": "Appendices A Experiment Details We run all single-task UNISTBASE experiments on NVIDIA RTX 2080Ti GPUs , and all UNISTLARGE and multi-task experiments on NVIDIA RTX A5000 GPUs .", "label": [[80, 102, "Hardware-device"], [154, 175, "Hardware-device"]], "Comments": []}
{"id": 164723, "text": "Inference After training the input encoder and prompt encoder , we encode the entire set of training examples with E ( · ) in a pre-processing step using FAISS [ Johnson et al. , , 2017 ] .", "label": [[154, 159, "Software-Entity"]], "Comments": []}
{"id": 164730, "text": "Zooming in on different inference LMs , GPT-J performs slightly better than GPT-NEO across the board , since it was Figure 3 : A t-SNE projection and clustering of the representations learned by EPR for the training examples in BREAK .", "label": [[129, 134, "Software-Entity"]], "Comments": []}
{"id": 164731, "text": "Figure [ 3 ] shows a t-SNE [ Hinton and Roweis , 2002 ] projection of the embeddings learned by EPR for the training examples of BREAK , with a link to an interactive version , where we applied the OPTICS [ Ankerst et al. , , 1999 ] [ Schubert and Gertz , 2018 ] clustering algorithm .", "label": [[21, 26, "Software-Entity"]], "Comments": []}
{"id": 164734, "text": "Training details To train EPR , we use the Adam optimizer [ Kingma and Ba , 2015 ] with batch size 120 and learning rate 1e-4 on eight RTX 3090 .", "label": [[129, 134, "Device-Count"], [135, 143, "Hardware-device"]], "Comments": []}
{"id": 164737, "text": "Table 12 : Example of a cluster from the t-SNE projection of EPR on BREAK .", "label": [[41, 46, "Software-Entity"]], "Comments": []}
{"id": 164738, "text": "Table 13 : Example of a cluster from the t-SNE projection of EPR on BREAK .", "label": [[41, 46, "Software-Entity"]], "Comments": []}
{"id": 164748, "text": "We report the results where necessity and sufficiency are calculated with masking rather than perturbing the chosen tokens in Appendix [ C. ] As baselines , we calculate the average importance of the tokens corresponding to target groups with SHAP [ 4 ] and LIME [ 5 ] .", "label": [[243, 247, "Software-Entity"]], "Comments": []}
{"id": 164751, "text": "6.1 Comparison of Average SHAP and LIME Values with Necessity and Sufficiency The average SHAP and LIME values for the two targets are presented in Table [ 4 . ]", "label": [[26, 30, "Software-Entity"], [35, 39, "Software-Entity"], [90, 94, "Software-Entity"], [99, 103, "Software-Entity"]], "Comments": []}
{"id": 164761, "text": "The training takes approximately 2.5 hours on a Tesla V100-SXM2 GPU .", "label": [[48, 67, "Hardware-device"]], "Comments": []}
{"id": 164762, "text": "This results in a total of 66,120 perturbed instances , and takes approximately 6 hours to generate on a 2.3 GHz Quad-Core Intel Core i7 CPU .", "label": [[105, 140, "Hardware-device"]], "Comments": []}
{"id": 164764, "text": "We fine-tune a BERT model from the Hugging Face library [ 12 ] on each of these datasets on a single Tesla V100-SXM2 GPU .", "label": [[35, 47, "Software-Entity"], [94, 100, "Device-Count"], [101, 120, "Hardware-device"]], "Comments": []}
{"id": 164773, "text": "We restrict these experiments to NQ and TriviaQA due to the high cost of running them for all datasets and baselines . 4.4 Implementation Details We base our implementation on the official code of DPR [ Karpukhin et al. , , 2020 ] , which is built on Hugging Face Transformers [ Wolf et al. , , 2020 ] .", "label": [[251, 263, "Software-Entity"]], "Comments": []}
{"id": 164776, "text": "We utilize eight 80GB A100 GPUs for pretraining , which takes roughly two days .", "label": [[11, 16, "Device-Count"], [17, 21, "Device-Memory"], [22, 31, "Hardware-device"]], "Comments": []}
{"id": 164777, "text": "In our ablation study ( see Section [ 5.4 ] , we lower the learning rate to 10− and the batch size to 512 in order to fit in eight Quadro RTX 8000 GPUs . [ 6 ] Each ablation takes two days .", "label": [[125, 130, "Device-Count"], [131, 151, "Hardware-device"]], "Comments": []}
{"id": 164780, "text": "One ablation does involve a batch size of 1,024 , and was trained using A100 GPUs as well .", "label": [[72, 81, "Hardware-device"]], "Comments": []}
{"id": 164783, "text": "Retrieval When performing dense retrieval , we apply exact search using FAISS [ Johnson et al. , , 2021 ] .", "label": [[72, 77, "Software-Entity"]], "Comments": []}
{"id": 164784, "text": "BM25 ) , we utilize the Pyserini library [ Lin et al. , 2021 ] , built on top of Anserini [ Yang et al. , ] [ 2017 , 2018 ] .", "label": [[24, 32, "Software-Entity"], [81, 89, "Software-Entity"]], "Comments": []}
{"id": 164785, "text": "The The model was downloaded from Hugging Face model hub 200,000 times during December 2021 .", "label": [[34, 46, "Software-Entity"]], "Comments": []}
{"id": 164802, "text": "We initialize the reranker from the BERT model trained on MS MARCO [ [ Nguyen et al. , ] [ 2016 ] ] ( page-10-13 ) by NBoost [ [ Thienes and Pertschuk , ] [ 2019 ] ] ( page-11-5 ) and available through Hugging Face [ 3 ] .", "label": [[202, 214, "Software-Entity"]], "Comments": []}
{"id": 164804, "text": "After DPR Stage 1 training the passages from the corpus are indexed with a Hierarchical Navigable Small World ( HNSW ) [ [ Malkov and Yashunin , ] [ 2018 ] ] ( page-10-14 ) using FAISS [ [ John ] [ son et al. , ] [ 2017 ] ] ( page-9-13 ) .", "label": [[179, 184, "Software-Entity"]], "Comments": []}
{"id": 164815, "text": "The random seed for python , numpy and pytorch was 42 .", "label": [[29, 34, "Software-Entity"], [39, 46, "Software-Entity"]], "Comments": []}
{"id": 164817, "text": "Computing infrastructure Using a single NVIDIA V100 GPU DPR training of two epochs takes approximately 24 hours for T-REx and less than 12 hours for FEVER and WoW .", "label": [[33, 39, "Device-Count"], [40, 55, "Hardware-device"]], "Comments": []}
{"id": 164818, "text": "Using a two NVIDIA P100 GPUs generation training for 370k T-REx instances takes two days , while FEVER and WoW training completes in half a day .", "label": [[8, 11, "Device-Count"], [12, 28, "Hardware-device"]], "Comments": []}
{"id": 164826, "text": "In order to assess typical levels of such noise in user feedback , we conduct a pilot study on Amazon Mechanical Turk ( Mturk ) , and evaluate the accuracy of feedback from Mturk users on two different text classification problems .", "label": [[95, 127, "Cloud-Platform"], [173, 178, "Cloud-Platform"]], "Comments": []}
{"id": 164830, "text": "The 20 newsgroup dataset * 20news * consists of 18k news articles and headers , organized into 20 classes . 3.1 Pilot Study : Real World User Feedback To understand the dynamics of user feedback noise , we conduct a pilot study using Amazon Mechanical Turk ( Mturk ) .", "label": [[234, 266, "Cloud-Platform"]], "Comments": []}
{"id": 164843, "text": "We train the model on an 8-GPU ( Nvidia V100s ) machine for up to 50 rounds with early stopping enabled .", "label": [[25, 30, "Device-Count"], [33, 45, "Hardware-device"]], "Comments": []}
{"id": 164863, "text": "We identify entities in a question using spaCy [ 3 ] .", "label": [[41, 46, "Software-Entity"]], "Comments": []}
{"id": 164875, "text": "We first construct templates from the questions by applying an existing NER system ( e.g. , spaCy ) to identify entities in the questions .", "label": [[92, 97, "Software-Entity"]], "Comments": []}
{"id": 164878, "text": "Our approach and the baseline are implemented in Py-Torch [ Paszke et al. , , 2019 ] .", "label": [[49, 57, "Software-Entity"]], "Comments": []}
{"id": 164879, "text": "We train our models on 4 NVIDIA Tesla V100 GPUs , with approximately 8–10 mins per epoch .", "label": [[23, 24, "Device-Count"], [25, 47, "Hardware-device"]], "Comments": []}
{"id": 164891, "text": "Training details All implementation is done in PyTorch [ Paszke et al. , , 2019 ] and Transformers [ Wolf et al. , , 2020 ] .", "label": [[47, 54, "Software-Entity"], [86, 98, "Software-Entity"]], "Comments": []}
{"id": 164896, "text": "Training was done for 4.5 hours with eight 32GB GPUs .", "label": [[37, 42, "Device-Count"], [43, 52, "Device-Memory"]], "Comments": []}
{"id": 164920, "text": "A Experiment Details Our experiments were run on a single V100 or RTX2080 GPU .", "label": [[51, 57, "Device-Count"], [58, 62, "Hardware-device"], [66, 77, "Hardware-device"]], "Comments": []}
{"id": 164935, "text": "Computational Resources We used Nvidia GeForce RTX 2080 to perform all our experiments except the experiment using the GPT-2 model which was ran on CPU for memory constraints .", "label": [[32, 55, "Hardware-device"]], "Comments": []}
{"id": 164936, "text": "Model details We used the small DialoGPT model from Hugging face [ 6 ] .", "label": [[52, 64, "Software-Entity"]], "Comments": []}
{"id": 164937, "text": "The GPT-2 [ 7 ] and Unitary Toxic-bert [ 8 ] models were also adapted from Hugging face .", "label": [[75, 87, "Software-Entity"]], "Comments": []}
{"id": 164944, "text": "The dataset is generated in Unity , a game engine seeing increased use by researchers [ Deitke et al. , , 2020 ] [ Gan et al. , , 2020 ] for its accessible rendering and physics simulation via the underlying Nvidia PhysX physics engine .", "label": [[28, 33, "Software-Entity"]], "Comments": []}
{"id": 164960, "text": "We run our experiments on a single RTX 8000 GPU , and each experiment takes 30–60 minutes per epoch .", "label": [[28, 34, "Device-Count"], [35, 47, "Hardware-device"]], "Comments": []}
{"id": 164963, "text": "Finetuning on our models took ∼6 hours per epoch , using four 48GB RTX8000 GPUs for finetuning our models .", "label": [[57, 61, "Device-Count"], [62, 79, "Hardware-device"]], "Comments": []}
{"id": 164980, "text": "[ 11 ] The dataset consists of 1 annotated entity per document , so we run spaCy ( `` en_core_web_lg '' model ) [ Honnibal and Montani , 2017 ] to identify additional mentions to allow our model and baselines to utilise other mentions to disambiguate the annotated entity mention .", "label": [[75, 80, "Software-Entity"]], "Comments": []}
{"id": 164985, "text": "This requires approximately 4 days when using 8 V100 GPUs .", "label": [[46, 47, "Device-Count"], [48, 57, "Hardware-device"]], "Comments": []}
{"id": 164986, "text": "B Training Details We use the Hugging Face implementation of RoBERTa [ Wolf et al. , , 2019 ] and optimise our model using Adam [ Kingma and Ba , 2015 ] with a linear learning rate schedule .", "label": [[30, 42, "Software-Entity"]], "Comments": []}
{"id": 164991, "text": "By avoiding the bilinear layer , our implementation is also faster to train , achieving 106.2 seconds per epoch on the DOCRED dataset on a single Tesla V100 GPU , compared to 155.7 seconds for the baseline model with a 128-dimension bilinear layer , and 343.2 seconds for the more accurate baseline model with a 256 dimension bilinear layer .", "label": [[139, 145, "Device-Count"], [146, 160, "Hardware-device"]], "Comments": []}
{"id": 164995, "text": "Full details about the differences between the two models can be found in Appendix [ C. ] 3.3 Experiment Setup We use the Hugging Face [ Wolf et al. , , 2020 ] implementation of XLM-RoBERTa-base [ Conneau et al. , , 2020 ] for Chinese .", "label": [[122, 134, "Software-Entity"]], "Comments": []}
{"id": 165001, "text": "We train all the models for 30 epochs on a NVIDIA Tesla V100 ( 16 GB ) GPU .", "label": [[43, 60, "Hardware-device"], [63, 68, "Device-Memory"]], "Comments": []}
{"id": 165014, "text": "All models are tested on one NVIDIA Tesla-V100 GPU with the same batch size and the beam size of IBR sets to 1 for a fair comparison .", "label": [[25, 28, "Device-Count"], [29, 50, "Hardware-device"]], "Comments": []}
{"id": 165015, "text": "We implement our model based on PyTorch along with Huggingface-Transformers toolkit [ 6 ] .", "label": [[32, 39, "Software-Entity"], [51, 62, "Software-Entity"]], "Comments": []}
{"id": 165017, "text": "All experiments are run on NVIDIA Tesla-V100 GPUs .", "label": [[27, 49, "Hardware-device"]], "Comments": []}
{"id": 165026, "text": "We set the other parameters the same as the baseline [ Kirstain et al. , 2021 ] . [ 5 ] All our experiments are performed on a single NVIDIA Tesla V100 32G GPU .", "label": [[127, 133, "Device-Count"], [134, 151, "Hardware-device"], [152, 159, "Device-Memory"]], "Comments": []}
{"id": 165036, "text": "For the other three datasets , we generate the back-translation data by ourselves ( with Fairseq toolkit [ Ott et al. , , 2019 ] ) .", "label": [[89, 96, "Software-Entity"]], "Comments": []}
{"id": 165039, "text": "All our experiments were run on a GeForce RTX 2080 Ti GPU and each experiment takes around 5 hours .", "label": [[34, 57, "Hardware-device"]], "Comments": []}
{"id": 165053, "text": "Human Evaluation Following [ Madotto et al. , , 2019 ] , We randomly sampled 100 sentences generated on the target domain and distributed a questionnaire at Amazon Mechanical Turk asking each worker to rank the content retention ( 0 to 5 ) , style transfer ( 0 to 5 ) and fluency ( 0 to 5 ) : human score = Average ( Pscorestyle + P P scorecontent + scoref luency ) , human score ∈ [ 0 , 100 ] .", "label": [[157, 179, "Cloud-Platform"]], "Comments": []}
{"id": 165056, "text": "To visualize how well DAML-ATM performs on the new unseen domain , we use t-SNE [ Van der Maaten and Hinton , 2008 ] plots to analyze the degree of separation between the source domain sentences and the generated target domain sentences .", "label": [[74, 79, "Software-Entity"]], "Comments": []}
{"id": 165059, "text": "We train our framework using the Adam optimizer [ Kingma and Ba , 2014 ] with the initial learning rate 1e-5 , and we employ a linear schedule for the learning rate , all models are trained on 8 RTX 3090 GPUs .", "label": [[193, 194, "Device-Count"], [195, 208, "Hardware-device"]], "Comments": []}
{"id": 165060, "text": "A.2 Details on Human Evaluation For the results generated by each method , following [ Krishna et al. , , 2020 ] , we randomly selected 100 sentences to be placed in the Amazon Mechanical Turk [ 1 ] questionnaire .", "label": [[170, 192, "Cloud-Platform"]], "Comments": []}
{"id": 165063, "text": "We randomly sampled 100 sentences generated on the target domain and distributed a questionnaire at Amazon Mechanical Turk asking each worker to rank the content retention ( 0 to 5 ) , style transfer ( 0 to 5 ) , and fluency ( 0 to 5 ) .", "label": [[100, 122, "Cloud-Platform"]], "Comments": []}
{"id": 165068, "text": "In preliminary experiments we found similar results when using the byte-level BPE tokenizer [ Sennrich et al. , , 2016 ] as used in RoBERTa [ Liu et al. , , 2019 ] .", "label": [[78, 81, "Software-Entity"]], "Comments": []}
{"id": 165070, "text": "5.1 Experimental setup For all our experiments , we employ the uncased BERT-base model [ Devlin et al. , , 2019 ] as implemented in the MaChAmp v0.2 toolkit [ van der Goot et al. , 2021 ] , since it has been shown to achieve state-of-the-art performance in hate speech detection [ Tran et al. , , 2020 ] .", "label": [[136, 148, "Software-Entity"]], "Comments": []}
{"id": 165073, "text": "All experiments have been run on a NVIDIA Tesla V100-SXM2 GPU , with a training time ranging from 10 to 40 minutes each .", "label": [[35, 61, "Hardware-device"]], "Comments": []}
{"id": 165083, "text": "A.2 Implementation Details Spacy 3.0.3 is used in data preprocessing .", "label": [[27, 38, "Software-Entity"]], "Comments": []}
{"id": 165084, "text": "Experiments are conducted on NVIDIA GTX 1080Ti , and the training time is about four hours .", "label": [[29, 46, "Hardware-device"]], "Comments": []}
{"id": 165089, "text": "We tokenize the bitext using a joint SentencePiece [ 7 ] unigram model [ Kudo , 2018 ] , with a character coverage of 1.0 and a maximum sentence length of 4096 tokens and create a vocabulary of 10K subwords .", "label": [[37, 50, "Software-Entity"]], "Comments": []}
{"id": 165092, "text": "We evaluate translation quality with BLEU [ Papineni et al. , 2002 ] using SacreBLEU [ Post , 2018 ] [ 9 ] and ChrF [ Popovic´ , 2015 ] .", "label": [[75, 84, "Software-Entity"]], "Comments": []}
{"id": 165099, "text": "As a baseline , we evaluated the zero-shot performance of M2M-100 ( not finetuned , ✗ ) on FLORES [ 10 ] using spBLEU ( i.e . sentencepiece BLEU [ Goyal et al. , , 2021 ] ) .", "label": [[126, 139, "Software-Entity"]], "Comments": []}
{"id": 165103, "text": "D Model Hyper-parameters and Reproducibility of Results For the pre-trained models , we fine-tune the models using HuggingFace transformer tool [ Wolf et al. , , 2020 ] with the default learning rate ( 5e − 5 ) , batch size of 10 , maximum source length & maximum target length of 200 , beam size of 10 , and number of epochs is 3 except for models trained on only NEWS which we set to 10 .", "label": [[115, 126, "Software-Entity"]], "Comments": []}
{"id": 165104, "text": "We make All the experiments were performed on a single GPU ( Nvidia V100 ) .", "label": [[48, 58, "Device-Count"], [61, 72, "Hardware-device"]], "Comments": []}
{"id": 165105, "text": "For mB ] ART50 , we pretrain with learning rate of 5e − 5 [ for ] 50 , 000 steps Table 11 : Model names on HuggingFace Model Hub .", "label": [[107, 118, "Software-Entity"]], "Comments": []}
{"id": 165106, "text": "using Fairseq ( Ott et al. , 2019 ) without modifying the mBART50 vocabulary .", "label": [[6, 13, "Software-Entity"]], "Comments": []}
{"id": 165107, "text": "[ Tabl ] e 11 has the names of all the models that are publicly available on HuggingFace Model Hub .", "label": [[77, 88, "Software-Entity"]], "Comments": []}
{"id": 165128, "text": "All MIPS retrieval is implemented using FAISS [ Johnson et al. , , 2019 ] .", "label": [[40, 45, "Software-Entity"]], "Comments": []}
{"id": 165134, "text": "A.2 Implementation Details We implement our models architectures based on HuggingFace 's Transformers [ Wolf et al. , , 2020 ] and run our experiments on 16 V100 GPUs .", "label": [[74, 88, "Software-Entity"], [154, 156, "Device-Count"], [157, 166, "Hardware-device"]], "Comments": []}
{"id": 165145, "text": "We try to explain this issue in Section [ 6.3 . ] For Seq2Edit , we additionally present results with other PLMs besides StructBERT , including BERT [ Devlin et al. , , 2019 ] , RoBERTa [ Liu et al. , , 2019 ] , and MacBERT [ Cui et al. , , 2020 ] from the Hugging Face [ 7 ] website .", "label": [[257, 269, "Software-Entity"]], "Comments": []}
{"id": 165169, "text": "D Experimental Setup Details All our experimental codes are based on the HuggingFace [ Wolf et al. , , 2020 ] .", "label": [[73, 84, "Software-Entity"]], "Comments": []}
{"id": 165171, "text": "We ran all experiments with one NVIDIA 2080Ti GPU with 16 GB of memory .", "label": [[32, 49, "Hardware-device"], [55, 70, "Device-Memory"]], "Comments": []}
{"id": 165186, "text": "We used word representations of h and c from fastText ( Bojanowski et al. , 2017 ) trained on Wikipedia as a simple setting of our model .", "label": [[45, 53, "Software-Entity"]], "Comments": []}
{"id": 165192, "text": "For picker 's label creation , we used stop words from NLTK for English and from [ sto ] pwordsiso for Chinese .", "label": [[55, 59, "Software-Entity"]], "Comments": []}
{"id": 165193, "text": "For lemmatization and stemming , NLTK 's Word-NetLemmatizer and PorterStemmer were adopted for English , while lemmatization and stemming were skipped for Chinese .", "label": [[33, 40, "Software-Entity"]], "Comments": []}
{"id": 165195, "text": "All models were trained on a single Tesla P100 GPU .", "label": [[29, 35, "Device-Count"], [36, 50, "Hardware-device"]], "Comments": []}
{"id": 165224, "text": "For all the experiments reported in this work , we use the Stanford CoreNLP parser [ Manning et al. , , 2014 ] .", "label": [[59, 82, "Software-Entity"]], "Comments": []}
{"id": 165235, "text": "We used the natural language toolkit [ 1 ] and the Bash parser [ 2 ] shared by the NLC2CMD competition organizers for preprocessing . 5 Experiments Section [ 5.1 ] describes the experiments conducted to measure our method 's accuracy .", "label": [[12, 36, "Software-Entity"], [51, 62, "Software-Entity"]], "Comments": []}
{"id": 165236, "text": "We fine-tuned the CodeT5-small checkpoint by huggingface [ Wolf et al. , , 2020 ] on our dataset .", "label": [[45, 56, "Software-Entity"]], "Comments": []}
{"id": 165247, "text": "The benchmarks are run on a 6 core Intel ( R ) Core ( TM ) i5-10400 CPU , using torch.utils.benchmark available in Py-Torch [ Paszke et al. , , 2019 ] .", "label": [[28, 34, "Device-Count"], [35, 71, "Hardware-device"], [115, 123, "Software-Entity"]], "Comments": []}
{"id": 165250, "text": "For instance We use the Integrated Gradient implementation provided by the Captum library - [ https : //www.github.com/ ] ( https : //www.github.com/pytorch/captum ) [ pytorch/captum ] ( https : //www.github.com/pytorch/captum ) Table 4 : Results from the evaluation of attributions produced by Integrated Gradients for 100 random examples from the test set .", "label": [[75, 81, "Software-Entity"]], "Comments": []}
{"id": 165256, "text": "Our method relies on Stanford CoreNLP parser [ Manning et al. , , 2014 ] for constituency parsing .", "label": [[21, 44, "Software-Entity"]], "Comments": []}
{"id": 165258, "text": "The most closely related work to ours is PrivFT [ Badawi et al. , , 2020 ] , a homomorphic encryption based method for privacy preserving text classification built on fastText [ Joulin et al. , , 2017 ] .", "label": [[167, 175, "Software-Entity"]], "Comments": []}
{"id": 165274, "text": "For implementation , we use a dual-NVLink Nvidia Quadro RTX6000 GPU with 24 GiBs of memory , on a server with a Intel Xeon Gold 6242R CPU ( 80 core ) and 125 GiBs of RAM . 3.2 Embedding Inversion As a quantitative evaluation of inversion risk , we adopt sentence embedding inversion .", "label": [[30, 34, "Device-Count"], [42, 67, "Hardware-device"], [73, 90, "Device-Memory"], [112, 137, "Hardware-device"], [140, 147, "Device-Memory"], [154, 169, "Device-Memory"]], "Comments": []}
{"id": 165311, "text": "A Appendix A.1 Details of Experiment Settings We run our code on Tesla V100 GPU with 16 GB memory .", "label": [[65, 79, "Hardware-device"], [85, 97, "Device-Memory"]], "Comments": []}
{"id": 165318, "text": "The train and test sets were annotated using Prodigy over the curated data .", "label": [[45, 52, "Software-Entity"]], "Comments": []}
{"id": 165319, "text": "Each of the participants had access to a separate instance of the Prodigy annotation tool [ Montani ] [ and Honnibal , 2018 ] , pre-loaded with the candidate annotation instances .", "label": [[66, 73, "Software-Entity"]], "Comments": []}
{"id": 165321, "text": "We train each model architecture for 10 epochs on a single NVIDIA Tesla T4 GPU with 15GB of GPU memory , which takes roughly 7 hours to train for each model .", "label": [[52, 58, "Device-Count"], [59, 78, "Hardware-device"], [84, 88, "Device-Memory"]], "Comments": []}
{"id": 165322, "text": "We then perform domainadaptive training against this dataset using the Hugging Face Transformers library .", "label": [[71, 83, "Software-Entity"]], "Comments": []}
{"id": 165323, "text": "This pretraining took roughly 8 hours using four 15GB NVIDIA Tesla T4 GPUs .", "label": [[44, 48, "Device-Count"], [49, 53, "Device-Memory"], [54, 74, "Hardware-device"]], "Comments": []}
{"id": 165334, "text": "All models are publicly available from Huggingface [ Wolf et al. , , 2019 ] .", "label": [[39, 50, "Software-Entity"]], "Comments": []}
{"id": 165336, "text": "Special thanks to the Machine Learning for Language Group at NYU for their wonderful NLP toolkit , JIANT [ Phang et al. , , 2020 ] .", "label": [[99, 104, "Software-Entity"]], "Comments": []}
{"id": 165338, "text": "Our model training and testing pipeline is modified from the JIANT toolkit .", "label": [[61, 66, "Software-Entity"]], "Comments": []}
{"id": 165339, "text": "All our experiments are implemented with models publicly available from Huggingface Transformers [ Wolf et al. , , 2020 ] [ 2 ] .", "label": [[72, 83, "Software-Entity"]], "Comments": []}
{"id": 165341, "text": "We use the AdamW [ Loshchilov and Hutter , 2019 ] as our optimizer . < https : //github.com/huggingface/transformers > Infrastructure All experiments are done with one single Geforce RTX 3090 ( 24GB ) .", "label": [[168, 174, "Device-Count"], [175, 191, "Hardware-device"], [194, 198, "Device-Memory"]], "Comments": []}
{"id": 165357, "text": "We used sklearn 's ridge-regression with default parameters , 10-fold cross-validation , Stochastic-Average-Gradient Descent Optimizer , Huggingface for Transformer models , MSE loss function , and L2-decay ( λ ) as 1.0 .", "label": [[8, 18, "Software-Entity"], [137, 148, "Software-Entity"]], "Comments": []}
{"id": 165359, "text": "All experiments were conducted on a machine with 1 NVIDIA GEFORCE-GTX GPU with 16GB GPU RAM .", "label": [[49, 50, "Device-Count"], [51, 73, "Hardware-device"], [79, 91, "Device-Memory"]], "Comments": []}
{"id": 165367, "text": "Note that we did not finetune any of these models ourselves but leveraged the state-of-the-art finetuned models available on Huggingface .", "label": [[125, 136, "Software-Entity"]], "Comments": []}
{"id": 165374, "text": "Note that we did not finetune any of these models ourselves but leveraged the state-of-the-art finetuned models available on Huggingface .", "label": [[125, 136, "Software-Entity"]], "Comments": []}
{"id": 165386, "text": "5.5 Case Study FACTGRAPH-E computes factuality scores for each edge of the AMR summary graph and those predictions are aggregated to generate a sentence- We use the JAMR aligner [ Flanigan et al. , , 2014 ] to obtain node-to-word alignments .", "label": [[165, 169, "Software-Entity"]], "Comments": []}
{"id": 165388, "text": "B Details of Models and Hyperparameters The experiments were executed using the version 3.3.1 of the * transformers * library released by Hugging Face [ Wolf et al. , , 2019 ] .", "label": [[103, 115, "Software-Entity"], [138, 150, "Software-Entity"]], "Comments": []}
{"id": 165391, "text": "The execution of the AMR parsers is parallelized using four Tesla V100 GPUs .", "label": [[55, 59, "Device-Count"], [60, 75, "Hardware-device"]], "Comments": []}
{"id": 165393, "text": "As shown in Table [ 9 , ] DAE 's preprocessing is much faster compared to this phase in FACTGRAPH-E , since DAE employs a fast en- < https : //catalog.ldc.upenn.edu/LDC2020T02 > hanced dependency model from the Stanford CoreNLP tool [ Manning et al. , , 2014 ] .", "label": [[211, 227, "Software-Entity"]], "Comments": []}
{"id": 165399, "text": "( 6 ) L+ ( · ) aims to increase the probability P ( zx , y≤ = 1|x , y < i ) of the gold label y since it is a valid extension of the translation prefix y < i : L− ( · ) is designed to reduce the probability P ( zx , y < i· = 1|x , y < i ) for all labels w except for the gold label y : Appendix [ C ] provides an implementation of SCONES in JAX [ Bradbury et al. , , 2018 ] .", "label": [[341, 344, "Software-Entity"]], "Comments": []}
{"id": 165401, "text": "We trained Transformer models ( Table [ 1 ] in six translation directions – German-English ( deen ) , Finnish-English ( en-fi ) , Lithuanian-English ( lt-en ) , and the reverse directions – on the WMT19 [ Barrault et al. , , 2019 ] training sets as provided by TensorFlow Datasets .", "label": [[261, 271, "Software-Entity"]], "Comments": []}
{"id": 165402, "text": "The training sets were filtered using language ID and simple length-based heuristics , and split into subwords using joint 32K SentencePiece [ Kudo and Richardson , 2018 ] models .", "label": [[127, 140, "Software-Entity"]], "Comments": []}
{"id": 165403, "text": "All our models were trained until convergence on the development set ( between 100K and 700K training steps ) using the LAMB [ You et al. , , 2020 ] optimizer in JAX [ Bradbury et al. , , 2018 ] .", "label": [[162, 165, "Software-Entity"]], "Comments": []}
{"id": 165404, "text": "We evaluate our models on the WMT19 test sets [ Barrault et al. , 2019 ] with SacreBLEU [ Post , 2018 ] , [ 6 ] using the WMT18 test sets as development sets to tune α .", "label": [[78, 87, "Software-Entity"]], "Comments": []}
{"id": 165408, "text": "Replacing beam-4 search with greedy search corresponds to a 3.9x speed-up ( 2.76 → 10.64 sentences per second ) on an entry-level NVIDIA Quadro P1000 GPU with a batch size of 4 . [ 7 ] Fig . [ 2 ] shows the BLEU scores for all six translation directions as a function of decoding speed .", "label": [[130, 153, "Hardware-device"]], "Comments": []}
{"id": 165413, "text": "C Implementation of SCONES in JAX Fig . [ 10 ] provides an implementation of the SCONES loss function ( Sec . [ 2 ] in JAX [ Brad ] [ bury et al. , , 2018 ] .", "label": [[30, 33, "Software-Entity"], [119, 122, "Software-Entity"]], "Comments": []}
{"id": 165417, "text": "21 return l o s s ∗ w e i g h t s / w e i g h t s .sum ( ) `` ` Figure 10 : JAX implementation of the SCONES loss function .", "label": [[76, 79, "Software-Entity"]], "Comments": []}
{"id": 165418, "text": "The JAX implementation generalizes the SCONES loss defined in the main paper in Eq .", "label": [[4, 7, "Software-Entity"]], "Comments": []}
{"id": 165419, "text": "2 We present strong baselines for the task including a new SpanBERT ( Joshi et al. , 2020 ) trained from scratch , and domain-adapted variants ( Gururangan et al. , 2020 ) , which we will release on the HuggingFace platform ( Wolf et al. , 2020 ) .", "label": [[203, 214, "Software-Entity"]], "Comments": []}
{"id": 165420, "text": "We used an open source text annotation tool named DOCCANO ( Nakayama et al. , 2018 ) .", "label": [[50, 57, "Software-Entity"]], "Comments": []}
{"id": 165421, "text": "BERTbase ( Devlin et al. , 2019 ) An out-ofthe-box BERTbase model ( bert-base-cased ) from the HuggingFace library ( Wolf et al. , 2020 ) functioning as a baseline .", "label": [[95, 106, "Software-Entity"]], "Comments": []}
{"id": 165424, "text": "We take the official TensorFlow implementation of SpanBERT by Ram et al . ( 2021 ) .", "label": [[21, 31, "Software-Entity"]], "Comments": []}
{"id": 165425, "text": "We continue training the BERT model for three epochs ( default in HuggingFace ) with a batch size of 16 .", "label": [[66, 77, "Software-Entity"]], "Comments": []}
{"id": 165432, "text": "We use the MACHAMP toolkit ( van der Goot et al. , 2021 ) for our experiments .", "label": [[11, 18, "Software-Entity"]], "Comments": []}
{"id": 165438, "text": "We use the default hyperparameters in MACHAMP ( van der Goot et al. , 2021 ) as shown in Table 4 .", "label": [[38, 45, "Software-Entity"]], "Comments": []}
{"id": 165440, "text": "All experiments with MACHAMP were ran on an NVIDIA® TITAN X ( Pascal ) 12 GB GPU and an Intel® Xeon® Silver 4214 CPU .", "label": [[21, 28, "Software-Entity"], [44, 59, "Hardware-device"], [71, 80, "Device-Memory"], [88, 116, "Hardware-device"]], "Comments": []}
{"id": 165468, "text": "This work was supported by Tencent Cloud and Tencent Youtu Lab .", "label": [[27, 40, "Cloud-Platform"]], "Comments": []}
{"id": 165471, "text": "We use 8 V100 GPUs and set gradient accumulation steps to 8 .", "label": [[7, 8, "Device-Count"], [9, 18, "Hardware-device"]], "Comments": []}
{"id": 165482, "text": "We use the grid search to search our hyperparameters , and the scope of each hyperparameter are included in Table [ 7 . ] We train our model on a single NVIDIA A40 GPU with 48GB memory .", "label": [[146, 152, "Device-Count"], [153, 167, "Hardware-device"], [173, 184, "Device-Memory"]], "Comments": []}
{"id": 165509, "text": "Our implementation is based on Pytorch [ Paszke et al. , , 2019 ] and Transformers [ Wolf et al. , , 2020 ] .", "label": [[31, 38, "Software-Entity"], [70, 82, "Software-Entity"]], "Comments": []}
{"id": 165510, "text": "We run experiments on a commodity server with a GeForce RTX 2080 GPU .", "label": [[48, 68, "Hardware-device"]], "Comments": []}
{"id": 165538, "text": "Each model is trained using one GPU ( Tesla_v100 sxm2-16gb ) , which takes 20 hours on average .", "label": [[28, 35, "Device-Count"], [38, 53, "Hardware-device"], [54, 58, "Device-Memory"]], "Comments": []}
{"id": 165545, "text": "We use Jieba [ 2 ] /NLTK [ Loper and Bird , 2002 ] for word tokenization of STORAL-ZH/ STORAL-EN .", "label": [[7, 12, "Software-Entity"], [20, 24, "Software-Entity"]], "Comments": []}
{"id": 165555, "text": "We perform our retrieval-augmented algorithm based on the post-trained models , called * RA-RoBERTa * and * RA-T5 * , respectively . 5.2 Experiment Settings We implement the pretrained models based on the codes and pretrained checkpoints of Hugging-Face 's Transformers [ Wolf et al. , , 2020 ] .", "label": [[241, 256, "Software-Entity"]], "Comments": []}
{"id": 165565, "text": "We conducted evaluation on STORAL-EN using Amazon Mechanical Turk ( AMT ) .", "label": [[43, 73, "Cloud-Platform"]], "Comments": []}
{"id": 165568, "text": "B Experiments B.1 Implementation We implement the pretrained models used in our experiment mainly based on the register models of HuggingFace [ Wolf et al. , , 2020 ] .", "label": [[130, 141, "Software-Entity"]], "Comments": []}
{"id": 165569, "text": "Note that we use LongLMbase [ Guan et al. , , 2022 ] as the T5 model for experiments on STORAL-ZH , which has not been registered on HuggingFace .", "label": [[133, 144, "Software-Entity"]], "Comments": []}
{"id": 165570, "text": "All results in the main paper and the appendix are based on one NVIDIA Tesla v100 ( 16G memory ) .", "label": [[60, 63, "Device-Count"], [64, 81, "Hardware-device"], [84, 94, "Device-Memory"]], "Comments": []}
{"id": 165571, "text": "The CPU is Intel Xeon Gold 5218 .", "label": [[11, 31, "Hardware-device"]], "Comments": []}
{"id": 165572, "text": "We set the hyper-parameters following the default parameters of HuggingFace .", "label": [[64, 75, "Software-Entity"]], "Comments": []}
{"id": 165615, "text": "Experiment Details We eval [ uat ] [ e case-sensitive ] detokenized BLEU using sacreBLEU ( [ Pos ] t , 2018 ) on MuST-C tst-COMMON [ set .", "label": [[79, 88, "Software-Entity"]], "Comments": []}
{"id": 165618, "text": "We jointly [ tokenize the bilingual ] * t [ ext ] * using SentencePiece ( Kudo and Richardson , 2018 ) , wit [ h a ] [ vocabulary s ] ize of 10k , which is the same as Ye et al . ( 2021 ) 's setup .", "label": [[58, 71, "Software-Entity"]], "Comments": []}
{"id": 165626, "text": "To visualize it , we plot th [ e b ] ivariate kernel density estimation ( Parzen , 1962 ) ( KDE ) contour of their dim-reduced features , where T-SNE ( Van der Maaten and Hinton , 2008 ) is used to reduce the dimension into two ( F [ igu ] re 5 ) .", "label": [[144, 149, "Software-Entity"]], "Comments": []}
{"id": 165633, "text": "We train the models in 8 Nvidia Tesla V100 GPUs for each experiment .", "label": [[23, 24, "Device-Count"], [25, 47, "Hardware-device"]], "Comments": []}
{"id": 165634, "text": "We use Fairseq [ Ott et al. , , 2019 ] as the code-base for our implementation .", "label": [[7, 14, "Software-Entity"]], "Comments": []}
{"id": 165652, "text": "For the tokenizer , we use the same byte-level BPE tokenizer as in GPT-2 ( Radford et al. , 2019 ) .", "label": [[47, 50, "Software-Entity"]], "Comments": []}
{"id": 165654, "text": "The model is trained for a total of 300 billion tokens , which takes approximately 21 days using 64 NVIDIA A100 GPUs .", "label": [[97, 99, "Device-Count"], [100, 116, "Hardware-device"]], "Comments": []}
{"id": 165659, "text": "We use Huggingface transformers ( Wolf et al. , 2020 ) to train the model , and use the learning rate . − , batch size 128 , the number of training epochs 3 .", "label": [[7, 18, "Software-Entity"]], "Comments": []}
{"id": 165692, "text": "In order to do this , we use the existing library named as spaCy [ 5 ] , and opensourced implementation of Entity Linker [ 6 ] .", "label": [[59, 64, "Software-Entity"]], "Comments": []}
{"id": 165698, "text": "B Experimental Setup In this section , we introduce the detailed setups for our models and baselines used in Table [ 1 , ] [ 2 , ] and [ 4 . ] B.1 Implementation Details We use the Pytorch [ Paszke et al. , , 2019 ] for the implementation of all models .", "label": [[181, 188, "Software-Entity"]], "Comments": []}
{"id": 165699, "text": "Also , to easily implement the language model , we use the huggingface library [ Wolf et al. , , 2020 ] containing various transformer-based pre-trained language models ( PLMs ) and their checkpoints .", "label": [[59, 70, "Software-Entity"]], "Comments": []}
{"id": 165708, "text": "B.3 Training details All experiments are constrained to be done with a single 12GB Geforce RTX 2080 Ti GPU for fairness in terms of memory and the availability on the academic budget , except for the DAPT and generative QA which use a single 48GB Quadro 8000 GPU .", "label": [[71, 77, "Device-Count"], [78, 82, "Device-Memory"], [83, 106, "Hardware-device"], [235, 241, "Device-Count"], [242, 246, "Device-Memory"], [247, 262, "Hardware-device"]], "Comments": []}
{"id": 165727, "text": "Specifically , we measure the computational cost on the NewsQA dataset with BERT-base , where we use the single Geforce RTX 2080 Ti GPU on the same machine .", "label": [[105, 111, "Device-Count"], [112, 135, "Hardware-device"]], "Comments": []}
{"id": 165731, "text": "Figure 15 : A textual example from NewsQA with predictions from each method ( DAPT and KALA ) , and also the T-SNE plot of contextualized representations from the last layer of BERT obtained by each method .", "label": [[109, 114, "Software-Entity"]], "Comments": []}
{"id": 165742, "text": "Models are trained on the Nvidia Superpod which consists of 1,024 A100 GPUs spread across 128 nodes .", "label": [[26, 41, "Hardware-device"], [60, 65, "Device-Count"], [66, 75, "Hardware-device"]], "Comments": []}
{"id": 165775, "text": "We conduct all experiments on one Tesla V100 SXM2 16GB .", "label": [[30, 33, "Device-Count"], [34, 49, "Hardware-device"], [50, 54, "Device-Memory"]], "Comments": []}
{"id": 165785, "text": "All the experiments are conducted based on the TRANSFORMERS library from HUGGINGFACE [ Wolf et al. , , 2020 ] .", "label": [[73, 84, "Software-Entity"]], "Comments": []}
{"id": 165786, "text": "F Computing Infrastructure We run all our experiments on a single NVIDIA TI-TAN RTX with 24GB GPU memory .", "label": [[59, 65, "Device-Count"], [66, 83, "Hardware-device"], [89, 104, "Device-Memory"]], "Comments": []}
{"id": 165794, "text": "[ We make ] all machine translations using pretrained 6-layer transformer networ [ ks ( Vaswani ] [ et al. , ] 2017 ) from MarianMT ( Tiedemann and Thottingal , 2020 ) , which are trained on a collection of web-based texts in the OPUS dataset ( Tiedemann , 2012 ) .", "label": [[123, 131, "Software-Entity"]], "Comments": []}
{"id": 165798, "text": "In addition to the word 's original form , we apply lemmatization or stemming to the original form using NLTK [ Bird et al. , , 2009 ] to find matching dictionary entries .", "label": [[105, 109, "Software-Entity"]], "Comments": []}
{"id": 165799, "text": "To check for Part-of-Speech ( POS ) tags , we apply the Flair tagger [ Akbik et al. , , 2018 ] on the context sentence with the slang expression replaced by a mask token and use counts from Histwords [ Hamilton et al. , , 2016 ] to determine POS tags for individual words .", "label": [[56, 61, "Software-Entity"]], "Comments": []}
{"id": 165802, "text": "We train the model for 20 epochs on a Nvidia Titan V GPU and took 12 hours to complete .", "label": [[38, 56, "Hardware-device"]], "Comments": []}
{"id": 165803, "text": "The contrastive models are trained on a Nvidia Titan V GPU for 4 epochs .", "label": [[40, 58, "Hardware-device"]], "Comments": []}
{"id": 165804, "text": "We check the similarity between two expressions in Equation [ 9 ] by comparing their fastText [ Bo ] [ janowski et al. , , 2017 ] embeddings .", "label": [[85, 93, "Software-Entity"]], "Comments": []}
{"id": 165805, "text": "For collaborative filtering , the neighborhood of words L ( S ) in Equation [ 8 ] is defined as the 5 closest words ( including the query word itself ) in the dataset 's slang expression vocabulary to the query word , measured in terms of cosine similarity between their respective fastText embeddings .", "label": [[282, 290, "Software-Entity"]], "Comments": []}
{"id": 165806, "text": "We use the list of stopwords from NLTK [ Bird et al. , , 2009 ] to check whether a word is a content word .", "label": [[34, 38, "Software-Entity"]], "Comments": []}
{"id": 165807, "text": "We apply the * simple_preprocess * routine from Gensim [ Re ] [ hurek and Sojka , 2011 ] before checking for the degree of content word overlap between two sentences .", "label": [[48, 54, "Software-Entity"]], "Comments": []}
{"id": 165812, "text": "DCUF takes about 5 hours to run on a single NVIDIA A100 Tensor Core GPU ( 40GB ) .", "label": [[37, 43, "Device-Count"], [44, 71, "Hardware-device"], [74, 78, "Device-Memory"]], "Comments": []}
{"id": 165813, "text": "The RoBERTa-based re-ranker is initialized from the hugging face checkpoint [ 2 ] without further fine-tuning .", "label": [[52, 64, "Software-Entity"]], "Comments": []}
{"id": 165815, "text": "We use NLTK [ 5 ] to remove stop words and lemmatize the remains .", "label": [[7, 11, "Software-Entity"]], "Comments": []}
{"id": 165828, "text": "Specifically , we compare to the well-known static word embedding approach fastText [ Joulin et al. , , 2016 ] , and an approach that extracts contextualized representations of words from a pretrained BERT language model [ Vulic´ et al. , 2020b ] , which we call `` BERT . ''", "label": [[75, 83, "Software-Entity"]], "Comments": []}
{"id": 165829, "text": "We re-implement the BERT baseline [ 7 ] , but use pretrained word embeddings for fastText [ 8 ] .", "label": [[81, 89, "Software-Entity"]], "Comments": []}
{"id": 165834, "text": "Since BERT and fastText can form representations from arbitrary string inputs , they have no out-of-vocabulary ( OOV ) words , while all methods relying on external resources ( ARES and COLEX ) require synset information for any word included in our experiments .", "label": [[15, 23, "Software-Entity"]], "Comments": []}
{"id": 165835, "text": "Figure [ 1 ] shows the overall correlation All experiments were performed on an x86-64 server with a 32-core Intel ( R ) Xeon ( R ) Silver 4215R CPU and 754GB RAM .", "label": [[101, 148, "Hardware-device"], [153, 162, "Device-Memory"]], "Comments": []}
{"id": 165841, "text": "The values reported below the diagonal are from COLEXcross while those above are from cross-lingual fastText embeddings created using the fully-supervised configuration of VECMAP [ Artetxe et al. , , 2018 ] .", "label": [[100, 108, "Software-Entity"]], "Comments": []}
{"id": 165842, "text": "We also compared COLEXcross to fastText ( the best-performing baseline on the LSIM task ) , on CLSIM ( Cross-lingual Semantic Similarity ) , a task identical to LSIM , except the words in each word pair are from different languages [ Vulic et al . ] ´ , [ 2020a ] ( see Table [ 3 ] .", "label": [[31, 39, "Software-Entity"]], "Comments": []}
{"id": 165843, "text": "We rely on VECMAP [ Artetxe et al. , , 2018 ] to map two monolingual fastText embedding spaces to a common bilingual space .", "label": [[69, 77, "Software-Entity"]], "Comments": []}
{"id": 165844, "text": "For every language pair , COLEXcross outperforms fastText , often by a significant margin .", "label": [[49, 57, "Software-Entity"]], "Comments": []}
{"id": 165846, "text": "We use the Perl script [ 15 ] from linguatools to convert xml format to raw text , excluding paragraph and heading mark-ups , and math and table tags .", "label": [[11, 15, "Software-Entity"]], "Comments": []}
{"id": 165852, "text": "B Implementation Details GPT2 : This model was implemented using the Pytorch Huggingface Transformers library [ Wolf et al. , 2020 ] and the Pytorch-lightning library [ 2 ] .", "label": [[69, 76, "Software-Entity"], [77, 88, "Software-Entity"], [141, 158, "Software-Entity"]], "Comments": []}
{"id": 165856, "text": "We follow training details in [ Rashkin et al. , 2021b ] and implement this model using the Pytorch Huggingface Transformers library and the Pytorch-lightning library .", "label": [[92, 99, "Software-Entity"], [100, 111, "Software-Entity"], [141, 158, "Software-Entity"]], "Comments": []}
{"id": 165858, "text": "Training for all models is done on an Nvidia V100 GPU 32GB and for inference , we use nucleus sampling with p=0.6 .", "label": [[38, 53, "Hardware-device"], [54, 58, "Device-Memory"]], "Comments": []}
{"id": 165862, "text": "We again hire college students to verify instance labels via Amazon Mechanical Turk .", "label": [[61, 83, "Cloud-Platform"]], "Comments": []}
{"id": 165864, "text": "Thus , we also directly probe the pretrained LMs in a complementary `` likelihood scoring '' experiment , described in Appendix [ C. ] Due to the size of MNLI and SNLI , we only evaluate available checkpoints from the Huggingface Transformers model hub .", "label": [[218, 229, "Software-Entity"]], "Comments": []}
{"id": 165867, "text": "During label verification , we host our questions on Amazon Mechanical Turk .", "label": [[53, 75, "Cloud-Platform"]], "Comments": []}
{"id": 165870, "text": "The first three are implemented with HuggingFace Transformers [ 20 ] , and the last is from OpenAI 's standard API [ 21 ] .", "label": [[37, 48, "Software-Entity"], [92, 101, "Software-Entity"]], "Comments": []}
{"id": 165873, "text": "Due to the size of MNLI and SNLI , we use existing checkpoints available on the Huggingface Transformers model hub .", "label": [[80, 91, "Software-Entity"]], "Comments": []}
{"id": 165874, "text": "For all other datasets , we finetune the pretrained models using the SequenceClassification pipeline on Huggingface , or the standard prompt completion finetuning API on OpenAI .", "label": [[104, 115, "Software-Entity"], [170, 176, "Software-Entity"]], "Comments": []}
{"id": 165875, "text": "[ 23 ] The finetuning scripts are adapted from the text-classification example in the HuggingFace Transformers repository .", "label": [[86, 97, "Software-Entity"]], "Comments": []}
{"id": 165876, "text": "[ 24 ] We performed hyperparameter search in the following range : The optimal hyperparameter values and finetuned models are available on the HuggingFace model hub .", "label": [[143, 154, "Software-Entity"]], "Comments": []}
{"id": 165877, "text": "We run our finetuning experiments on an NVIDIA GeForce RTX 2080 Ti GPU , with halfprecision floating point format ( FP16 ) .", "label": [[40, 70, "Hardware-device"]], "Comments": []}
{"id": 165878, "text": "The optimal hyperparameter values and finetuned models on the full 200 examples of each RNPC task are available on the HuggingFace model hub .", "label": [[119, 130, "Software-Entity"]], "Comments": []}
{"id": 165881, "text": "Specifically , the linear classifier is an SGDClassifier implemented with scikitlearn .", "label": [[74, 85, "Software-Entity"]], "Comments": []}
{"id": 165883, "text": "] ( https : //huggingface.co/transformers/v4.8.2/pretrained_models.html ) [ pretrained_models.html ] ( https : //huggingface.co/transformers/v4.8.2/pretrained_models.html ) or [ https : ] ( https : //beta.openai.com ) [ //beta.openai.com ] ( https : //beta.openai.com ) [ https : //beta.openai.com/docs/ ] ( https : //beta.openai.com/docs/api-reference/fine-tunes ) < https : //scikit-learn.org > The optimal hyperparameter values for the bestperforming models of each RNPC task from Section [ E.2 ] are available on the HuggingFace model hub .", "label": [[521, 532, "Software-Entity"]], "Comments": []}
{"id": 165892, "text": "We also control that all translation models use the same Transformer architecture [ Vaswani et al. , , 2017 ] by fairseq [ Ott et al. , , 2019 ] , with experimental details in Appendix [ C. ] A side benefit of controlling the training data size is that our experiments can help answer what the best nature ( i.e. , human translation direction ) of the training data given a fixed annotation or computation budget is .", "label": [[113, 120, "Software-Entity"]], "Comments": []}
{"id": 165901, "text": "We use the Python library Stanza [ Qi et al. , , 2020 ] to tokenize the sentences when [ ca ] lculating the number of sentences per sample .", "label": [[26, 32, "Software-Entity"]], "Comments": []}
{"id": 165902, "text": "For computational efficiency , we use NLTK [ Bird et al. , , 2009 ] to tokenize the words and co [ un ] t the vocabulary .", "label": [[38, 42, "Software-Entity"]], "Comments": []}
{"id": 165903, "text": "We use the Python library spaCy [ Ho ] [ nnibal and ] Montani , 2017 ) to calculate the punctuation per [ sample .", "label": [[26, 31, "Software-Entity"]], "Comments": []}
{"id": 165904, "text": "C Implementation Details C.1 Preprocessing To prepare the text for the models , we follow the preprocessing scripts of fairseq [ Ott et al. , , 2019 ] .", "label": [[119, 126, "Software-Entity"]], "Comments": []}
{"id": 165905, "text": "[ 11 ] Specifically , we use the Moses tokenizer [ Koehn et al. , 2007 ] , [ 12 ] the default byte pair encoding ( BPE ) size of 40K subwords , and remove sentence pairs that of larger than 1.5 length ratio from the training set .", "label": [[33, 38, "Software-Entity"]], "Comments": []}
{"id": 165906, "text": "C.2 Evaluation Script We use the fairseq script [ 13 ] to calculate the BLEU score [ Papineni et al. , , 2002 ] of each translation model , with a beam width of 5 , BPE removed , and detokenized by moses .", "label": [[33, 40, "Software-Entity"]], "Comments": []}
{"id": 165907, "text": "C.3 Model Details We use the sequence-to-sequence Transformer model [ Vaswani et al. , , 2017 ] implemented by the fairseq library [ Ott et al. , , 2019 ] .", "label": [[115, 122, "Software-Entity"]], "Comments": []}
{"id": 165911, "text": "All experiments are run on NVIDIA RTX2080 GPUs .", "label": [[27, 46, "Hardware-device"]], "Comments": []}
{"id": 165927, "text": "For German , Italian and Spanish input texts , we apply the tokenizer from JAMR parser [ 9 ] before sentencepiece tokenization .", "label": [[75, 79, "Software-Entity"], [100, 113, "Software-Entity"]], "Comments": []}
{"id": 165928, "text": "For Chinese , we directly apply the sentencepiece tokenizer .", "label": [[36, 49, "Software-Entity"]], "Comments": []}
{"id": 165934, "text": "C Implementation Details Our models are implemented with FAIRSEQ toolkit [ Ott et al. , , 2019 ] , trained and tested on a single NVIDIA Tesla A100/V100 GPU with 40-80GB memory .", "label": [[57, 64, "Software-Entity"], [123, 129, "Device-Count"], [130, 156, "Hardware-device"], [162, 176, "Device-Memory"]], "Comments": []}
{"id": 165937, "text": "EN is decoded on NVIDIA Tesla A100 and all other languages , on NVIDIA Tesla V100 .", "label": [[17, 34, "Hardware-device"], [64, 81, "Hardware-device"]], "Comments": []}
{"id": 165942, "text": "Experiments were conducted on four models , all based on huggingface 's bert-base-uncased [ Wolf et al. , , 2019 ] .", "label": [[57, 71, "Software-Entity"]], "Comments": []}
{"id": 165946, "text": "For all models , I ( D , Z ) < I ( D , Z1 ) + I ( D , Z2 ) ; i.e. , one gains The QA model was downloaded from huggingface model repository under `` twmkn9/bert-base-uncased-squad2 '' Figure 3 : Mean and standard deviation probabilities over 5 trials for plural candidates using the original embeddings ( green ) or counterfactual embeddings favoring plural ( dashed red ) or singular ( solid blue ) parses .", "label": [[111, 122, "Software-Entity"]], "Comments": []}
{"id": 165952, "text": "Training a single probe on a single layer took approximately 2 minutes on an NVIDIA GeForce 3080 ; generating counterfactuals took approximately 1 second per counterfactul on similar hardware .", "label": [[77, 96, "Hardware-device"]], "Comments": []}
{"id": 165962, "text": "Details and statistics of the two datasets are provided in [ A.3 . ] Implementation Our baseline implementation is adapted from the PyTorch COREF model by [ Xu and Choi , 2020 ] and the ATLOP RE model by [ Zhou et al. , 2021 ] .", "label": [[132, 139, "Software-Entity"]], "Comments": []}
{"id": 165963, "text": "The proposed Joint-M , +GP , +GC models are further coded in PyTorch .", "label": [[61, 68, "Software-Entity"]], "Comments": []}
{"id": 165970, "text": "All training is conducted on a Nvidia TITAN RTX GPU .", "label": [[31, 51, "Hardware-device"]], "Comments": []}
{"id": 165973, "text": "We implement nextTokens using a trie and a CFG for Overnight and SMCalFlow , respectively . 2.2 OpenAI language models OpenAI operates a service offering GPT-3 [ Brown et al. , 2020 ] through a networked API .", "label": [[119, 125, "Software-Entity"]], "Comments": []}
{"id": 165977, "text": "We used the models available in late 2021 ; OpenAI may change them from time to time . 2.3 Experimental setup We used two of the datasets from [ Shin et al. , 2021 ] for our experiments .", "label": [[44, 50, "Software-Entity"]], "Comments": []}
{"id": 166007, "text": "We use the default hyper-parameters of the Huggingface LED implementation ( W [ olf et ] al. , 2020 ) .", "label": [[43, 73, "Software-Entity"]], "Comments": []}
{"id": 166009, "text": "A Appendix A.1 Training Configurations For the joint related work tagger training , we use GeForce GTX 1080 11 GB GPUs .", "label": [[91, 107, "Hardware-device"], [108, 118, "Device-Memory"]], "Comments": []}
{"id": 166010, "text": "The training process lasts 2.5 hours on a single GPU using Huggingface 's [ Wolf et al. , , 2020 ] SciBERT , BERT-base or Roberta-base as the paragraph encoders , and it lasts 6.5 hours using LED-base encoder .", "label": [[42, 52, "Device-Count"], [59, 73, "Software-Entity"]], "Comments": []}
{"id": 166012, "text": "For training the citation span generation model , we use Tesla V100s-PCIE-32GB GPUs .", "label": [[57, 73, "Hardware-device"], [74, 83, "Device-Memory"]], "Comments": []}
{"id": 166014, "text": "The Huggingface models [ Wolf et al. , , 2020 ] we develop upon are released under Apache License 2.0 .", "label": [[4, 15, "Software-Entity"]], "Comments": []}
{"id": 166022, "text": "We pre-train the model on eight V100 GPUs for 100,000 steps with an Adam optimizer and batch size of 32 per GPU in both Stage I and Stage II .", "label": [[26, 31, "Device-Count"], [32, 41, "Hardware-device"]], "Comments": []}
{"id": 166025, "text": "All experiments are conducted on the AWS p3dn.24xlarge instance , consisting of 96 Intenl Xeon CPUs with 768 GB of RAM and 8 Nvidia V100 GPUs with 32 GB of memory each .", "label": [[37, 40, "Cloud-Platform"], [41, 54, "Hardware-device"], [80, 82, "Device-Count"], [83, 99, "Hardware-device"], [105, 118, "Device-Memory"], [123, 124, "Device-Count"], [125, 141, "Hardware-device"], [147, 162, "Device-Memory"]], "Comments": []}
{"id": 166038, "text": "We used four V-100 GPUs with 32 GB memory to train each model .", "label": [[8, 12, "Device-Count"], [13, 23, "Hardware-device"], [29, 41, "Device-Memory"]], "Comments": []}
{"id": 166048, "text": "We modified Wikiextractor [ Attardi , 2015 ] to extract 18 GB size of documents with category labels from the Wikipedia dump [ 5 ] released on March 20 , 2021 .", "label": [[12, 25, "Software-Entity"]], "Comments": []}
{"id": 166050, "text": "We implement our models using Huggingface API [ Wolf et al. , , 2020 ] . < https : //dumps.wikimedia.org > < https : //petscan.wmflabs.org > Table 7 : Hyperparamters for pre-training ConfliBERT using two strategies , pre-training from scratch ( SCR ) and continual pre-training ( Cont ) .", "label": [[30, 41, "Software-Entity"]], "Comments": []}
{"id": 166056, "text": "The algorithm is implemented in PyTorch-1.10 and experiments are conducted on Nvidia RTX-6000 and RTX-A6000 GPU .", "label": [[32, 44, "Software-Entity"], [78, 93, "Hardware-device"], [98, 111, "Hardware-device"]], "Comments": []}
{"id": 166057, "text": "Correcting bias in the optimizer is already fixed by the default optimizer in Huggingface Transformer and training longer surely will lead to further over-fitting in our extreme data scarce scenario .", "label": [[78, 89, "Software-Entity"]], "Comments": []}
{"id": 166064, "text": "For IWSLT'14 De-En , we follow the preprocessing steps provided by fairseq [ 2 ] [ Ott et al. , , 2019 ] to split the data , and process the text into bytepair encoding ( BPE ) [ Sennrich et al. , , 2016 ] .", "label": [[67, 74, "Software-Entity"]], "Comments": []}
{"id": 166068, "text": "4.3 Implementation Details All the algorithms are implemented in Pytorch with fairseq toolkit [ Ott et al. , , 2019 ] , and all the experiments are conducted on a machine with 8 NVIDIA GTX 1080Ti GPUs .", "label": [[65, 72, "Software-Entity"], [78, 85, "Software-Entity"], [176, 177, "Device-Count"], [178, 200, "Hardware-device"]], "Comments": []}
{"id": 166074, "text": "We use FAISS [ Johnson et al. , , 2017 ] for the nearest neighbor search .", "label": [[7, 12, "Software-Entity"]], "Comments": []}
{"id": 166090, "text": "A.2 Hyper-parameters Setting All the algorithms are implemented in Pytorch with fairseq toolkit [ Ott et al. , , 2019 ] , and all the experiments are conducted on a machine with 8 NVIDIA GTX 1080Ti GPUs with the hyperparameters reported in Table [ 6 . ]", "label": [[67, 74, "Software-Entity"], [80, 87, "Software-Entity"], [178, 179, "Device-Count"], [180, 202, "Hardware-device"]], "Comments": []}
{"id": 166091, "text": "Note that during training , we are using the dynamic batching provided by fairseq , and choose the max tokens according to the GPU memory constraint .", "label": [[74, 81, "Software-Entity"]], "Comments": []}
{"id": 166097, "text": "To support future work with the data , we also release an API to download and preprocess it into a format compatible with Fairseq [ Ott et al. , , 2019 ] .", "label": [[122, 129, "Software-Entity"]], "Comments": []}
{"id": 166102, "text": "All models are trained for about 48 hours on V100 GPUs .", "label": [[45, 54, "Hardware-device"]], "Comments": []}
{"id": 166113, "text": "4.1 Experimental Setup Architecture , Input and Hyperparameters We follow the GPT-3 [ Brown et al. , , 2020 ] architecture ( small , medium , large , and XL ) implemented in Fairseq [ Ott et al. , , 2019 ] .", "label": [[174, 181, "Software-Entity"]], "Comments": []}
{"id": 166115, "text": "For each model , we report test perplexity after a single run of 48 hours of training on differing numbers of NVIDIA V100 32GB GPUs ( Table [ 2 ] . 4.2 Models DENSE The first baseline is a dense LM , implemented with distributed data parallel [ Li , 2021 ] .", "label": [[110, 121, "Hardware-device"], [122, 131, "Device-Memory"]], "Comments": []}
{"id": 166120, "text": "We report test perplexity after adapting to each domain for 1 hour with 8 NVIDIA V100 32GB GPUs , tracking validation perplexity every 10 minutes for early stopping .", "label": [[72, 73, "Device-Count"], [74, 85, "Hardware-device"], [86, 95, "Device-Memory"]], "Comments": []}
{"id": 166134, "text": "All models are trained for about 48 hours on V100 GPUs .", "label": [[45, 54, "Hardware-device"]], "Comments": []}
{"id": 166157, "text": "The experiments with RoBERTa-base are carried out on one NVIDIA RTX-A6000 with 48 GB of memory .", "label": [[53, 56, "Device-Count"], [57, 73, "Hardware-device"], [79, 94, "Software-Entity"]], "Comments": []}
{"id": 166158, "text": "Experiments with RoBERTa-large require 4x NVIDIA RTX-8000 ( or RTX-A6000 ) with 192 ( 4x 48 ) GB of momery .", "label": [[39, 40, "Device-Count"], [42, 57, "Hardware-device"], [63, 72, "Hardware-device"], [80, 106, "Device-Memory"]], "Comments": []}
{"id": 166165, "text": "A definition may have one or The model is based on a BERT+LSTM+classifier architecture , and is the default SRL model of the widely used AllenNLP toolkit [ Gardner et al. , , 2017 ] as of August 2021 .", "label": [[137, 145, "Software-Entity"]], "Comments": []}
{"id": 166166, "text": "Each model uses the default hyperparameters from HuggingFace Transformers ( Wolf et al. , 2019 ) without tuning .", "label": [[49, 60, "Software-Entity"]], "Comments": []}
{"id": 166167, "text": "A Modeling Details All our models are implemented upon the Hugging-Face Transformers library , which provides streamlined model sharing functions .", "label": [[59, 71, "Software-Entity"]], "Comments": []}
{"id": 166169, "text": "We run all experiments on NVIDIA V100 GPUs with 16G memory .", "label": [[26, 42, "Hardware-device"], [48, 58, "Device-Memory"]], "Comments": []}
{"id": 166172, "text": "We leveraged Prodigy [ 8 ] for an efficient annotation process . 3.5 Annotation Guidelines A set of comprehensive annotation guidelines was constructed for the annotators to consult when labeling each document to ensure the quality of labels .", "label": [[13, 20, "Software-Entity"]], "Comments": []}
{"id": 166173, "text": "[ 9 ] Table 3 : Mask identifiers in CoDA 3.6 Additional Processing & Text Masking As the Dark Web contains webpages in various languages , we label each of the documents in CoDA with the language of its content using fastText [ Joulin et al. , , 2016a ] [ Joulin et al. , , 2016b ] .", "label": [[217, 225, "Software-Entity"]], "Comments": []}
{"id": 166177, "text": "We aggregate all texts in each category into a single file , lemmatize each word using spaCy [ 13 ] , and use scikit-learn [ 14 ] [ Pedregosa et al. , , 2011 ] to retrieve the word frequency per category .", "label": [[87, 92, "Software-Entity"], [110, 122, "Software-Entity"]], "Comments": []}
{"id": 166178, "text": "Similar methods are used for TF-IDF using scikit-learn [ Pedregosa et al. , , 2011 ] .", "label": [[42, 54, "Software-Entity"]], "Comments": []}
{"id": 166179, "text": "We exclude English stopwords as defined in NLTK [ Bird et al. , 2009 ] , but preserve the mask IDs to capture < https : //spacy.io/ > < https : //scikit-learn.org/stable/ > Word frequency distribution is shown in Figure [ 4 . ] important mask IDs in each category .", "label": [[43, 47, "Software-Entity"]], "Comments": []}
{"id": 166180, "text": "We build our classifier using TfidfVectorizer , LinearSVC , and GridSearchCV classes in scikit-learn .", "label": [[88, 100, "Software-Entity"]], "Comments": []}
{"id": 166181, "text": "Using PyTorch , we build a CNN model with a GloVe embedding layer ( 6B.300d ) , 2D convolution layers , and a fully-connected layer [ Pennington et al. , , 2014 ] .", "label": [[6, 13, "Software-Entity"]], "Comments": []}
{"id": 166183, "text": "We use the pretrained bert-base-uncased model in the PyTorch version of the HuggingFace library [ Wolf et al. , , 2020 ] with a fully-connected classification layer on top of the [ CLS ] token . 5.2 Results Table [ 7 ] summarizes the performance of the three classifiers on CoDA .", "label": [[53, 60, "Software-Entity"], [76, 87, "Software-Entity"]], "Comments": []}
{"id": 166188, "text": "B Experimental Details The data analysis experiments were performed on a machine with Intel Xeon E5-2630 v4 CPU @ 2.2 GHz ( with no GPU usage ) , and the classification experiments were performed on a machine with Intel Xeon Gold 6258R CPU @ 2.70 GHz and Nvidia GeForce RTX 3090 .", "label": [[86, 121, "Hardware-device"], [214, 250, "Hardware-device"], [255, 278, "Hardware-device"]], "Comments": []}
{"id": 166192, "text": "To obtain the universal PoS of words in the dataset , we utilize the PoS tagger in spaCy .", "label": [[83, 88, "Software-Entity"]], "Comments": []}
{"id": 166193, "text": "Following [ Choshen et al. , 2019 ] , we define content words as words tagged by spaCy into to one of the following PoS tags : { ADJ , ADV , NOUN , PROPN , VERB , , NUM } and define all other words as function words .", "label": [[81, 86, "Software-Entity"]], "Comments": []}
{"id": 166194, "text": "D.2 Discussion The use of spaCy for the comparison of PoS distribution and CF ratios may raise questions as spaCy has not been pretrained on Dark Web content , which may yield a higher error rate on the Dark Web results .", "label": [[26, 31, "Software-Entity"], [108, 113, "Software-Entity"]], "Comments": []}
{"id": 166195, "text": "If the result of the analysis is heavily skewed by the presence of Dark Web content and additional training is necessary for the pretrained spaCy model , then this implies that there are meaningful lexical differences in the Dark Web .", "label": [[140, 145, "Software-Entity"]], "Comments": []}
{"id": 166204, "text": "The text generation model was run on a single GPU ( NVIDIA GeForce GTX TITAN X ) .", "label": [[39, 49, "Device-Count"], [52, 78, "Hardware-device"]], "Comments": []}
{"id": 166205, "text": "[ 2 ] C IPW Details C.1 Background The ATE is defined as τ = E [ Yi ( 1 ) −Yi ( 0 ] , where Yi ( t ) is the potential outcome for unit i under We used the implementation from the HuggingFace repository , < https : //huggingface.co/gpt2 > .", "label": [[179, 190, "Software-Entity"]], "Comments": []}
{"id": 166210, "text": "We employ the WMT-19 DE↔EN model from fairseq [ 3 ] [ Ott et al. , ] [ https : //github.com/jasonwei20/eda_nlp ] ( https : //github.com/jasonwei20/eda_nlp ) 3 < https : //github.com/pytorch/fairseq > [ 2019 ] to do the back-translation on unlabeled samples , and exploit them for consistency training .", "label": [[38, 45, "Software-Entity"]], "Comments": []}
{"id": 166214, "text": "ToDKAT [ Zhu et al. , , 2021 ] improves performance by combining commonsense knowledge using COMET and topic discovery using VHRED [ Serban et al. , , 2017 ] to the model .", "label": [[93, 98, "Software-Entity"], [125, 130, "Software-Entity"]], "Comments": []}
{"id": 166219, "text": "Sentiment labels are not provided , but sentiment classes can be grouped as follows : positive : { joyful , peaceful , powerful } , negative : { scared , mad , sad } , neutral : { neutral } 4.2 Training Setup We use the pre-trained model from the huggingface library [ 2 ] .", "label": [[247, 258, "Software-Entity"]], "Comments": []}
{"id": 166220, "text": "All experiments are conducted on one V100 GPU with 32GB memory . 4.3 Previous Method We show that the proposed approach is effective by comparing it with various baselines and the stateof-the-art methods .", "label": [[33, 36, "Device-Count"], [37, 45, "Hardware-device"], [51, 62, "Device-Memory"]], "Comments": []}
{"id": 166236, "text": "Our model was imple [ mented in ] [ Pytor ] ch 1.8 ( Paszke et al. , 2019 ) using the hugging face reimplementation of RoBERTa ( Wolf et al. , 2019 [ a ] [ nd was trained on eigh ] t NVIDIA RTX A4000 GPUs to achieve the best perform [ anc ] [ e. ] Table 1 : Parameters used for training HyperMatch .", "label": [[34, 50, "Software-Entity"], [86, 98, "Software-Entity"], [183, 204, "Hardware-device"]], "Comments": []}
{"id": 166245, "text": "time of each method on a TiTan XP GPU with batch size=8 .", "label": [[25, 37, "Hardware-device"]], "Comments": []}
{"id": 166247, "text": "A Appendix A.1 Implementation Details We implement our method based on the huggingface transformers [ 3 ] .", "label": [[75, 86, "Software-Entity"]], "Comments": []}
{"id": 166263, "text": "Table 3 : Results for different query sampling strategies on the in-domain task 6 Analysis of Results 6.1 Experimental Setup All our models are based on BERTBASE ( Devlin et al. , 2019 ) implemented using Huggingface 's Transformers ( Wolf et al. , 2020 ) and trained using mixed precision .", "label": [[205, 219, "Software-Entity"]], "Comments": []}
{"id": 166265, "text": "All models were trained on either NVIDIA V100 or NVIDIA 3090 GPUs .", "label": [[34, 45, "Hardware-device"], [49, 65, "Hardware-device"]], "Comments": []}
{"id": 166288, "text": "We use the Pytorch framework [ Paszke et al. , 2019 ] and Apex for mixed-precision training .", "label": [[11, 18, "Software-Entity"]], "Comments": []}
{"id": 166319, "text": "We conducted our experiments on 2 Tesla V100 .", "label": [[32, 33, "Device-Count"], [34, 44, "Hardware-device"]], "Comments": []}
{"id": 166363, "text": "All sentences are pre-processed via byte-pair encoding ( BPE ) [ Sennrich et al. , , 2016 ] into sub-word units .", "label": [[36, 62, "Software-Entity"]], "Comments": []}
{"id": 166368, "text": "We compare our approach with a series of previous MT models on applying lexical 4 https : //iate.europa.eu constraints : Implementation Details We use and extend the FairSeq framework [ Ott et al. , , 2019 ] for training our models .", "label": [[166, 173, "Software-Entity"]], "Comments": []}
{"id": 166369, "text": "We keep mostly the default parameters of FairSeq , such as setting dmodel = 512 , dhidden = 2,048 , nheads = 8 , nlayers = 6 and pdropout = 0.3 .", "label": [[41, 48, "Software-Entity"]], "Comments": []}
{"id": 166370, "text": "All models are trained with a batch size of 16,000 tokens for maximum of 300,000 steps with Adam optimizer [ Kingma and Ba , 2014 ] on 2 NVIDIA GeForce RTX 3090 GPUs with gradient accumulation of 4 batches .", "label": [[135, 136, "Device-Count"], [137, 165, "Hardware-device"]], "Comments": []}
{"id": 166373, "text": "As seen in Table [ 9 , ] we categorize and count the constraints into five classes based on their Part-Of-Speech tagging with NLTK [ Bird et al. , , 2009 ] .", "label": [[126, 130, "Software-Entity"]], "Comments": []}
{"id": 166378, "text": "It is worth noting that , since the number of users is large in the datasets , we choose to use the dot-product to compute the similarity of the users so that the whole process can be implemented by dense retrieval libraries , such as Faiss [ Johnson et al. , , 2021 ] , which is very efficient .", "label": [[235, 240, "Software-Entity"]], "Comments": []}
{"id": 166405, "text": "Table 3 : Pre-training time ( w/o evaluation during training ) of IMP and TAMT on a single on a single 32GB Nvidia V100 GPU . `` h '' , `` m '' and `` s '' denote hour , minute and second , respectively .", "label": [[96, 102, "Device-Count"], [103, 107, "Device-Memory"], [108, 123, "Hardware-device"]], "Comments": []}
{"id": 166407, "text": "Training and evaluation are implemented on Nvidia V100 GPU .", "label": [[43, 58, "Hardware-device"]], "Comments": []}
{"id": 166408, "text": "The codes are based on the Pytorch framework [ 3 ] and the huggingface * Transformers * library [ 4 ] [ Wolf et al. , , 2020 ] .", "label": [[27, 34, "Software-Entity"], [59, 70, "Software-Entity"]], "Comments": []}
{"id": 166414, "text": "Concretely , we use the tool PyPinyin [ 2 ] to generate pinyin for each erroneous Chinese character and insert pinyin tokens after the problematic token .", "label": [[29, 37, "Software-Entity"]], "Comments": []}
{"id": 166416, "text": "4.2 Training Details Consistent with previous work [ Leng et al. , , 2021 ] , we use the ESPnet [ Watanabe et al. , , 2018 ] toolkit to train an ASR model on AISHELL-1 training set .", "label": [[89, 95, "Software-Entity"]], "Comments": []}
{"id": 166422, "text": "We also test the inference speed of the correction models on NVIDIA V100 GPU and the test batch size is set to 1 sentence to match the online serving environment . between most words of the source and target is definite .", "label": [[61, 76, "Hardware-device"]], "Comments": []}
{"id": 166424, "text": "Moreover , we want to emphasize that the original FastCorrect used NVIDIA V100 and P40 GPU to test the model delay respectively .", "label": [[67, 78, "Hardware-device"], [83, 90, "Hardware-device"]], "Comments": []}
{"id": 166425, "text": "To ensure the consistency of the results , we reimplement all baseline methods and test the inference speed on NVIDIA V100 GPU .", "label": [[111, 126, "Hardware-device"]], "Comments": []}
{"id": 166434, "text": "Our guidelines reuse the definitions of Hate by [ Ward , 1997 ] and Counter-hate by [ Mathew et al. , 2019 ] and [ Vidgen et al. , 2021 ] : Annotation Process We chose Amazon Mechanical Turk ( MTurk ) as the crowdsourcing platform .", "label": [[168, 200, "Cloud-Platform"]], "Comments": []}
{"id": 166440, "text": "In compliance with Reddit 's policy , we would like to make sure that our dataset will be reused for non-commercial research only . [ 7 ] The Reddit comments in this dataset were annotated by annotators using Amazon Mechanical Turk .", "label": [[209, 231, "Cloud-Platform"]], "Comments": []}
{"id": 166442, "text": "C Hyperparamters to Fine-tune the Systems The neural model takes about half an hour on average to train on a single GPU of NVIDIA TITAN Xp .", "label": [[109, 119, "Device-Count"], [123, 138, "Hardware-device"]], "Comments": []}
{"id": 166455, "text": "B Details of Model Development Training Details for Temporal Adaptation We train GPT2 over each domain and timestamp for k steps using Huggingface 's implementation of GPT2 .", "label": [[135, 149, "Software-Entity"]], "Comments": []}
{"id": 166456, "text": "Training Details for Temporal Finetuning We use Huggingface 's implementation of GPT2 for finetuning for both the classification and summarization tasks .", "label": [[48, 62, "Software-Entity"]], "Comments": []}
{"id": 166457, "text": "We train on Quadro RTX 800 GPUs .", "label": [[12, 31, "Hardware-device"]], "Comments": []}
{"id": 166475, "text": "All RL experiments are run using a single Nvidia A100 GPU for at most 12 hours per 15 , 000 steps .", "label": [[35, 41, "Device-Count"], [42, 57, "Hardware-device"]], "Comments": []}
{"id": 166478, "text": "However , with an ever-changing landscape of transformer models and pre-training techniques ( with over 10000 [ 1 ] different fine-tuned models available on the HuggingFace hub [ Wolf et al. , 2020 ] ) , finding the best model for a given task has become a time-consuming process since , in order to compare multiple models , they each need to be separately fine-tuned on the task .", "label": [[161, 172, "Software-Entity"]], "Comments": []}
{"id": 166487, "text": "For example , a from-scratch training of BERT takes 6.4 days on an 8 GPU Nvidia V100 server [ 3 ] . [ Devlin et al. , 2019 ] recommend finetuning the language model between 2-4 epochs for a given task .", "label": [[67, 68, "Device-Count"], [69, 84, "Hardware-device"]], "Comments": []}
{"id": 166493, "text": "We use the pre-trained transformer models which are available on the HuggingFace [ Wolf et al. , 2020 ] hub .", "label": [[69, 80, "Software-Entity"]], "Comments": []}
{"id": 166495, "text": "4.2 Experimental Setup All of our classification and similarity experiments were run on an Nvidia RTX 2080 GPU .", "label": [[91, 110, "Hardware-device"]], "Comments": []}
{"id": 166502, "text": "We used Hugging Face 's Transformers to build the system and the NVIDIA A100 SXM4 GPU with a GPU memory size of 40 GB .", "label": [[8, 23, "Software-Entity"], [65, 85, "Hardware-device"], [112, 117, "Device-Memory"]], "Comments": []}
{"id": 166508, "text": "4.3.2 Manual Evaluation We use Amazon Mechanical Turk to manually evaluate whether the generated responses are natural and persona-based .", "label": [[31, 53, "Cloud-Platform"]], "Comments": []}
{"id": 166514, "text": "Figure 2 : An example of tasks given to workers on Amazon Mechanical Turk for the manual evaluation .", "label": [[51, 73, "Cloud-Platform"]], "Comments": []}
{"id": 166521, "text": "Because the MM-IMDb dataset is an imbalanced dataset , we assign different ω for different classes . 4 Experiment System Configuration During the training phase , we used a single Nvidia RTX 3090 with a batch size of 12 .", "label": [[173, 179, "Device-Count"], [180, 195, "Hardware-device"]], "Comments": []}
{"id": 166522, "text": "We implemented our model using PyTorch [ Paszke et al. , , 2019 ] and DGL [ Wang et al. , 2020 ] on top of MMBT code available on the public repository . [ 1 ] For every encoder , we used pre-trained models to reduce the computational cost and maximize their performance .", "label": [[31, 38, "Software-Entity"]], "Comments": []}
{"id": 166545, "text": "K Computational Resource usage of this work The person responsible for developing the method took about one year to do so and used a workstation with a single NVIDIA RTX2060 GPU with 6GB of GPU memory to test different approaches .", "label": [[152, 158, "Device-Count"], [159, 177, "Hardware-device"], [183, 186, "Device-Memory"]], "Comments": []}
{"id": 166546, "text": "To conduct the experiments using the final version of our methods , we used our SLURM compute cluster with an array of shared NVIDIA Tesla V100 GPUs .", "label": [[80, 85, "Software-Entity"], [126, 148, "Hardware-device"]], "Comments": []}
{"id": 166555, "text": "Furthermore , we follow [ Karpukhin et al. , 2020 ] to build an offline searchable dense vector index of these embeddings for our SMIkb using the FAISS [ Johnson et al. , , 2017 ] library for faster lookup .", "label": [[146, 151, "Software-Entity"]], "Comments": []}
{"id": 166557, "text": "Thus , we additionally performed human evaluation of the responses with the highest BLEU ( k = 5 ) through Amazon Mechanical Turk , on the following three metrics : * Relevance * , whether the response is relevant to the utterance ; * Engagement * , whether the response makes the conversation engaging ; and * Knowledge * , whether the response seems knowledgeable or sensible .", "label": [[107, 129, "Cloud-Platform"]], "Comments": []}
{"id": 166559, "text": "We use LIME [ Ribeiro et al. , , 2016 ] , a model agnostic explainer that can be used on textual data , to identify attribute tokens .", "label": [[7, 11, "Software-Entity"]], "Comments": []}
{"id": 166560, "text": "Although very effective , using LIME can increase computational time , especially for long text sequences .", "label": [[32, 36, "Software-Entity"]], "Comments": []}
{"id": 166561, "text": "To use LIME to detect attribute words , we first need to train a text classifier f that predicts whether a given text is biased .", "label": [[7, 11, "Software-Entity"]], "Comments": []}
{"id": 166567, "text": "Some statistics of the datasets are given in [ A ] Table [ 3 ] See Section [ 7 ] All experiments are run on a Tesla V100-SXM3 GPU with 32Gb memory . 4.1.1 Jigsaw dataset : The Jigsaw datasets [ 5 ] consists of comments that are labeled by humans with regard to bias towards or against particular demographics .", "label": [[110, 129, "Hardware-device"], [135, 146, "Device-Memory"]], "Comments": []}
{"id": 166580, "text": "Lastly , we con- duct an ablation study to show the effectiveness of the proposed similarity loss and consistent loss . * * 241 * * 5.1 Experiment Setup We use Huggingface [ Wolf et al. , , 2020 ] and Py- torch [ Paszke et al. , , 2019 ] libraries to implement each model .", "label": [[160, 171, "Software-Entity"], [201, 210, "Software-Entity"]], "Comments": []}
{"id": 166581, "text": "We use 4 TX1080 and V100 NVIDIA to train models in 5 epochs with a learning rate of 1e-5 , batch size of 32 .", "label": [[7, 8, "Device-Count"], [9, 15, "Hardware-device"], [20, 31, "Hardware-device"]], "Comments": []}
{"id": 166598, "text": "In this study , we use spaCy [ 3 ] [ Honnibal et al. , , 2020 ] for dependency parsing and prune nodes whose depths are deeper than half of the dependency tree in the extractive summarization step .", "label": [[23, 28, "Software-Entity"]], "Comments": []}
{"id": 166603, "text": "In detail , we use the implementation in the fairseq [ 5 ] [ Ott et al. , , 2019 ] for our experiments .", "label": [[45, 52, "Software-Entity"]], "Comments": []}
{"id": 166616, "text": "Therefore , ExtraPhrase can generate Table 4 : Cost on pseudo data generation using Amazon Elastic Compute Cloud ( Amazon EC2 ) .", "label": [[84, 112, "Cloud-Platform"], [115, 125, "Cloud-Platform"]], "Comments": []}
{"id": 166618, "text": "This table also shows costs when we use Amazon EC2 , which is a cloud computing service , to construct pseudo data .", "label": [[40, 50, "Cloud-Platform"]], "Comments": []}
{"id": 166643, "text": "Table 10 : Estimated per-mention inference times of the selected methods . mGENRE is run on Nvidia GeForce GTX TITAN X , while UIScore and NS were executed on a single 2.5 GHz core of Intel Xeon E5-2680 processor .", "label": [[92, 118, "Hardware-device"], [168, 202, "Hardware-device"]], "Comments": []}
{"id": 166656, "text": "Training infrastructure We trained all models on a single 1080 Ti Nvidia GPU .", "label": [[51, 57, "Device-Count"], [58, 76, "Hardware-device"]], "Comments": []}
{"id": 166658, "text": "Training infrastructure We trained all models on a single 1080 Ti Nvidia GPU .", "label": [[51, 57, "Device-Count"], [58, 76, "Hardware-device"]], "Comments": []}
{"id": 166661, "text": "A.4 BERT unsupervised parsing When using BERT to perform unsupervised parsing , we use the implementation of BERT-base model from the Transformers library [ 11 ] .", "label": [[134, 146, "Software-Entity"]], "Comments": []}
{"id": 166676, "text": "To ensure a faithful comparison with the baseline model , the small architecture of the S2T Transformer in Fairseq [ Wang et al. , , 2020a ] , all our models consist of 12 encoder layers , 6 decoder layers and 4 heads in each attention layer .", "label": [[107, 114, "Software-Entity"]], "Comments": []}
{"id": 166684, "text": "Furthermore , kNN-LM uses * FAISS * , which is an efficient library that allows them to quickly find kNNs .", "label": [[28, 33, "Software-Entity"]], "Comments": []}
{"id": 166685, "text": "We trained the each language model on a Tesla V100 with 40GB of RAM .", "label": [[40, 50, "Hardware-device"], [56, 67, "Device-Memory"]], "Comments": []}
{"id": 166708, "text": "Experiments have been implemented in PyTorch and performed on a single NVIDIA 8GB GPU in Ubuntu 16.04.6 LTS .", "label": [[37, 44, "Software-Entity"], [64, 70, "Device-Count"], [78, 85, "Device-Memory"]], "Comments": []}
{"id": 166718, "text": "The CPU is a 3.1 GHz Quad-Core Intel Core i7 .", "label": [[13, 44, "Hardware-device"]], "Comments": []}
{"id": 166720, "text": "These sentence embeddings are obtained as average of token embeddings from last model layers of the BART models . [ 2 ] We then used the cosine similarity between embeddings to build a fast and efficient retrieval index with efficient FAISS library [ Johnson et al. , , 2019 ] .", "label": [[235, 240, "Software-Entity"]], "Comments": []}
{"id": 166725, "text": "Each utterance was represented by its BART-based embedding and indexed using FAISS library [ Johnson et al. , , 2019 ] . [ 6 ] With FAISS computation cost of updating indexing was kept to bare minimum .", "label": [[77, 82, "Software-Entity"], [132, 137, "Software-Entity"]], "Comments": []}
{"id": 166746, "text": "Leveraging AllenNLP [ Gardner et al. , 2017 ] textual entailment classifier trained on the MNLI [ Williams et al. , , 2018 ] dataset , we calculate the frequency of contradicting turns between the interlocutors , and classify an interlocutor as positive if no contradictions are found , negative if more than 1 contradictions are found , and neutral otherwise .", "label": [[11, 19, "Software-Entity"]], "Comments": []}
{"id": 166748, "text": "6 Experiments and Results 6.1 Experiment Set-up We used the pre-trained 139M parameters ( base ) version of BART [ Lewis et al. , , 2020 ] , and the 400M parameters distilled BlenderBot [ Roller et al. , , 2020 ] from the Huggingface library [ Wolf et al. , , 2020 ] as our base models , and added 24 new tokens comprising of speaker identifiers ( agent_1 , agent_2 ) , traits and intent control codes to the embedding layer .", "label": [[222, 233, "Software-Entity"]], "Comments": []}
{"id": 166750, "text": "We clipped [ Pascanu et al. , , 2013 ] the gradients to unit norm , and used AdamW [ Loshchilov and Hutter , 2019 ] with default Py-Torch parameters for optimization .", "label": [[129, 137, "Software-Entity"]], "Comments": []}
{"id": 166754, "text": "Considering only the BART based models as they resulted in better BLEU and ROUGE scores in table [ 2 , ] we sampled 100 examples from the test sets of each dataset , and utilized Amazon Mechanical Turk for performing human evaluation .", "label": [[179, 201, "Cloud-Platform"]], "Comments": []}
{"id": 166756, "text": "A.4 Amazon Mechanical Turk for Evaluation We leveraged Amazon Mechanical Turk ( AMT ) in order to perform human evaluations on our model Table 10 : Intent Automatic Annotation Evaluation .", "label": [[55, 85, "Cloud-Platform"]], "Comments": []}
{"id": 166762, "text": "We use Amazon Mechanical Turk to collect the annotations , and more details are described in the Supplementary Material .", "label": [[7, 29, "Cloud-Platform"]], "Comments": []}
{"id": 166765, "text": "When training the models , weights of the retriever and the generator are initialized with the pre-trained Bi-encoder 256M and Blender 90M , respectively , For Blender 90M , we use the model released by ParlAI [ Miller et al. , , 2017 ] , which is fine-tuned on the BST+ dataset .", "label": [[203, 209, "Software-Entity"]], "Comments": []}
{"id": 166766, "text": "For Bi-encoder 256M , we fine-tune the model released by ParlAI on the BST+ dataset , and we follow the hyperparameter settings of [ Humeau et al. , 2019 ] , which are implemented in the ParlAI library .", "label": [[187, 193, "Software-Entity"]], "Comments": []}
{"id": 166767, "text": "We use NVIDIA DGX Station A100 for training the models .", "label": [[26, 30, "Hardware-device"]], "Comments": []}
{"id": 166770, "text": "B.1 Pair-wise Human Evaluation As we described in Section 5.3 , we use Amazon Mechanical Turk to collect the annotations .", "label": [[71, 93, "Cloud-Platform"]], "Comments": []}
{"id": 166771, "text": "When calculating BLEU , we use sentence_bleu function in nltk python package [ Loper and Bird , 2002 ] .", "label": [[57, 61, "Software-Entity"]], "Comments": []}
{"id": 166772, "text": "We test our model on NVIDIA DGX Station A100 with PyTorch 1.7.1 , CUDA 11.0 , CuDNN 8.0 , and here we adopt the generation strategy we describe above .", "label": [[40, 44, "Hardware-device"], [50, 63, "Software-Entity"], [66, 75, "Software-Entity"], [78, 87, "Software-Entity"]], "Comments": []}
{"id": 166773, "text": "When we measure the inference time , we only use a single GPU ( NVIDIA A100 GPU , 40GB Memory ) , and the inference time is measured as the average inference time of 100 response generations .", "label": [[51, 61, "Device-Count"], [64, 79, "Hardware-device"], [82, 93, "Device-Memory"]], "Comments": []}
{"id": 166785, "text": "4 Empirical Study 4.1 Experimental Setting We implement all the models following public code from [ Zhang et al. , 2020 ] , based on the HuggingFace Transformers library [ Wolf et al. , ] Table 3 : Statistics of CLINC-Single-Domain-OOS and BANKING77-OOS dataset .", "label": [[137, 148, "Software-Entity"]], "Comments": []}
{"id": 166790, "text": "Experiments were conducted on single NVIDIA Tesla V100 GPU with 32GB memory .", "label": [[30, 36, "Device-Count"], [37, 58, "Hardware-device"], [64, 75, "Device-Memory"]], "Comments": []}
{"id": 166792, "text": "To facilitate end-to-end fine-tuning for this scenario , we collected a large dataset of 4232 dialogues between two people negotiating on goods in a simulated market on Amazon Mechanical Turk ( AMT ) .", "label": [[169, 199, "Cloud-Platform"]], "Comments": []}
{"id": 166801, "text": "4 Experiments 4.1 Training Details We used PyTorch to implement our models , and used the pretrained T5-base model from Hugging Face .", "label": [[43, 50, "Software-Entity"], [120, 132, "Software-Entity"]], "Comments": []}
{"id": 166804, "text": "Supervised finetuning was run on a single RTX 2080Ti GPU .", "label": [[35, 41, "Device-Count"], [42, 56, "Hardware-device"]], "Comments": []}
{"id": 166810, "text": "References A FruitStand Interface Figure 3 : The interface we use for collecting dataset on the Amazon Mechanical Turk .", "label": [[96, 118, "Cloud-Platform"]], "Comments": []}
{"id": 166812, "text": "Dependency parses and labels associated with each dependent word were identified using a pre-trained transformer model from spaCy .", "label": [[124, 129, "Software-Entity"]], "Comments": []}
{"id": 166825, "text": "Each finetuning epoch takes 1.5 hours on a Nvidia V100 GPU .", "label": [[43, 58, "Hardware-device"]], "Comments": []}
{"id": 166826, "text": "GPT2-small was accessed from HuggingFace Transformers library with 125M parameters , context window 1024 , 768-hidden , 768-hidden , 12-heads , dropout = 0.1 .", "label": [[29, 40, "Software-Entity"]], "Comments": []}
{"id": 166827, "text": "Each training epoch took around 0.5 hour on an Nvidia V100 GPU with a batch size of 16 .", "label": [[47, 62, "Hardware-device"]], "Comments": []}
{"id": 166828, "text": "BERT-base-uncased was accessed from HuggingFace Transformers library ( with 12-layer , 768-hidden , 12-heads , 110M parameters , dropout = 0.1 ) .", "label": [[36, 47, "Software-Entity"]], "Comments": []}
{"id": 166829, "text": "Each training epoch took around 1 hour on an Nvidia V100 GPU with a batch size of 10.Validation was done every 0.25 epochs during training . 5 different seeds ( 40-44 ) were set for 5 separate runs .", "label": [[45, 60, "Hardware-device"]], "Comments": []}
{"id": 166840, "text": "We survey four benchmark dataset sharing platforms : HuggingFace , PaperswithCode , Tensorflow , and Pytorch to diagnose the current practices of how the dataset is shared - * which metadata is shared and omitted * .", "label": [[53, 64, "Software-Entity"], [84, 94, "Software-Entity"], [101, 108, "Software-Entity"]], "Comments": []}
{"id": 166841, "text": "HuggingFace , PyTorch , Tensorflow ) .", "label": [[0, 11, "Software-Entity"], [14, 21, "Software-Entity"], [24, 34, "Software-Entity"]], "Comments": []}
{"id": 166843, "text": "This resulted in four main platforms : HuggingFace [ 1 ] , PaperswithCode [ 2 ] , Tensorflow [ 3 ] , and PyTorch [ 4 ] .", "label": [[39, 50, "Software-Entity"], [82, 92, "Software-Entity"], [105, 112, "Software-Entity"]], "Comments": []}
{"id": 166844, "text": "HuggingFace HuggingFace provides an infrastructure so ML researchers can easily leverage models and datasets .", "label": [[12, 23, "Software-Entity"]], "Comments": []}
{"id": 166845, "text": "The idea of Hugging-Face is similar to Github , where the codes and data are shared .", "label": [[12, 24, "Software-Entity"]], "Comments": []}
{"id": 166846, "text": "In HuggingFace , it is the language model trained by different groups of researchers in NLP that is shared .", "label": [[3, 14, "Software-Entity"]], "Comments": []}
{"id": 166847, "text": "Of many language models available in HuggingFace , what made HuggingFace famous is Transformers , which enabled loading thousands of deep learning frameworks ( PyTorch , Tensorflow , JAX ) as well as language models ( e.g. , BERT , RoBERTa , GPT ) with a single line of code .", "label": [[37, 48, "Software-Entity"], [61, 72, "Software-Entity"], [160, 167, "Software-Entity"], [170, 180, "Software-Entity"], [183, 186, "Software-Entity"]], "Comments": []}
{"id": 166851, "text": "Furthermore , the terminology that HuggingFace is using can be misleading .", "label": [[35, 46, "Software-Entity"]], "Comments": []}
{"id": 166852, "text": "HuggingFace uses 'Curators ' to show * '' people involved in collecting the dataset and their affiliation ( s ) '' * [ 5 ] .", "label": [[0, 11, "Software-Entity"]], "Comments": []}
{"id": 166855, "text": "HuggingFace data cards have a section that links the contributors , those who upload the dataset to the platform to write down what the data curation rationale is and how the annotations were made .", "label": [[0, 11, "Software-Entity"]], "Comments": []}
{"id": 166857, "text": "As Tensorflow and PyTorch were focused more on its technical use of the datasets , only the information pertaining to how to practically use the datasets was documented .", "label": [[3, 13, "Software-Entity"], [18, 25, "Software-Entity"]], "Comments": []}
{"id": 166858, "text": "For Tensorflow and PyTorch , as the main focus of these platforms are on redistribution , and enhancing the reusability of the users , the documentation did not include any discussions of the social impacts of the datasets .", "label": [[4, 14, "Software-Entity"], [19, 26, "Software-Entity"]], "Comments": []}
{"id": 166862, "text": "PaperswithCode , Tensorflow , and PyTorch emphasized descriptive and administrative metadata while neglecting the importance of the social impact that the benchmark dataset can bring .", "label": [[17, 27, "Software-Entity"], [34, 41, "Software-Entity"]], "Comments": []}
{"id": 166864, "text": "However , metadata for social impact were absent in PaperswithCode , Tensorflow , and PyTorch .", "label": [[69, 79, "Software-Entity"], [86, 93, "Software-Entity"]], "Comments": []}
{"id": 166888, "text": "4.2 Implementation Details Our model was developed using PyTorch [ Paszke et al. , 2017 ] on top of the implementations released by [ Xin et al. , 2020 ] and [ Zhou et al. , 2020 ] , as well as the HuggingFace Transformers library [ Wolf et al. , 2020 ] .", "label": [[57, 64, "Software-Entity"], [198, 209, "Software-Entity"]], "Comments": []}
{"id": 166889, "text": "Because the focus of this paper was to introduce an alternative architecture of dynamic Transformers and not achieve state of the art results we use the default parameters and architectures from the Transformers library .", "label": [[199, 211, "Software-Entity"]], "Comments": []}
{"id": 166890, "text": "Each experiment was run using a single 11GB NVIDIA graphics accelerator , which allows for training on the complete batch using 32-bit precision and without needing gradient accumulation .", "label": [[32, 38, "Device-Count"], [39, 43, "Device-Memory"]], "Comments": []}
{"id": 166904, "text": "To mimic a typical use-case , we obtained these two pre-trained models from the HuggingFace library [ 2 ] – hence our analysis can be easily extended to any HuggingFace model .", "label": [[80, 91, "Software-Entity"], [157, 168, "Software-Entity"]], "Comments": []}
{"id": 166905, "text": "This version of the LED model utilized the Longformer-chunks implementation that achieves high compute efficiency at the cost of higher memory by chunking the key and query matrices such that only a single matrix multiplication operation from PyTorch is needed .", "label": [[242, 250, "Software-Entity"]], "Comments": []}
{"id": 166906, "text": "The two versions of the model are stored on HuggingFace as allenai/led-base-16384 and allenai/led-large-16384 .", "label": [[44, 55, "Software-Entity"]], "Comments": []}
{"id": 166908, "text": "This model is stored on HuggingFace as google/bigbird-pegasus-large-pubmed .", "label": [[24, 35, "Software-Entity"]], "Comments": []}
{"id": 166909, "text": "We only performed experiments on the large version of this model as the base version is not released on HuggingFace .", "label": [[104, 115, "Software-Entity"]], "Comments": []}
{"id": 166914, "text": "These initial experiments informed our decision to use a fixed resource budget of 1 Nvidia RTX A6000 GPU for both fine-tuning and inference of all models on the summarization tasks , since increasing the number of GPUs does not have a positive effect on the model accuracy .", "label": [[82, 83, "Device-Count"], [84, 104, "Hardware-device"]], "Comments": []}
{"id": 166915, "text": "On the other hand , for the question answering task , we used a much larger fixed resource budget of 8 Nvidia RTX A6000 GPUs ( on the same server ) for both finetuning and inference to allow for larger batch sizes that can obtain much better model accuracy .", "label": [[101, 102, "Device-Count"], [103, 124, "Hardware-device"]], "Comments": []}
{"id": 166917, "text": "To control for the effects of memory on our metrics , for each sequence length and model , we selected the largest batch size that can fit on the 48GB A6000 GPU .", "label": [[146, 150, "Device-Memory"], [151, 160, "Hardware-device"]], "Comments": []}
{"id": 166923, "text": "The training and inference speeds are provided by the HuggingFace library while the total energy consumed and the power efficiency of the GPU ( s ) were collected with the help of the Weights and Biases ( wandb ) tool .", "label": [[54, 65, "Software-Entity"], [184, 202, "Software-Entity"], [205, 210, "Software-Entity"]], "Comments": []}
{"id": 166938, "text": "For any hyperparameters that are not listed in this table , we used the default values provided from the HuggingFace Trainer Library [ 7 ] . and abstract of a paper , models have to be able to generate the answer to a question about the paper .", "label": [[105, 124, "Software-Entity"]], "Comments": []}
{"id": 166939, "text": "B SCROLLS Model Hyperparameters All the experiments conducted in this project were built upon the pre-trained models from the HuggingFace library .", "label": [[126, 137, "Software-Entity"]], "Comments": []}
{"id": 166940, "text": "Unless specified in Table [ 4 , ] hyperparameters take on default values from the HuggingFace Trainer library .", "label": [[82, 101, "Software-Entity"]], "Comments": []}
{"id": 166941, "text": "[ 6 ] As mentioned in Section [ 3.4 , ] we selected the largest batch sizes that can fit on the NVIDIA RTX A6000 GPU ( s ) during fine-tuning for each model and dataset in order to control for the effects of memory on our metrics .", "label": [[95, 116, "Hardware-device"]], "Comments": []}
{"id": 166951, "text": "We provide a package to add these scores to a Scikit-learn [ Pedregosa et al. , , 2011 ] classification report at [ https : //github.com/DFKI-NLP/ ] ( https : //github.com/DFKI-NLP/weighting-schemes-report ) [ weighting-schemes-report ] ( https : //github.com/DFKI-NLP/weighting-schemes-report ) .", "label": [[46, 58, "Software-Entity"]], "Comments": []}
{"id": 166953, "text": "Since the official code of BERTEM is not available , we implement this method using the HuggingFace Transformers library [ Wolf et al. , , 2020 ] and PyTorch [ Paszke et al. , , 2019 ] , and make our code base available at [ https : //github .", "label": [[88, 99, "Software-Entity"], [150, 157, "Software-Entity"]], "Comments": []}
{"id": 166954, "text": "It takes a single RTX-A6000 GPU approximately 10 hours to complete all 5 runs on TACRED .", "label": [[11, 17, "Device-Count"], [18, 31, "Hardware-device"]], "Comments": []}
{"id": 166955, "text": "B.2 BERTEM We use the pre-trained language model ( PLM ) bert-large-uncased from the HuggingFace model hub and directly fine-tune the model for the RC task , without matching-the-blank pre-training .", "label": [[85, 96, "Software-Entity"]], "Comments": []}
{"id": 166957, "text": "It takes a single RTX-A6000 GPU 30 minutes to complete all 5 runs on SemEval .", "label": [[11, 17, "Device-Count"], [18, 31, "Hardware-device"]], "Comments": []}
{"id": 166959, "text": "It takes 4 Quadro-P5000 GPUs 84 hours to complete 5 runs on TACRED , and it takes 8 Titan-V GPUs 9 hours on SemEval .", "label": [[82, 83, "Device-Count"], [84, 96, "Hardware-device"]], "Comments": []}
{"id": 166965, "text": "Each job uses a single RTX3090 GPU ( Ampere ) with 24GB of shared memory and 8 CPUs .", "label": [[16, 22, "Device-Count"], [23, 34, "Hardware-device"], [51, 55, "Device-Memory"], [77, 83, "Device-Count"]], "Comments": []}
{"id": 166966, "text": "All experiments are built using the machine learning framework * PyTorch * at version 1.9.0 and NVIDIA 's graphics programming interface * CUDA * with toolkit version 11.1 .", "label": [[65, 72, "Software-Entity"], [139, 143, "Software-Entity"]], "Comments": []}
{"id": 166968, "text": "It is implemented in Python and builds upon PyTorch 's [ Paszke et al. , , 2019 ] .", "label": [[44, 54, "Software-Entity"]], "Comments": []}
{"id": 166969, "text": "Therefore , several generic embedding models are already available within the model.kge package , which extend PyTorch 's default nn.Module by integrating commonly required methods in knowledge representation learning .", "label": [[111, 121, "Software-Entity"]], "Comments": []}
{"id": 166981, "text": "We implemented our model in PyTorch [ Paszke et al. , 2019 ] .", "label": [[28, 35, "Software-Entity"]], "Comments": []}
{"id": 166982, "text": "All experiments were conducted on one Nvidia Tesla V100 GPU with 32GB VRAM .", "label": [[38, 59, "Hardware-device"], [65, 74, "Device-Memory"]], "Comments": []}
{"id": 166987, "text": "The autoencoder is implemented in Python 3.x via Tensorflow 2.x , consisting of an encoder with two hidden layers and a decoder with a single hidden layer .", "label": [[49, 59, "Software-Entity"]], "Comments": []}
{"id": 166988, "text": "The autoencoder model used to produce the encoded vectors was fitted to pretrained word vectors from * FastText * [ Bojanowski et al. , , 2016 ] ( Common Crawl , 600B tokens ) [ 2 ] .", "label": [[103, 111, "Software-Entity"]], "Comments": []}
{"id": 166990, "text": "Calculations were performed on a machine with an Intel i7-8565U CPU and 8GB RAM .", "label": [[49, 67, "Hardware-device"], [72, 79, "Device-Memory"]], "Comments": []}
{"id": 167001, "text": "Fine-tuning and evaluation was performed on a Tesla V100 GPU .", "label": [[46, 60, "Hardware-device"]], "Comments": []}
{"id": 167013, "text": "We used GeForce GTX 1080 GPU .", "label": [[8, 28, "Hardware-device"]], "Comments": []}
{"id": 167025, "text": "The SGNS embeddings were trained using our custom implementation . [ 6 ] The PMI and BPMI matrices were extracted using the HYPERWORDS tool of [ Levy et al. , 2015 ] and SVD was performed using the PYTORCH library of [ Paszke et al. , 2019 ] .", "label": [[124, 134, "Software-Entity"], [198, 205, "Software-Entity"]], "Comments": []}
{"id": 167033, "text": "Framework implementation We implement our low-resource NER encoder evaluation framework using the HuggingFace Transformers li- brary [ Wolf et al. , , 2020 ] , Hydra [ Yadan , 2019 ] , and PyTorch [ Paszke et al. , , 2019 ] .", "label": [[98, 109, "Software-Entity"], [160, 165, "Software-Entity"], [189, 196, "Software-Entity"]], "Comments": []}
{"id": 167037, "text": "A Additional Training Details We used a single RTXA6000-GPU for all experiments .", "label": [[40, 46, "Device-Count"], [47, 59, "Hardware-device"]], "Comments": []}
{"id": 167038, "text": "Constrastive pre-training was also performed on the same single RTXA6000-GPU , and took approximately 1 hour of GPU-time , including hyperparameter search .", "label": [[57, 63, "Device-Count"], [64, 76, "Hardware-device"]], "Comments": []}
{"id": 167040, "text": "All pre-trained models evaluated in this study were used as they are available from Hugging-Face 's model hub , without any modifications .", "label": [[84, 99, "Software-Entity"]], "Comments": []}
{"id": 167041, "text": "We used Hugging-Face 's dataset hub for all datasets except the dataset by [ Zhang et al. , 2020 ] , which is used here with the permission of the authors .", "label": [[8, 23, "Software-Entity"]], "Comments": []}
{"id": 167100, "text": "Additionally , training the multitude of large-scale language models presented in this paper required the use of 32 Cloud TPU v3 cores for several hundred hours , which has a significant environmental impact [ Strubell et al . ] , [ 2019 ] ) .", "label": [[113, 115, "Device-Count"], [116, 134, "Hardware-device"]], "Comments": []}
{"id": 167122, "text": "We use a standard Wikipedia extractor wikiextractor by Attardi ( 2015 ) and a redirect extractor.10 We use both Wikipedia and Wikidata dumps from 2019-10-01 .", "label": [[38, 51, "Software-Entity"]], "Comments": []}
{"id": 167126, "text": "We implemented , trained , and evaluated our model A.4 Training using the fariseq library ( Ott et al. , 2019 ) .", "label": [[74, 81, "Software-Entity"]], "Comments": []}
{"id": 167129, "text": "Training was done on 384 GPUs ( Tesla V100 with 32GB of memory ) and it completed in ≈72h for a total of ≈27,648 GPU hours or ≈1,152 GPU days .", "label": [[21, 29, "Device-Count"], [32, 42, "Hardware-device"], [48, 62, "Device-Memory"]], "Comments": []}
{"id": 167146, "text": "The experiments are conducted on 1080Ti GPUs . 4.4 Main Results Following Wei and Zou ( 2019 ) , for each classifi- cation dataset , we randomly sample x percentage of training data for model training .", "label": [[33, 44, "Hardware-device"]], "Comments": []}
{"id": 167159, "text": "The VILA structures in the three component datasets are curated differently , which helps to evaluate the generality of VILA-based methods . 5 Experimental Setup 5.1 Implementation Details Our models are implemented using PyTorch ( Paszke et al. , 2019 ) and the transformers library ( Wolf et al. , 2020 ) .", "label": [[222, 229, "Software-Entity"], [263, 275, "Software-Entity"]], "Comments": []}
{"id": 167160, "text": "A series of baseline and VILA models are fine-tuned on 4-GPU RTX8000 or A100 machines .", "label": [[55, 56, "Device-Count"], [57, 68, "Hardware-device"], [72, 76, "Hardware-device"]], "Comments": []}
{"id": 167161, "text": "As for S2-VL , given its smaller size , we use 5-fold cross validation and report averaged scores , and weights available in the transformers library .", "label": [[129, 141, "Software-Entity"]], "Comments": []}
{"id": 167163, "text": "All models are tested on an isolated machine with a single V100 GPU .", "label": [[52, 58, "Device-Count"], [59, 67, "Hardware-device"]], "Comments": []}
{"id": 167164, "text": "This Positional embeddings are not used in these models . [ 11It takes 10.5 h ] ours to finish fine-tuning I-VILA on the GROTOAP2 dataset using a 4 RTX 8000 machine , equivalent to around 60 V100 GPU hours , approximately 5 % of the 1280 hours of the pretraining time for LayoutLM .", "label": [[146, 147, "Device-Count"], [148, 156, "Hardware-device"], [188, 190, "Device-Count"], [191, 199, "Hardware-device"]], "Comments": []}
{"id": 167184, "text": "We follow Cui et al . ( 2020 ) to use a sentencepiece vocabulary of 32,000 wordpieces ( Kudo and Richardson , 2018 ) .", "label": [[40, 53, "Software-Entity"], [75, 85, "Software-Entity"]], "Comments": []}
{"id": 167185, "text": "We pretrained LongLM for 2.5M steps . using eight NVIDIA V100 GPUs .", "label": [[44, 49, "Device-Count"], [50, 66, "Hardware-device"]], "Comments": []}
{"id": 167200, "text": "We used Wikiextractor to extract the text .", "label": [[8, 21, "Software-Entity"]], "Comments": []}
{"id": 167211, "text": "DPR Retriever is trained on four 40GB A100 GPUs , whereas DPR Reader and FiD are trained on 8 32GB V100 GPUs .", "label": [[28, 32, "Device-Count"], [33, 37, "Device-Memory"], [38, 47, "Hardware-device"], [92, 93, "Device-Count"], [94, 98, "Device-Memory"], [99, 108, "Hardware-device"]], "Comments": []}
{"id": 167250, "text": "In parti [ cular , ] [ they ] have extremely high search latency : Processing a single text query with an ima [ ge ] [ collection ] [ of ] [ 1M ] [ items ] [ may ] [ take ] [ up ] [ to ] [ 36 ] [ minutes ] [ u ] s [ ing ] [ a ] single NVIDIA V100 GPU ( see Table 3 ) .", "label": [[228, 234, "Device-Count"], [235, 250, "Hardware-device"]], "Comments": []}
{"id": 167278, "text": "Type Model en de [ fr ] cs mean Model NVIDIA V100 CPU tups ) .", "label": [[38, 49, "Hardware-device"]], "Comments": []}
{"id": 167280, "text": "CPU is an Intel Xeon Gold 6154 .", "label": [[10, 30, "Hardware-device"]], "Comments": []}
{"id": 167283, "text": "The time includes biencoding images and text , i.e. , the embeddings are not pre-computed . * denotes extrapolated values . in PyTorch without additional optimization such as multi-processing or optimized nearest-neighbor search libraries like FAISS ( Johnson et al. , 2021 ) .", "label": [[127, 134, "Software-Entity"], [244, 249, "Software-Entity"]], "Comments": []}
{"id": 167367, "text": "The proposed * [ Surrogate ] * model is based on [ PyTorch . ] [ InferSent ] [ Conneau ] et al. , 2017 ) and SBERT ( Reimers and Gurevych , 2019 ) are based on PyTorch .", "label": [[51, 58, "Software-Entity"], [160, 167, "Software-Entity"]], "Comments": []}
{"id": 167368, "text": "Universal Sent Encoder ( Cer et al. , 2018 ) is based on Tensorflow and the model is from the Tensorflo [ w ] [ Hub . ] [ Model ] [ e ] f [ ficien ] cy is measure [ d ] [ on ] [ a ] [ server ] [ with ] [ Inte ] l [ i7-58 ] [ 20K CPU ] [ @ 3 . ] 30GHz , Nvidia Tesla V100 GPU , CUDA 10.2 , and cuDNN .", "label": [[57, 67, "Software-Entity"], [94, 103, "Software-Entity"], [253, 274, "Hardware-device"], [277, 286, "Software-Entity"], [293, 298, "Software-Entity"]], "Comments": []}
{"id": 167387, "text": "We used four GeForce RTX 2080 GPUs for training ; we set the batch size to 8 ( i.e. , one [ samp ] le for each GPU ) , and accumul [ ate ] [ gradients ] every 32 steps .", "label": [[8, 12, "Device-Count"], [13, 34, "Hardware-device"]], "Comments": []}
{"id": 167400, "text": "8 Human Evaluation Following previous work ( Xu and Lapata , 2021 , 2020 ) , we also evaluated query-focused summaries in a judgment elicitation study via Amazon Mechanical Turk .", "label": [[155, 177, "Cloud-Platform"]], "Comments": []}
{"id": 167409, "text": "( We use PyTorch ; implementation details are discussed below . )", "label": [[9, 16, "Software-Entity"]], "Comments": []}
{"id": 167414, "text": "We considered normalized frequencies as BoW features . 6.2 Experimental Setup Table 2 describes all the methods that we compared in the empirical studies . 6The dataset only includes papers that are not cross listed Python 's sklearn package with default settings .", "label": [[226, 233, "Software-Entity"]], "Comments": []}
{"id": 167415, "text": "We implemented the BERT method with the transformers Python library.7 The BERT method uses the raw text as input , with a maximum token length of 128 .", "label": [[40, 52, "Software-Entity"]], "Comments": []}
{"id": 167417, "text": "STMs , sLDA , and HSTM were all implemented with PyTorch .", "label": [[49, 56, "Software-Entity"]], "Comments": []}
{"id": 167419, "text": "For stochastic optimization with Adam , we use automatic differentiation in PyTorch .", "label": [[76, 83, "Software-Entity"]], "Comments": []}
{"id": 167421, "text": "These methods and BERT were trained on Titan GPUs .", "label": [[39, 49, "Hardware-device"]], "Comments": []}
{"id": 167446, "text": "Retrieval/Inference The bi-encoder design enables DPR to perform an approximate nearest neighbour search ( ANN ) using tools like FAISS [ Johnson et al. , , 2021 ] , where the representations of the corpus passages are indexed offline .", "label": [[130, 135, "Software-Entity"]], "Comments": []}
{"id": 167459, "text": "To identify examples of pragmatic inference patterns involving the connective * and * , we selected instances where the premise or the hypothesis contains two main clauses linked by * and * using the SpaCy dependency parser [ Honnibal and Montani , 2017 ] .", "label": [[200, 205, "Software-Entity"]], "Comments": []}
{"id": 167461, "text": "For our experiments , we used the SpaCy pipeline en_core_web_s from the most recent version 3.2 .", "label": [[34, 39, "Software-Entity"]], "Comments": []}
{"id": 167473, "text": "A PyTorch [ Paszke et al. , , 2019 ] implementation of BERTBASE ( 110M parameters ) pre-trained on Japanese Wikipedia from HuggingFace [ Wolf et al. , 2020 ] [ 2 ] was fine-tuned on WRIME [ Kajiwara et al. , 2021 ] .", "label": [[2, 9, "Software-Entity"], [123, 134, "Software-Entity"]], "Comments": []}
{"id": 167474, "text": "Training was conducted with an NVIDIA Tesla K80 and finished in 3 hours .", "label": [[31, 47, "Hardware-device"]], "Comments": []}
{"id": 167476, "text": "The second model we compared to uses fastText [ Bojanowski et al. , , 2017 ] and an SVM ( fastText+SVM ) .", "label": [[37, 45, "Software-Entity"]], "Comments": []}
{"id": 167487, "text": "* * 4.4 Model Implementation * * We performed hyperparameter tuning , pretraining , fine-tuning and evaluation for our model on Google TPU using TensorFlow 2.5.0 with Python3 .", "label": [[128, 138, "Hardware-device"], [145, 161, "Software-Entity"]], "Comments": []}
{"id": 167490, "text": "Table 4 : Mapping quality between MWE and F-MWE using N-BL We also investigated the quality of T-BL by translating the English-side words in T-BL to Malay using Google Cloud Translation API , resulting in a new set of bilingual word pairs ( * * N-BL * * ) .", "label": [[161, 188, "Cloud-Platform"]], "Comments": []}
{"id": 167491, "text": "However , realising some of the English NN were noise , we filtered out those not in Words Corpus by Natural Language Processing Toolkit ( NLTK ) .", "label": [[139, 143, "Software-Entity"]], "Comments": []}
{"id": 167492, "text": "The remaining neighbours were then translated to Malay using Google Cloud Translation API .", "label": [[61, 89, "Cloud-Platform"]], "Comments": []}
{"id": 167507, "text": "We also used models , based on multiple different transformer architec- Table 1 : Models used in experiments - inference times , number of parameters , and languages used in pre-training , base model and data used in pre-training * mUSE models were used in TensorFlow implementation in contrast to others in torch Base model is either monolingual version on which it was based or another multilingual model which was used and adopted Colossal Clean Crawled Corpus in multilingual version ( mC4 ) multiple datasets from OPUS website ( https : //opus.nlpl.eu ) , bilingual dictionaries from MUSE ( https : //github.com/facebookresearch/MUSE ) , just titles from wiki articles in multiple languages tures ( T5 , BERT , RoBERTa ) .", "label": [[257, 267, "Software-Entity"]], "Comments": []}
{"id": 167508, "text": "We searched for datasets in various sources , like Google Scholar , GitHub repositories , and the HuggingFace datasets library .", "label": [[98, 109, "Software-Entity"]], "Comments": []}
{"id": 167513, "text": "This last scenario evaluated the fine-tuning of all transformer-based models ( referred to as fine-tuning ) , with an exception made for mUSE-transformer because it was not possible to do with our implementation in Py-Torch with Huggingface models .", "label": [[215, 223, "Software-Entity"], [229, 240, "Software-Entity"]], "Comments": []}
{"id": 167520, "text": "Due to computation resources limitations , we selected eight models available in Huggingface for finetuning .", "label": [[81, 92, "Software-Entity"]], "Comments": []}
{"id": 167523, "text": "A Appendices A.1 Hardware and Software We performed our experiments using Python 3.9 and PyTorch ( 1.8.1 ) ( and Tensorflow ( 2.3.0 ) for original mUSE ) .", "label": [[89, 106, "Software-Entity"], [113, 133, "Software-Entity"]], "Comments": []}
{"id": 167524, "text": "Our experimental setup consists of Intel ( R ) Xeon ( R ) CPU E5-2630 v4 @ 2.20GHz and Nvidia Tesla V100 16GB .", "label": [[35, 82, "Hardware-device"], [87, 104, "Hardware-device"], [105, 109, "Device-Memory"]], "Comments": []}
{"id": 167549, "text": "Figure 3 : t-SNE plots of the learned embeddings on Dev and Test sets of EmoMoham .", "label": [[11, 16, "Software-Entity"]], "Comments": []}
{"id": 167552, "text": "The pre-training and surrogate fine-tuning models are trained on eight Nvidia V100 GPUs ( 32G each ) .", "label": [[65, 70, "Device-Count"], [71, 87, "Hardware-device"], [90, 93, "Device-Memory"]], "Comments": []}
{"id": 167553, "text": "All the models are implemented by Huggingface Transformers [ Wolf et al. , , 2020 ] .", "label": [[34, 45, "Software-Entity"]], "Comments": []}
{"id": 167556, "text": "All downstream task models are fine-tuned on four Nvidia V100 GPUs ( 32G each ) .", "label": [[45, 49, "Device-Count"], [50, 66, "Hardware-device"], [69, 72, "Device-Memory"]], "Comments": []}
{"id": 167567, "text": "[ Respectively , as language-specific competi ] [ tors ( LS-EMO ) , we use the FEEL-IT ( Bianchi ] [ et al. , ] [ 2021a ) as found on HuggingFace ] [ 9 ] and EmoNet [ Abdul-Mageed and Ungar , 2017 ] as found on GitHub [ 10 ] [ .", "label": [[79, 86, "Software-Entity"], [134, 145, "Software-Entity"], [158, 164, "Software-Entity"]], "Comments": []}
{"id": 167571, "text": "Models are trained on a Nvidia GeForce RTX 2080 Ti .", "label": [[24, 50, "Hardware-device"]], "Comments": []}
{"id": 167581, "text": "The author obtains their gold scores by averaging the submissions from different participants . 3.2 Implementation Details We train the model using the Pytorch [ 2 ] [ Paszke et al. , 2019 ] on the NVIDIA A100 GPU and use the hugging-face [ 3 ] [ Wolf et al. , , 2020 ] framework .", "label": [[152, 159, "Software-Entity"], [198, 213, "Hardware-device"], [226, 238, "Software-Entity"]], "Comments": []}
{"id": 167594, "text": "In our experiments , we used the DeBERTa Transformer [ He et al. , , 2021 ] , specifically the * microsoft/debertaxlarge-mnli * model from Hugging Face .", "label": [[139, 151, "Software-Entity"]], "Comments": []}
{"id": 167597, "text": "We employed the OpenAI Davinci 's model as it is the most capable one , often with less context .", "label": [[16, 22, "Software-Entity"]], "Comments": []}
{"id": 167598, "text": "In case of disagreement or tie , we selected the emotion given by the supervised model . 4 Experimental Setup All the transformer based models have been finetuned on a single NVIDIA Ampere A100 GPU by making use of the Hugging Face 's transformer library [ Wolf et al. , , 2019 ] .", "label": [[168, 174, "Device-Count"], [175, 197, "Hardware-device"], [219, 234, "Software-Entity"]], "Comments": []}
{"id": 167606, "text": "Stopwords were removed using the nltk [ Loper and Bird , 2002 ] library .", "label": [[33, 37, "Software-Entity"]], "Comments": []}
{"id": 167607, "text": "4.2 Training Setup We used the pretrained 'roberta-base ' model from the Huggingface Transformers [ 1 ] library .", "label": [[73, 84, "Software-Entity"]], "Comments": []}
{"id": 167608, "text": "All other modules used in our methodology were built using PyTorch .", "label": [[59, 66, "Software-Entity"]], "Comments": []}
{"id": 167610, "text": "A single Nvidia P100-16GB GPU provided by Google Colab was used to train all models . 4.3 Baselines Our goal in this work is to examine if concatenating emotional-specific features to pre-existing transformer models leads to an increase in the emotion classification performance of these models .", "label": [[2, 8, "Device-Count"], [9, 29, "Hardware-device"]], "Comments": []}
{"id": 167621, "text": "batch size = 16 ( for maximum utilization of the GPU ) imported from the Tensorflow Hub ( [ https : //www .", "label": [[73, 83, "Software-Entity"]], "Comments": []}
{"id": 167627, "text": "We used Google translate API for translating essays back and forth .", "label": [[8, 28, "Cloud-Platform"]], "Comments": []}
{"id": 167629, "text": "The input tokens were made using the Roberta Tokenizer imported from the transformer library .", "label": [[73, 84, "Software-Entity"]], "Comments": []}
{"id": 167630, "text": "The dataset was shuffled using Pytorch [ Paszke et al. , , 2019 ] data loader .", "label": [[31, 38, "Software-Entity"]], "Comments": []}
{"id": 167639, "text": "The provided dataset of WASSA shared work contains 1860 training samples . 3.2 Implementation Details In these tasks , we are mainly based on the hugging face framework [ 2 ] [ Wolf et al. , , 2020 ] .", "label": [[146, 158, "Software-Entity"]], "Comments": []}
{"id": 167641, "text": "We implemented the code of training and reasoning based on PyTorch [ 3 ] [ Paszke et al. , , 2019 ] in three NVIDIA A100 GPUs .", "label": [[59, 66, "Software-Entity"], [103, 108, "Device-Count"], [109, 125, "Hardware-device"]], "Comments": []}
{"id": 167647, "text": "The weights of each class were determined using the sklearn library . [ 1 ] 3 Baselines The following section describes different approaches we tried before shifting to our proposed methodology .", "label": [[52, 59, "Software-Entity"]], "Comments": []}
{"id": 167655, "text": "The training was done on Nvidia RTX 2080 TI ( 11 GB ) and took about one hour for each model finetuning . 6 Results Table [ 4 ] shows the results of our dev set .", "label": [[25, 43, "Hardware-device"], [46, 51, "Device-Memory"]], "Comments": []}
{"id": 167657, "text": "4.2 Preprocessing When preprocessing text inputs , we use spaCy [ 4 ] for tokenization , filter punctuation tokens , replace hyperlinks with < URL > and separate posts in a thread with < SEP > .", "label": [[58, 63, "Software-Entity"]], "Comments": []}
{"id": 167658, "text": "The numeric features are scaled to the [ 0 , 1 ] interval using scikitlearn 's [ 5 ] MinMaxScaler .", "label": [[64, 78, "Software-Entity"]], "Comments": []}
{"id": 167660, "text": "All experiments were ran on four Nvidia GTX 1080 graphics cards . [ https : //developer.twitter.com/en/ ] ( https : //developer.twitter.com/en/docs/twitter-api ) [ docs/twitter-api ] ( https : //developer.twitter.com/en/docs/twitter-api )", "label": [[28, 32, "Device-Count"], [33, 48, "Hardware-device"]], "Comments": []}
{"id": 167673, "text": "We use T5 [ Raffel et al. , , 2020 ] to paraphrase each item , based on the * T5ForConditionalGeneration * model provided by HuggingFace [ 5 ] .", "label": [[125, 136, "Software-Entity"]], "Comments": []}
{"id": 167675, "text": "] ( https : //huggingface.co/transformers/model_doc/t5.htmlt5forconditionalgeneration ) [ htmlt5forconditionalgeneration ] ( https : //huggingface.co/transformers/model_doc/t5.htmlt5forconditionalgeneration ) < https : //github.com/minimaxir/gpt-2-simple > Seed set to 42 via PyTorchLighting * seed_everything * , learning rate 10− , batch size of 16 , optimization with Adam [ Kingma and Ba , 2014 ] .", "label": [[276, 291, "Software-Entity"]], "Comments": []}
{"id": 167679, "text": "4.3 Model Introspection To provide some insights on the decision process by the classification models , we provide one example for each personality trait from the Tweet corpus with LIME explanations [ Ribeiro et al. , , 2016 ] in Table [ 3 . ]", "label": [[181, 185, "Software-Entity"]], "Comments": []}
{"id": 167683, "text": "A.2 Implementation Details We performed the experiments on 4 NVIDIA GeForce GTX 1080 Ti GPUs with Intel ( R ) Xeon ( R ) CPU E5-2650 v4 @ 2.20GH .", "label": [[59, 60, "Device-Count"], [61, 92, "Hardware-device"], [98, 144, "Hardware-device"]], "Comments": []}
{"id": 167687, "text": "After the creation of the frameworks , job students were hired to annotate the data using the INCEpTION annotation tool .", "label": [[94, 103, "Software-Entity"]], "Comments": []}
{"id": 167691, "text": "For this feature-based approach , we used a combination of token-shape features , linguistic information extracted via the LeTs pre-processing toolkit [ Van de Kauter et al. , , 2013 ] and dependency parsing information obtained from the Dutch dependency parser implemented within the open-source Spacy toolkit [ 6 ] .", "label": [[123, 127, "Software-Entity"], [297, 302, "Software-Entity"]], "Comments": []}
{"id": 167692, "text": "To this purpose we again relied on a supervised machine learning model , namely a Support Vector Machine , using the algorithm as implemented in Scikit Learn 's C-Support Vector Classification [ 7 ] , which is based on LibSVM [ Chang and Lin , 2011 ] .", "label": [[145, 160, "Software-Entity"], [219, 225, "Software-Entity"]], "Comments": []}
{"id": 167693, "text": "We implemented a combination of lexico-semantic features and Word2Vec embeddings on the training data using Gensim ( Reh˚u ˇ [ ˇrek and Sojka , 2010 ] .", "label": [[108, 114, "Software-Entity"]], "Comments": []}
{"id": 167698, "text": "The SentEMO Front Office is built with Docker containers [ 13 ] ( as shown by Figure [ 10 ] : a custom Node.js [ 14 ] application container , a PostgreSQL relational database container , and a RabbitMQ message queue container .", "label": [[39, 45, "Software-Entity"]], "Comments": []}
{"id": 167717, "text": "GRU was accessed from PyTorch , with K set to 33 and a hidden dimension of 33 .", "label": [[22, 29, "Software-Entity"]], "Comments": []}
{"id": 167718, "text": "RoBERTa RoBERTa-base-uncased with 12 layer , 768-hidden ( KR ) , 12-heads , 110M parameters , 0.1 dropout was used , accessed from HuggingFace Transformers library [ Wolf et al. , , 2020 ] .", "label": [[131, 142, "Software-Entity"]], "Comments": []}
{"id": 167723, "text": "Training takes 20 minutes on an Nvidia P100 GPU .", "label": [[32, 47, "Hardware-device"]], "Comments": []}
{"id": 167726, "text": "Training took 18 hours on a Nvidia P100 GPU .", "label": [[28, 43, "Hardware-device"]], "Comments": []}
{"id": 167728, "text": "Training took 15 hours on a Nvidia P100 GPU .", "label": [[28, 43, "Hardware-device"]], "Comments": []}
{"id": 167731, "text": "Training took 1 hour on a Nvidia P100 GPU .", "label": [[26, 41, "Hardware-device"]], "Comments": []}
{"id": 167732, "text": "A.4 Sequentiality experimental details GPT2-small was accessed from HuggingFace Transformers library and used without further finetuning .", "label": [[68, 79, "Software-Entity"]], "Comments": []}
{"id": 167735, "text": "Training each model took around 30 minutes on an Nvidia P100 GPU .", "label": [[49, 64, "Hardware-device"]], "Comments": []}
{"id": 167738, "text": "Training took 4 hours on a Nvidia P100 GPU .", "label": [[27, 42, "Hardware-device"]], "Comments": []}
{"id": 167748, "text": "We use scikit-learn to create our classification pipeline .", "label": [[7, 19, "Software-Entity"]], "Comments": []}
{"id": 167755, "text": "We identify All BERT experiments were performed on Google Colab with Tesla V100-SXM2-16GB GPU , and we use BERTForSequenceClassification from huggingface for our implementation .", "label": [[69, 84, "Hardware-device"], [85, 93, "Device-Memory"], [142, 153, "Software-Entity"]], "Comments": []}
{"id": 167762, "text": "For hateful cases , XTC performs best in terms of F1 score on Portuguese ( 83.5 ) and worst on Polish ( 76.1 ) , but performance differences are We use the XLM-T implementation hosted on Hugging-Face : [ huggingface.co/cardiffnlp/twitter-xlm-roberta-base .", "label": [[187, 199, "Software-Entity"]], "Comments": []}
{"id": 167766, "text": "Annotators were recruited on Appen , a crowdworking provider .", "label": [[29, 34, "Cloud-Platform"]], "Comments": []}
{"id": 167771, "text": "G XLM-T Model Details Model Architecture We implemented XLM-T model [ Barbieri et al. , , 2021 ] using the transformers Python library [ Wolf et al. , , 2020 ] .", "label": [[107, 119, "Software-Entity"]], "Comments": []}
{"id": 167773, "text": "Computation We ran all computations on an AWS `` g4dn.2xlarge '' server equipped with one NVIDIA T4 GPU card .", "label": [[42, 45, "Cloud-Platform"], [49, 61, "Hardware-device"], [90, 103, "Hardware-device"]], "Comments": []}
{"id": 167775, "text": "With each user at most requiring server usage of one NVIDIA GeForce RTX 2080 , with reference to existing online benchmarks [ Ignatov , 2021 ] the latency for 1 image ( CNN ) and text ( LSTM ) model would be 5.1ms and 4.8ms respectively .", "label": [[53, 76, "Hardware-device"]], "Comments": []}
{"id": 167779, "text": "To reduce redundancies in the KG triples , we cluster the mined sentences with similar content together using the fast clustering method for community detection implemented in the SentenceTransformers [ 6 ] [ Reimers and ] [ Gurevych , 2019 ] library .", "label": [[180, 200, "Software-Entity"]], "Comments": []}
{"id": 167782, "text": "We assess inter-annotator agreement using the average observed agreement ( OA ) as calculated using the NLTK agreement [ 10 ] function , which does not penalize repeated entries of a single value [ 11 ] unlike other common metrics ( e.g .", "label": [[104, 108, "Software-Entity"]], "Comments": []}
{"id": 167787, "text": "Models For the knowledge integration experiments , we use the sequence classification pipeline in the simpletransformers [ 12 ] library .", "label": [[102, 120, "Software-Entity"]], "Comments": []}
{"id": 167789, "text": "Environmental Impact Our models are trained on Titan X GPUs with 12GB RAM .", "label": [[47, 59, "Hardware-device"], [65, 73, "Device-Memory"]], "Comments": []}
{"id": 165988, "text": "We used speculative constrained decoding with a width of 10 and a temperature of 0.5 .", "label": [], "Comments": []}
{"id": 167901, "text": "However , the absolute correlation values are still larger than 0.7 .", "label": [], "Comments": []}
{"id": 167902, "text": "As shown in Figure [ 4 , ] we can get log g n i by applying convolution on log P , with yi : i+ as the kernels : where Onehot ( · ) maps each token to its corresponding one-hot representation and Conv ( · , · ) is the convolution operation with the first argument as input and the second as the kernel .", "label": [], "Comments": []}
{"id": 167903, "text": "For each relation , we initialize the refining threshold p % ( Section [ 3.3 ] as 0.7 .", "label": [], "Comments": []}
{"id": 167904, "text": "1 Introduction Semantic parsing is the task of mapping natural language to a target meaning representation .", "label": [], "Comments": []}
{"id": 167905, "text": "Finally , we present some best practices for working with such a massive collection of datasets and models from a multilingual perspective . 1 Introduction Multilingual text representations are becoming increasingly important in science as well as the business community .", "label": [], "Comments": []}
{"id": 167906, "text": "We find that most general-typed tables are column tables , which means they only contain column headers .", "label": [], "Comments": []}
{"id": 167907, "text": "As expected , the combination of multiple types of REPLACE operations facilitates the attack success rate while increasing the number of queries .", "label": [], "Comments": []}
{"id": 167908, "text": "This aligns with the known language hierarchy for spoken language understanding [ Nastase et al. , , 2020 ] .", "label": [], "Comments": []}
{"id": 167909, "text": "By constructing a template for each class , it queries each span with each class separately .", "label": [], "Comments": []}
{"id": 167910, "text": "For zero-shot transfer , these scores drop to 35 LAS and 52 UAS , with some language pairs seeing differences of up to 85 points : e.g .", "label": [], "Comments": []}
{"id": 167911, "text": "The generative story consists of different steps such as distortion ( word reordering ) , fertility ( 1 : n word mappings ) , and lexical translation ( word-to-word translation ) that describe the translation process .", "label": [], "Comments": []}
{"id": 167912, "text": "The results for SenPos and Out-Gen are shown in Tables 15 and 16 , respectively .", "label": [], "Comments": []}
{"id": 167913, "text": "This section briefly describes these benchmark models .", "label": [], "Comments": []}
{"id": 167914, "text": "In Table [ 8 , ] we report the hyperparameters related to our KG construction .", "label": [], "Comments": []}
{"id": 167915, "text": "Each pair of sibling nodes , have a coupled nuclearity label .", "label": [], "Comments": []}
{"id": 167916, "text": "Table 3 : Action-level parameters , which add variety in the execution of each action primitive in the dataset .", "label": [], "Comments": []}
{"id": 167917, "text": "Experiments on the Fisher Spanish-English dataset show that the proposed framework yields improvement of 6.7 BLEU compared with a baseline direct S2ST model that predicts spectrogram features .", "label": [], "Comments": []}
{"id": 167918, "text": "For the query , the concatenation of question and one candidate answer , we also have two versions , with or without knowledge .", "label": [], "Comments": []}
{"id": 167919, "text": "Specially , the input layer X ( 0 ) is the embeddings of words .", "label": [], "Comments": []}
{"id": 167920, "text": "We present the top k = 10 most informative tokens for the * hateful * class along with their scores in Table [ 7 . ] Table 7 : Top 10 most informative tokens for the hateful class on the Reddit dataset according to PMI .", "label": [], "Comments": []}
{"id": 167921, "text": "As a result , although these studies perform better than directly applying their base SDS models [ See et al. , ] K is set to 7 in [ Lebanoff et al. , 2018 ] and 15 in [ Zhang et al. , 2018 ] .", "label": [], "Comments": []}
{"id": 167922, "text": "Note that a limitation of IScore is that it treats the words New and York separately , although they should be treated as a single noun phrase .", "label": [], "Comments": []}
{"id": 167923, "text": "Lastly , individual languages , like the ten included in MHC , are not monolithic but vary between speakers , especially across geographic regions and sociodemographic groups .", "label": [], "Comments": []}
{"id": 167924, "text": "According to the individual language heatmaps ( Fig . [ 2 ] , the languages that seem to benefit most from the cross-lingual colexifica- Figure 3 : ( a ) Performance on LSIM task for each evaluation language using 9 , 20 , 50 , 100 , 200 , or 499 input languages to build the COLEXcross graph .", "label": [], "Comments": []}
{"id": 167925, "text": "The dip in puzzles in 1993-1996 is due to an unavailability of NYT puzzles from those years .", "label": [], "Comments": []}
{"id": 167926, "text": "We follow the same learning rate schedule in [ Vaswani et al. , 2017 ] .", "label": [], "Comments": []}
{"id": 167927, "text": "Our models are trained by Adam optimizer [ Kingma and Ba , 2015 ] with default hyperparameters .", "label": [], "Comments": []}
{"id": 167928, "text": "Summarized in Table [ 1 , ] based on the majoritarian vote , most sentences ( 57.5 % ) contain no event boundaries while 16.6 % and 13.0 % of sentences contains expected and surprising event boundaries , respectively .", "label": [], "Comments": []}
{"id": 167929, "text": "Lastly , we remove pairs if the same author posted the * Parent * and * Target * .", "label": [], "Comments": []}
{"id": 167930, "text": "The dataset MNLI is built from multiple genre ( the original SNLI dataset is in fact built from the Flickr images thus has a strong visual connection ) and QNLI is purely based on English Wikipedia ( The same as SQuAD [ Rajpurkar et al. , 2016 ] ) .", "label": [], "Comments": []}
{"id": 167931, "text": "Span alignment annotation : Next , we solicit finegrained annotations between the narratives and the proverb in form of aligned spans .", "label": [], "Comments": []}
{"id": 167932, "text": "Comprehensive comparisons show the benefits of learning memory representations contextualized with up-to-date information .", "label": [], "Comments": []}
{"id": 167933, "text": "First , we observe that supportive subreddits are most similar to other supportive subreddits that cater towards the same marginalized community .", "label": [], "Comments": []}
{"id": 167934, "text": "However , persuasive essays usually convince readers by directly presenting arguments but not narrating a story .", "label": [], "Comments": []}
{"id": 167935, "text": "1 Introduction Recent large neural models like BERT and GPT-3 exhibit impressive performance on a large variety of linguistic tasks , from sentiment analysis to question-answering [ Devlin et al. , , 2019 ] [ Brown et al. , 2020 ] .", "label": [], "Comments": []}
{"id": 167936, "text": "To better evaluate our proposed methods , we design a new benchmark suite , Semantic Scholar Visual Layout-enhanced Scientific Text Understanding Evaluation ( S2-VLUE ) .", "label": [], "Comments": []}
{"id": 167937, "text": "The arrows indicate centering transitions with two different transition types , continue and shift .", "label": [], "Comments": []}
{"id": 167938, "text": "A popular approach to controllable text generation is to design prompts , which control high-level linguistic features ( i.e. , text style and sentiment ) by prefixing simple context to the input of a language model .", "label": [], "Comments": []}
{"id": 167939, "text": "Association for Computational Linguistics .", "label": [], "Comments": []}
{"id": 167940, "text": "G A2C Policy Learning A policy gradient-based reinforcement learning approach [ Williams , 1992 ] allows optimizing on nondifferentiable metrics , and eliminates the exposure bias that occurs with traditional training methods , like cross-entropy , on generation tasks [ Ranzato et al. , 2016 ] .", "label": [], "Comments": []}
{"id": 167941, "text": "grained interaction between any token in question and any entity in KG through dense bidirectional attention , and perform multi-step joint reasoning by stacking several interaction layers .", "label": [], "Comments": []}
{"id": 167942, "text": "The Multiformer proposes to modify the self-attention module on each encoder layer by a MHMA , since we believe that this module could be helpful to deal with speech .", "label": [], "Comments": []}
{"id": 167943, "text": "Our model integrates multiple components such as sequence labeling , multi-scale cross-modality learning , and object detection .", "label": [], "Comments": []}
{"id": 167944, "text": "Adjectives : A uniform semantic approach .", "label": [], "Comments": []}
{"id": 167945, "text": "A Experimental Setup A.1 Datasets The dataset statistics for all the datasets are reported in Table [ 5 . ] It is worth to mention that IWSLT datasets are under the Creative Commons BY-NC-ND license , and the multi-domains translation datasets are under the BSD license .", "label": [], "Comments": []}
{"id": 167946, "text": "We train our model on entity linked documents using cross-entropy loss .", "label": [], "Comments": []}
{"id": 167947, "text": "Another drawback is that it needs parallel data for model training on specific tasks .", "label": [], "Comments": []}
{"id": 167948, "text": "Table 14 : Some summarization data examples whose weights learned by our framework are close to 0 .", "label": [], "Comments": []}
{"id": 167949, "text": "However , machine translation , in either direction can introduce limiting artifacts [ Artetxe et al. , , 2020 ] with poor generalization due to how `` translationese '' training data diverges from gold test utterances [ Riley et al. , , 2020 ] .", "label": [], "Comments": []}
{"id": 167950, "text": "On the contrary , both kNN-MT and our kNN-KD increase the probabilities of the reasonable target tokens , and these two models have similar predicted probabilities .", "label": [], "Comments": []}
{"id": 167951, "text": "Both tasks are binary * hate * / * neutral * classification tasks .", "label": [], "Comments": []}
{"id": 167952, "text": "The values of T are the same as the maximum sequence length values in section [ B . ] The final token representations are derived by taking the average of the token representations in each segment .", "label": [], "Comments": []}
{"id": 167953, "text": "5 Related Works 5.1 Neural Machine Translation Machine translation has developed rapidly in recent years .", "label": [], "Comments": []}
{"id": 167954, "text": "Some of the most well-known interventions are ad blockers , and tools that improve productivity online ( e.g . by removing the Facebook newsfeed [ Lyngs et al. , , 2020b ] ) .", "label": [], "Comments": []}
{"id": 167955, "text": "Baselines . [ Li and Jurafsky , 2017 ] propose a neural model based on cliques , that are sets of adjacent sentences .", "label": [], "Comments": []}
{"id": 167956, "text": "We focus on providing fine-grained analysis of zero-shot system performance on a broad range of linguistic phenomena .", "label": [], "Comments": []}
{"id": 167957, "text": "[ He et al. , 2017 ] addressed the knowledge-grounded conversation task by iteratively updating the knowledge base embeddings to generate informative responses .", "label": [], "Comments": []}
{"id": 167958, "text": "Explicitly , several single-domain models were created by fine-tuning each pre-trained LM with training data from one SE domain , either restaurants ( R ) or laptops ( L ) .", "label": [], "Comments": []}
{"id": 167959, "text": "the combined training set of UFET , TACRED and MAVEN , and report F1 performance on their respective test sets by following their respective evaluation protocol .", "label": [], "Comments": []}
{"id": 167960, "text": "This is particularly true for the ShadowLink SHADOW-DOC split , as this split contains a larger number of tail entities which Note that some examples may contain more than one source of error ( or contain an error not clearly in any category ) , so the sum of the rows will not necessarily be 50 .", "label": [], "Comments": []}
{"id": 167961, "text": "( iii ) Different from previous work that mainly relies on HowTo100M , we include additional video datasets for pre-training , encouraging the model to learn from richer and more divserse visual content .", "label": [], "Comments": []}
{"id": 167962, "text": "The added term acts as a bridge between coreference and relations , thereby providing explicit task interactions that circumvents independent decoding of each task .", "label": [], "Comments": []}
{"id": 167963, "text": "We remove the words related to race attributes such as `` white '' and `` black '' .", "label": [], "Comments": []}
{"id": 167964, "text": "Text Adapters ' Pretraining .", "label": [], "Comments": []}
{"id": 167965, "text": "Specifically , ( 1 ) we remove the extra semantic encoder and construct the sentence-level representations by averaging the sequence of outputs of the vanilla sentence encoder .", "label": [], "Comments": []}
{"id": 167966, "text": "Reliable UE of DNN predictions that does not introduce high computational overhead is an open research question [ Van Amersfoort et al. , , 2020 ] .", "label": [], "Comments": []}
{"id": 167967, "text": "Memory efficiency enables applications in low-resource environments .", "label": [], "Comments": []}
{"id": 167968, "text": "Our dataset contains narratives ( N1 and N2 ) paired against proverbs ( P ) along with a fine-grained annotation of * aligned spans * between the narratives and proverbs .", "label": [], "Comments": []}
{"id": 167969, "text": "From the perspective of UT , we believe the lazy transition is a good alternative to the combination of step encodings and ACT .", "label": [], "Comments": []}
{"id": 167970, "text": "Finally , we sample from the GPT-2 language model [ 1 ] to generate the paragraph text Wij , starting from the prompt W ij and with a vocabulary distribution shift defined by Zij .", "label": [], "Comments": []}
{"id": 167971, "text": "while applying efficient attention mechanisms .", "label": [], "Comments": []}
{"id": 167972, "text": "We can also see the similar results for three Blog models of different sizes in Table [ 2 . ] Blog and Blog 54B achieve similar performance .", "label": [], "Comments": []}
{"id": 167973, "text": "As shown in Figure [ 4 , ] each layer takes the concatenation of outputs from all preceding layers as its input .", "label": [], "Comments": []}
{"id": 167974, "text": "We further augment the dataset with Twitter biographical user data and thread metadata retrieved via the Twitter API [ 7 ] .", "label": [], "Comments": []}
{"id": 167975, "text": "We also included the newly collected French sentences and their translation into English .", "label": [], "Comments": []}
{"id": 167976, "text": "The LP network is optimized to discriminate the source language from z , but the encoder is now optimized adversarially * against * this objective .", "label": [], "Comments": []}
{"id": 167977, "text": "Source of Definitions .", "label": [], "Comments": []}
{"id": 167978, "text": "All hyperparameters are the same for DEMIX and DENSE training .", "label": [], "Comments": []}
{"id": 167979, "text": "Figure 1 : Label distribution in * Target * s depending on whether commentw/ hateword is the * Parent * or the * Target * . with even more hatred ( and the ground truth label is Hate ) .", "label": [], "Comments": []}
{"id": 167980, "text": "Directional Encoder .", "label": [], "Comments": []}
{"id": 167981, "text": "To fill in above gap , we propose a lightweight POS-Enhanced Iterative Co-Attention Network ( * POI-Net * ) as the first attempt of unified modeling with pertinence , to handle diverse discriminative MRC tasks synchronously .", "label": [], "Comments": []}
{"id": 167982, "text": "Each convolutional layer has a stride of 4 and d channels .", "label": [], "Comments": []}
{"id": 167983, "text": "We believe that the two kinds of shift—task shift and language shift—are difficult to disentangle , and we do not attempt to do so in this work .", "label": [], "Comments": []}
{"id": 167984, "text": "Label it as non-factual hallucination . ( 4 ) If the entity is mentioned in the source document , but it is used in the wrong context and misrepresents information from the document .", "label": [], "Comments": []}
{"id": 167985, "text": "To see which out-of-vocab words frequently appear in each corpus , we rank them by their frequency .", "label": [], "Comments": []}
{"id": 167986, "text": "However , this correlation does not suffice to explain the totality of character information learned by PLMs .", "label": [], "Comments": []}
{"id": 167987, "text": "In Dialogue 2 , there are multiple topics in a session ; eating , health and exercising .", "label": [], "Comments": []}
{"id": 167988, "text": "PTR [ Han et al. , 2021 ] is based on prompt-tuning .", "label": [], "Comments": []}
{"id": 167989, "text": "We also provide clustering results based on the sentence embeddings extracted from a finetuned model in Appendix [ A.4 . ] We merge the clusters iteratively and stop the procedure when all the clusters have at least 20 examples or only 5 clusters are left .", "label": [], "Comments": []}
{"id": 167990, "text": "Using the development and test sets of ParaNMT , [ Chen et al. , 2020 ] also provide a curated set of triplets formed by a target sentence ( * target * ) , a semantic source ( * sem_src * ) , and a syntactic source ( * syn_src * ) .", "label": [], "Comments": []}
{"id": 167991, "text": "Consequently , they experience ineffective supervision and uninterpretable model predictions .", "label": [], "Comments": []}
{"id": 167992, "text": "We extract at most eight phrases for each story , and each phrase contains no more than eig [ ht ] [ words . ]", "label": [], "Comments": []}
{"id": 167993, "text": "Table 6 : Effects of using personal attributes to augment Blender on Personachat .", "label": [], "Comments": []}
{"id": 167994, "text": "On the large- scale semi-supervised setting , our model is also consistently better than others , showing the effectiveness of the model on a large training set .", "label": [], "Comments": []}
{"id": 167995, "text": "Table 2 : Thematic categories for significantly predictors of Helpfulness .", "label": [], "Comments": []}
{"id": 167996, "text": "While Sabo et al .", "label": [], "Comments": []}
{"id": 167997, "text": "The batch size was set to 8 .", "label": [], "Comments": []}
{"id": 167998, "text": "Table 3 : The test-set BLEU of CMLM trained with our EISL , compared to other recent fully non-autoregressive methods .", "label": [], "Comments": []}
{"id": 167999, "text": "In the federated framework , two types of models are needed , i.e. , the local model and the central model , which share the same network structure but have different permissions to access private data .", "label": [], "Comments": []}
{"id": 168000, "text": "It evaluates the minimal cost of operations which have to applied to one of the strings to obtain the other string : where { ops } denotes the set of operations , len ( · ) is the string length . where LCS ( s , t ) is the longest common substring of s and t .", "label": [], "Comments": []}
{"id": 168001, "text": "Acknowledgements We would like to thank Adam Pauls , Jason Eisner , Matt Gardner , and other colleagues at Microsoft Semantic Machines , who provided helpful feedback and engaged in stimulating discussions which have greatly improved the paper .", "label": [], "Comments": []}
{"id": 168002, "text": "The motivation behind EAG is two-fold : 1 ) Although identical source or target sentences between bilingual examples from different language pairs are scarce , highly similar sentences in source or target side are more wide-spread ; 2 ) Based on the pre-extracted candidate aligned examples which have highly similar source or target sentences , EAG can generate the final aligned examples by only refining the sentences partly with a few modifications .", "label": [], "Comments": []}
{"id": 168003, "text": "Personalization across a wide range of tasks ( recommending food , movies and music by multi-task dialogue agents such as Alexa , Siri and Assistant ) however can require orders of magnitude more personal attribute features .", "label": [], "Comments": []}
{"id": 168004, "text": "To the west , a corridor leads back to your laboratory .", "label": [], "Comments": []}
{"id": 168005, "text": "We observe that human-written negative answers suffer from serious bias ( * i.e . * , models can learn to predict correctly without absorbing any information from the video or subtitles ) .", "label": [], "Comments": []}
{"id": 168006, "text": "Moreover , the authors only release the document screenshots and token information parsed using PDFMiner12 instead of the source PDF files , which causes additional issues when using the dataset .", "label": [], "Comments": []}
{"id": 168007, "text": "3.1 Implementation Victim Model In this work , we choose fine-tuned BERT as the victim model for the all evaluation tasks .", "label": [], "Comments": []}
{"id": 168008, "text": "By using the corpus as a source of knowledge these models can extend the information available to the model by tens or even hundreds of gigabytes with a sub-linear scaling in computation cost .", "label": [], "Comments": []}
{"id": 168009, "text": "We think a classifier with a closed decision boundary could better deal with the Universum class in documentlevel event argument extraction , as illustrated by the purple line in Figure [ 2 . ] The contribution of this work is three-fold .", "label": [], "Comments": []}
{"id": 168010, "text": "2.2 Neighbourhood Models kNN models have been used for a number of NLP tasks such as part of speech tagging ( Daelemans et al. , 1996 ) and morphological analysis ( Bosch et al. , 2007 ) , among many others .", "label": [], "Comments": []}
{"id": 168011, "text": "5 Analysis 5.1 Ablation Study To explore the effects between different components in our model , we consider the following ablations : 1 ) IBR +Gold-Parent : given the gold parent nodes during inference to explore the accuracy of child node prediction ; 2 ) IBR +Gold-Child : given the gold child nodes to verify the accuracy of parent node prediction ; 3 ) * w/o * QA : removing QA task in loss to check its impact on proof generation ; 4 ) * w/o * node LSTM : using mean pooling rather than LSTM encoding to get the representations of nodes ; 5 ) * w/o * focus LSTM : Removing the supplementary LSTM in path focus selection .", "label": [], "Comments": []}
{"id": 168012, "text": "For IWSLT14 En→De , there are 160k sentence pairs for training and 7584 sentence pairs for validation .", "label": [], "Comments": []}
{"id": 168013, "text": "We also compare the model 's generation performance with * Hierarchical Recurrent Encoder-Decoder * ( HRED ) and * Graph-Structured Network * ( GSN ) [ Hu et al. , , 2019 ] .", "label": [], "Comments": []}
{"id": 168014, "text": "Transfer Learning of the [ FLF ] Weights .", "label": [], "Comments": []}
{"id": 168015, "text": "[ Clark et al. , , 2018 ] designed auxiliary prediction modules with restricted views of the input to encourage consistency across views .", "label": [], "Comments": []}
{"id": 168016, "text": "Our approach is empirically compared against an array of existing metrics on three human summary evaluation datasets .", "label": [], "Comments": []}
{"id": 168017, "text": "Although glimpses of personality traits are observable in existing open domain conversation corpora , leveraging generic language modelling for response generation overlooks the interlocutor idiosyncrasies , resulting in non-customizable personality agnostic responses .", "label": [], "Comments": []}
{"id": 168018, "text": "We exclude posts written in a language other than English and remove strings containing numbers , references to users , and hyperlinks .", "label": [], "Comments": []}
{"id": 168019, "text": "In summary , our diagnostic study through inoculation exposes a diverse set of dataset and model weaknesses . 5.3 Hypothesis-only Bias To determine if models can leverage spurious artifacts in the hypotheses of each phenomenon , we compare full models to hypothesis-only baselines .", "label": [], "Comments": []}
{"id": 168020, "text": "In contrast , our KALA predicts the * Jaime Andrade * as an answer , which is the ground truth .", "label": [], "Comments": []}
{"id": 168021, "text": "Detailed survey instruction forms of our attack and defense are included in Figures [ 13 ] and [ 14 . ]", "label": [], "Comments": []}
{"id": 168022, "text": "These values indicate that the association between the two annotations is statistically significant , suggesting that our metric provides a good approximation of the existence of framing bias .", "label": [], "Comments": []}
{"id": 168023, "text": "Model parameters are averaged over the top 3 and top 5 models .", "label": [], "Comments": []}
{"id": 168024, "text": "3.6 Training and Optimization The generator is optimized by maximizing the generation probability of the ground-truth y : In practice , we find that the token refiner is hard to train .", "label": [], "Comments": []}
{"id": 168025, "text": "By observing row 2 & 3 , the performance of UT significantly degrades without using step encodings , which indicates a mechanism for step identification is beneficial for UT .", "label": [], "Comments": []}
{"id": 168026, "text": "We also provide a novel dataset of medical newswire queries linked to research literature .", "label": [], "Comments": []}
{"id": 168027, "text": "The * Temporal * model has lowest perplexities on both the seen and unseen slices of evaluation data .", "label": [], "Comments": []}
{"id": 168028, "text": "The final outputs are selected based on those scores .", "label": [], "Comments": []}
{"id": 168029, "text": "The [ g ] round-truth VILA labels in S2-VL can be used to fine-tune visual layout detection models , and paper PDFs are also included , making PDF-based structure parsing feasible ; this enables VILA annotations to be created by different means , which is helpful for benchmarking new VILA-based models .", "label": [], "Comments": []}
{"id": 168030, "text": "We call these biases exhibited in a neighborhood of instances local group bias in contrast with global group bias which is evaluated on the entire corpus .", "label": [], "Comments": []}
{"id": 168031, "text": "A.3 Additional Results Table [ 8 ] reports the F1 scores of the best performing models for predicting each of the additional tasks in the multi-task learning framework .", "label": [], "Comments": []}
{"id": 168032, "text": "The distillation process consists of epochs .", "label": [], "Comments": []}
{"id": 168033, "text": "Rather , his conversation partner expressed his passion for animals in previous dialogue contexts .", "label": [], "Comments": []}
{"id": 168034, "text": "This model is denoted as `` DAN ( KD+FT ) '' . fully connected layers , ( W1 , b1 ) and ( W2 , b2 ) , to produces the final logits zˆ , i.e. , zˆ = Ms ( x ) = W2 ( ReLu ( W1h + b1 ) ) + b ∈ R n .", "label": [], "Comments": []}
{"id": 168035, "text": "Fine tuning the mLOWNER models on the WNUT train set , we achieve a new state of the art result ( cf . [ Shahzad et al. , , 2021 ] ) in WNUT with F1=0.507 for our approach , which is 9.7 % higher than the baseline .", "label": [], "Comments": []}
{"id": 168036, "text": "However , since it relies on crowdsourcing , it is limited in its coverage and is not easily extendable .", "label": [], "Comments": []}
{"id": 168037, "text": "We can see that training with small training datasets of only a few hundred pairs can produce a response with a natural and consistent personality , as shown in Table [ 4 . ]", "label": [], "Comments": []}
{"id": 168038, "text": "[ Bal ] [ abantaray et al. , 2012 ] and [ Wang et al. , 2012 ] engineered overlapping sets of features from Tweets , with shared features including n-grams , POS , adjectives , and lexicon-based sentiment polarity scores .", "label": [], "Comments": []}
{"id": 168039, "text": "For a detailed description of training parameters , see appendix [ A . ] 4.2 Experiments Description First , we trained two architectures based on a single attention mechanism ( Local or ConvAttention ) , in order to obtain a comparison between models with and without diversity .", "label": [], "Comments": []}
{"id": 168040, "text": "Models .", "label": [], "Comments": []}
{"id": 168041, "text": "Categorizing Factual Errors .", "label": [], "Comments": []}
{"id": 168042, "text": "Dropout [ Srivastava et al. , , 2014 ] , residual connection [ He et al. , , 2016 ] and layer normalization [ Ba et al. , 2016 ] are also applied following each sublayer .", "label": [], "Comments": []}
{"id": 168043, "text": "Then , TIE is utilized to predict the answer node n for the question q given the flattened HTML codes c and the corresponding DOM tree D and NPR graphs G ( see Sec . [ 3.2 ] .", "label": [], "Comments": []}
{"id": 168044, "text": "We first pretrain the LED-base model on the masked language modeling ( MLM ) task ( Devlin et al. , 2019 ) using related work sectio [ ns from ] [ S2ORC pape ] rs in the computer science domain , as well as on the cross-document language modeling ( CDLM ) task ( Caciularu et al. , 2021 ) , which aligns masked citat [ ion sentences with their ] context sentences and the full text of their cited papers .", "label": [], "Comments": []}
{"id": 168045, "text": "5 Conclusion We aimed to improve the raw conversational ability of dialogue systems by grounding the responses in much more human-like social media interactions .", "label": [], "Comments": []}
{"id": 168046, "text": "Yet , the extension of the proposed idea to all different approaches is straightforward .", "label": [], "Comments": []}
{"id": 168047, "text": "This task is proposed for this purpose , and the originally shared task was released at the CHIP2020 conference .", "label": [], "Comments": []}
{"id": 168048, "text": "Due to this temporal misalignment , performance will degrade after deployment , or any in evaluations that use test data temporally distant from the training data .", "label": [], "Comments": []}
{"id": 168049, "text": "In contrast , sentences with * no event * boundary typically continue or elaborate on the preceding event , such as a person liking a dog given that they get along with the dog ( Figure [ 1 ] .", "label": [], "Comments": []}
{"id": 168050, "text": "[ Webb et al. , 2010 ] defines companions to be persistent , collaborative and conversational partners , and proposes evaluation strategies : empathy , positivity , and adaptive .", "label": [], "Comments": []}
{"id": 168051, "text": "The alarm bell continues to ring .", "label": [], "Comments": []}
{"id": 168052, "text": "Reward : 0 ===================== Value : Bad for others 1 Observation : Reward : 0 Planetfall : `` Join the Patrol , and see the Galaxy ! ''", "label": [], "Comments": []}
{"id": 168053, "text": "General co-occurrence .", "label": [], "Comments": []}
{"id": 168054, "text": "We also included models trained on multilingual corpus like Wikipedia ( Wiki ) or Common Crawl ( CC ) as well as models trained with the use of parallel datasets .", "label": [], "Comments": []}
{"id": 168055, "text": "Similarly to [ Polyak et al. , , 2020 ] we report voice decision error ( VDE ) [ Nakatani et al. , 2008 ] , which measures the portion of frames with voicing decision error and F0 Frame Error Figure A.3 : Speaker linear F0 mean and standard deviation distributions on VCTK .", "label": [], "Comments": []}
{"id": 168056, "text": "When the inference speed is at millisecond level ( * e.g . * , with our DAN model ) , preprocessing time can become non-negligible .", "label": [], "Comments": []}
{"id": 168057, "text": "We trained a model for occupation prediction without fine-tuning the underlying RoBERTa model .", "label": [], "Comments": []}
{"id": 168058, "text": "For this reason , we propose a fusion forget gate ( FFG ) to filter low-level cross-modal adaptation information of each modality generated by the CFG .", "label": [], "Comments": []}
{"id": 168059, "text": "Due to the lack of program labels , existing methods will sample label consistent programs for model training .", "label": [], "Comments": []}
{"id": 168060, "text": "We address these issues by drawing inspiration from abstractive question answering ( QA ) .", "label": [], "Comments": []}
{"id": 168061, "text": "Flickr30k * in Neural Information Processing Systems 24 : 25th Annual Conference on Neural Information Processing Systems 2011 .", "label": [], "Comments": []}
{"id": 168062, "text": "Our method enforces both structure-awareness and transformation-invariance to such models .", "label": [], "Comments": []}
{"id": 168063, "text": "The datasets used in previous work ( Jigsaw Multilingual and WUL ) provide English-only training data and observe the performance of different models in zero-shot transfer learning settings .", "label": [], "Comments": []}
{"id": 168064, "text": "However , obtaining contextual relations between words from the sequence texts plays a crucial role in understanding the complete meaning of sentences .", "label": [], "Comments": []}
{"id": 168065, "text": "Read from row to column : E.g. , in COMBINED STL-JobBERT ( row ) is stochastically dominant over STL-BERTbase ( column ) with ✏min of 0.00 . 2001 ) layer .", "label": [], "Comments": []}
{"id": 168066, "text": "If models understand * necessarily * and * not necessarily * correctly , they should find the task easy .", "label": [], "Comments": []}
{"id": 168067, "text": "The following describes the detailed updating process : Update of the standard K-way classifier and the class-sentence matching classifier : The update is performed on labeled and unlabeled data at the same time .", "label": [], "Comments": []}
{"id": 168068, "text": "RQ2 : * How do existing disentanglement objectives compare for this task ? * Notably , the INF objective alone results in good disentanglement according to our three desiderata , suggesting that supervision alone is sufficient for disentanglement .", "label": [], "Comments": []}
{"id": 168069, "text": "Hence , this list is a strong indicator of a non-actionable task ( in AT ) . 5 Related Work To-do Management : Intelligent systems can assist users with task management in many ways [ Gil et al. , 2012 ] .", "label": [], "Comments": []}
{"id": 168070, "text": "Furthermore , AMR captures semantics at a high level of abstraction explicitly modeling relations in the text and reducing the negative influence of diverse text surface variances with the same meaning .", "label": [], "Comments": []}
{"id": 168071, "text": "In literature , there are several examples of reviews , which focus on * traditional * sentiment analysis methods ( e.g. , lexicon-based , lexical features engineering , shallow models ) , while not mentioning any embeddingbased methods [ Dashtipour et al. , , 2016 ] [ Sagnika et al. , 2020 ] .", "label": [], "Comments": []}
{"id": 168072, "text": "We train a projection matrix that maps embeddings of word-initial forms of bases to wordinternal embeddings .", "label": [], "Comments": []}
{"id": 168073, "text": "It is inspired by parallel co-attention [ Lu et al. , , 2016 ] , which computes an affinity matrix between two sequences , while we apply two unidirectional matrices instead of assigning shared parameters to both directions , and use scaled dot-product attention [ Vaswani et al. , , 2017 ] .", "label": [], "Comments": []}
{"id": 168074, "text": "In this case , most of our methods improve macroaveraged precision and recall of the BERT model on WNUT .", "label": [], "Comments": []}
{"id": 168075, "text": "Details are in Appendix [ B.1 . ] Note that even the smallest percentage of attack effectiveness ( e.g. , 10 % -20 % ) poses a major risk for real-world conversational systems when those systems are deployed at scale .", "label": [], "Comments": []}
{"id": 168076, "text": "2.3 Model-Agnostic Meta-Learning Model-agnostic meta-learning ( MAML ) [ Finn et al. , 2017 ] provides a general method to adapt to parameters in different domains .", "label": [], "Comments": []}
{"id": 168077, "text": "To match the size of the dataset with its Dark Web counterparts , the aggregate Surface Web dataset is trimmed by randomly sampling a portion of documents from each category .", "label": [], "Comments": []}
{"id": 168078, "text": "SMLM ( * ) used different KG which has strong alignment with each task ( * e.g . * AT for SIQA ) . reasoning for a-NLI , physical commonsense for PIQA , and pronoun resolution ability for WG . [ 3 ] The details are presented in Appendix [ G. ] 4.1.2 Baselines We compare our framework with the following baselines .", "label": [], "Comments": []}
{"id": 168079, "text": "More recent systems employ specialized components [ to ] [ aggregate ] [ multip ] [ le ] [ pieces ] [ of ] [ evid ] ence .", "label": [], "Comments": []}
{"id": 168080, "text": "3 Defense Approaches The defense against adversarial attacks has two components ( a ) detecting the attack and ( b ) mitigating its effect by ensuring that the defender does not generate a toxic response .", "label": [], "Comments": []}
{"id": 168081, "text": "Finally , as described in more detail in Section [ 2.2 ] and Appendix [ A6 , ] RELiC differs significantly from existing NLP and IR retrieval datasets in domain , linguistic complexity , and query length .", "label": [], "Comments": []}
{"id": 168082, "text": "Table 5 : A comparison between our ITA ( ITA-All+CVA with I+T inputs ) model and the baseline ( BERT-CRF ) in precision ( P ) , recall ( R ) and F1 . ∆ represents the relevant improvement of ITA over the Baseline . aligning image features into textual space .", "label": [], "Comments": []}
{"id": 168083, "text": "Knowledge Distillation without Dtrain .", "label": [], "Comments": []}
{"id": 168084, "text": "This DPR ensemble performs poorly , scoring 74.5 R @ 20 ( not shown in tables ) , 6.8 % below the equivalent DrBoost , confirming that the boosting formulation is important , not simply having several ensembled dense retrievers .", "label": [], "Comments": []}
{"id": 168085, "text": "The inferior results demonstrate that these two methods are not qualified for enhancing the ATE task . 5 Analysis , 5.1 Perplexities of Language Models 2GV 8KY 8KY 8KY In this section , we present the perplexities of language models pre-trained on different datasets .", "label": [], "Comments": []}
{"id": 168086, "text": "All p-values are smaller than α = 0.05 .", "label": [], "Comments": []}
{"id": 168087, "text": "A Fleiss Kappa of 0.25 was achieved for sentence relevance selection , the first task .", "label": [], "Comments": []}
{"id": 168088, "text": "We normalized the ratings to be ∈ [ 0 , 1 ] , and studied the regression task of predicting the nor [ m ] alized rating from text .", "label": [], "Comments": []}
{"id": 168089, "text": "We align the topics in the base and augmented SCHOLAR models using a variation of competitive linking , which produces a greedy approximation to optimal weighted bipartite graph matching [ Melamed , 2000 ] .", "label": [], "Comments": []}
{"id": 168090, "text": "In this section , we consider the setting where we have an already trained model on the 2010–18 slices , as well as new data from the 2019 slice .", "label": [], "Comments": []}
{"id": 168091, "text": "The first clear difference is that there is a far higher count of gold * SAME AS * facts in the AIDA dataset , which is potentially explained by pages on Wikipedia generally having hyperlinks for the first mention of an entity only .", "label": [], "Comments": []}
{"id": 168092, "text": "The KL loss can be transformed into the following function by expanding and marginalizing p ( y|x ) [ Liu and Hockenmaier , 2020 ] : where q is the target distribution , ψ 0 is the feature function , Z is the normalization function .", "label": [], "Comments": []}
{"id": 168093, "text": "Table [ 19 ] and [ 20 ] show interactions between the system and human workers in the process of Section [ 3.3 . ] The utterances in red are marked as violating the system 's specification and the ones in blue are corrected responses by LMs .", "label": [], "Comments": []}
{"id": 168094, "text": "The data splits of each dataset are presented in Table [ 2 . ] Table 2 : Social meaning data . opt . : : Optimism , sad . : Sadness , off . : offensive , sarc . : sarcastic , IC : Ironic by clash , SI : Situational irony , OI : Other irony , NI : Nonironic , neg . : Negative , neu . : Neutral , pos . : Positive .", "label": [], "Comments": []}
{"id": 168095, "text": "We further experimentally show the overall superiority of EmbedHalluc when comparing to regularization methods proposed to address the problem of over-fitting during fine-tuning of LMs , such as Mixout [ Lee et al. , , 2019 ] and Re-Init [ Zhang et al. , , 2021 ] .", "label": [], "Comments": []}
{"id": 168096, "text": "AI : Plus , you need to exercise !", "label": [], "Comments": []}
{"id": 168097, "text": "Last , given the examples from Figure 1 , we formulate the task as both as a sequence labeling and a multi-task learning ( MTL ) problem , i.e. , training on both skill and knowledge components jointly ( Caruana , 1997 ) .", "label": [], "Comments": []}
{"id": 168098, "text": "[ Damonte and ] [ Cohen , 2018 ] ; [ Sheth et al. , 2021 ] propose annotation projection of English AMR graphs to target languages to train cross-lingual parsers , using word alignments .", "label": [], "Comments": []}
{"id": 168099, "text": "Labeling more spans within the same context is more efficient .", "label": [], "Comments": []}
{"id": 168100, "text": "This provides us with an explicit representation of the most important items in sentences leading to the notion of focus .", "label": [], "Comments": []}
{"id": 168101, "text": "The two highest-correlated metrics for each dataset are shown in bold .", "label": [], "Comments": []}
{"id": 168102, "text": "They were married in 1977 . . . twin daughters : properties of extracted entities .", "label": [], "Comments": []}
{"id": 168103, "text": "4.2 Experiments We perform five-fold cro [ ss-v ] alidation to tune the model hyper-parameters .", "label": [], "Comments": []}
{"id": 168104, "text": "For example , [ Zhang and Chen , 2018 ] used distance metric information , and [ Li et al. , 2020 ] used distance features like shortest path and landing probabilities between pair of nodes in subgraphs as additional features .", "label": [], "Comments": []}
{"id": 168105, "text": "Cloud providers routinely spend 40-50 % of the cost towards electricity as well as powering and cooling the servers , and this cost is increasing .", "label": [], "Comments": []}
{"id": 168106, "text": "B Additional Details B.1 DAN Variations Due to space limits we have omitted the details for the DAN variations we studied in [ §4.1 . ] We introduce these variations in the following .", "label": [], "Comments": []}
{"id": 168107, "text": "Group tournament ranking system . [ https : //en.wikipedia.org/wiki/ ] ( https : //en.wikipedia.org/wiki/Group_tournament_ranking_system ) [ Group_tournament_ranking_system ] ( https : //en.wikipedia.org/wiki/Group_tournament_ranking_system ) .", "label": [], "Comments": []}
{"id": 168108, "text": "As for our methods , we can see that , UniTE-MRA achieves better results on all tasks , demonstrating the effectiveness of monotonic attention flows for cross-lingual interactions .", "label": [], "Comments": []}
{"id": 168109, "text": "As Table [ 5 ] shows , our model achieves the best performance on eight out of 15 datasets .", "label": [], "Comments": []}
{"id": 168110, "text": "Essentially , the embedding vectors remain unchanged for the Malay words but are significantly smaller in size .", "label": [], "Comments": []}
{"id": 168111, "text": "In this work , we explore the use of InfoNCE to train our event representation model .", "label": [], "Comments": []}
{"id": 168112, "text": "Evaluation Metric We extract word-level sentiments from the phrase structure tree ( PTB ) in SST dataset .", "label": [], "Comments": []}
{"id": 168113, "text": "6 Conclusion We studied the possibility of generating imperceptible attacks against conversational agents that , while fluent and coherent , target the model into generating toxic responses .", "label": [], "Comments": []}
{"id": 168114, "text": "3.5 Cluster Fusion Next , we would like to merge the paraphrastic propositions in each cluster , while consolidating complementary details , to generate a new coherent summary sentence .", "label": [], "Comments": []}
{"id": 168115, "text": "Figure [ 3 ] shows the comparison between 1.3Bsized model and 6.9B-sized model .", "label": [], "Comments": []}
{"id": 168116, "text": "Moreover , the product image can indicate that the term `` * black * '' is not an attribute value of the current product ; thus , it should not be extracted .", "label": [], "Comments": []}
{"id": 168117, "text": "We only experiment with SP because it is the best retrieval model , providing a larger improvement headroom .", "label": [], "Comments": []}
{"id": 168118, "text": "The summarized representations are then used as the downstream model 's input .", "label": [], "Comments": []}
{"id": 168119, "text": "The text snippets we show here are restored from the original websites .", "label": [], "Comments": []}
{"id": 168120, "text": "As supported by our human evaluation , we believe the underlying reason is that our model can leverage pre-trained generators to generate coherent and relevant partner personas .", "label": [], "Comments": []}
{"id": 168121, "text": "This shows the superiority of our methods , even when compared to models trained simply with MLM with ∼ 3× more data ( 850M tweets for BERTweet vs. only 276M for our best method ) .", "label": [], "Comments": []}
{"id": 168122, "text": "Overall , there are 27 functional tests grouped into 11 classes for each of the ten languages in MHC , except for Mandarin , which has 25 functional tests .", "label": [], "Comments": []}
{"id": 168123, "text": "From the decoding speed perspective , we should bear in mind various use-cases for the model deployment , such as the hardware environment or batching conditions .", "label": [], "Comments": []}
{"id": 168124, "text": "Results .", "label": [], "Comments": []}
{"id": 168125, "text": "During this phase , distances are computed between each word 's r nearest neighbors and stored in a distance matrix M [ Werner and Laber , 2019 ] .", "label": [], "Comments": []}
{"id": 168126, "text": "We randomly sample 200 utterances from the test set , and each sample is rated by 8 raters on a scale of 1–5 , with 1 being the worst and 5 being the best . 4.5 Results setup .", "label": [], "Comments": []}
{"id": 168127, "text": "This work was supported by National Natural Science Foundation of China No . 61872370 and No . 61832017 , Beijing Outstanding Young Scientist Program NO .", "label": [], "Comments": []}
{"id": 168128, "text": "Note that TextHide will encode such representation vectors into the training vectors for downstream tasks .", "label": [], "Comments": []}
{"id": 168129, "text": "If the baseline shows poor performance , it gives evidence that the model is not taking short-cuts .", "label": [], "Comments": []}
{"id": 168130, "text": "We also found that LaMemo increases the utilization of older memory states when predicting the target tokens , and yields a higher performance boost when extrapolating to longer memory length , which indicates the effectiveness of recontextualizing the memory under the current context .", "label": [], "Comments": []}
{"id": 168131, "text": "In future work , we https : //www.allsides.com/blog/does-unbiased-newsreally-exist will explore the application of our methodology to different cultures or languages .", "label": [], "Comments": []}
{"id": 168132, "text": "Further details of the dataset and the hyperparameter settings are described in the Appendix [ C. ] 4.4 Main Results We show the results of word-level language modeling benchmark Wikitext-103 in Table [ 1 . ]", "label": [], "Comments": []}
{"id": 168133, "text": "Within the same framework we compare the size of models used in our experiments .", "label": [], "Comments": []}
{"id": 168134, "text": "References Apoorv Agarwal , Anup Kotalwar , and Owen * Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) * , pages 352–361 .", "label": [], "Comments": []}
{"id": 168135, "text": "We have previously hypothesized that DANs will have superior inference speed due to its simple and sparse architecture .", "label": [], "Comments": []}
{"id": 168136, "text": "Learning from ample labeled examples is the predominant paradigm in many NLP tasks [ Schick ] [ and Schütze , 2021 ] , including SRL .", "label": [], "Comments": []}
{"id": 168137, "text": "ACT is designed based on two major ideas : * 1 ) Constrained Training * : bridging the discrepancy between training and constrained inference ; * 2 ) Alignment Prompting * : helping the model understand the context of the constraints .", "label": [], "Comments": []}
{"id": 168138, "text": "On the other hand , recent work [ Raffel et al. , 2020 ] [ Zhang et al. , , 2020 ] [ Schick and Schütze , ] In general , better token-level calibration does n't guarantee better sequence-level performance . < http : //cs.umd.edu/~snover/tercom/ > Table 10 : Case Study on CNNDM .", "label": [], "Comments": []}
{"id": 168139, "text": "The baselines do not have a retrieval step and therefore do not have an effect due to changing k. bold refers to the best scores across all k among the generated responses .", "label": [], "Comments": []}
{"id": 168140, "text": "Attribution analysis is also conducted to analyze the method .", "label": [], "Comments": []}
{"id": 168141, "text": "To prevent private information leakage , we leverage federated learning without sharing raw privacysensitive medical texts .", "label": [], "Comments": []}
{"id": 168142, "text": "7 Discussion We addressed the problem of modeling the rela- tionship between text and an outcome of interest in service of text analysis and supervised prediction .", "label": [], "Comments": []}
{"id": 168143, "text": "The dataset contains 504 common pediatric diseases , 7,085 body parts , 12,907 clinical symptoms , and 4,354 medical procedures in total .", "label": [], "Comments": []}
{"id": 168144, "text": "Please read their terms of use [ 3 ] for more details .", "label": [], "Comments": []}
{"id": 168145, "text": "However , designing a suitable system of canonical utterances is a non-trivial effort .", "label": [], "Comments": []}
{"id": 168146, "text": "To illustrate , we find that 21 % of the 25 % least popular [ 1 ] entities in Wikidata have neither an English description nor any entity type [ 2 ] , leaving no mechanism for models which rely on these two sources of information alone to disambiguate them ( other than their label ) .", "label": [], "Comments": []}
{"id": 168147, "text": "SIGHAN Bakeoff therefore defines two types of evaluation settings , closed test limits all the data for learning not to be beyond the given training set , while open test does not take this limitation [ Emer ] [ son , 2005 ] .", "label": [], "Comments": []}
{"id": 168148, "text": "Both fined-tuned BERT and MLM-phonetics perform weakly because of the inherent fixed-length limitation of their generation .", "label": [], "Comments": []}
{"id": 168149, "text": "The figure visualizes the distribution of the embeddings of the `` buy < grocery > '' and `` call < person > '' to-do texts [ 6 ] expressed in two ways : ( 1 ) the descriptions `` buy < grocery > '' and `` call < person > '' are paired with generic list names ( `` to do '' and `` reminders '' ) ; or ( 2 ) the descriptions `` < grocery > '' and `` < person > '' are paired with specific list names ( indicating the actions `` to buy '' and `` to call '' ) .", "label": [], "Comments": []}
{"id": 168150, "text": "Our model , LITE , can successfully * ignore * the generic lists and group similar tasks together . 3 Method : Multi-task Learning ( MTL ) We propose a multi-task learning ( MTL ) framework to represent to-do descriptions along with their list names ( Fig . [ 3 ] .", "label": [], "Comments": []}
{"id": 168151, "text": "Figure 5 : A case study on one context of the NCBI-Disease dataset .", "label": [], "Comments": []}
{"id": 168152, "text": "The hidden dimensions of the generator are 128 , 256 , 512 , 1024 .", "label": [], "Comments": []}
{"id": 168153, "text": "These illustrate cases with and without temporal sensitivity .", "label": [], "Comments": []}
{"id": 168154, "text": "In the first case , the model generates a response that exhibits a strong character style but is incoherent to the input context .", "label": [], "Comments": []}
{"id": 168155, "text": "The results suggest that our model performance reflects the variability and agreements in human annotations of event boundaries .", "label": [], "Comments": []}
{"id": 168156, "text": "As seen in Groups A & B in Table [ 7 , ] NAUS with length transfer is slightly worse than NAUS trained on the correct length , which is understandable .", "label": [], "Comments": []}
{"id": 168157, "text": "Table 9 : Bias detection on toxic classification using LOGAN .", "label": [], "Comments": []}
{"id": 168158, "text": "How this quantity affect the performance of D 3 ?", "label": [], "Comments": []}
{"id": 168159, "text": "Appendix C Multilingual Analyses Model Details : We only consider mBART [ Liu et al. , 2020 ] with 610M parameters and 250k vocab size .", "label": [], "Comments": []}
{"id": 168160, "text": "E is Electronics .", "label": [], "Comments": []}
{"id": 168161, "text": "All tasks are in English .", "label": [], "Comments": []}
{"id": 168162, "text": "Complex embeddings for simple link prediction .", "label": [], "Comments": []}
{"id": 168163, "text": "PRC uses k-nearest neighbours to find the MeSH annotations of similar citations in MEDLINE .", "label": [], "Comments": []}
{"id": 168164, "text": "Code and trained models at : [ https : //github.com/ ] ( https : //github.com/OriShapira/InterExp_DeepRL ) [ OriShapira/InterExp_DeepRL ] ( https : //github.com/OriShapira/InterExp_DeepRL ) task requirements .", "label": [], "Comments": []}
{"id": 168165, "text": "This score is 0.107 , which is comparable to a value of 0.098 for unigram overlap between pairs of narratives from different proverbs .", "label": [], "Comments": []}
{"id": 168166, "text": "For example , if we can tolerate a 10 % relative drop in accuracy from the best performing system 's exact search , DrBoost requires 16 ( 4 ) probes for MSMARCO ( NQ ) to reach the required accuracy , whereas DPR will require 1024 ( 16 ) , meaning DrBoost can be operated approximately 64x ( 4x ) faster .", "label": [], "Comments": []}
{"id": 168167, "text": "Figure 3 : Document comparison by WORDTOUR .", "label": [], "Comments": []}
{"id": 168168, "text": "Compared to a vanilla VRNN , structured attention enables a model to focus on different parts of the source sentence embeddings while enforcing a structural inductive bias .", "label": [], "Comments": []}
{"id": 168169, "text": "3.3 Training The newly added embedding layer embeds persona info tokens , and the embedding layer of the pretrained language model embeds each pair of utterance and response ( which consists of tokens already generated during training ) .", "label": [], "Comments": []}
{"id": 168170, "text": "As the clustering partition presumably captures information about salient features in the corpus , feeding this information into the model could lead to representations that are better geared to perform the target task .", "label": [], "Comments": []}
{"id": 168171, "text": "We observe that the performance of TAPT decreases with the increased number of iterations , which could be due to forgetting of the knowledge from the PLM .", "label": [], "Comments": []}
{"id": 168172, "text": "4.1 Experimental Settings The Multiformer architectures have been trained on 3 different language directions of the MuST-C dataset [ Cattoni et al. , , 2021 ] .", "label": [], "Comments": []}
{"id": 168173, "text": "Second , we link the worlds of non-autoregressive translation and optimization of autoregressive models to provide a better understanding of the results achieved in the related work .", "label": [], "Comments": []}
{"id": 168174, "text": "This dataset contains 8,628 sentence pairs from three categories : captions , news , and forums , and is split into 5,749/1,500/1,379 sentence pairs , respectively , for training/dev/test .", "label": [], "Comments": []}
{"id": 168175, "text": "Contributions We release XLM-EMO which is a multilingual emotion detection model for social media text .", "label": [], "Comments": []}
{"id": 168176, "text": "Table 3 : Modal dependency parsing F scores . 3.5 Cross-lingual Comparison Our experimental results show that English MDP results are in general better than Chinese .", "label": [], "Comments": []}
{"id": 168177, "text": "However , the models can be run in parallel if latency is a concern , and if needed , these models can be distilled into a single model with little drop in accuracy .", "label": [], "Comments": []}
{"id": 168178, "text": "In particular , with one source language and N target languages one needs to train : i ) N+1 different MAD-X LAs ( one for L and one for each of the N Lts ) ; ii ) N different BAD-X BAs ( one for each ( Ls , Lt ) pair ) and , iii ) only one MA ( using L and all Lts at once ) .", "label": [], "Comments": []}
{"id": 168179, "text": "Our analysis starts with a one-dimensional example ; the conclusion generalizes to higher-dimensional cases .", "label": [], "Comments": []}
{"id": 168180, "text": "Thus , this is a common issue in MRPC . 5.2.1 Proposed Amendments to MRPC With the aim to improve the precision of sentence pairs labelled as paraphrases in MRPC , we proposed some amendments to MRPC , including the following specific objectives : Our process to achieve this has two main steps .", "label": [], "Comments": []}
{"id": 168181, "text": "We enhance DiffKS by applying KI on its context encoder .", "label": [], "Comments": []}
{"id": 168182, "text": "The classification loss forces the model to predict a label for the query .", "label": [], "Comments": []}
{"id": 168183, "text": "[ Challenges in au ] ( https : //aclanthology.org/2021.eacl-main.274 ) [ tomated debiasing for toxic language detection .", "label": [], "Comments": []}
{"id": 168184, "text": "Fourth , removing both LLCT and PLCT decreases the performance of recall , F1 , and Jaccard score by 15.8 % , 13.2 % , 13.4 % , and 6.1 % .", "label": [], "Comments": []}
{"id": 168185, "text": "The causal analysis is a more appropriate method to isolate the influence of the data-model direction match .", "label": [], "Comments": []}
{"id": 168186, "text": "4.2.2 Audio Quality and Vocal Tone Quality To thoroughly evaluate our proposed model in audio quality and vocal tone quality , we compare subjective metric MOS-Q , MOS-V and objective metric MCD of audio samples generated by NVSB with the systems including : 1 ) * GT Mel * , amateur ( A ) and professional ( P ) version , where we first convert ground truth audio into mel-spectrograms , and then convert the mel-spectrograms back to audio using HiFi-GAN introduced in Section [ 4.1 ; ] 2 ) * Baseline * : the baseline model for SVB based on WaveNet with the number of parameters similar to * NSVB * , which adopts the same pitch correction method ( SADTW ) as * NSVB * does , and takes in the condition cˆ defined in Section [ 3.3 ] to generate the mel-spectrogram optimized by the L distance to xp .", "label": [], "Comments": []}
{"id": 168187, "text": "Here we aim to improve the language model representations by relying on clustering of data from the target domain .", "label": [], "Comments": []}
{"id": 168188, "text": "The relatively low performance of baseline models on this task suggests that while recognizing helpfulness in language is trivial for typically-developing humans , they remain challenging for machines .", "label": [], "Comments": []}
{"id": 168189, "text": "Consider two documents , d with high coverage , and d with low coverage , along with two claims c and c from the respective documents , extracted with the same confidence .", "label": [], "Comments": []}
{"id": 168190, "text": "Table 11 : Example 4 .", "label": [], "Comments": []}
{"id": 168191, "text": "In the pre-training stage , 10 % of the English Wikipedia was used for 3 epochs .", "label": [], "Comments": []}
{"id": 168192, "text": "Examples include Boko Haram , Al Qaeda , Sinaloa Cartel , PCC , FARC , Mara , among others .", "label": [], "Comments": []}
{"id": 168193, "text": "Then , the episode 's support set is used for prototype computations while the query set is used for performance evaluation .", "label": [], "Comments": []}
{"id": 168194, "text": "[ Ta ] [ ble 1 ] provides an overview of the focus languages , including the language families , location and number of speakers , and the source and original language for our corpus .", "label": [], "Comments": []}
{"id": 168195, "text": "No standardization was done in the case of NRC scores , as we wanted to feed our model a vector of raw emotion-intensity scores for each of the six emotions considered in our NRC representation .", "label": [], "Comments": []}
{"id": 168196, "text": "* We publicly release our metric code for easy use by other researchers [ 7 ] .", "label": [], "Comments": []}
{"id": 168197, "text": "In summary , the contributions of our study are the following : 2 Data Collection In this paper , we discuss the behavior of two conversational agents negotiating given imperfect information .", "label": [], "Comments": []}
{"id": 168198, "text": "In addition to minimizing the loss between the input and reconstruction , as in a standard AE , the VAE uses an additional KL divergence term to keep the approximate posterior close to the prior distribution .", "label": [], "Comments": []}
{"id": 168199, "text": "Increasing the Beam Width While theoretically a larger beam width ( i.e .", "label": [], "Comments": []}
{"id": 168200, "text": "For our approach , we jointly train a generatorretriever model where the retriever searches through pre-indexed SMIkb and feeds the related information together with the input utterance to the generative seq2seq model , allowing for additional context at the time of generation .", "label": [], "Comments": []}
{"id": 168201, "text": "Compared to the approaches that replace words in the embedding space [ Cheng et al. , , 2020 ] , our approach also demonstrates superior performance , which reveals that sentence-level augmentation with continuous semantics works better on generalizing to unseen instances .", "label": [], "Comments": []}
{"id": 168202, "text": "Abductive NLI ( a-NLI ) [ Bhagavatula et al. , , 2020 ] asks to infer the most plausible explanation based on the given causal situation to test abductive reasoning in narratives .", "label": [], "Comments": []}
{"id": 168203, "text": "Unlike Multistream , which leverages fine-grained region-level features , our results are reported on global framelevel features .", "label": [], "Comments": []}
{"id": 168204, "text": "While external memory allows models to cache sequential information and retrieve relevant multimodal content [ Patel et al. , 2021 ] , latest models still suffer from decreased performance , for example on ambiguous questions that require deeper reasoning abilities .", "label": [], "Comments": []}
{"id": 168205, "text": "Despite using less space , ICOREF retains the same accuracy as C2F-COREF .", "label": [], "Comments": []}
{"id": 168206, "text": "We include the following languages : English–EN , Spanish– ES , Dutch–NL , Russian–RU , Turkish–TR , Korean– KO , Farsi–FA , a mix of high and low resource languages .", "label": [], "Comments": []}
{"id": 168207, "text": "Experimental results on the How2 dataset show that our proposed model achieves a new state-of-the-art performance .", "label": [], "Comments": []}
{"id": 168208, "text": "In the zero-shot setting , we do see that that the largest model instruction-tuned with the most datasets ( T0++ ) improves a model 's sensitivity to prompt semantics .", "label": [], "Comments": []}
{"id": 168209, "text": "θ 18 : θ = θ − β∇ E L val Tj ( θ old i , θnew i , γ ) Domain Adaptive Meta-Validation After meta-training phase , DAML-ATM has already learned a temporary model ( θ old i , θnew i ) in the meta-training domains Dtr .", "label": [], "Comments": []}
{"id": 168210, "text": "Table 2 : Comparison of different methods in F1-scores .", "label": [], "Comments": []}
{"id": 168211, "text": "Though successful in retrieving suitable entities or facts from the KG , these systems fail to provide explainability to the recommendations .", "label": [], "Comments": []}
{"id": 168212, "text": "However , back-translation on summarization is an unrealistic problem because a model is required to restore deleted information in the given summary without any guide .", "label": [], "Comments": []}
{"id": 168213, "text": "In this work , we instead devise a novel framework to simulate realistic noisy user feedback on publiclyavailable , human-annotated data , and defer the task of real world deployment to future work .", "label": [], "Comments": []}
{"id": 168214, "text": "Figure [ 3 ] presents the locations of extracted keyphrases by different predictors .", "label": [], "Comments": []}
{"id": 168215, "text": "This method extracts a specific aspect d of the input embeddings . ( 2 ) PCA-1 orders in ascending order of the top PCA component . ( 3 ) [ Mu and Viswanath , 2018 ] reported that a few leading PCA components were not informative .", "label": [], "Comments": []}
{"id": 168216, "text": "Specifically for the matrix multiplication BC of two arbitrary matrices B and C , [ Drineas et al. , 2006 ] approximate it with BSS C and set the sampling probability p in S proportional to the product B ( i ) ∥2∥C ( i ) ∥2 , where ( i ) is the i-th column in matrix B and C ( i ) is the i-th row in matrix C .", "label": [], "Comments": []}
{"id": 168217, "text": "Each predictor takes the textual and visual embeddings given by Eq .", "label": [], "Comments": []}
{"id": 168218, "text": "When distance ✏min = 0.0 , one can claim that A stochastically dominant over B with a predefined significance level .", "label": [], "Comments": []}
{"id": 168219, "text": "Privacy-preserving embeddings .", "label": [], "Comments": []}
{"id": 168220, "text": "Therefore , it is crucial for the architecture to address the graph 's density and sparsity .", "label": [], "Comments": []}
{"id": 168221, "text": "2 Related Work Aspect Term Extraction Early researches for ATE mainly involve pre-defined rules [ Hu and Liu , 2004 ] [ Popescu and Etzioni , 2005 ] [ Wu et al. , , 2009 ] [ Qiu et al. , , 2011 ] and hand-craft features [ Li et al. , , 2010 ] [ Liu et al. , ] [ 2012 , 2013 ] [ Chen et al. , , 2014 ] .", "label": [], "Comments": []}
{"id": 168222, "text": "Building on both the structural and tree depth probes [ Hewitt and Manning , 2019 ] , [ Kulmizev et al. , 2020 ] extract * directed * dependency graphs from mBERT for 13 languages ( Section [ 3.2 ] .", "label": [], "Comments": []}
{"id": 168223, "text": "In the sequel , we omit the condition x and the subscript θ for simplicity . 3.1 The Difficulty of Cross Entropy Loss The standard approach to learn the sequence model is to minimize the negative log-likelihood ( NLL ) of the target sequence , i.e. , minimizing the CE loss L CE ( θ ) = log ( ∗ ) .", "label": [], "Comments": []}
{"id": 168224, "text": "`` I never threatened anyone , '' Conway told a gaggle of journalists at the White House .", "label": [], "Comments": []}
{"id": 168225, "text": "We collect a small test set from wikiHow , a website of how-to articles .", "label": [], "Comments": []}
{"id": 168226, "text": "R [ efere ] ( https : //arxiv.org/abs/2107.07170 ) nce [ s ] ( https : //arxiv.org/abs/2107.07170 ) * Systems 33 : Annual Conference on Neural Information Processing Systems 2020 , NeurIPS 2020 , December 6-12 , 2020 , virtual * .", "label": [], "Comments": []}
{"id": 168227, "text": "Training At each time step of pretraining , we take a batch of m examples { ( q 0 i , p i , p− i ) } m =1 , and optimize the cross-entropy loss with respect to the positive passage p + i for each query q 0 i in a contrastive fashion ( i.e. , with in-batch negatives ) , similar to [ Karpukhin et al. , 2020 ] : 3.2 Hybrid Dense-Sparse Retrieval It is well established that the strong lexical matching skills of sparse models such as BM25 [ Robert ] [ son and Zaragoza , 2009 ] are complementary to dense representation models .", "label": [], "Comments": []}
{"id": 168228, "text": "* * Pre-training * * : We set the dimension for a unidirectional LSTM at both word level and sentence level to 200 dimensions and the context vectors required in the word attention to 400 dimensions .", "label": [], "Comments": []}
{"id": 168229, "text": "Important factors are highlighted with red color .", "label": [], "Comments": []}
{"id": 168230, "text": "References [ you need . ] ( https : //proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html ) In * Advances in Neural Information Processing Systems 30 : Annual Conference on Neural Information Processing Systems 2017 , December 4-9 , 2017 , Long Beach , CA , USA * , pages 5998–6008 .", "label": [], "Comments": []}
{"id": 168231, "text": "Finally , researchers have analyzed conversation graphs for topological indicators of abuse [ Papegnies et al. , , 2017 ] .", "label": [], "Comments": []}
{"id": 168232, "text": "We annotate a multimodal product attribute value dataset that contains 87,194 instances , and the experimental results on this dataset demonstrate that explicitly modeling the relationship between attributes and values facilitates our method to establish the correspondence between them , and selectively utilizing visual product information is necessary for the task .", "label": [], "Comments": []}
{"id": 168233, "text": "A classic example of this issue is the sentence `` Thank you '' with its two equally probable German translations `` Danke schön '' and `` Vielen Dank '' [ Gu et al. , , 2018 ] .", "label": [], "Comments": []}
{"id": 168234, "text": "Table 1 : Micro-averaged precision , recall and F1-scores for ATE on the held-out test sets in all domains . 4.2 Aspect Category Classification For the Aspect Category Classification sub-task , a classifier was required that was capable of labeling a large number of classes ( cfr .", "label": [], "Comments": []}
{"id": 168235, "text": "That is because diversity is usually the first thing that comes to human judges ' minds while doing the evaluation .", "label": [], "Comments": []}
{"id": 168236, "text": "Hausa and Mossi have slightly more than 200K parallel sentences , while Fon and Naija have around 30K sentences .", "label": [], "Comments": []}
{"id": 168237, "text": "We evaluate the ScalingUp model to predict the value for each given attribute on our dataset , and the result is unsatisfactory .", "label": [], "Comments": []}
{"id": 168238, "text": "Unlike our approach , they attempted to predict * all * words in the target sentence with a bag-of-words loss function .", "label": [], "Comments": []}
{"id": 168239, "text": "The fact that DEP-PROBE nonetheless identifies syntax-specific relations such as nsubj , and to a lesser degree obj and obl , indicates the presence of contextdependent syntactic information in addition to PoS .", "label": [], "Comments": []}
{"id": 168240, "text": "Oracle is oracle model .", "label": [], "Comments": []}
{"id": 168241, "text": "Here , 0 < γ ≤ 1 is the discount factor to weigh more on earlier rewards .", "label": [], "Comments": []}
{"id": 168242, "text": "Novel pre-training tasks are proposed to capture temporal alignment both locally and globally .", "label": [], "Comments": []}
{"id": 168243, "text": "The majority of them have length less than 15 seconds .", "label": [], "Comments": []}
{"id": 168244, "text": "Summary-level correlations are excluded for QAGS as this dataset does not contain annotations for multiple models , which is necessary to compute this score .", "label": [], "Comments": []}
{"id": 168245, "text": "We found the loss on held out CUSTOMNEWS was still improving at the end of 300K steps , but the overall trends were stable ; to limit the experimentation time we did not explore longer training runs .", "label": [], "Comments": []}
{"id": 168246, "text": "We evaluated the robustness of our model and its ablated versions by training each five times with five different seeds .", "label": [], "Comments": []}
{"id": 168247, "text": "Y denotes the set of candidate labels , which may include a large number of free-form phrases [ Choi et al. , , 2018 ] or ontological labels [ Zhang et al. , , 2017 ] .", "label": [], "Comments": []}
{"id": 168248, "text": "Yes .", "label": [], "Comments": []}
{"id": 168249, "text": "We perform an ablation study to investigate the contribution of individual components .", "label": [], "Comments": []}
{"id": 168250, "text": "One key factor in FL is * communication * , which is defined by the Pull and Push steps in this algorithm .", "label": [], "Comments": []}
{"id": 168251, "text": "Our method maximizes the success of cross-lingual transfer for a parser , trained on English paired data ( EN → LF ) , to accurately generate logical forms from new languages ( X → LF ) .", "label": [], "Comments": []}
{"id": 168252, "text": "However its ability to generalize to the attention of the future tokens remains unknown , since both the distance and the direction need to be taken into consideration .", "label": [], "Comments": []}
{"id": 168253, "text": "Rather than rely only on KILT 's Wikipedia snapshot , KILT-WEB 2 creates SPHERE as a knowledge source .", "label": [], "Comments": []}
{"id": 168254, "text": "The values of distinct-1 and distinct-2 are shown in Table [ 1 . ] The evaluation values are the average of all the generation results of the persona , general eval datasets from each model corresponding to the three types of personas .", "label": [], "Comments": []}
{"id": 168255, "text": "On 33.3 % of the phenomena , both the full-model and the baseline achieve high accuracy showing the possibility that the model exploits artifacts from the hypothesis to reach high accuracy .", "label": [], "Comments": []}
{"id": 168256, "text": "Analyzing Performance on Old Compositions .", "label": [], "Comments": []}
{"id": 168257, "text": "On the German questions , the annotators agreed in 81.38 % of the cases , and α is 0.78 ( N=1,200 ) .", "label": [], "Comments": []}
{"id": 168258, "text": "The average gains are around 10.6 % absolute improvement in terms of We experimented with this approach by replacing its GloVE and ELMo embeddings with XLMR contextual embeddings , however , the performance was suboptimal , and thus conclude that the these two embeddings in the respective languages are crucial for the model 's performance .", "label": [], "Comments": []}
{"id": 168259, "text": "For each bot , we obtain the per-metric rankings ( 1-7 ) first and then we sum up the 7 per-metric rankings for each bot to get their overall scores .", "label": [], "Comments": []}
{"id": 168260, "text": "A.5 Implementation Details We extract 2304-dimensional Slowfast ( Feichtenhofer et al. , 2019 ) features at a fixed frame rate ( TV : 2/3 frame per second , HowTo100M : 1/2 frame per second ) .", "label": [], "Comments": []}
{"id": 168261, "text": "Intuitively , if a passage is ranked high , then some sentences in the passage should be selected as relevant .", "label": [], "Comments": []}
{"id": 168262, "text": "We present a comparison in Appendix [ A . ] Significantly , the iterative refinement in UT is reformed to : h t = h t−1 + f ( h t−1 i ) , where f is the same for every iteration step t , i.e. , a single-layer structure that the parameters are tied throughout the iteration .", "label": [], "Comments": []}
{"id": 168263, "text": ". . , In } , we train function that predicts binary label y i j for all j where i is an index number of an entity and j is an index number of classes .", "label": [], "Comments": []}
{"id": 168264, "text": "Abstract Towards building intelligent dialogue agents , there has been a growing interest in introducing explicit personas in generation models .", "label": [], "Comments": []}
{"id": 168265, "text": "The target domains are news articles ( also from SemEval 2018 Task 6 ) and reports from food security warning systems including the UN World Food Programme and the Famine Early Warning Systems Network .", "label": [], "Comments": []}
{"id": 168266, "text": "For the fourth case , we observe that the annotator sometimes converses based on the partner profile rather than his own traits .", "label": [], "Comments": []}
{"id": 168267, "text": "Effect of Retrieval Quality .", "label": [], "Comments": []}
{"id": 168268, "text": "Hallucination : a response 's factual correctness can not be fully verified from the knowledge-snippet ( even if it is true in the real world ) .", "label": [], "Comments": []}
{"id": 168269, "text": "Reward : 0 ===================== Act : wait Observation : Time passes ...", "label": [], "Comments": []}
{"id": 168270, "text": "Non-English Hate Speech Data English is by far the most common language for hate speech datasets , as recent reviews by [ Vidgen and Derczyn ] [ ski , 2020 ] and [ Poletto et al. , 2021 ] confirm .", "label": [], "Comments": []}
{"id": 168271, "text": "Table 12 : Denotation accuracy for the Overnight dataset compared across reconstruction language ablations .", "label": [], "Comments": []}
{"id": 168272, "text": "Abstract In data-to-text ( D2T ) generation , training on in-domain data leads to overfitting to the data representation and repeating training data noise .", "label": [], "Comments": []}
{"id": 168273, "text": "Label the entity as intrinsic hallucination .", "label": [], "Comments": []}
{"id": 168274, "text": "Figure [ 2 ] presents an overview of our proposed approach , which contains two parts : the weakly-supervised contrastive learning method ( left ) and the prototype-based clustering method ( right ) .", "label": [], "Comments": []}
{"id": 168275, "text": "( a ) The high-level architecture of * GreaseTerminator * .", "label": [], "Comments": []}
{"id": 168276, "text": "We form an action matrix A using h , where the j-th row A corresponds to the j-th sentence ( s ) in D .", "label": [], "Comments": []}
{"id": 168277, "text": "Rank is the average over the rank of the performance on each dataset over the different models ( the lower , the better ) .", "label": [], "Comments": []}
{"id": 168278, "text": "In our work , we mitigate this risk by employing a state-of-the-art AMR parser [ Bevilacqua et al. , , 2021 ] .", "label": [], "Comments": []}
{"id": 168279, "text": "One widely used approach is to compress a long text into a short one , and to reconstruct it to the long text by a cycle consistency loss [ Miao and Blunsom , 2016 ] [ Wang and ] [ Lee , 2018 ] [ Baziotis et al. , , 2019 ] .", "label": [], "Comments": []}
{"id": 168280, "text": "Because Multilingual BART consists of characters from different scripts and because its tokens are not explicitly separated by languages , for our Multilingual BART experiments we consider all tokens that consist exclusively of characters from the target alphabet .", "label": [], "Comments": []}
{"id": 168281, "text": "Furthermore , * k * NN benchmark evaluations place WCD consistently ahead of RWMD and Rel-RWMD , even surpassing WMD on the * BBC Sport * dataset .", "label": [], "Comments": []}
{"id": 168282, "text": "They are provided in Table [ 5 . ] For learning rate and number of epochs , we followed the proposed values by [ Wu et al. , 2020 ] .", "label": [], "Comments": []}
{"id": 168283, "text": "This suggests that it is possible to use our method to optimize any specific target metric , making our method an alternative to reinforcement learning or minimum risk training . ( 2 ) Our model that is trained on one evaluation metric ( e.g .", "label": [], "Comments": []}
{"id": 168284, "text": "They also differ regarding covered languages , from 16 up to more than a hundred .", "label": [], "Comments": []}
{"id": 168285, "text": "This process , known as * exemplification * , occurs in diverse forms , including hypothetical examples , personal anecdotes , and analogies [ Clouse , 2013 ] .", "label": [], "Comments": []}
{"id": 168286, "text": "If the CoM and PM are based on different backbones , we consider them to be unaligned with respect to each other 's output representations .", "label": [], "Comments": []}
{"id": 168287, "text": "For this , a third BERT model is used that receives as input the concatenation of the textual context of the mention and the title and description of the candidate entity .", "label": [], "Comments": []}
{"id": 168288, "text": "`` * in order [ https : //github.com/esmab/ ] ( https : //github.com/esmab/necessity-sufficiency ) [ necessity-sufficiency ] ( https : //github.com/esmab/necessity-sufficiency ) to determine whether the event was a cause of the observed outcome [ Pearl , 2009 ] .", "label": [], "Comments": []}
{"id": 168289, "text": "We plan to study such extensions in future work .", "label": [], "Comments": []}
{"id": 168290, "text": "Second , we filter out sentences , in which the links could not be resolved to Wikidata entities .", "label": [], "Comments": []}
{"id": 168291, "text": "As mentioned in Section [ 4.1.2 , ] li-clust-ent tends to sample nested entity mentions , which may become redundant for annotators to label .", "label": [], "Comments": []}
{"id": 168292, "text": "Abstract While there have been advances in Natural Language Processing ( NLP ) , their success is mainly gained by applying a self-attention mechanism into single or multi-modalities .", "label": [], "Comments": []}
{"id": 168293, "text": "Kmax is a constant value indicating the maximum size of the support set as a whole .", "label": [], "Comments": []}
{"id": 168294, "text": "autoregressive modeling along frequency , we modify the vanilla mel-spectrogram decoder in baseline model to support autoregressive generation along frequency ( which is called FreqAR decoder ) and feed y < f to the FreqAR decoder to model P ( y |y < f ) as shown in Figure [ 4a . ]", "label": [], "Comments": []}
{"id": 168295, "text": "Context related Complications : The following are kinds of complications found when the evidence lies in the wider part of the context .", "label": [], "Comments": []}
{"id": 168296, "text": "While time sampling does not represent a viable extension for evaluating timesensitive embedding models , it allows examining the significance of temporal facts for individual benchmark datasets .", "label": [], "Comments": []}
{"id": 168297, "text": "For the summary of training setups , please see Table [ 10 ] and [ 12 . ] Fine-tuning Setup In the following three paragraphs , we explain the setting of fine-tuning for QA , NER , and generative QA tasks .", "label": [], "Comments": []}
{"id": 168298, "text": "In contrast , our approach can operate on monolingual language models fully pre-trained in total isolation from the source language encoder .", "label": [], "Comments": []}
{"id": 168299, "text": "In our method , the augmented distilled samples are encouraged to have different semantics with the original ones and improve the data diversity , and thus continue to get improvements on the strong pretrained GPT2 .", "label": [], "Comments": []}
{"id": 168300, "text": "B Experimental Specification B.1 Detailed Data Preprocessing We perform minimal preprocessing on the textual input : differently than in the BerTweet paper [ Nguyen et al. , , 2020 ] , we perform only URL normalization and lowercasing .", "label": [], "Comments": []}
{"id": 168301, "text": "During trainin [ g we limit the size ] of Repisode to 1 .", "label": [], "Comments": []}
{"id": 168302, "text": "Our findings support our claim that latent cross-lingual similarity can be improved using auxiliary objectives and we specifically identify that a combination of approaches yields superior parsing .", "label": [], "Comments": []}
{"id": 168303, "text": "Zooming in on the performance over individual characters , we observe that , relative to the control task , some English characters consistently perform much better when using syntactic features .", "label": [], "Comments": []}
{"id": 168304, "text": "The vertical dashed and dotted lines show updated sample-specific difficulty thresholds for easy and hard samples respectively . tively utilizing local sample-level information to determine difficulty .", "label": [], "Comments": []}
{"id": 168305, "text": "( b ) Without DCNN on the abstract channel .", "label": [], "Comments": []}
{"id": 168306, "text": "Dialogue for each video clip is also provided .", "label": [], "Comments": []}
{"id": 168307, "text": "On the other hand , next sentence prediction ( NSP ) determines whether one sentence follows another one in the same document .", "label": [], "Comments": []}
{"id": 168308, "text": "2015 .", "label": [], "Comments": []}
{"id": 168309, "text": "How about we talk about X ?", "label": [], "Comments": []}
{"id": 168310, "text": "Based on these findings , we propose 5393 a new probe design that encourages the probe to use all relevant representations of syntax in model embeddings .", "label": [], "Comments": []}
{"id": 168311, "text": "Our pretraining scheme directly controls for term overlap across pseudo queries and relevant passages , thus allowing to model both lexical and semantic relations between them .", "label": [], "Comments": []}
{"id": 168312, "text": "Learning rate was linearly decayed ( over a max epoch of 8 ) with 100 warm-up steps .", "label": [], "Comments": []}
{"id": 168313, "text": "The performance drop on both sides shows the forgetting effect . ( Right ) F1 scores on TEMPLAMA grouped by the number of years for which the answer to a query persists .", "label": [], "Comments": []}
{"id": 168314, "text": "Our work has a focus on building a visuallysupervised language pre-training frameworks to improve general language understanding .", "label": [], "Comments": []}
{"id": 168315, "text": "See Table [ 4 ] for statistics and classifier accuracy .", "label": [], "Comments": []}
{"id": 168316, "text": "We restrict the vocabulary to word types that occur at least ten times in StreetEasy and in the multimodal Wikipedia dataset .", "label": [], "Comments": []}
{"id": 168317, "text": "* * 110 * * [ 2020 ] proposes a joint model to tackle commu- * * 111 * * nity question answering such that the model can * * 112 * * simultaneously select the set of correct answers * * 113 * * from candidates and generate an abstractive sum- * * 114 * * mary for each selected answer .", "label": [], "Comments": []}
{"id": 168318, "text": "We observe that the largest DEMIX LM achieves competitive results with the GPT-3 * Da-Vinci * result with a fraction of the computation , and gives large performance boosts on this benchmark over our other DENSE baselines .", "label": [], "Comments": []}
{"id": 168319, "text": "From Figure [ 2b , ] we observe that hypothesis-only baseline performs poorly on a majority of the phenomena .", "label": [], "Comments": []}
{"id": 168320, "text": "Compared with training an autoregressive model from search [ Li et al. , , 2020 ] , non-autoregressive generation predicts all the words in parallel , further improving inference efficiency by several times .", "label": [], "Comments": []}
{"id": 168321, "text": "In this section , we want to test how these two hyper-parameters affect the final translation performance and how they work with each other .", "label": [], "Comments": []}
{"id": 168322, "text": "HERB+SSAN sorts documents by HERB scores for high coverage and then lets SSAN process them in this order .", "label": [], "Comments": []}
{"id": 168323, "text": "Formally , a prompt consists of a template function Tprompt ( · ) that converts the input x to a prompt input xprompt = Tprompt ( x ) , and a set of Figure 2 : Comparison of different fine-tuning methods for NER .", "label": [], "Comments": []}
{"id": 168324, "text": "Sports news use bellicose language similar to that of conflict stories with words such as attack , shoot , and defeat , thus presenting a classification challenge .", "label": [], "Comments": []}
{"id": 168325, "text": "The remaining approaches just predict the correctness of statements but never concern about generating correct programs .", "label": [], "Comments": []}
{"id": 168326, "text": "This can then be used to formulate a maximum likelihood training objective with cross-entropy for each individual Figure 3 : An overview of RL training for the agent .", "label": [], "Comments": []}
{"id": 168327, "text": "We conduct MOS evaluation and compute Var to compare different modeling methods including MAE ( denoted as * MAE * ) , Laplacian mixture loss ( denoted as * LM * ) , structural similarity index ( denoted as * SSIM * ) , generative adversarial network ( denoted as * GAN * ) , Glow ( denoted as * Glow * ) .", "label": [], "Comments": []}
{"id": 168328, "text": "Had the interlocutor been introverted , the response could have been different .", "label": [], "Comments": []}
{"id": 168329, "text": "It is motivated by FFNs 's analogy with the self-attention mechanism ( as described in Section [ 2 ] , because self-attention scores are usually used as a strong attribution baseline [ Kovaleva et al. , 2019 ] [ Voita et al. , , 2019 ] [ Hao et al. , , 2021 ] .", "label": [], "Comments": []}
{"id": 168330, "text": "The outputs of these unconditional discriminators are then concatenated and linearly projected to form the output .", "label": [], "Comments": []}
{"id": 168331, "text": "Each entry in the data has the following fields : * DateTime , Open , High , Low , Close , Volume * . * DateTime * is in US Eastern Time , in the format YY-MM-DD h : m : s .", "label": [], "Comments": []}
{"id": 168332, "text": "The hidden representation h is structurally relevant to hi if h 's associated RST node is an ancestor of the RST node associated to h .", "label": [], "Comments": []}
{"id": 168333, "text": "Abstract Bash is a Unix command language used for interacting with the Operating System .", "label": [], "Comments": []}
{"id": 168334, "text": "It 's unbearable to listen to .", "label": [], "Comments": []}
{"id": 168335, "text": "The reason we choose to change the size of the training data is that data sizes often have a large influence on a model 's generalization ability , which could help reveal relevant affecting factors .", "label": [], "Comments": []}
{"id": 168336, "text": "Data curriculum .", "label": [], "Comments": []}
{"id": 168337, "text": "In this work , we further investigate the marriage between semisupervised learning and a pre-trained language model .", "label": [], "Comments": []}
{"id": 168338, "text": "Specifically , with probability β , each position of sentence y 2 j can be noised by either removed directly , inserted or substituted with any other words in the dictionary Wb , which is constructed from the corpus Y 2 .", "label": [], "Comments": []}
{"id": 168339, "text": "6.6 Scalability of DL-MNAV Although our methods show improvements over the proposed baseline in both tasks the results are currently severely lacking , especially compared to the state-of-the-art * supervised * learning approaches on both data sets ( 65.92 % F for DocRED ( Xu et al. , 2021 ) and 52.0 % F for sciERC ( Ye et al. , 2022 ) ) .", "label": [], "Comments": []}
{"id": 168340, "text": "Therefore , variable episode construction is not utilized in this step .", "label": [], "Comments": []}
{"id": 168341, "text": "Comparing the supervised baselines VGVAE and SynPG greatly differ in scores .", "label": [], "Comments": []}
{"id": 168342, "text": "However , in a bot-bot setting , two bots , including the same bot in the previous conversation , start explaining their hobbies to each other , producing a more interesting conversation .", "label": [], "Comments": []}
{"id": 168343, "text": "To address these two issues , we propose a multistage fusion network with the fusion forget gate module , which builds upon this approach by modeling fine-grained interactions between the multisource modalities through a multistep fusion schema and controlling the flow of redundant information between multimodal long sequences via a forgetting module .", "label": [], "Comments": []}
{"id": 168344, "text": "If the underlying distribution is multimodal with wide spread , the learned distribution would be significantly underfitting with a mean far from the modes .", "label": [], "Comments": []}
{"id": 168345, "text": "6 Conclusion We propose a pre-training network architecture with three objectives , which can incorporate intraspan and inter-span information into pre-trained models .", "label": [], "Comments": []}
{"id": 168346, "text": "Formally , a semantic role is assigned to each argument of a predicate in a sentence .", "label": [], "Comments": []}
{"id": 168347, "text": "Empathy Quotient Questionnaire The short form of Empathy Quotient ( EQ ) questionnaire [ Wakabayashi et al. , , 2006 ] was used to measure empathy ( details are in appendix [ A ] .", "label": [], "Comments": []}
{"id": 168348, "text": "We describe how to derive such edits in detail in Section [ 6.2 . ] We can see that the number of edit is tightly correlated with sentence length .", "label": [], "Comments": []}
{"id": 168349, "text": "Secondly , since edit distance only utilizes three operations , i.e. , removal , insertion , or substitution , it is easier to mimic these operations in the process of generating the final aligned examples ( we leave the explanation in the next subsection ) .", "label": [], "Comments": []}
{"id": 168350, "text": "Table 10 : Proverb prediction accuracy in MCQ setting . 5.2 Narrative Generation 5.2.1 Task details One of the important use-cases for NLP models in the creative writing domain is to use these models to generate content .", "label": [], "Comments": []}
{"id": 168351, "text": "[ github.com/chujiezheng/DiffKS ] ( github .", "label": [], "Comments": []}
{"id": 168352, "text": "according to the wall street journal , the allegations were made in a text message between a us politician and an associate of trump lawyer rudy giuliani .", "label": [], "Comments": []}
{"id": 168353, "text": "We will take advantage of the speaker information into both extraction and dialogue understanding models as our future work . 6 Conclusion As far as we know , we are the first work bringing the dependency information of dialogues into the multi-turn response selection task .", "label": [], "Comments": []}
{"id": 168354, "text": "Since there can be multiple combinations to create these feature set , we carefully chose five features that shows the best performance empirically : * director * , * producer * , * writer * , * cinematographer * , and * art director * .", "label": [], "Comments": []}
{"id": 168355, "text": "Data diversification .", "label": [], "Comments": []}
{"id": 168356, "text": "To train AfriMT5 and ByT5 , we start with MT5 and ByT5 .", "label": [], "Comments": []}
{"id": 168357, "text": "Figure 5 : The relaiton F1 score ( exactly match ) with respect to the number of entities and the sentence length for each sentence on ACE05 test data .", "label": [], "Comments": []}
{"id": 168358, "text": "The usual pipeline consists of finetuning BERT by adapting and retraining its classification head to meet the requirements of a specific NLP task .", "label": [], "Comments": []}
{"id": 168359, "text": "Even though we do not explicitly condition on f ( c ) = y ′ , Ds ( x ) is biased towards such c because the interventions are conditioned on y ′ .", "label": [], "Comments": []}
{"id": 168360, "text": "For example , for BMO ( from the show Adventure Time ) response generated by our method mentions terms such as `` core system drivers '' and `` MO Factory '' that are relevant to the fact that BMO is an animated video game console in the show .", "label": [], "Comments": []}
{"id": 168361, "text": "Due to these biases , we can not guarantee that annotators ' guesses always match reality .", "label": [], "Comments": []}
{"id": 168362, "text": "To train the shared discrete embedding space , we warm-started from the baseline model with a learning rate of 1e-5 .", "label": [], "Comments": []}
{"id": 168363, "text": "Rowling ) , its relevant knowledge tokens ( e.g. , British ) are likely to receive high probabilities and be selected in the following steps ( see the J.K Rowling example in Fig [ 1 ( b ) . ]", "label": [], "Comments": []}
{"id": 168364, "text": "Results of other baselines are from [ Cheng et al. , 2022 ] .", "label": [], "Comments": []}
{"id": 168365, "text": "We train a model from this self-supervision in a contrastive fashion .", "label": [], "Comments": []}
{"id": 168366, "text": "Kliment Ohridski '' , Bulgaria Checkstep , UK , University of Massachusetts , Amherst , University of Copenhagen , Denmark , smsarwar @ amazon.com , { didi , momchil , yoan.dinkov , isabelle , preslav.nakov } @ checkstep.com Abstract [ ( e.g. , the U ] ( mailto : smsarwar @ amazon.com ) .S .", "label": [], "Comments": []}
{"id": 168367, "text": "This conceptualization of definition modeling is an important starting point for addressing our task .", "label": [], "Comments": []}
{"id": 168368, "text": "We weight the retrieved KB facts by multiplying the initial candidate entity score for the subject entity , the predicted score for the relation , and the initial candidate entity score for the object entity .", "label": [], "Comments": []}
{"id": 168369, "text": "Table-to-text generation seeks to generate textual descriptions for tabular data .", "label": [], "Comments": []}
{"id": 168370, "text": "This work was completed in partial fulfillment for the Ph.D degree of Ohad Rubin .", "label": [], "Comments": []}
{"id": 168371, "text": "Given a sample x , we create a set S of increasingly diverse samples in the dataset relative to x using an operation labeled SSET and thresholds τ and τc : where τ = T·max ( Mi ) at each step of training , and T ∈ ( 0 , 1 ) is a hyperparameter . τ helps in sampling the most diverse samples relative to x .", "label": [], "Comments": []}
{"id": 168372, "text": "We have 142 unique tasks in total , covering a variety of problems including text classification , question answering ( QA ) , natural language inference ( NLI ) and paraphrase detection .", "label": [], "Comments": []}
{"id": 168373, "text": "As depicted in [ Table 2 , ] our best [ VLCN ] model achieves an overall accuracy of 36.01 % on MSRVTT-QA – the second best performance after CoMVT which achieves 37.30 % accuracy when trained from scratch and 39.50 % after pretaining on HowToFUP .", "label": [], "Comments": []}
{"id": 168374, "text": "We tackle this objective by utilizing a * Graph Neural Network ( GNN ) * which learns feature representations of each node using a neighborhood aggregation scheme [ Hamilton et al. , , 2017 ] , as follows : where N ( e ; G ) is a set of neighboring entities of the entity e , AGG is the function that aggregates embeddings of neighboring entities of e , and UPDATE is the function that updates the representation of e with the aggregated messages from AGG .", "label": [], "Comments": []}
{"id": 168375, "text": "This observation shows that , regardless of the properties of NLP models , the nature of many tasks changes over time , if only because the output distribution changes .", "label": [], "Comments": []}
{"id": 168376, "text": "In many real-world scenarios , obtaining even a couple of hundred of labeled examples per class is challenging .", "label": [], "Comments": []}
{"id": 168377, "text": "Next , we meta-train the MT 0-shot baseline and MetaICL where , for each iteration of meta-training , we similarly map label words with random words .", "label": [], "Comments": []}
{"id": 168378, "text": "Regarding a concrete example , please see format ( a ) in Figure [ 6 . ] In the example , `` Text '' indicates the entity mention within the input x , the `` start '' and `` end '' indicates its mention position denoted as ( mα , m ) , and `` id '' indicates the wikidata id for the entity identification used in the next step .", "label": [], "Comments": []}
{"id": 168379, "text": "References Appendices A Neural Architecture for Fine-tuning [ Figure 4 ] displays our neural architecture adopted in the fine-tuning step .", "label": [], "Comments": []}
{"id": 168380, "text": "The model shows competitive performance in a zero-shot setting , suggesting it is helpful in the context of lowresource languages .", "label": [], "Comments": []}
{"id": 168381, "text": "The discourse structure of the output text y is controlled by R and R .", "label": [], "Comments": []}
{"id": 168382, "text": "For hybrid retrieval , we set k 0 = 1000 similar to [ Ma et al. , 2021 ] . 5 Results Our experiments show that Spider significantly improves performance in the challenging unsupervised retrieval setting , even outperforming strong supervised models in many cases .", "label": [], "Comments": []}
{"id": 168383, "text": "Acknowledgements T [ his work was partially supported by the Ger ] ( https : //arxiv.org/abs/1706.02677v1 ) m [ an Fe ] ( https : //arxiv.org/abs/1706.02677v1 ) deral Ministry of Education and Research ( BMBF ) as part of the pr [ oject IIDI ( 01IS21026D ] ( https : //doi.org/10.1109/ACCESS.2020.2996642 ) a [ nd the Smart Data Innovation Lab as part of the ] ( https : //doi.org/10.1109/ACCESS.2020.2996642 ) S [ mart Data Innovati ] ( https : //doi.org/10.1109/ACCESS.2020.2996642 ) on Challenges ( 01IS19030A ) .", "label": [], "Comments": []}
{"id": 168384, "text": "Wikinews-7 For the purpose of testing a model 5.1 Performance Evaluation on languages unseen during training , we extract mention-entity pairs from Wikinews in 7 languages that are not in the Mewsli-9 language set.4 Table 9 in Appendix A.3 reports statistics of this dataset .", "label": [], "Comments": []}
{"id": 168385, "text": "BART-aug+RL Summary : It is possible that the old flapper is stuck in the outflow pipe and is causing the leak .", "label": [], "Comments": []}
{"id": 168386, "text": "The retrieve step , where efficiency is paramount , is again executed by the BE sub-model , and the precision-oriented rerank step is conducted via the CE sub-model .", "label": [], "Comments": []}
{"id": 168387, "text": "PUBCLS We carry out additional postprocessing and ensure that each of the three labels ( Fox News , New York Times , and Washington Post ) have an equal distribution across years .", "label": [], "Comments": []}
{"id": 168388, "text": "Data Filtering We selected question threads for annotation from the StackExchange data release [ 2 ] , as it is publicly available and has been shared using a Creative Commons ShareAlike license .", "label": [], "Comments": []}
{"id": 168389, "text": "Results .", "label": [], "Comments": []}
{"id": 168390, "text": "Nicholas Tomlin is supported by the National Science Foundation Graduate Research Fellowship .", "label": [], "Comments": []}
{"id": 168391, "text": "Table 9 : Dataset statistics for entity disambiguation datasets .", "label": [], "Comments": []}
{"id": 168392, "text": "A 256-length context is used at evaluation time . † numbers are due to [ Kasai et al. , 2021b ] .", "label": [], "Comments": []}
{"id": 168393, "text": "In preliminary experiments , we initially use newline characters as appeared in [ Brown et al. , 2020 ] 's Appendix G .", "label": [], "Comments": []}
{"id": 168394, "text": "Based on this speculated inflation of MT performance due to translationese in the test set , further work inspects what previous conclusions a [ bout the ] [ effect ] i [ veness ] of MT models should be recalibrated .", "label": [], "Comments": []}
{"id": 168395, "text": "However , for the opposite dire [ ctio ] n , we achieve considerable performance gains with the JOINT+ COOPOSCAR variant .", "label": [], "Comments": []}
{"id": 168396, "text": "Groundtruth is highlighted with the green bar under the video frames .", "label": [], "Comments": []}
{"id": 168397, "text": "Note that entire images represent nodes , not segments of images .", "label": [], "Comments": []}
{"id": 168398, "text": "In the experiments , we confirm that WORDTOUR provides high-quality embeddings via qualitative comparison , user studies , and document classification .", "label": [], "Comments": []}
{"id": 168399, "text": "Additionally , the marking scheme is limited in its expressiveness as it is hard to mark missing information in the answer .", "label": [], "Comments": []}
{"id": 168400, "text": "To answer this question , we conduct a retrieval experiment , * i.e . * finding the nearest ( smallest cosine similarity ) transcript based on the speech representation .", "label": [], "Comments": []}
{"id": 168401, "text": "Abstract Pre-trained language models ( PLMs ) have achieved remarkable success on various natural language understanding tasks .", "label": [], "Comments": []}
{"id": 168402, "text": "For example , * if your wife is a stay at home wife and your a business man making lots of money , the reasoning is that you would not have the same amount of time to dedicate to work if you had to look after your own house and/or children .", "label": [], "Comments": []}
{"id": 168403, "text": "DrBoost is trained in stages : each component model is learned sequentially and * specialized * by focusing only on retrieval mistakes made by the current ensemble .", "label": [], "Comments": []}
{"id": 168404, "text": "This work was supported by National Key Research and Development Project ( No . 2020AAA0109302 ) , Shanghai Science and Technology Innovation Ac- tion Plan ( No.19511120400 ) and Shanghai Municipal Science and Technology Major Project ( No.2021SHZDZX0103 ) .", "label": [], "Comments": []}
{"id": 168405, "text": "To make a fair comparison with other modeling methods , we only take the output hidden states of the encoder as the condition to each step of flows and use spherical Disc3 H'Ph Disc2 Disc1 HPh H'Ph H'Ph Figure 8 : The architecture of multiple random window discriminators .", "label": [], "Comments": []}
{"id": 168406, "text": "[ 4 ] We remedy this in the final stage of the BCS with local search ( LS ) , where we take a `` secondpass '' through the puzzle and score alternate proposals that are a small edit distance away from the BP solution .", "label": [], "Comments": []}
{"id": 168407, "text": "The total number of model parameters is 249M .", "label": [], "Comments": []}
{"id": 168408, "text": "All images are shaped to a size of 608 × 608 .", "label": [], "Comments": []}
{"id": 168409, "text": "B StreetEasy dataset preprocessing The dataset consists of 29,347 English-language real estate listings from the StreetEasy website from June 2019 .", "label": [], "Comments": []}
{"id": 168410, "text": "Note that unlike our approach , ICT is trained to produce representations to * corrupted * passages .", "label": [], "Comments": []}
{"id": 168411, "text": "II .", "label": [], "Comments": []}
{"id": 168412, "text": "indicates if the item has been shown to correlate positively or negatively to the respective concept .", "label": [], "Comments": []}
{"id": 168413, "text": "Among all class prototypes , the O-type serves as negative examples and covers all the miscellaneous spans that are not entities , which poses new challenges to identify the entity boundaries .", "label": [], "Comments": []}
{"id": 168414, "text": "Quantitatively , we find the average cosine distance between the sentence-mean of parallel utterances reduces from 0.58 to 0.47 .", "label": [], "Comments": []}
{"id": 168415, "text": "Self-training has been applied to a variety of domain adaptation scenarios [ Ruder ] [ and Plank , 2018 ] [ Yu et al. , , 2015 ] [ Cui and Bollegala , 2019 ] , but always with the assumption that the original labeled data L is available at each iteration .", "label": [], "Comments": []}
{"id": 168416, "text": "Performance on MSRVTT-QA should thus benefit from the knowledge acquired while training on MSVD-QA [ Pan and Yang , 2010 ] .", "label": [], "Comments": []}
{"id": 168417, "text": "Trigger Masking ( TM ) Baseline In this baseline , we consider masking the adversarial trigger tokens .", "label": [], "Comments": []}
{"id": 168418, "text": "This is due to the inability to resolve ambiguous cases , which highlights the difficulty of the task , and that gazetteers alone lead to noisy NER .", "label": [], "Comments": []}
{"id": 168419, "text": "However , the correlation of malevolence labels in different groups is not well captured .", "label": [], "Comments": []}
{"id": 168420, "text": "Table 2 : Feature windows of different models . i ( j ) is the index of current character ( word ) . an alternative for performance improvement other than designing better models [ Yang et al. , , 2017 ] .", "label": [], "Comments": []}
{"id": 168421, "text": "We calculate the inter-annotator agreement ( IAA ) score before and after adjudication using Cohen 's kappa [ Cohen , 1960 ] .", "label": [], "Comments": []}
{"id": 168422, "text": "In this paper , we propose a multimodal method to jointly predict product attributes and extract values from textual product descriptions with the help of the product images .", "label": [], "Comments": []}
{"id": 168423, "text": "The initial state of the proof is only the representation of the question hQ , then the rest of the reasoning path will be constructed based on it .", "label": [], "Comments": []}
{"id": 168424, "text": "Various experiments demonstrate the effectiveness of our method .", "label": [], "Comments": []}
{"id": 168425, "text": "Our model can give predictions for different data items formatted as REF , SRC , or SRC+REF setting , unifying all evaluation tasks into one single model without additional modifications .", "label": [], "Comments": []}
{"id": 168426, "text": "We performed paired t-tests to compare BERTIT : CLUST with BERT and BERTIT : MLM , pooling together all datasets and repetitions for a given Table 2 : BERTIT : CLUST outperforms BERT in topical datasets .", "label": [], "Comments": []}
{"id": 168427, "text": "One such approach clustered topically-related sentences , after which cluster properties were leveraged for rating sentence salience [ Radev et al. , , 2004 ] [ Wang et al. , 2008 ] [ Wan and Yang , 2008 ] .", "label": [], "Comments": []}
{"id": 168428, "text": "Aligned spans are shown with matching colors and indicate correspondences in roles between proverbs and narratives .", "label": [], "Comments": []}
{"id": 168429, "text": "Figure adapted from [ Tenney et al. , 2019 ] .", "label": [], "Comments": []}
{"id": 168430, "text": "We note that the distribution of labels is skewed ( e.g. , there are disproportionately very few samples labeled as `` surprise '' ) .", "label": [], "Comments": []}
{"id": 168431, "text": "Here , we explore the task of identifying narrative analogy by modeling 'similarity ' between narratives based Table 11 : Automatic evaluation for narrative generation on 'seen ' and 'unseen ' proverbs using 'base ' versions of LLMs .", "label": [], "Comments": []}
{"id": 168432, "text": "I Scientific Artifacts License .", "label": [], "Comments": []}
{"id": 168433, "text": "2 Personal Attribute Tasks Based on the usefulness of personal attributes for dialogue personalization , we propose the task of obtaining personal attributes from natural language sentences .", "label": [], "Comments": []}
{"id": 168434, "text": "Another problem arises if we consider that examples with novel labels can be added to the source dataset .", "label": [], "Comments": []}
{"id": 168435, "text": "Previous work has shown that induced sparsity can improve both generalization [ Zhao et al. , , 2021 ] and model interpretability [ Wong et al. , , 2021 ] .", "label": [], "Comments": []}
{"id": 168436, "text": "Recently , pre-trained models [ Devlin et al. , , 2018 ] [ Dong et al. , , 2019 ] have substantially advanced a variety of NLP tasks , including entity relation extraction [ Li et al. , , 2019 ] [ Wadden et al. , , 2019 ] .", "label": [], "Comments": []}
{"id": 168437, "text": "We evaluate the performance of baselines with 3 sampled instances .", "label": [], "Comments": []}
{"id": 168438, "text": "Additionally , we propose [ a novel ] setting extracted from Wikinews2 where we train [ a ] [ mo ] d [ el ] [ on ] [ a ] [ set ] [ of ] [ languag ] es , and we tes [ t ] [ it ] [ on ] [ unseen ] [ ones ] .", "label": [], "Comments": []}
{"id": 168439, "text": "Smith . 2021 .", "label": [], "Comments": []}
{"id": 168440, "text": "This extra annotation layer of the YASO evaluation data is available online ( see [ §1 ] .", "label": [], "Comments": []}
{"id": 168441, "text": "HERB takes about 2 seconds , on average , to p [ ro- ] cess one document , whereas SSAN requires 13.6 seconds—a factor of 6.8 higher in speed and resource consumption .", "label": [], "Comments": []}
{"id": 168442, "text": "Recently , pretrained models like BERT have offered an attractive choice of teacher model , used successfully for a variety of tasks such as sentiment classification and paraphrasing [ Tang et al. , ] [ 2019a , ] [ b ] .", "label": [], "Comments": []}
{"id": 168443, "text": "2 Related Work 2.1 Event Argument Extraction Most previous event argument extraction models make predictions at sentence-level [ Nguyen et al. , , 2016 ] [ Liu et al. , , 2018 ] [ Yang et al. , , 2019b ] [ Du ] [ and Cardie , 2020b ] [ Wei et al. , , 2021 ] [ Wang et al. , , 2021 ] [ Dutta et al. , , 2021 ] .", "label": [], "Comments": []}
{"id": 168444, "text": "Each fact is surrounded by special tokens denoting the beginning ( < s > ) and the end ( < /s > ) of the fact .", "label": [], "Comments": []}
{"id": 168445, "text": "The reviews were initially selected at random , and then some reviews were removed by two conditions : reviews that are rated as * not useful * ( with useful=0 ) and reviews of businesses with no business categories .", "label": [], "Comments": []}
{"id": 168446, "text": "tag ( PoS ; e.g. , for `` Jane '' , NNP for a proper noun ) , Coarse-Grained Part of Speech tag ( Coarse-grained PoS ; e.g. , for `` Jane '' , PROPN for proper noun ) , and a Named Entity Recognition tag ( NER ; e.g. , for `` Jane '' , PERSON for a personal name ) .", "label": [], "Comments": []}
{"id": 168447, "text": "Therefore , modulation is model-agnostic and similarly allows for a parameter-efficient extension of static embedding models . 2.3 Regularization-based extensions Regularization-based extensions include techniques that impose consistency constraints on learnable feature representations .", "label": [], "Comments": []}
{"id": 168448, "text": "We train with 500 warm-up steps and 20,000 total steps and pick the model with the best label-smoothed cross-entropy [ Szegedy et al. , , 2016 ] validation loss .", "label": [], "Comments": []}
{"id": 168449, "text": "Application of Document Graphs .", "label": [], "Comments": []}
{"id": 168450, "text": "1 % from the original training set while fixing the validation and test sets .", "label": [], "Comments": []}
{"id": 168451, "text": "In particular , given an event chain which is a series of events , this task requires a reasoning system to distinguish the next event from a small set of randomly drawn events .", "label": [], "Comments": []}
{"id": 168452, "text": "[ 2018 , ] p. 157 ) , only 1 out of the 4 teams participating in the task integrated images into their model , by training a CNN on Spanish and Catalan flags ( with the underlying intuition that using them would hint to the user 's stance with respect to the topic of Catalan independence ) [ 3 ] .", "label": [], "Comments": []}
{"id": 168453, "text": "Best viewed in bold . corresponding outputs are 3,072 , 1,024 , and 1 , respectively .", "label": [], "Comments": []}
{"id": 168454, "text": "It includes texts from social media , news sites , blogs and forums .", "label": [], "Comments": []}
{"id": 168455, "text": "Matrices are denoted by bold , uppercase letters such as X .", "label": [], "Comments": []}
{"id": 168456, "text": "We pre-trained on an extended version of the cc100 ( Conneau et al. , 2020 ; Wenzek et al. , 2020 ) c [ orpora ] [ available ] [ he ] re7 where we increased the number of common crawl snapshots for low resource languages from 12 to 60 .", "label": [], "Comments": []}
{"id": 168457, "text": "SIM ( x , y ) measures the similarity of texts x and y , and D is a fully concatenated version of document set D .", "label": [], "Comments": []}
{"id": 168458, "text": "While this increases the difficulty of the challenge , this also resembles a more realistic application of few-shot relation extraction methods : A key motivation for few-shot learning is to develop methods which can be applied to new data without the need for large-scale manual annotation .", "label": [], "Comments": []}
{"id": 168459, "text": "We also evaluate the accuracy of our aggregation model , using triples ordered according to the plans from [ Ferreira et al. , 2018 ] as input .", "label": [], "Comments": []}
{"id": 168460, "text": "For instance , the hospital admission information and diagnosis report can be processed by language models to predict the readmission rate of a patient [ Lehman et al. , , 2021 ] .", "label": [], "Comments": []}
{"id": 168461, "text": "Reward : 0 ===================== Act : push button Observation : You ca n't see any button here !", "label": [], "Comments": []}
{"id": 168462, "text": "To aid Claudius politically , young Nero was adopted in 50 and took the name Nero Claudius Caesar Drusus Germanicus ( see adoption in Rome ) .", "label": [], "Comments": []}
{"id": 168463, "text": "Equally contributed . to choose results from different tactics using macro-level features .", "label": [], "Comments": []}
{"id": 168464, "text": "We introduce a new method for text classification on encrypted data .", "label": [], "Comments": []}
{"id": 168465, "text": "However , the direct use of Gaussian sketching matrix , i.e . the approximation D−1ASSTV [ Wang et al. , ] [ 2020b , ] Eqn . ( 5 ) ) , requires the computation of the whole matrix A .", "label": [], "Comments": []}
{"id": 168466, "text": "For each item the information is available if it is correlated positively or negatively with the concept to be measured .", "label": [], "Comments": []}
{"id": 168467, "text": "Table 5 : The example spans from QBCOREF documents that are sampled with each active learning strategy .", "label": [], "Comments": []}
{"id": 168468, "text": "Style classifier for each task is trained using the same codebase and hyperparameters as in training the character style classifier in the HLA-chat dataset .", "label": [], "Comments": []}
{"id": 168469, "text": "Most researchers actually seek help from joint learning , extra learning resources including dictionaries , pre-trained embedding , deeper information extracted from training set and so on .", "label": [], "Comments": []}
{"id": 168470, "text": "This will serve as a baseline for other approaches .", "label": [], "Comments": []}
{"id": 168471, "text": "But since only discrete tokens are used to represent video frames , rich video frame features are not fully utilized .", "label": [], "Comments": []}
{"id": 168472, "text": "We compare the pinyin of the incorrect tokens of X with their https : //github.com/mozillazg/python-pinyin Figure 3 : The process of generating training samples .", "label": [], "Comments": []}
{"id": 168473, "text": "C.6 WikiAnn LOC , ORG , PER .", "label": [], "Comments": []}
{"id": 168474, "text": ", on Microsoft Research Paraphrase Corpus ( MRPC ) [ Dolan and Brockett , 2005 ] Quora Question Pairs ( QQP ) [ Chen et al. , , 2018 ] for Paraphrase Similarity Matching , Multi-Genre Natural Language Inference ( MNLI ) [ Williams et al. , , 2017 ] , and Recognizing Textual Entailment ( RTE ) [ Bentivogli et al. , , 2009 ] for Natural Language inference .", "label": [], "Comments": []}
{"id": 168475, "text": "Datasets and the task ontology are taken from CROSSFIT [ Ye et al. , , 2021 ] and UNIFIEDQA [ Khashabi et al. , , 2020 ] .", "label": [], "Comments": []}
{"id": 168476, "text": "Formally , this means choosing the set of types J as Our results in [ §6 ] focus on KM and GMM , as we observe that k-medoids , spherical KM and von Mises-Fisher tend to perform worse than KM and GMM ( see App . [ A , ] App . [ B ] .", "label": [], "Comments": []}
{"id": 168477, "text": "While the overall layer-wise trends of multilingual models look similar to some monolingual models ( mBERT vs .", "label": [], "Comments": []}
{"id": 168478, "text": "[ Chen et al. , 2018 ] combined federated learning with meta learning for the recommendation .", "label": [], "Comments": []}
{"id": 168479, "text": "In this way , we model an equilibrium [ Bai et al. , , 2019 ] [ Lan et al. , , 2020 ] between two representations of the same token from consecutive steps .", "label": [], "Comments": []}
{"id": 168480, "text": "By removing the external memory of the [ FLF ] block and using a plain [ LSTM ] network , [ VLCN+ ] LSTM falls behind on all question length bins resulting in an overall accuracy decrease of 0.84 % and 0.8 % on MSVD-QA and MSRVTT-QA , respectively , compared to [ VLCN ] ( see Tables [ 3 ] and [ 4 ] .", "label": [], "Comments": []}
{"id": 168481, "text": "For question ( 2 ) , we perform a case study where we analyze DPR 's individual encoders under a data efficiency setting .", "label": [], "Comments": []}
{"id": 168482, "text": "Then in a similar way to how L O h , t , r is generated with SAIS All , we design two types of evidencebased data augmentation as follows : the pipeline and get the confidence L M h , t , r .", "label": [], "Comments": []}
{"id": 168483, "text": "For this reason , hypothetically , the downstream performance of IMP is also better than OMP .", "label": [], "Comments": []}
{"id": 168484, "text": "DIRPROBE [ Kulmizev et al. , , 2020 ] is reimplemented based on the authors ' algorithm description and uses their reported hyperparameters .", "label": [], "Comments": []}
{"id": 168485, "text": "EDA augments each input sentence to produce 8-16 new sentences .", "label": [], "Comments": []}
{"id": 168486, "text": "The document comes from British Broadcasting Corporation ( BBC ) online articles .", "label": [], "Comments": []}
{"id": 168487, "text": "HATECHECK 's English test cases provide a starting point for MHC , but experts were encouraged to creatively adapt cases rather than providing literal translations , so as to retain relevance and realism .", "label": [], "Comments": []}
{"id": 168488, "text": "The initial bilingual word pairs were constructed without supervision by exploiting similarity matrices of each language .", "label": [], "Comments": []}
{"id": 168489, "text": "Downstream Fine-Tuning .", "label": [], "Comments": []}
{"id": 168490, "text": "WORDTOUR is time efficient as well .", "label": [], "Comments": []}
{"id": 168491, "text": "We created a LanguageARC project [ 1 ] that divided the < https : //languagearc.com/projects/19 > data collection into three tasks : Participants were recruited through calls for volunteers posted to social media and mailing lists in the French research community .", "label": [], "Comments": []}
{"id": 168492, "text": "Our model can recognize it because the graph2token module can feed back the coreference information to `` this action '' .", "label": [], "Comments": []}
{"id": 168493, "text": "We design a memory-augmented archi- In this work , we explore ways to combine an after speaking that unicorns have one horn . tecture that stores relations from a knowledge graph and investigate the effect of conditioning on this relational memory in an autoregressive language model .", "label": [], "Comments": []}
{"id": 168494, "text": "An initial manual evaluation by us revealed that almost all the generated candidates were fluent , which can be attributed to the extensive pre-training of the stateof-the-art base language models .", "label": [], "Comments": []}
{"id": 168495, "text": "Given its success in the end-to-end experiments , one natural question to ask is whether the high correlation with the human evaluation comes from the bot-bot set-up or the seven scoring metrics .", "label": [], "Comments": []}
{"id": 168496, "text": "For example , given a sentence `` * The red collar and golden buttons in the shirt form a colorful fashion topic * '' and the predicted product attribute `` * Color * '' , it is easy to recognize the value `` * golden * '' corresponding to attribute `` * Color * '' instead of `` * Material * '' .", "label": [], "Comments": []}
{"id": 168497, "text": "Highly imbalanced data poses added difficulty , as most learners will exhibit bias towards the majority class and , in extreme cases , may ignore the minority class altogether [ Johnson ] [ and Khoshgoftaar , 2019 ] .", "label": [], "Comments": []}
{"id": 168498, "text": "We hope that MHC can contribute to closing this language gap and that by diagnosing specific model weaknesses across languages it can support the development of better multilingual hate speech detection models in the future .", "label": [], "Comments": []}
{"id": 168499, "text": "2019 .", "label": [], "Comments": []}
{"id": 168500, "text": "This method is based on the idea of Borda count [ Ho et al. , , 1994 ] [ Emerson , 2013 ] , which provides more precise and well-distributed synthetic data labels than Z-score normalization .", "label": [], "Comments": []}
{"id": 168501, "text": "A common weakness seen in the literature is the use of weak baseline models .", "label": [], "Comments": []}
{"id": 168502, "text": "21DZ1100100 ) .", "label": [], "Comments": []}
{"id": 168503, "text": "As abusive content can be very culture-specific , there will be cases , even within the same language , where some utterances will be offensive in one culture , but not in another .", "label": [], "Comments": []}
{"id": 168504, "text": "Correct Label Errors Given the VILA structures , we can easily correct some previously mentioned texts that are not covered by the detected blocks errors like incorrect labels for `` Figure * '' by applying majority voting for token labels in a text block .", "label": [], "Comments": []}
{"id": 168505, "text": "These stories are under licenses that allow use and redistribution for research purposes .", "label": [], "Comments": []}
{"id": 168506, "text": "References [ dangers of stochastic parrots : Can language mod ] ( https : //doi.org/10.1145/3442188.3445922 ) [ els be too big ? ] ( https : //doi.org/10.1145/3442188.3445922 ) In * Proceedings of the 2021 ACM Conference on Fairness , Accountability , and Transparency * , FAccT '21 , page 610–623 , New York , NY , USA .", "label": [], "Comments": []}
{"id": 168507, "text": "E Derivations of the Equation Let E be the loss of training , L be the number blocks of the model , and y be the model output .", "label": [], "Comments": []}
{"id": 168508, "text": "B Low-Resource Machine Translation For the low-resource scenario , we choose the IWSLT14 English-German ( En→De ) and IWSLT17 English-French ( En→Fr ) tasks .", "label": [], "Comments": []}
{"id": 168509, "text": "Therefore , the proposed benchmark , FREDo , consists of 2 main tasks with a 1-Doc and a 3-Doc subtask each : • The in-domain tasks for which an approach which has been trained on documents sampled from DocRED is evaluated on 15k episodes generated using documents from DocRED .", "label": [], "Comments": []}
{"id": 168510, "text": "Figure [ 7 ] show the questions asked of the participants , in line with the annotation framework , to better understand the differences in the perceived social values of exhibited behavior across demographic groups .", "label": [], "Comments": []}
{"id": 168511, "text": "3.2.1 Input repr [ ese ] ntation As shown in Figure 2 , we introduced three kinds of special tokens into the input text ; [ X1 ] , [ X2 ] and < s > .", "label": [], "Comments": []}
{"id": 168512, "text": "For example , for a criterion like `` 2 points if the response mentions * Western culture * '' , the phrase * Western culture * would be marked in the response , if present .", "label": [], "Comments": []}
{"id": 168513, "text": "We adapt their model to our task by finetuning their model on XENT .", "label": [], "Comments": []}
{"id": 168514, "text": "The reported results are means and standard deviations over five different runs .", "label": [], "Comments": []}
{"id": 168515, "text": "Formally , MMR defines the score of a sentence s at time t as m = λS ( s , D ) − ( 1 − λ ) maxe∈E R ( s , e ) , where λ ∈ [ 0 , 1 ] is the weight balancing salience and redundancy .", "label": [], "Comments": []}
{"id": 168516, "text": "This kind of data was first studied by [ Weston et al. , 2006 ] under the name Universum .", "label": [], "Comments": []}
{"id": 168517, "text": "In the common supervised setting , the training objective is to learn a transformation from the source space to the target space X 7→ Y : f ( y|x ; Θ ) with the usage of parallel data .", "label": [], "Comments": []}
{"id": 168518, "text": "In Dialogue 1 , topic changes from exercise to a nearby sauna , which is n't included in sub-topics of example dialogues ( Table [ 13 ] .", "label": [], "Comments": []}
{"id": 168519, "text": "In the abstractive summarization , we need summaries as sentences in the target language but it is hard to obtain corpus containing summaries only .", "label": [], "Comments": []}
{"id": 168520, "text": "The most prominent static models in this area are Dist-Mult [ Yang et al. , , 2015 ] , SimplE [ Kazemi and ] [ Poole , 2018 ] , ComplEx [ Trouillon et al. , , 2016 ] and TuckER [ Balazevic et al. , , 2019 ] .", "label": [], "Comments": []}
{"id": 168521, "text": "The dataset contains tweets dated from 2014 to 2019 , each annotated with the mentions of named entities and their types ( * Person * , * Organization * , or * Location * ) .", "label": [], "Comments": []}
{"id": 168522, "text": "3.3 Neural Sentence Extraction We briefly describe the SDS sentence extraction module [ Chen and Bansal , 2018 ] that we base our work on , and elaborate in Sec [ 3.4 ] how we adapt it for better MDS performance with MMR guidance .", "label": [], "Comments": []}
{"id": 168523, "text": "The baseline approaches have access to full feature vectors .", "label": [], "Comments": []}
{"id": 168524, "text": "Each training batch contains at most 4,096 source/target tokens .", "label": [], "Comments": []}
{"id": 168525, "text": "In this example , we would expect to see , on average , 9.09 points of deterioration for each year of misalignment .", "label": [], "Comments": []}
{"id": 168526, "text": "However , most of existing work are either assuming the existence of the alignment information between tasks and KGs [ Banerjee and Baral , 2020 ] or an integrated KG [ Ma et al. , , 2021 ] .", "label": [], "Comments": []}
{"id": 168527, "text": "This is probably because the number of training data is too scarce in these tasks for sparse PLMs to perform well .", "label": [], "Comments": []}
{"id": 168528, "text": "We provide OOV words for each language when querying BabelNet , because all COLEX approaches and ARES rely on BabelNet synset annotations .", "label": [], "Comments": []}
{"id": 168529, "text": "Also , we apply a quality control step filtering subjects who do not score some faked and known sentences properly .", "label": [], "Comments": []}
{"id": 168530, "text": "We summarize it in Table 1 , depicting state-of-the-art approaches , level of annotations , what kind of competences are annotated , the modeling approaches , the size of the dataset ( if available ) , type of skills annotated for , baseline models , and whether they release their annotations and guidelines .", "label": [], "Comments": []}
{"id": 168531, "text": "Cross-attention of all decoder layers is used for action-source alignment .", "label": [], "Comments": []}
{"id": 168532, "text": "In this paper , we investigate the extent to which * trajectories * ( i.e . the position and rotation of objects over time ) naturally encode verb semantics .", "label": [], "Comments": []}
{"id": 168533, "text": "Table 5 : Impact of Different Values of λ .", "label": [], "Comments": []}
{"id": 168534, "text": "6.3 One-stage training on distilled + annotated dataset We observed that models pretrained on the Troy-Blogs dataset show good results on Stage I , but lose their advantage after training on Stage II .", "label": [], "Comments": []}
{"id": 168535, "text": "The rule-based classifier that relied on lexicon matching to assign emotions was not able to capture the overall context of the tweets and thus was likely to assign inaccurate emotions .", "label": [], "Comments": []}
{"id": 168536, "text": "Since CDLM was provided only as a base-sized model , we pretrained a larger version of the CDLM model , and hence used both the base and large versions of both Longformer and CDLM .", "label": [], "Comments": []}
{"id": 168537, "text": "All reported results use LR as the default readout method , unless specified otherwise , as we found LR to perform best on average ( see Section [ 4.4 ] .", "label": [], "Comments": []}
{"id": 168538, "text": "Figure 7 : Number of beam search errors for German-English as a function of the beam size . search did not find the global best translation , as a function of beam size .", "label": [], "Comments": []}
{"id": 168539, "text": "Based on Table [ 2 , ] in most of the cases , our learned strategy outperforms other strategies .", "label": [], "Comments": []}
{"id": 168540, "text": "This leads to a higher risk of gradient vanishing when L is larger .", "label": [], "Comments": []}
{"id": 168541, "text": "The first block r [ ep ] orts performance for the ORACLE upper bound and GOLD , which was estimated by comparing a ( randomly selected ) reference summary against the remaining two or t [ hree ] [ refe ] rence summaries .", "label": [], "Comments": []}
{"id": 168542, "text": "But what if those 2 % increase in performance does not make a difference in a production use case ?", "label": [], "Comments": []}
{"id": 168543, "text": "Sentenc [ e ] [ Embedding ] [ Base ] d Methods by incorporating supervision from downstream tasks ( Huang et al. , 2016 ) , introducing hierarchical optimal transport over topics ( Yurochkin resentations for sentences .", "label": [], "Comments": []}
{"id": 168544, "text": "A task trained this way is still evaluated as a classification task by performing the same transformation into discrete classes on the outputs of the regression model .", "label": [], "Comments": []}
{"id": 168545, "text": "We apply gradient penalty with weight of loss 100 for training the cWGAN .", "label": [], "Comments": []}
{"id": 168546, "text": "Finally , the representative domain of each review was assigned to all its selected sentences .", "label": [], "Comments": []}
{"id": 168547, "text": "We find the use of questions positively improves performance compared to crawled sentences .", "label": [], "Comments": []}
{"id": 168548, "text": "On GPT2 , many methods fail to improve the various metrics consistently .", "label": [], "Comments": []}
{"id": 168549, "text": "The inputs to the BART model are sequences ( s = p [ SEP ] q ) that comprise a query q plus retrieved passage p .", "label": [], "Comments": []}
{"id": 168550, "text": "In this work , we explore transfer learning from representations learned for ten popular natural language processing tasks ( two syntactic and eight semantic ) for predicting brain responses from two diverse datasets : Pereira ( subjects reading sentences from paragraphs ) and Narratives ( subjects listening to the spoken stories ) .", "label": [], "Comments": []}
{"id": 168551, "text": "Abstract TextHide was proposed to protect the training data via instance encoding in the natural language domain .", "label": [], "Comments": []}
{"id": 168552, "text": "3 Literary Evidence Retrieval Having established that the examples in RELiC contain complex interplay between literary quotation and scholarly analysis , we now shift to measuring how well neural models can understand these interactions .", "label": [], "Comments": []}
{"id": 168553, "text": "All of our reported results are the mean of 5 results with different seeds , which are ran- https : //nlp.stanford.edu/projects/tacred/ https : //github.com/DFKI-NLP/tacrev Table 2 : Results on Tacred and Tacred Revisited .", "label": [], "Comments": []}
{"id": 168554, "text": "References A More Results Figure [ 4 ] shows the model confidence level on the development set of the `` Credit cards '' domain in the CLINC-Single-Domain-OOS dataset .", "label": [], "Comments": []}
{"id": 168555, "text": "It can be observed that temporal finetuning has a greater impact than temporal pretraining .", "label": [], "Comments": []}
{"id": 168556, "text": "Our contributions are as follows : [ https : //github.com/cambridge-wtwt/ ] ( https : //github.com/cambridge-wtwt/acl2022-wtwt-stocks ) [ acl2022-wtwt-stocks ] ( https : //github.com/cambridge-wtwt/acl2022-wtwt-stocks ) 4 .", "label": [], "Comments": []}
{"id": 168557, "text": "The search target we want to maximize is the normalized average of all automatic metrics listed in Table [ 2 ] when inferencing on the test set , except PPL .", "label": [], "Comments": []}
{"id": 168558, "text": "Table 2 : ConST versus the cascaded ST systems on MuST-C En-De/Fr/Ru test sets . [ Ye et al. , 2021 ] and [ Xu et al. , 2021 ] are two strong cascaded models . 5 Analysis 5.1 Is contrastive loss effective ?", "label": [], "Comments": []}
{"id": 168559, "text": "Each encoder in the model can distill the critical information from each dialogue thread or the candidate response .", "label": [], "Comments": []}
{"id": 168560, "text": "Very recently , [ Zhou et al. , 2021 ] formulates personas generation as a Seq2Seq task for improved downstream response generation via multi-task learning .", "label": [], "Comments": []}
{"id": 168561, "text": "Furthermore , we show that entailment and QA-based metrics offer complementary signals through a combined metric that achieves state-of-the-art performance on this benchmark .", "label": [], "Comments": []}
{"id": 168562, "text": "The first question refers to the users ' informational goal that they received , in order to follow a consistent objective goal during their exploration .", "label": [], "Comments": []}
{"id": 168563, "text": "If users annotate the antecedents of `` the bonds '' and other ambiguous entity mentions , then these labels help adapt a model trained on ONTONOTES to new domains .", "label": [], "Comments": []}
{"id": 168564, "text": "Specifically , we split the articles into sentences x and then identify * salient spans * y in the text corresponding to named entities and dates .", "label": [], "Comments": []}
{"id": 168565, "text": "Rep .", "label": [], "Comments": []}
{"id": 168566, "text": "Overall parsing Table [ 3 ] presents a comparison of our prompt-based model with previous results and our own baseline .", "label": [], "Comments": []}
{"id": 168567, "text": "[ Borkan et al. , , 2019 ] and Wiki Toxic [ Wulczyn et al. , 2017 ] dataset for toxicity detection .", "label": [], "Comments": []}
{"id": 168568, "text": "Interestingly , even in the presence of a diverse TSA labeled data , our approach was comparable to the performance obtained with that data .", "label": [], "Comments": []}
{"id": 168569, "text": "F Augmentation with 20 % of input tokens In the main paper , we compare our augmentation strategy ( random templates and random demonstrations ) to standard augmentation techniques on 10 % of input tokens for creating multi-view of inputs to apply the SupCon loss .", "label": [], "Comments": []}
{"id": 168570, "text": "Rather than conducting a * hard * retrieval for individual tokens , we turn to retrieve the tokens ' counterparts according to their contexts .", "label": [], "Comments": []}
{"id": 168571, "text": "The method without ensemble component only achieves 0.2408 AVG .", "label": [], "Comments": []}
{"id": 168572, "text": "BCR ) and the ratio of instances in those biased clusters ( i.e. , BIR ) are 57.7 % and 54.9 % for K-Means and LOGAN , respectively .", "label": [], "Comments": []}
{"id": 168573, "text": "I have 2 dogs as pets .", "label": [], "Comments": []}
{"id": 168574, "text": "To avoid pairing all the entities from two KGs and control the size of the PCG , our approach selects entity-pairs having high equivalent possibilities as nodes in the PCG .", "label": [], "Comments": []}
{"id": 168575, "text": "Being a resource-rich language , English is a highly competitive area for the GEC task . 2 Base System Overview 2.1 GECToR architecture Our tagging models are inherited from GECToR [ Omelianchuk et al. , , 2020 ] .", "label": [], "Comments": []}
{"id": 168576, "text": "We then find that Multi-task 0 shot baselines do not outperform the best raw LM baseline in most settings , despite being supervised on a large set of meta-training tasks .", "label": [], "Comments": []}
{"id": 168577, "text": "Models like XLM-RoBERTa ( XLMR ) [ Conneau et al. , , 2020 ] offer advantages as they can be applied on several languages with little fine-tuning to obtain optimal NER performance , with an F1 score of 92.92 for English and an average of 89.43 across all languages in CoNLL [ Sang ] [ and Meulder , 2003 ] .", "label": [], "Comments": []}
{"id": 168578, "text": "CMU-MOSEI Sentiment The analysis of the routing coefficients rij is included in the main paper .", "label": [], "Comments": []}
{"id": 168579, "text": "Lastly , we would like to note that while crossvalidating though the interpolation parameter λ we note that for all models , λ = 0.3 works the best which is similar to the finding in [ Khandelwal et al. , , 2020 ] .", "label": [], "Comments": []}
{"id": 168580, "text": "We adapt the teacher using mini-episodic adaptation .", "label": [], "Comments": []}
{"id": 168581, "text": "Table 8 : Coverage of human-defined concepts using coarse POS and SEM labels across all clusters from a given model F Robustness of Methodology across Datasets and Settings Figure [ 8 ] shows the layer-wise patterns using 600 clusters instead of 1000 as used in the main paper .", "label": [], "Comments": []}
{"id": 168582, "text": "A single distribution over all sequences represents uncertainty by assigning probabilities , but it can not distinguish between different kinds of uncertainty ( e.g . model uncertainty versus intrinsic uncertainty ) .", "label": [], "Comments": []}
{"id": 168583, "text": "It also indicates whether s is already a complete , well-formed c or m by including EOS in the result ; if nextTokens ( s ) = { EOS } , then s is a valid canonical utterance or meaning representation with no possible extensions .", "label": [], "Comments": []}
{"id": 168584, "text": "There is a lot of variation in disagreement across languages , with most having less than 5 % disagreement , and only Mandarin and French more than 10 % .", "label": [], "Comments": []}
{"id": 168585, "text": "This finding offers a new way of explaining residual networks in the view of numerical algorithms .", "label": [], "Comments": []}
{"id": 168586, "text": "Finally , we want to explore the synergy between the ASAG scoring and classification tasks and feedback generation .", "label": [], "Comments": []}
{"id": 168587, "text": "We evaluate our method on 15 classification tasks studied in LM-BFF and follow the same setup as them to allow fair comparisons ( see Appendix [ A ] for more training details ) .", "label": [], "Comments": []}
{"id": 168588, "text": "In contrast , the BLEU score of ExtraPhrase is much lower than one of Self-training .", "label": [], "Comments": []}
{"id": 168589, "text": "This is an assumption that might either result in effective communication or conversely hurt the client .", "label": [], "Comments": []}
{"id": 168590, "text": "Adversarial NLI ( ANLI ) is a new benchmark collected `` via an iterative , adversarial human-and-model-in-the-loop procedure . '' [ Nie et al. , , 2020 ] .", "label": [], "Comments": []}
{"id": 168591, "text": "In total , we crawled 3 , 564 triplets ( 10 , 692 articles ) .", "label": [], "Comments": []}
{"id": 168592, "text": "The output from this stage serves as input to the clustering stage .", "label": [], "Comments": []}
{"id": 168593, "text": "We find that the larger ODE block is sufficient to take the role of several ODE blocks with first-order solutions .", "label": [], "Comments": []}
{"id": 168594, "text": "3.2 Directed Probing Apart from the structural probe B , [ Hewitt and Man ] [ ning , 2019 ] also probe for tree depth .", "label": [], "Comments": []}
{"id": 168595, "text": "We then combine the two representations of local coherence and the context vector , simply averaged sentence representations .", "label": [], "Comments": []}
{"id": 168596, "text": "We choose linear classifiers because more expressive ones like Multi-Layer Perceptron are more likely to capture the target feature themselves [ He ] [ witt and Liang , 2019 ] .", "label": [], "Comments": []}
{"id": 168597, "text": "Thus , our contributions in this paper are that we ( 1 ) assemble a corpus from publicly available psychometric tests for the 'Big Five ' variables of openness , conscientiousness , extraversion , agreeableness , and neuroticism [ Costa and McCrae , 1992 ] , which have been shown to be principled factors of personality [ Cattell , 1945 ] .", "label": [], "Comments": []}
{"id": 168598, "text": "A similar context-based approach has been explored in a previous study led by Ni and Wang ( 2017 ) showing that a sequence-to-sequence model trained directly on a large number of pairs of slang-contained sentences along with their corresponding definitions from Urban Dictionary can be a useful starting point toward the automated interpretation of slang .", "label": [], "Comments": []}
{"id": 168599, "text": "This precludes the outer-product memory computation and efficient recurrences in ABC .", "label": [], "Comments": []}
{"id": 168600, "text": "The WASSA 2022 dataset provides several numerical demographic features , namely : gender , education , race , age , and income .", "label": [], "Comments": []}
{"id": 168601, "text": "A few previous works have demonstrated that attention-based models will be distracted by noisy attended information , and accurate attention supervisions can be very beneficial [ Liu et al. , , 2016 ] [ Hsu et al. , 2018 ] .", "label": [], "Comments": []}
{"id": 168602, "text": "For supervised states set to 300 .", "label": [], "Comments": []}
{"id": 168603, "text": "As for the BANKING77 dataset , we randomly separate 27 intents from the 77 intents and use them as the ID-OOS samples , following the above process .", "label": [], "Comments": []}
{"id": 168604, "text": ". . , Pfm ] v1 , .", "label": [], "Comments": []}
{"id": 168605, "text": "We asked commercial annotation teams to extract stories and morals from the crawled raw texts .", "label": [], "Comments": []}
{"id": 168606, "text": "Here we explore how to control the complexity of generated definitions .", "label": [], "Comments": []}
{"id": 168607, "text": "However , existing knowledge bases suffer from missing relations and often do not contain relation triples related to observed contexts i [ n ] [ a ] [ target ] [ corpus ] [ , ] [ even ] though resear [ ch ] [ on ] [ knowledge ] [ bas ] e completion has resulted in significant advances ( Bordes et al. , 2013 ; Trouillon et al. , 2016 ; Zhang et al. , 2019 ) .", "label": [], "Comments": []}
{"id": 168608, "text": "Hypothetical examples : In contrast , hypothetical examples are scenarios constructed by the author .", "label": [], "Comments": []}
{"id": 168609, "text": "For each story , they should select a sentence as the right candidate that can be reasoned based on the context and common sense .", "label": [], "Comments": []}
{"id": 168610, "text": "F Ethical Considerations F.1 Limitations Assumptions .", "label": [], "Comments": []}
{"id": 168611, "text": "We then train all parameters with back propagation .", "label": [], "Comments": []}
{"id": 168612, "text": "Results for the first eight methods are taken from [ Li et al. , 2018 ] , while other results are the averaged scores of 5 runs with random initialization .", "label": [], "Comments": []}
{"id": 168613, "text": "However , not all predicates are similarly affected by time or show reduced sensitivity to temporal changes in related facts .", "label": [], "Comments": []}
{"id": 168614, "text": "For now , we decide not to break the sentences when releasing in order to be consistent with the sources where the data comes from .", "label": [], "Comments": []}
{"id": 168615, "text": "However , the statement itself is not necessarily stereotypical as it could describe the attitude of a person named Marie .", "label": [], "Comments": []}
{"id": 168616, "text": "Prototypes in Neural Networks The idea of prototypes ( or templates ) originates from information retrieval ( IR ) approaches for sentence matching tasks like response generation [ Ji et al. , , 2014 ] [ Hu et al. , , 2014 ] .", "label": [], "Comments": []}
{"id": 168617, "text": "Let s denote the sentence containing e , we add token markers < EVENT > , < /EVENT > before and after the event span in s to get the prompt for ei .", "label": [], "Comments": []}
{"id": 168618, "text": "In this use case , we hope to explain the deep sentiment classification model and obtain sentiment factors that drive the model to identify the sentiment .", "label": [], "Comments": []}
{"id": 168619, "text": "In a word , the threads and response can be encoded as follows : where m = 1 , 2 , ... M and M is the number of dialogue threads .", "label": [], "Comments": []}
{"id": 168620, "text": "From each utterance , one can extract ( 1 ) a set of * forwardlooking centers * ( C ) ranked according to their prominence , ( 2 ) a single * backward-looking center * ( Cb ) connected with one of the C of the immediately preceding utterance , and ( 3 ) a * preferred center * ( Cp ) which is the most salient center in C .", "label": [], "Comments": []}
{"id": 168621, "text": "Furthermore , other factors—such as the audience reached by the claim , and the intentions and forms [ of the claim—are often ] [ considered .", "label": [], "Comments": []}
{"id": 168622, "text": "While direct text manipulations are sometimes possible ( e.g. , modifying `` The man works as a ... '' to `` The woman works as a ... '' ) , several methods rely on constructing counterfactual representations to measure model behavior [ Kaushik et al. , , 2020 ] [ Ravfogel et al. , , 2020 ] .", "label": [], "Comments": []}
{"id": 168623, "text": "Second , we perform * reranking * , * jointly * encoding a step with each of its candidate goals in a supervised fashion to allow for more expressive contextualized embeddings .", "label": [], "Comments": []}
{"id": 168624, "text": "More details are in Appendix [ C. ] Table 1 : Accuracy on topic classification datasets under the various SSL settings . 10 , 200 , 2500 denote the number of labeled samples per class used during training .", "label": [], "Comments": []}
{"id": 168625, "text": "Equation [ 3 ] * * 221 * * enforces that all the relevant sentences should have * * 222 * * higher similarity with the passage than all the ir- * * 223 * * relevant sentences by a margin m ; otherwise , the * * 224 * * model would be penalized .", "label": [], "Comments": []}
{"id": 168626, "text": "This might become more challenging for non-standard tasks .", "label": [], "Comments": []}
{"id": 168627, "text": "We chose the cased BERT [ Devlin et al. , , 2019 ] large model as the monolingual model for encoding English text .", "label": [], "Comments": []}
{"id": 168628, "text": "Image-Audio : Places [ Harwath et al. , , 2017 ] contains over 400k pairs of images from the Places 205 dataset [ Zhou et al. , , 2014 ] and corresponding spoken audio captions averaging 10 seconds .", "label": [], "Comments": []}
{"id": 168629, "text": "Thus , we investigate different sources of CR model uncertainty . 3.1.1 Clustered Entropy To sample spans for learning CR , [ Li et al. , 2020 ] propose a strategy called * clustered entropy * .", "label": [], "Comments": []}
{"id": 168630, "text": "Eventually , the generator produces a highly over-fitted response to the exemplar by directly copying the tokens of the exemplar . 4 Method We hypothesize that selecting semantically relevant but lexically distanced exemplars from the gold response could solve the drawbacks above .", "label": [], "Comments": []}
{"id": 168631, "text": "This highlights the importance of monolingual encoders for the target language , which seems to be more important than having access to a monolingual encoder for the source language .", "label": [], "Comments": []}
{"id": 168632, "text": "While our proposed framework is similar in that it also discretizes the audio sequence Table 1 : Cross-Modal retrieval results on S-MiT , Places , and MSR-VTT .", "label": [], "Comments": []}
{"id": 168633, "text": "Specifically , the language of political actors involves strategic and complex semantics .", "label": [], "Comments": []}
{"id": 168634, "text": "There are 980 test cases , and we randomly choose 100 test cases for the human evaluation .", "label": [], "Comments": []}
{"id": 168635, "text": "Compared to the ground truth personas that are limited sets of traits , our generator can leverage the power of pre-trained models for better diversity .", "label": [], "Comments": []}
{"id": 168636, "text": "C = { ( k , vi ) } .", "label": [], "Comments": []}
{"id": 168637, "text": "This finding has n't been shown in Humeau 's work [ 2019 ] .", "label": [], "Comments": []}
{"id": 168638, "text": "We apply the multitask learning framework to jointly model the product attribute prediction and value extraction .", "label": [], "Comments": []}
{"id": 168639, "text": "This performance pattern is observed not only in classification tasks , but also in NER tasks where named entity tokens are the minority compared to non-entity tokens [ Mao et al. , , 2007 ] [ Kuperus et al. , , 2013 ] .", "label": [], "Comments": []}
{"id": 168640, "text": "Results and Discussion : Table [ 5 ] shows that our model consistently outperforms others .", "label": [], "Comments": []}
{"id": 168641, "text": "Formally , the path between concept u and v is represented as suv= [ e ( u , k1 ) , e ( k1 , k2 ) , ... , e ( km , v ] , where e indicates the relation label between two concepts and k1 : are the relay nodes .", "label": [], "Comments": []}
{"id": 168642, "text": "3 The Overshadowing Effect of Train-Test Direction Match The first analysis of this paper aims to calibrate the most studied relationship of the test-model direction match an [ d MT performance ] ( https : //github.com/armsp/active_or_passive ) by considering the additional effect of the * train-t [ est ] direction match * .", "label": [], "Comments": []}
{"id": 168643, "text": "In FewDocs , they read fewer documents and label roughly seven spans per document .", "label": [], "Comments": []}
{"id": 168644, "text": "ConfliBERT improves downstream tasks for conflict research while significantly alleviating the annotation bottleneck .", "label": [], "Comments": []}
{"id": 168645, "text": "Since JobBERT and JobSpan-BERT are performing similarly , we apply both to the test set and BERTbase .", "label": [], "Comments": []}
{"id": 168646, "text": "Table 2 shows the class distribution of the Malay tweets .", "label": [], "Comments": []}
{"id": 168647, "text": "F.3 Did you run * computational experiments * ?", "label": [], "Comments": []}
{"id": 168648, "text": "The recall for intent and slot retrieval is 15 % lower for complex utterances . [ 12 ] Thus , highlighting one reason for a performance gap between simple and complex frames .", "label": [], "Comments": []}
{"id": 168649, "text": "We conducted experiments on two open-ended [ VideoQA ] datasets : MSVD-QA and MSRVTT-QA [ Xu et al. , 2017 ] .", "label": [], "Comments": []}
{"id": 168650, "text": "However , there are a number of caveats for such a framework .", "label": [], "Comments": []}
{"id": 168651, "text": "But they are not the best on CANARD ( Elgohary et al. , 2019 ) , which requires more abstraction for restoration .", "label": [], "Comments": []}
{"id": 168652, "text": "Superscripts indicate the order of this encoding .", "label": [], "Comments": []}
{"id": 168653, "text": "This results in large improvements in retrieval performance and , as a consequence , in the downstream tasks .", "label": [], "Comments": []}
{"id": 168654, "text": "In the following example , we pass `` * I keep a glass of water next to my bed when I sleep . * '' as an input to be paraphrased by the model .", "label": [], "Comments": []}
{"id": 168655, "text": "We obtain a vector representation e i b for each time point i by concatenating : where e b i1 and e b i2 are the time embeddings for p b i1 and p b i2 ( same for the target ) .", "label": [], "Comments": []}
{"id": 168656, "text": "Supervised learning of universal sentence repre- Graph optimal transport for cross-domain alignment .", "label": [], "Comments": []}
{"id": 168657, "text": "Similarly , check-worthiness is a subjective concept varying along axes including target audience , recency , and geography .", "label": [], "Comments": []}
{"id": 168658, "text": "For corpora with more than 150B tokens , we make the training corpora 150B tokens via random sampling and construct a validation set with 10,000 examples randomly sampled from the remaining .", "label": [], "Comments": []}
{"id": 168659, "text": "Semantic graphs are uniquely suitable to detect factual errors in abstractive summaries as they abstract away from the lexical surface forms of documents and summaries , enabling direct comparisons of the underlying semantic concepts and relations of a document-summary pair .", "label": [], "Comments": []}
{"id": 168660, "text": "KGEB [ Ding et al. , , 2016 ] incorporates knowledge graph information .", "label": [], "Comments": []}
{"id": 168661, "text": "models .", "label": [], "Comments": []}
{"id": 168662, "text": "Ency is a collection of encyclopedic texts including Korean Wikipedia .", "label": [], "Comments": []}
{"id": 168663, "text": "For example , Adiwardana et al .", "label": [], "Comments": []}
{"id": 168664, "text": "We compute the most-frequent answers and find , as expected , that noun phrases related to `` person '' are the most frequent : the word `` man '' appears in 5.7 % of total original phrases and 1.2 % of total annotations ( see Figure [ 5 ] in the Appendix ) .", "label": [], "Comments": []}
{"id": 168665, "text": "Other hyperparameters are applied according to default settings .", "label": [], "Comments": []}
{"id": 168666, "text": "We then asked AMT workers to rate the generated attack 's fluency , relevancy of the attack to previous utterances , and overall conversation coherency on a likert scale of 1 to 3 representing poor , moderate , and good qualities respectively .", "label": [], "Comments": []}
{"id": 168667, "text": "Observation : Time passes ...", "label": [], "Comments": []}
{"id": 168668, "text": "l '' represents the average number of tokens in the input sentence . ?", "label": [], "Comments": []}
{"id": 168669, "text": "Acknowledgements We thank the anonymous reviewers for their helpful comments .", "label": [], "Comments": []}
{"id": 168670, "text": "We consider the definition of meaningful entity names when not available an interesting future research direction .", "label": [], "Comments": []}
{"id": 168671, "text": "This sets a new state-of-the-art on these tasks with fewer parameters .", "label": [], "Comments": []}
{"id": 168672, "text": "To aid visualization of what the metric measures , the common words are underlined and coloured to aid comparison .", "label": [], "Comments": []}
{"id": 168673, "text": "We will integrate the ordering model into our framework .", "label": [], "Comments": []}
{"id": 168674, "text": "Do the models learn how soon an answer is likely to change in the future ?", "label": [], "Comments": []}
{"id": 168675, "text": "5.1 Slot filling error analysis To understand the types of errors Re2G makes we sampled 50 instances of the development set of the T-REx dataset where the Accuracy and token-level F1 score was zero .", "label": [], "Comments": []}
{"id": 168676, "text": "For both MOCPT and MOPREF , we encode the story and candidates using an input encoder , and then predict a probability distribution over the candidates by normalizing the dot-product scores between the representations of the story and each candidate .", "label": [], "Comments": []}
{"id": 168677, "text": "Language Features .", "label": [], "Comments": []}
{"id": 168678, "text": "Story Filtering To ensure the quality of LOT examples , we asked annotators to judge whether each cr [ awle ] d story meets the following definition : `` anything which is told in the form of a coherent event sequence involving several specific and related characters '' ( Mostafazadeh et al. , 2016 ) .", "label": [], "Comments": []}
{"id": 168679, "text": "• CTRL [ Rashkin et al. , , 2021b ] augments the GPT2 model with control tokens [ Keskar et al. , , 2019 ] that guide the generation towards less subjective and more entailed content .", "label": [], "Comments": []}
{"id": 168680, "text": "This suggests that the relevant dialogue history for the query can provide valuable information for response generation .", "label": [], "Comments": []}
{"id": 168681, "text": "Similarly , a challenge across domains are the diverging named entity distributions or ambiguities that surface forms resolve to different entity types .", "label": [], "Comments": []}
{"id": 168682, "text": "This has led to an interest in whether this aspect can be leveraged to get better generalization on unseen goals made up of familiar terms [ Oh et al. , , 2017 ] [ Hermann et al. , 2017 ] .", "label": [], "Comments": []}
{"id": 168683, "text": "TinyBERT fine-tuned checkpoints are obtained from their repository .", "label": [], "Comments": []}
{"id": 168684, "text": "This work was supported by the Key Development Program of the Ministry of Science and Technology ( No.2019YFF0303003 ) , the National Natural Science Foundation of China ( No.61976068 ) and `` Hundreds , Millions '' Engineering Science and Technology Major Special Project of Heilongjiang Province ( No.2020ZX14A02 ) .", "label": [], "Comments": []}
{"id": 168685, "text": "To avoid such phenomena , one solution is to provide the conversation model with relevant knowledge to guide the response generation [ Parthasarathi and Pineau , 2018 ] [ Ghazvinine ] [ jad et al. , , 2018 ] [ Dinan et al. , , 2019 ] .", "label": [], "Comments": []}
{"id": 168686, "text": "Dataset Characteristics After filtering dupli- cates , irrelevant URLs like social media handles , and video-content websites , we obtained a total of 31,366 documents for 520 entities .", "label": [], "Comments": []}
{"id": 168687, "text": "10 ] shows the hyperparameters and dev F1 score for TACRED experiments .", "label": [], "Comments": []}
{"id": 168688, "text": "The concatenation of test2010 , test2011 , test2012 , test2013 , test2014 and test2015 is used as the test set .", "label": [], "Comments": []}
{"id": 168689, "text": "For inference , we obtain the entity-level relation labels by simply averaging the mention-level relation scores from the cartesian product of the predicted entity clusters , denoted as s ( eh , et ) = MEAN { s ( mh , mt ) } , ( mh , mt ) .", "label": [], "Comments": []}
{"id": 168690, "text": "Specifically , a CMLM predicts a target sequence T given a source text S and part of the target Tmasked , where Tmasked is the target sequence with a random entity being masked .", "label": [], "Comments": []}
{"id": 168691, "text": "Even with this change , we divide very long documents into chunks , estimating logits for each chunk and taking the pointwise mean .", "label": [], "Comments": []}
{"id": 168692, "text": "As neural abstractive summarizers have become popular , their issues with correctness have sparked much recent work that focus specifically on model hallucinations and summary factuality [ Kryscinski et al. , , 2020 ] .", "label": [], "Comments": []}
{"id": 168693, "text": "Active learning for CR adaptation is wellmotivated , but the implementation is neither straightforward nor well-studied .", "label": [], "Comments": []}
{"id": 168694, "text": "With a transferred model , one can further improve performance with domain-specific information .", "label": [], "Comments": []}
{"id": 168695, "text": "Each image is encoded into 49 codewords ( 7 × 7 for height , width ) and every 16 consecutive frames from the spectrogram is encoded into 1 codeword .", "label": [], "Comments": []}
{"id": 168696, "text": "We compare with BERT and latent tree learning models .", "label": [], "Comments": []}
{"id": 168697, "text": "Our annotators agreed with each other 54.2 % on a single system and 91.7 % when accounting for ties .", "label": [], "Comments": []}
{"id": 168698, "text": "Reviews from the OPINOSIS source [ Ganesan et al. , , 2010 ] were assigned a label of * electronics * , * automotive * or * hotels * , based on the topic provided in that corpus for each review .", "label": [], "Comments": []}
{"id": 168699, "text": "We present additional results in our appendices : To the best of our knowledge , the other two nonautoregressive supervised summarization models are [ Yang et al. , 2021 ] and [ Qi et al. , 2021 ] .", "label": [], "Comments": []}
{"id": 168700, "text": "[ Xiao et al. , 2015 ] and [ Gibson et al. , 2015 ] found that language use relating to asking for others ' perspective ( * e.g .", "label": [], "Comments": []}
{"id": 168701, "text": "Co-occurring events are then incorporated as additional positives , weighted by a normalized co-occurrence frequency .", "label": [], "Comments": []}
{"id": 168702, "text": "We also show the transferability of our vokens to other frameworks ( i.e. , RoBERTa ) . 2 Visually-Supervised Language Models Contextual language representation learning is driven by self-supervision without considering explicit connections ( grounding ) to the external world .", "label": [], "Comments": []}
{"id": 168703, "text": "In order to better characterize our tasks in relation to the common N-way K-shot format we measure the distribution of N and K across our test sets .", "label": [], "Comments": []}
{"id": 168704, "text": "However , there are several important negative results in these experiments as well .", "label": [], "Comments": []}
{"id": 168705, "text": "The fine-tuning helps in this task because it exposes our model to how a complete Malay tweet can be formed from words and sentences .", "label": [], "Comments": []}
{"id": 168706, "text": "Each task proposes a Figure 4 : Probability distributions of 50 tasks ( the noun in the masked tokens ) in Flickr-shift data stream .", "label": [], "Comments": []}
{"id": 168707, "text": "Figure 1 : An illustration of our dataset annotation pipeline .", "label": [], "Comments": []}
{"id": 168708, "text": "Also , infusing the neural representation of the current summary with explicit MMR measures significantly reduces summary redundancy .", "label": [], "Comments": []}
{"id": 168709, "text": "XLM-EMO performs better than the selected baseline . model every 50 steps and get the best checkpoint ) and 5 % of data for the test .", "label": [], "Comments": []}
{"id": 168710, "text": "The F1 measure combines precision and recall ( using a harmonic mean ) , which are both important to consider for automatic abusive language detection systems .", "label": [], "Comments": []}
{"id": 168711, "text": "And our reward is defined below . where T ype = { `` function '' , `` cell '' , `` column '' } , { rκ|κ ∈ T ype } are relevances , { σκ|κ ∈ T ype } are hyper-parameters , and yˆ is the label predicted by accessing the table T with the program z .", "label": [], "Comments": []}
{"id": 168712, "text": "Precision ( P ) , recall ( R ) , and F1-score ( F1 ) are used to evaluate the model 's performance .", "label": [], "Comments": []}
{"id": 168713, "text": "A smaller a indicates a large domain difference between this summarization pair and the text classification data , and this pair should be down-weighted during the training of the summarization model .", "label": [], "Comments": []}
{"id": 168714, "text": "The ratings were obtained using a five-point Likert scale .", "label": [], "Comments": []}
{"id": 168715, "text": "Miscellaneous : 18 of the 200 samples were not literary analysis , though some were still related to literature ( for example , analysis of the the film adaptation of * The Age of Innocence * ) .", "label": [], "Comments": []}
{"id": 168716, "text": "All models are tested on XSUM 's official test set .", "label": [], "Comments": []}
{"id": 168717, "text": "Another feasible solution is to generate training examples with pivot-based translation where the source sentence cascades through the pre-trained source → English and English → target systems to generate the target sentence [ Cheng et al. , , 2016 ] .", "label": [], "Comments": []}
{"id": 168718, "text": "While there exists many augmentation techniques like Cutmix [ Yun et al. , , 2019 ] , Mixup [ Zhang et al. , , 2018 ] in computer vision and EDA [ Wei and Zou , 2019 ] , AEDA [ Karimi et al. , , 2021 ] for text , data augmentation remains challenging .", "label": [], "Comments": []}
{"id": 168719, "text": "As BERT is fine-tuned , structures tend to become more standard and present slightly more similarities with linguistic patterns .", "label": [], "Comments": []}
{"id": 168720, "text": "Following [ Zhang et al. , 2020 ] , we report the sacre-BLEU on the average of the 15 non-English language pairs .", "label": [], "Comments": []}
{"id": 168721, "text": "In our experiments we fine-tune the backbone model for the GLUE tasks using the default values of the hyper-parameters .", "label": [], "Comments": []}
{"id": 168722, "text": "We observe two error cases when this baseline struggles to predict correctly .", "label": [], "Comments": []}
{"id": 168723, "text": "1 Introduction Deep neural networks achieve high performance in many tasks , but typically require annotated training data for each new domain .", "label": [], "Comments": []}
{"id": 168724, "text": "3 Approach We first describe the teacher and student model architectures , followed by our proposed model training procedure .", "label": [], "Comments": []}
{"id": 168725, "text": "The schema , defining which relation types are relevant is highly dependent on the specific application and domain .", "label": [], "Comments": []}
{"id": 168726, "text": "The scraped documents covered [ a ] range of webs [ ite ] domains , including biographical sites , news articles , official company profiles , newsletters , and so on .", "label": [], "Comments": []}
{"id": 168727, "text": "[ https : //tianchi.aliyun.com/ ] ( https : //tianchi.aliyun.com/notebook-ai/ ) [ notebook-ai/ ] ( https : //tianchi.aliyun.com/notebook-ai/ ) Table 3 : Performance of baseline models on CBLUE benchmark .", "label": [], "Comments": []}
{"id": 168728, "text": "Moreover , MO2ST is similar to persuasive essay generation [ Stab and ] [ Gurevych , 2017 ] , which also requires conveying a viewpoint in generated texts .", "label": [], "Comments": []}
{"id": 168729, "text": "We also report the average over all tasks and the average of the standard deviation values in separate rows for analysis .", "label": [], "Comments": []}
{"id": 168730, "text": "In these cases , the model can only make the correct predictions by understanding long contexts .", "label": [], "Comments": []}
{"id": 168731, "text": "Our BERT consists of an 12 layers , 768-dimensional embeddings and 12 attention heads , resulting in 110M of total parameters .", "label": [], "Comments": []}
{"id": 168732, "text": "This structure motivates our hypothesis that such psychometric tests can be used directly to induce classifiers that profile individuals in social media without the existence of designated , manually annotated in-domain training data .", "label": [], "Comments": []}
{"id": 168733, "text": "Test Split of Novel Compositions .", "label": [], "Comments": []}
{"id": 168734, "text": "Under this perspective , we classified existing methods combating over-smoothness into two categories : simplifying data distributions and enhancing modeling methods , and conducted comprehensive analyses and studies on these methods .", "label": [], "Comments": []}
{"id": 168735, "text": "First , we briefly introduce the task of aspect-based sentiment analysis and emotion detection .", "label": [], "Comments": []}
{"id": 168736, "text": "Scaling Table [ 3 ] shows the effect of increasing model size on the overall F1 scores on CUSTOM-NEWS and TEMPLAMA .", "label": [], "Comments": []}
{"id": 168737, "text": "Table [ 10 ] reports on the total number of ( trainable ) parameters for each considered model .", "label": [], "Comments": []}
{"id": 168738, "text": "Thus , z contains syntactic information and , when we state that M `` uses '' syntactic information , we formally mean that ∇zdepMk+ ( z ) ̸= 0 .", "label": [], "Comments": []}
{"id": 168739, "text": "Complete results in Appendix [ C. ] sponding IE and AE prediction heads .", "label": [], "Comments": []}
{"id": 168740, "text": "We manually analyzed these mistakes by sampling 200 errors from the NYT 2021 puzzles and placing them in the same categories used in Table [ 1 . ] Figure [ 6 ] shows the results and indicates that knowledge , wordplay , and cross-reference clues make up the majority of errors .", "label": [], "Comments": []}
{"id": 168741, "text": "Most existing generation models for this task rely heavily on training with sufficient personabased dialogues .", "label": [], "Comments": []}
{"id": 168742, "text": "A shared attentional mechanism R F 0 × R F 0 × R F → R is used to computes attention coefficients : where ( i → j ) denotes the index of edge-type linking the i-th node to the j-th node , t ( i→j ) ∈ R F 0 is the vector representation of the edge-type ; a ∈ R 3F 0 is a weight vector of a single-layer feedforward neural network for computing the attention coefficients ; k represents concatenation of vectors .", "label": [], "Comments": []}
{"id": 168743, "text": "Details on the training setup and ( hyper- ) parameter settings are reported in Appendix [ B.2 ] for replication .", "label": [], "Comments": []}
{"id": 168744, "text": "This shows that incorporating external information can aid in the generation of more informative responses .", "label": [], "Comments": []}
{"id": 168745, "text": "This better generalization phenomenon is interesting and has recently been noted in the machine vision community in the context of investigating the regularization effects of batch normalization in classification settings [ Dauphin ] [ and Cubuk , 2021 ] .", "label": [], "Comments": []}
{"id": 168746, "text": "In this work , we design and introduce a text-based emotion recognition system using neural networks .", "label": [], "Comments": []}
{"id": 168747, "text": "The other 4,100 tweets were collected as part of the Italian hate speech monitoring project `` Contro l'Odio '' [ Capozzi et al. , , 2019 ] .", "label": [], "Comments": []}
{"id": 168748, "text": "As an alternative , we can also learn the alignment through Connectionist Temporal Classification ( CTC ) [ Graves et al. , 2006 ] , which is widely used for * variablelength * alignments , such as in ASR [ Audhkhasi et al. , 2019 ] , handwriting recognition [ Bluche et al. , , 2014 ] , etc .", "label": [], "Comments": []}
{"id": 168749, "text": "The second premise contains a noun other than N , denoted For example , other studies call category ( 3 ) `` nonsubsective '' instead , and further decompose it into `` privative '' ( X is a M N contradicts X is a N , e.g. , * fake * ) and `` nonprivative '' ( X is a M N is neutral to X is a N , e.g. , * alleged * ) .", "label": [], "Comments": []}
{"id": 168750, "text": "We observe a performance gap between one versus multiple source domains in pretrained students , specially when we opt out the source ATIS ; The performance of the pretrained student on TOP is 40.50 with source SNIPS and 57.16 with source ATIS and SNIPS .", "label": [], "Comments": []}
{"id": 168751, "text": "Some expressions are incomplete : For instance * sín azan + number ˇ * means * since xxx days * but * y * E * yw * E is not a number , and do not have any meaning in this context .", "label": [], "Comments": []}
{"id": 168752, "text": "The design is motivated by the translation invariance properties of ConvNets on images ( see Figure [ 3 ] , and captures the edit invariance properties of text n-grams in calculating the loss .", "label": [], "Comments": []}
{"id": 168753, "text": "Benchmark is BEA-2019 ( dev ) . the same encoders ' architectures in Base and Large configurations may provide slightly better results than we get for the Base and Large models separately ( see RoBERTa ( B ) + RoBERTa ( L ) in Table [ 6 ] .", "label": [], "Comments": []}
{"id": 168754, "text": "These models typically rely on pretraining with massive and heterogeneous corpora on a general Masked Language Modeling ( * MLM * ) task , i.e. , predicting a word that is masked in the original text .", "label": [], "Comments": []}
{"id": 168755, "text": "We evaluate our model on three downstream tasks showing that it is not only linguistically more sound than previous models but also that it outperforms them in end applications [ 1 ] . 1 Introduction Coherence describes the semantic relation between elements of a text .", "label": [], "Comments": []}
{"id": 168756, "text": "These weights are tentatively fixed at this stage and will be updated later .", "label": [], "Comments": []}
{"id": 168757, "text": "Spider narrows the gap between unsupervised dense retrievers and DPR on all benchmarks ( Figure [ 1 , ] Table [ 1 ] , outperforming all contrastive and end-to-end unsupervised models in top-5 & top-20 accuracy consistently across datasets .", "label": [], "Comments": []}
{"id": 168758, "text": "In this paper , we explore how to improve the accuracy and efficiency of structured content [ extraction from sc ] ientific documents by using VIsual LAyout ( VILA ) groups .", "label": [], "Comments": []}
{"id": 168759, "text": "Different positions of the blank tokens in the output sequence represent different alignments between the outputs and the ground-truth sequence .", "label": [], "Comments": []}
{"id": 168760, "text": "Adverbials are included if it concerns the manner of doing something .", "label": [], "Comments": []}
{"id": 168761, "text": "State-of-the-art methods are based on deep neural net [ works trained ] [ via di ] s [ tant supervision ( Li ] n et al. , 2016 ; Zhang et al. , 20 [ 17 ; Soares et al ] . , 2019 ; Yao et al. , 2019 ) .", "label": [], "Comments": []}
{"id": 168762, "text": "In addition , distantly supervised pre-training also have poor performances . 5 Related Work Research on entity relation extraction has been extensively investigated .", "label": [], "Comments": []}
{"id": 168763, "text": "indicates that results are reproduced by ourselves ; § indicates results are taken from Reimers and Gurevych ( 2019 ) ; * Surrogate * are results for our proposed method .", "label": [], "Comments": []}
{"id": 168764, "text": "The results are presented in three interactive dashboards , allowing companies to gain more insights into what stakeholders think of their products and services .", "label": [], "Comments": []}
{"id": 168765, "text": "[ e h k , e k , rk ] means the head entity , the tail entity and the relationship of the k th triple respectively .", "label": [], "Comments": []}
{"id": 168766, "text": "Track 3 : Personality Prediction ( PER ) , which consists in predicting the personality of the essay writer , knowing all his/her essays and the news article from which they reacted .", "label": [], "Comments": []}
{"id": 168767, "text": "In other words , the model is extremely reluctant to predict nonother entity types .", "label": [], "Comments": []}
{"id": 168768, "text": "More recent work on text-based retrieval and QA tasks [ has ] [ moved ] [ away ] [ from ] [ B ] [ oW ] [ methods ] [ towards ] learned ( neural ) models for the first retrieval step ( Karpukhin et al. , 202 [ 0 ; ] [ Qu ] [ et ] [ al. , 2021 ] Xiong et al. , 2021 ) .", "label": [], "Comments": []}
{"id": 168769, "text": "[ 13 ] Following this intuition , we thus remove duplicate instances after preprocessing .", "label": [], "Comments": []}
{"id": 168770, "text": "[ Li et al. , 2020 ] ; [ Garg and ] [ Ramakrishnan , 2020 ] ; [ Li et al. , 2021 ] perturb input using MLMs similar to ours but designed for an attack so inefficient for adversarial training .", "label": [], "Comments": []}
{"id": 168771, "text": "The text encoder , denoted by E , uses a pretrained encoder E , augmented with adapter modules which receives the summary S and document D and outputs a contextual text representation .", "label": [], "Comments": []}
{"id": 168772, "text": "Since this model also learns the user profile directly from the dialogue history , it is the most relevant baseline of our method .", "label": [], "Comments": []}
{"id": 168773, "text": "To mitigate this issue , a straightforward solution is increasing the number of references [ Sakaguchi et al. , , 2016 ] [ Choshen and Abend , 2018 ] .", "label": [], "Comments": []}
{"id": 168774, "text": "The global error might be larger if we run it for a number of times .", "label": [], "Comments": []}
{"id": 168775, "text": "Next , salient propositions are clustered based on their semantic similarity ( [ §3.3 ] .", "label": [], "Comments": []}
{"id": 168776, "text": "We find the contrastive loss particularly helpful when the amount of speech data is extremely small , like only 1 hour of speech .", "label": [], "Comments": []}
{"id": 168777, "text": "For evaluation , we fine-tune all models on the downstream AS2 and FEVER datasets : using the same IE and AE prediction heads exploited in pre-training for AS2 and using either IE or AE prediction heads for FEVER .", "label": [], "Comments": []}
{"id": 168778, "text": "Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein .", "label": [], "Comments": []}
{"id": 168779, "text": "4.2 Bas [ eline ] We co [ mp ] are our method with the following baseline methods .", "label": [], "Comments": []}
{"id": 168780, "text": "Alan Black and Prof .", "label": [], "Comments": []}
{"id": 168781, "text": "Contributions To the best of our knowledge , we are the first to ( i ) conduct a thorough investigation of lexical artifacts across online platforms ; ( ii ) disentangle artifacts into fine-grained categories ; and ( iii ) propose a viable data-centric approach based on masking that consistently improves fairness over * all * baselines across * all * platforms .", "label": [], "Comments": []}
{"id": 168782, "text": "As we shall see , the Silver pairs are useful to learn models . 4 Corpus Analysis Does conversational context affect if a comment is perceived as Hate or Counter-hate ?", "label": [], "Comments": []}
{"id": 168783, "text": "In this work , we revisit the clustering approach , grouping together sub-sentential * propositions * , aiming at more precise information alignment .", "label": [], "Comments": []}
{"id": 168784, "text": "Aggregating Input into Sentences Typically , multiple pieces of input information need to be merged into a single sentence .", "label": [], "Comments": []}
{"id": 168785, "text": "This corresponds to getting 6 out of the 7 puzzles perfect and 1 letter wrong on 1 puzzle .", "label": [], "Comments": []}
{"id": 168786, "text": "In this version , the result of each QK ( Eq . [ 2 ] in Self-Attention is masked so that each t in T only queries for information from t with j ≤ i .", "label": [], "Comments": []}
{"id": 168787, "text": "Everyone contributed to writing the paper .", "label": [], "Comments": []}
{"id": 168788, "text": "The language of the dataset is English .", "label": [], "Comments": []}
{"id": 168789, "text": "For unlabeled data , we follow UDA to use a student-teacher alike training strategy , that is , we first use the original sentence input x ∈ U to obtain the prediction target ( similar to a pseudo label ) and then enforce the prediction of the backtranslated version x a j of x being close to the prediction target .", "label": [], "Comments": []}
{"id": 168790, "text": "Table 2 : Evaluation results on restaurant domain ( Yelp ) .", "label": [], "Comments": []}
{"id": 168791, "text": "To create the string representation for the target , we follow [ Conforti et al. , 2020b ] 's representation of company names and acronyms , and add the official ( at the time of data collection ) Figure 4 : Confusion matrices for the our multi-modal model on the test merger ( when training , in turn , on the other three ) .", "label": [], "Comments": []}
{"id": 168792, "text": "Vacillating between different options to set up θ requires the engagement of both client and server and could in fact be more costly than simply pulling all tensors .", "label": [], "Comments": []}
{"id": 168793, "text": "This can be attributed to the fact that there were many instances in which Hyper-CLOVA begins its response with back-channeling .", "label": [], "Comments": []}
{"id": 168794, "text": "To implement this idea , the HSTM posits a joint generative model that captures how latent topics drive both text documents and outcomes .", "label": [], "Comments": []}
{"id": 168795, "text": "But cross-encoders come with an extremely high computational overhead , making them less suited for a production set- ting .", "label": [], "Comments": []}
{"id": 168796, "text": "Figure 9 : Layer-wise results on a separately sampled dataset .", "label": [], "Comments": []}
{"id": 168797, "text": "It outperforms the out-of-the-box MS MARCO model 7.7 points on average .", "label": [], "Comments": []}
{"id": 168798, "text": "Unsupervised Evaluation The results are shown in Table 4 , from which we can see that for both the unsupervised settings , the proposed models * Origin * and * Surrogate * outperform baseline models by a large margin , with over 10 points for the unsup [ ervised ] [ se ] tting and over 4 points for the supervised setting .", "label": [], "Comments": []}
{"id": 168799, "text": "See Figure [ 1 ] ( c ) for example , the length predictor produces a wrong alignment ( 0 2 1 1 ) , indicating that the decoder will drop the first source character and generate two target characters for the second source character .", "label": [], "Comments": []}
{"id": 168800, "text": "Then , an available proposition similarity model , trained from summarysource alignments [ Ernst et al. , , 2021 ] , provides the basis for agglomerative clustering [ Ward , 1963 ] .", "label": [], "Comments": []}
{"id": 168801, "text": "It shows how Kronecker product constraints the space of possible projections .", "label": [], "Comments": []}
{"id": 168802, "text": "Table 3 : Reconstruction self-BLEU and reconstruction perplexity ( PPL ) for each model on each data split .", "label": [], "Comments": []}
{"id": 168803, "text": "The guesser chooses class i with probability q equal to the fraction of i labels in the training set .", "label": [], "Comments": []}
{"id": 168804, "text": "( NAM ) Generated Case 2 : We should not be greedy and learn to others the importance of our desires .", "label": [], "Comments": []}
{"id": 168805, "text": "Models for SPTE are finetuned on MNLI and SNLI , while models for the other two tasks are finetuned on MPE and ADEPT , respectively .", "label": [], "Comments": []}
{"id": 168806, "text": "Crosswords test knowledge of word meanings , trivia , commonsense , and wordplay , while also requiring one to simultaneously reason about multiple intersecting answers .", "label": [], "Comments": []}
{"id": 168807, "text": "4.2.1 Teacher-forcing metrics A simple way to evaluate models is to measure its loss on hold-out data in a setup where for each step the full ground truth context is provided .", "label": [], "Comments": []}
{"id": 168808, "text": "We trained the classification layer with pre-trained RoBERTa on 300 epochs , but with fine-tuned RoBERTa , 10 epochs were sufficient .", "label": [], "Comments": []}
{"id": 168809, "text": "We motivate our handling of visual features using two intuitions .", "label": [], "Comments": []}
{"id": 168810, "text": "We apply our explainability metrics to test classifiers ' fairness towards identity-based groups ( e.g. , women , Muslims ) .", "label": [], "Comments": []}
{"id": 168811, "text": "The column Unclear shows the number of pairs for which we did not have enough information to assess their correctness . 4.2 Intrinsic Evaluation of Vector Space We first investigate the effect of fine-tuning on the latent representations of the entities .", "label": [], "Comments": []}
{"id": 168812, "text": "Further , continuing to pre-train to specialize models to African languages further improves performance .", "label": [], "Comments": []}
{"id": 168813, "text": "However , for applying RC to real-world problems , it is especially important to discover instances of relations that are not yet covered well in a given knowledge base .", "label": [], "Comments": []}
{"id": 168814, "text": "As a result of this step , we expect the performance of SBERT and SRoBERTa embeddings to be better that BERT and ALBERT embeddings .", "label": [], "Comments": []}
{"id": 168815, "text": "For each positive pair ( sq , v ) , we replace v or s with one other sample from in the same mini-batch to construct two sets of negative examples : ( sq , vˆ ) and ( ˆsq , v ) .", "label": [], "Comments": []}
{"id": 168816, "text": "MPNet remains competitive because it was trained on a large supervised corpus ( out-of-domain ) to learn semantics between paraphrasing sentences .", "label": [], "Comments": []}
{"id": 168817, "text": "Our work is motivated by the underlying hypothesis that partner personas generation is plausible given the self personas and dialogue context .", "label": [], "Comments": []}
{"id": 168818, "text": "However , the shared parameters in the frozen pretrained LM may limit modularity .", "label": [], "Comments": []}
{"id": 168819, "text": "Figure 8 : Defense human evaluation results .", "label": [], "Comments": []}
{"id": 168820, "text": "A similar phenomenon can be found in CNN [ Sanyal et al. , , 2018 ] , that a low-rank regularizer , such as SVD , applied to the representation of the intermediate layers can allow the model to have lower prediction errors .", "label": [], "Comments": []}
{"id": 168821, "text": "Each token in every other layer is sent to two of k experts , and this routing is updated via backpropagation .", "label": [], "Comments": []}
{"id": 168822, "text": "For small corpus , the model tends to be overfitting .", "label": [], "Comments": []}
{"id": 168823, "text": "Splits and dataname names consistent to those in Ye et al . ( 2021 ) and Khashabi et al . ( 2020 ) .", "label": [], "Comments": []}
{"id": 168824, "text": "Nevertheless , room for additional improvement remains , especially on the QA front .", "label": [], "Comments": []}
{"id": 168825, "text": "For example , among the subnetworks of OMP , IMP and TAMT at 50 % sparsity , the decrease in KD/MLM loss produces little or no downstream improvement ; at 60 % ∼ 80 % sparsity , OMP underperforms random pruning in MLM , while its downstream performance is better .", "label": [], "Comments": []}
{"id": 168826, "text": "Third , in addition to experiments on ToTTo and HiTab benchmarks , we evaluate our model on a harder version of ToTTo with a special focus on robustness to content-invariant structural transformations .", "label": [], "Comments": []}
{"id": 168827, "text": "The batch size is set to 1 [ github.com/facebookresearch/KILT/ ] ( github.com/facebookresearch/KILT/blob/main/kilt/retrievers/README.md ) [ blob/main/kilt/retrievers/README.md ] ( github.com/facebookresearch/KILT/blob/main/kilt/retrievers/README.md ) 5 < github.com/facebookresearch/FiD > < ai.facebook.com/tools/kilt/ > Table 2 : Main Results .", "label": [], "Comments": []}
{"id": 168828, "text": "The examples support again that knowledge neurons are activated by corresponding knowledge-expressing prompts . 5 Case Studies We present two preliminary studies to demonstrate the potential applications of knowledge neurons .", "label": [], "Comments": []}
{"id": 168829, "text": "A Language Characteristics [ Table 9 ] provides the details about the language characteristics .", "label": [], "Comments": []}
{"id": 168830, "text": "Table 4 : Results of MAD-X and all BAD-X variants on POS .", "label": [], "Comments": []}
{"id": 168831, "text": "Our experiments in Section [ 4 ] suggest that such design allows our model to outperform strong baseline models on both the EXTRACTION and INFERENCE tasks .", "label": [], "Comments": []}
{"id": 168832, "text": "A.2 Case study examples We show two examples for case study that explains how LCT and LCC work , as shown in Table [ 7 ; ] the description is in [ §5 ] of the main paper .", "label": [], "Comments": []}
{"id": 168833, "text": "One possible reason for these lower results is that finding an image that conveys the meaning of the whole sentence is hard .", "label": [], "Comments": []}
{"id": 168834, "text": "So in this paper , we propose a simple yet effective contrastive learning method to bridge the gap and to improve ST performance .", "label": [], "Comments": []}
{"id": 168835, "text": "[ 6 ] We have two variants : ( a ) using one instruction per meta-training task , and ( b ) using all available instructions including 267 instructions in total ( 8.3 per meta-training task ) which [ Sanh et al. , 2022 ] found to be better than ( a ) .", "label": [], "Comments": []}
{"id": 168836, "text": "To quantify this intuition , in Figure [ 4 ] we depict the Normalized Mutual Information ( NMI ) between sIB labels and the target task labels , calculated over the entire training set , versus the gain of using BERTIT : CLUST , reflected as the reduction in classification error rate between BERT and BERTIT : CLUST , at the extreme case of 64 fine-tuning samples .", "label": [], "Comments": []}
{"id": 168837, "text": "[ Wager et al. , 2020 ] propose a datadriven approach to predict pitch shifts depending on both amateur recording and its accompaniment .", "label": [], "Comments": []}
{"id": 168838, "text": "This is equiv- Object Position XYZ , Hand Position XYZ , and Object Rotation XYZW .", "label": [], "Comments": []}
{"id": 168839, "text": "We therefore use multiple measures of scientific writing complexity based on prior work in science communication and readability .", "label": [], "Comments": []}
{"id": 168840, "text": "In contrast to temporal domain adaptation , which does not mitigate temporal misalignment 's effects , finetuning on temporally-updated labeled data is more effective .", "label": [], "Comments": []}
{"id": 168841, "text": "SRL .", "label": [], "Comments": []}
{"id": 168842, "text": "We consider |A ( nlc ) | ≤ 5 and assume that there is only one ground truth command c corresponding to an invocation nlc .", "label": [], "Comments": []}
{"id": 168843, "text": "Each participant was payed US $ 0.50 cents based on US $ 10 dollars/hour .", "label": [], "Comments": []}
{"id": 168844, "text": "Our contributions are three-fold : ( 1 ) We design an MSP model to tackle the data noise problem .", "label": [], "Comments": []}
{"id": 168845, "text": "The relational triples describe relations between entities , and the attributional triples describe attributes of entities .", "label": [], "Comments": []}
{"id": 168846, "text": "RELiC is also challenging because of the large number of retrieval candidates : for * War and Peace * , the longest literary work in the dataset , models must choose from one of ∼ 32K candidate passages .", "label": [], "Comments": []}
{"id": 168847, "text": "To obtain a query embedding c , we feed the query encoder the surrounding context of a masked-out example , as in Figure [ 2 . ]", "label": [], "Comments": []}
{"id": 168848, "text": "Peak performance on within-distribution goals for prior methods in the same environment is typically reached at 2500 samples per goal , or 45,000 total samples .", "label": [], "Comments": []}
{"id": 168849, "text": "We view our contribution as consistent with and complementary to this formal semantics program .", "label": [], "Comments": []}
{"id": 168850, "text": "[ Rese ] arch so far has focused on justification production for claim verification , as the latter is often the most scrutinized stage in fact-checkin [ g. ] [ Nev ] ertheless , explainability may also be desirable and necessary for the other stages in our framework .", "label": [], "Comments": []}
{"id": 168851, "text": "The number of trainable parameter for all models are ≈110M . 5.2 Baselines We investigate the impact of spurious identityrelated and non identity-related lexical artifacts on the robustness and fairness of hate speech detection by employing the following data-centric baselines .", "label": [], "Comments": []}
{"id": 168852, "text": "For example , the number of instances in clusters could be uneven ( see Appendix [ A.3 ] .", "label": [], "Comments": []}
{"id": 168853, "text": "Reward : 0 ===================== Act : put key in bucket Observation : [ Those things are n't here ! ]", "label": [], "Comments": []}
{"id": 168854, "text": "Our simple setting enables us to implement minimal experiments to evaluate how much important token extraction makes the contribution to generation . 4 Settings and Evaluation Metrics Data We conducted all experiments on four [ wel ] lknown datasets of utterance rewriting in Table 1 .", "label": [], "Comments": []}
{"id": 168855, "text": "SQuAD2.0 [ Kudo and ] [ Richardson , 2018 ] also contains unanswerable questions .", "label": [], "Comments": []}
{"id": 168856, "text": "Given an annotation task , this interface presents a potentially wrong sentence and a text input box .", "label": [], "Comments": []}
{"id": 168857, "text": "Their distribution is dissimilar to each other , and thus they are easy to be distinguished from the known intent classes .", "label": [], "Comments": []}
{"id": 168858, "text": "S3 is another interesting example .", "label": [], "Comments": []}
{"id": 168859, "text": "Persona editing BERTper and GPT2per will be used for token-level editing and phrase-level editing respectively .", "label": [], "Comments": []}
{"id": 168860, "text": "SMART-KPE+R2J is our complete model equipped with the state-of-the-art extracting method ( RoBERTa2Joint ) . with the basic version of SMART-KPE which also uses BERT .", "label": [], "Comments": []}
{"id": 168861, "text": "C Monolingual Corpus PLMs adaptation [ Table 10 ] provides the details about the Monolingual corpus used to adapt the pre-trained language models ( PLMs ) , their size and source of corpora .", "label": [], "Comments": []}
{"id": 168862, "text": "`` Ukraine can not ignore such illegal activities on the territory of its own state , '' the Ministry of Internal Affairs of Ukraine said in a statement Thursday .", "label": [], "Comments": []}
{"id": 168863, "text": "[ Vidgen et al. , 2021 ] introduce a dataset of Reddit comments with annotations in 6 categories taking into account context .", "label": [], "Comments": []}
{"id": 168864, "text": "The second stage is the formal annotation phase , and during this stage , 6 annotators were divided into three groups , each with 2 persons .", "label": [], "Comments": []}
{"id": 168865, "text": "By the end of a simulation run , 300 spans are sampled from six documents .", "label": [], "Comments": []}
{"id": 168866, "text": "However , sometimes ( such as the datapoints exhibited in the GovReport dataset ) a similar accuracy can be obtained by LED-base model with a larger input length ( 2048 ) as opposed to LED-large with a smaller input length ( 1024 ) .", "label": [], "Comments": []}
{"id": 168867, "text": "With the attribution algorithm , we can identify a coarse set of knowledge neurons whose attribution scores are greater than a threshold t. 3.3 Knowledge Neuron Refining In order to identify knowledge neurons more accurately , we further propose a refining strategy .", "label": [], "Comments": []}
{"id": 168868, "text": "Existing synthetic data generation approaches are not well-suited to factuality evaluation of current summarization models and human-annotated data can improve factuality models [ Goyal and Durrett , 2021 ] .", "label": [], "Comments": []}
{"id": 168869, "text": "[ Sa ] bo et al. , 2021 ) argue that the way FewRel 2.0 models NOTA cases is not realistic due to the way NOTA instances are sampled , develop a framework for creating more realistic benchmarks and propose building such a benchmark using the s [ entence-level data ] set TACRED ( Zhang et al. , 2017 ) .", "label": [], "Comments": []}
{"id": 168870, "text": "Run times and results on the development set are provided in the Appendix .", "label": [], "Comments": []}
{"id": 168871, "text": "We simply use the word-initial instead of the word-internal form , segmenting the derivative into the prefix followed by the base , i.e. , un , allowed in our example .", "label": [], "Comments": []}
{"id": 168872, "text": "P can start from one or multiple nodes but must end in one node that directly entails or contradicts Q .", "label": [], "Comments": []}
{"id": 168873, "text": "Reward : 0 ===================== Act : south Observation : Second Floor Reward : 0 ===================== Act : west Observation : Kitchen This is a filthy kitchen .", "label": [], "Comments": []}
{"id": 168874, "text": "A lack of checkmark indicates where model performance fails to differentiate the two categories , i.e. , models do not understand the differences between the prompt categories .", "label": [], "Comments": []}
{"id": 168875, "text": "We extract the test samples of all eight experiments , and since each test set is 2K , there are 16K samples in total .", "label": [], "Comments": []}
{"id": 168876, "text": "However , current autoregressive approaches suffer from high latency .", "label": [], "Comments": []}
{"id": 168877, "text": "2015 .", "label": [], "Comments": []}
{"id": 168878, "text": "With our LCS approach , we are able to replace the first sentence of block quote above with `` When near the buildings I met a white man , in such an unexpected elegance of get-up that in the first moment I took him for a sort of vision . ''", "label": [], "Comments": []}
{"id": 168879, "text": "Most prior work in MLC focuses on classification and is not directly applicable to MT .", "label": [], "Comments": []}
{"id": 168880, "text": "A.2 Glucose relations training details Because the Glucose dataset [ Mostafazadeh et al. , , 2020 ] was not split initially , we randomly split the dataset into train/dev/test splits based on a 80/10/10 ratio .", "label": [], "Comments": []}
{"id": 168881, "text": "We also study the importance of different words in a sentence via TF-IDF by forcing ( a ) Sorting self-constraints by frequency . ( b ) Sorting self-constraints by TF-IDF .", "label": [], "Comments": []}
{"id": 168882, "text": "On MSVD-QA , [ VLCN ] achieves the best results on the most challenging questions , i.e . questions whose answers are not amongst the 100 most frequent , and performs on par with [ VLCN ] − [ FLF ] on questions with the 100 most frequent answers .", "label": [], "Comments": []}
{"id": 168883, "text": "The model can simply copy the equilibrium token representation to the next step for speed .", "label": [], "Comments": []}
{"id": 168884, "text": "There exist two other ways to calculate and combine F1-scores for a multi-class problem .", "label": [], "Comments": []}
{"id": 168885, "text": "Future work can also explore if there can be an end-to-end model jointly inferring the human translatio [ n direction and sig ] [ naling the model to ] deal with the aligned and unaligned directions differently .", "label": [], "Comments": []}
{"id": 168886, "text": "Existing summarization datasets contain many documentsummary pairs .", "label": [], "Comments": []}
{"id": 168887, "text": "The first 10 rows are for settings described in Section [ 4.1 ; ] the next two rows are for settings used for ablations on the diversity of meta-training tasks ( Table [ 7 ] of Section [ 5.2 ] ; the last two rows are for settings used for ablations on using natural instructions ( Table [ 8 ] of Section [ 5.2 ] .", "label": [], "Comments": []}
{"id": 168888, "text": "From these results , we note the following phenomena : The above finding supports our belief that using our added regularization , we are able to find better representations , that can subsequently be used more efficiently when for kNN LM .", "label": [], "Comments": []}
{"id": 168889, "text": "For instance , in the NP * a former American diplomat * , * former * negates * diplomat * ( N ) , but the person is still American ; while in * a former beginner drummer * , it negates * beginner * ( M2 ) , but the person may still be See Appendix [ E.2 ] for model and hyperparameter details .", "label": [], "Comments": []}
{"id": 168890, "text": "However , in-context learning with an LM achieves poor performance when the target task is very different from language modeling in nature or the LM is not large enough .", "label": [], "Comments": []}
{"id": 168891, "text": "By contrast , for baseline-identified neurons , the suppressing operation has a negligible influence ( 1.47 % decrease on average ) on the correct probability .", "label": [], "Comments": []}
{"id": 168892, "text": "Three linear-complexity models are compared : RFA ( with 256/128 cross/causal memory sizes ; [ Peng et al. , , 2021 ] , T2R ( 32/4 ; [ Kasai et al. , , 2021b ] , and ABCMLP ( 32/8 ) .", "label": [], "Comments": []}
{"id": 168893, "text": "For QBCOREF , we set k to twenty and forty .", "label": [], "Comments": []}
{"id": 168894, "text": "Results reveal that none of MLMs ( i.e. , pragmatic masking and random masking models ) improves the spatial anisotropy .", "label": [], "Comments": []}
{"id": 168895, "text": "Because providing thousands of candidates to a human evaluator is infeasible , we instead measure human performance on a simplified proxy task : we provide our evaluators with four sentences on either side of a missing quotation from * Pride and Prejudice * [ 16 ] and ask them to select one of only three candidates to fill in the blank .", "label": [], "Comments": []}
{"id": 168896, "text": "However , in combination with pre-trained language models , the proposed classifier gives useful predictions of document coverage .", "label": [], "Comments": []}
{"id": 168897, "text": "For FLOPs computation of our KALA , we additionally include the FLOPs of the entity embedding layer , linear layers for h1 , h2 , h3 , h4 , and GNN layer .", "label": [], "Comments": []}
{"id": 168898, "text": "Indeed , the author of * Target * expresses his/her attitude without vilifying others .", "label": [], "Comments": []}
{"id": 168899, "text": "It is typically used in first-stage retrieval , where the goal is to retrieve all potentially relevant documents from the large corpus .", "label": [], "Comments": []}
{"id": 168900, "text": "The result in Figure 2 indicates the necessity to develop better representations of discourse relations instead of relying only on increasing the data size .", "label": [], "Comments": []}
{"id": 168901, "text": "Our framework features leverage the relation dependency information in both encoding and decoding stages .", "label": [], "Comments": []}
{"id": 168902, "text": "Intuitively , AMR provides important benefits : First , it encodes core concepts as it strives for a more logical and less syntactic representation , which has been shown to benefit text summarization [ Hardy and Vlachos , 2018 ] [ Dohare et al. , , 2018 ] [ Lee et al. , , 2021 ] .", "label": [], "Comments": []}
{"id": 168903, "text": "3.1 Encoders We group encoders into four categories , depending on their type of pre-training : PLM These models are pre-trained on a large general corpus in a self-supervised manner without any task-specific fine-tuning .", "label": [], "Comments": []}
{"id": 168904, "text": "While generally a good thing , it may dilute the pool of data-driven work in the biomedical field even more than it already is , making it hard for experts to spot the relevant work .", "label": [], "Comments": []}
{"id": 168905, "text": "Association for Computational Linguistics .", "label": [], "Comments": []}
{"id": 168906, "text": "We hope that our work achieves better results using more data ( * e.g . * raw speech , raw text , ASR , MT data ) in the future .", "label": [], "Comments": []}
{"id": 168907, "text": "ADEPT . [ Emami et al. , 2021 ] introduce a dataset of the Adjective-Dependent Plausibility Task ( ADEPT ) .", "label": [], "Comments": []}
{"id": 168908, "text": "More broadly , transgender users are `` excluded , harmed , and misrepresented in existing These authors made equal contributions . platforms , algorithms , and research methods '' related to network analysis [ Stewart and Spiro , 2021 ] .", "label": [], "Comments": []}
{"id": 168909, "text": "In this work , we propose an efficient method for retrieving prompts for in-context learning using annotated data and an LM .", "label": [], "Comments": []}
{"id": 168910, "text": "We also adopt cross-entropy loss in these two tasks .", "label": [], "Comments": []}
{"id": 168911, "text": "And it is also memory-friendly through the memory comparison between the baseline and the RK variants in both base and big configurations .", "label": [], "Comments": []}
{"id": 168912, "text": "For example , the following regular expression used in the slot-error script : prices ? ( ? : range ) ? ( ? : w+ ) 0,3 high matches * '' ( ... ) price range and high customer rating ( ... ) '' * , incorrectly classifying the presence of the extra slot * priceRange [ high ] * .", "label": [], "Comments": []}
{"id": 168913, "text": "3 Dataset of Derivatives We base our study on a new dataset of derivatives in context similar in form to the one released by [ Vylomova et al. , 2017 ] , i.e. , it is based on sentences with a derivative ( e.g. , this jacket is unwearable . )", "label": [], "Comments": []}
{"id": 168914, "text": "Track 3 : Personality Prediction ( PER ) aims to predict the Big Five personality traits , and Track 4 : Interpersonal Reactivity Index Prediction ( IRI ) consists of predicting each dimension of assessment of empathy : perspective taking , fantasy , empathic concern .", "label": [], "Comments": []}
{"id": 168915, "text": "Table 1 : Statistical results on experimental test datasets .", "label": [], "Comments": []}
{"id": 168916, "text": "ST is the best selftraining model .", "label": [], "Comments": []}
{"id": 168917, "text": "Debiasing Techniques .", "label": [], "Comments": []}
{"id": 168918, "text": "Notation We refer to the pre-finetuning runs with their initials—Only-QA means that only question answering tasks were used in pre-finetuning the model and QA+C means that all questionanswering and classification tasks were used and so on .", "label": [], "Comments": []}
{"id": 168919, "text": "To better understand self-attention , we introduce a theoretical framework , sketching [ Woodruff , 2014 ] , to help explain the key ideas in Informer and Linformer from the perspective of matrix approximation .", "label": [], "Comments": []}
{"id": 168920, "text": "Classification heads IE1 , IE and AE all operate on the embedding of a single token , and are identical to the classification head of RoBERTa ( AE operates on the concatenation of two token embeddings , and contains double the number of parameters as the RoBERTa ) .", "label": [], "Comments": []}
{"id": 168921, "text": "Since there are 18 different goals in the training set , the total number of samples is 18 × N .", "label": [], "Comments": []}
{"id": 168922, "text": "However , the above studies do not meet our problem condition since they require a considerable amount of style-labeled texts or need further training procedure and target only specific styles .", "label": [], "Comments": []}
{"id": 168923, "text": "SRL : [ The most important thing about Disney ] 1-ARG is [ that [ it ] 2-ARG is [ a global brand ] 2-ARG ] 1-ARG .", "label": [], "Comments": []}
{"id": 168924, "text": "We visualize the improvement of performance for five benchmark development sets , leveraging heatmaps in Figure [ 6 . ]", "label": [], "Comments": []}
{"id": 168925, "text": "For example , Spider-NQ outperforms DPR-NQ ( initialized from BERT ) by 4-12 points .", "label": [], "Comments": []}
{"id": 168926, "text": "Ensuring that these systems are trustworthy is key to deploy systems safely at a large scale in real-world application , especially in high-stake domains [ Sambasivan et al. , 2021 ] .", "label": [], "Comments": []}
{"id": 168927, "text": "Firstly , we discover that the success of magnitude pruning can be attributed to the preserved pre-training performance , which correlates with the downstream transferability .", "label": [], "Comments": []}
{"id": 168928, "text": "We broadly follow the DPR training set-up of [ Oguz et al . ]", "label": [], "Comments": []}
{"id": 168929, "text": "One critical hyper-parameter in our model is θ .", "label": [], "Comments": []}
{"id": 168930, "text": "However , UTSC-1 is the strongest as it relies on most toxic utterances followed by UAT-LM .", "label": [], "Comments": []}
{"id": 168931, "text": "All hyperparameters are the same for DEMIX and DENSE training .", "label": [], "Comments": []}
{"id": 168932, "text": "Note the conflicting valence of the two rewards when the agent is in the Kitchen .", "label": [], "Comments": []}
{"id": 168933, "text": "The dataset provides multisource information , including video , audio , text transcription and human-generated summary .", "label": [], "Comments": []}
{"id": 168934, "text": "Meanwhile , the EntLM objective serves as a LM-based objective to reduce the gap between pre-training and finetuning .", "label": [], "Comments": []}
{"id": 168935, "text": "For example , some generated queries are statements instead of questions .", "label": [], "Comments": []}
{"id": 168936, "text": "Including too much context can aggravate the imbalance problem .", "label": [], "Comments": []}
{"id": 168937, "text": "For each method , we test 6 times and average the results as final time .", "label": [], "Comments": []}
{"id": 168938, "text": "Figure 1 : DEPPROBE extracts tree structure using transformation B , labels using L and infers directionality using root , based on contextualized embeddings .", "label": [], "Comments": []}
{"id": 168939, "text": "Our proposed MACLR consistently outperforms all comparing baselines by a large margin on all four datasets .", "label": [], "Comments": []}
{"id": 168940, "text": "Person-level demographic information ( age , gender , ethnicity , income , education level ) is included for each post .", "label": [], "Comments": []}
{"id": 168941, "text": "which may suggest a high popularity of these products in the Dark Web .", "label": [], "Comments": []}
{"id": 168942, "text": "Does token-level routing via GSHARD also result in a modular model ?", "label": [], "Comments": []}
{"id": 168943, "text": "The numbers inside parenthesis show the number of the examples included in the easy and hard subsets .", "label": [], "Comments": []}
{"id": 168944, "text": "In Table [ 1 ] we provide an overview of the datasets . 5.2 Implementation For implementation , we developed an extensible temporal knowledge representation learning framework CHRONOKGE [ 2 ] .", "label": [], "Comments": []}
{"id": 168945, "text": "However , existing methods require at least five to tens of dimensions .", "label": [], "Comments": []}
{"id": 168946, "text": "Negative gaps indicate that the model is tested on data from * before * the slice on which it was trained .", "label": [], "Comments": []}
{"id": 168947, "text": "Our work encourages more attention to be put on the above findings .", "label": [], "Comments": []}
{"id": 168948, "text": "After the RE scoring in Joint-M , we regard each mention candidate as a graph node and their relation scores as weighted graph edges .", "label": [], "Comments": []}
{"id": 168949, "text": "Success for a single triplet is achieved if a metric computes the lowest distance score for the most-related document pair , namely D and D .", "label": [], "Comments": []}
{"id": 168950, "text": "These models are trained to generate a final output given retrieved passages that can be irrelevant to an input query , leading to learning spurious cues or memorization .", "label": [], "Comments": []}
{"id": 168951, "text": "Our results show that , while commonsense and narrative features can help improve performance overall , detecting event boundaries that are more subjective remains challenging for our model .", "label": [], "Comments": []}
{"id": 168952, "text": "For gradient-based model with hyperparameters , we tuned them on the first 100 sentences in the test set .", "label": [], "Comments": []}
{"id": 168953, "text": "Others argue that BT can still improve on both test sets ( Edunov et al. , 2020 ) .", "label": [], "Comments": []}
{"id": 168954, "text": "In contrast , UNIST effectively captures the meaning of event types and learns to classify event triggers by only fine-tuning on MAVEN , while still achieving promising performance without the need of any additional annotated resources .", "label": [], "Comments": []}
{"id": 168955, "text": "For the models with OAXE loss , we train with XE loss for 100K updates and then train with OAXE loss for 200K updates .", "label": [], "Comments": []}
{"id": 168956, "text": "An intuition behind this is that these verbs involve manner distinctions , and in particular , rotations of the object relative to itself .", "label": [], "Comments": []}
{"id": 168957, "text": "EISL is designed to be robust to various noises and edits in the target sequences .", "label": [], "Comments": []}
{"id": 168958, "text": "Inference In an MDS , each child node only has one parent node .", "label": [], "Comments": []}
{"id": 168959, "text": "Maximum sequence length was 512 tokens , batch-size was 8 and epoch number was 2 .", "label": [], "Comments": []}
{"id": 168960, "text": "We follow the strategy of [ Devlin et al. , , 2019 ] [ Lan et al. , , 2020 ] , and equally weight the the two pre-training loss objectives : MLM and MSPP .", "label": [], "Comments": []}
{"id": 168961, "text": "Therefore , directly using the context-response pairs in HLA-Chat as in Gold Match could adversely affect the quality of subsequent responses in style strength and coherency .", "label": [], "Comments": []}
{"id": 168962, "text": "There is a long tail because many subreddits have low participation .", "label": [], "Comments": []}
{"id": 168963, "text": "They are selected from { 8 , 16 , 32 , 64 } , with cross attention 's equal to or larger than causal 's : cross attention is more important than causal attention in machine translation [ Michel et al. , , 2019 ] .", "label": [], "Comments": []}
{"id": 168964, "text": "C Why is scrubbing not as effective as subsampli [ ng ] ?", "label": [], "Comments": []}
{"id": 168965, "text": "We also thank Baosong Yang and Dayiheng Liu for their instructive suggestions and invaluable help .", "label": [], "Comments": []}
{"id": 168966, "text": "For example , a user can become emotionally attached to a bot , even codependent on it , which can divert attention away from real-world relationships and cause distress if the chatbot fails .", "label": [], "Comments": []}
{"id": 168967, "text": "We define the loss function to be : where C is the number of classes .", "label": [], "Comments": []}
{"id": 168968, "text": "We attribute this to a lack of introductory context in comparison to PER mentions ( e.g. , a mention of `` China '' in an article would typically not be followed by `` a state in East Asia '' ) .", "label": [], "Comments": []}
{"id": 168969, "text": "* , average , extrema , and greedy ) [ Chan et al. , , 2019 ] are introduced to measure the semantic similarity between the generated response and the groundtruth one .", "label": [], "Comments": []}
{"id": 168970, "text": "Although such language models are capable of generating human-like responses , they often come with their own set of predicaments .", "label": [], "Comments": []}
{"id": 168971, "text": "Thus h can be computed as a single inner product , making it a drop-in replacement for standard MIPS dense retrievers at test time .", "label": [], "Comments": []}
{"id": 168972, "text": "However , we can see that the MPNet proved to be the best ( for the linear scenario ) and not significantly worse than the best - XLR-M model - in the other two scenarios .", "label": [], "Comments": []}
{"id": 168973, "text": "Descriptive Statistics .", "label": [], "Comments": []}
{"id": 168974, "text": "3.1 Data Feature Groups Once finalized , our NLP preprint popularity dataset [ 2 ] ( henceforth NLPOP ) consists of four distinct input feature groups : ( 1 ) the preprint title and abstract text , encoded separately ( PAPER ) , ( 2 ) the Twitter thread text ( THREAD ) , ( 3 ) Twitter user biographical data ( BIO ) and ( 4 ) numeric metadata features ( NUM ) of the user profile and the Twitter thread .", "label": [], "Comments": []}
{"id": 168975, "text": "Specifically , BLEU is defined as a weighted geometric mean of n-gram precisions : where BP is a brevity penalty depending on the lengths of y and y ∗ ; N is the maximum n-gram order ( typically N = 4 ) ; { wn } are the weights which usually take 1/N ; prec is the n-gram precision , gram ( y ) is the set of unique n-gram subsequences of y ; and C ( s , y ) is the number of times a gram s occurs in y as defined in Eq .", "label": [], "Comments": []}
{"id": 168976, "text": "However , there is bias if the model consistently gives higher probability to one type of sentence over the other .", "label": [], "Comments": []}
{"id": 168977, "text": "In the future , we will investigate the feasibility of incorporating classical MDS guidance to abstractive models with large-scale pre-training [ Gu et al. , , 2020 ] and more challenging settings where each document set may contain hundreds or even thousands of documents .", "label": [], "Comments": []}
{"id": 168978, "text": "3.3 DEMIX Architecture Our design results in one expert in a DEMIX layer per domain ( i.e. , eight experts for eight training domains in our multi-domain corpus ) .", "label": [], "Comments": []}
{"id": 168979, "text": "The big gaps ( more than 40 % in MAPO and LPA ) between PT.Match and Ex.Acc accuracy suggest that with only lexical features , there are still many spurious programs being explored .", "label": [], "Comments": []}
{"id": 168980, "text": "piece of the .", "label": [], "Comments": []}
{"id": 168981, "text": "The output vector y is computed via where W and W are learnable weights and h = { h i t } L =1 are the hidden states of the [ LSTM ] controller .", "label": [], "Comments": []}
{"id": 168982, "text": "• For non-autoregressive models , we can de- Our code , model , and output are released at : [ https : ] ( https : //github.com/MANGA-UOFA/NAUS ) [ //github.com/MANGA-UOFA/NAUS ] ( https : //github.com/MANGA-UOFA/NAUS ) sign a length-control algorithm based on dynamic programming to satisfy the constraint of output lengths , which is typical in summarization applications but can not be easily achieved with autoregressive models .", "label": [], "Comments": []}
{"id": 168983, "text": "[ 1 ] The parser was trained on data annotated with the ClearNLP dependency schema that is similar to Universal Dependencies [ Nivre et al. , , 2016 ] .", "label": [], "Comments": []}
{"id": 168984, "text": "More details about the FEVEROUS dataset are in Appendix .", "label": [], "Comments": []}
{"id": 168985, "text": "The blurred BoW can be computed in O ( wn ) time , where n denotes the number of words in a document and w is the width of the filter .", "label": [], "Comments": []}
{"id": 168986, "text": "Blather enters , looking confused , and begins ranting madly at you . `` I said to return to your post , Ensign Seventh Class ! '' bellows Blather , turning a deepening shade of crimson .", "label": [], "Comments": []}
{"id": 168987, "text": "To discuss the different LMs results , we will first provide an overview of the models we tested for French .", "label": [], "Comments": []}
{"id": 168988, "text": "An overview of our architecture is shown in Figure [ 1 . ]", "label": [], "Comments": []}
{"id": 168989, "text": "After the final FL round , the server parameters are pushed back to clients so they can also benefit from the result of aggregation .", "label": [], "Comments": []}
{"id": 168990, "text": "Association for Computational Linguistics .", "label": [], "Comments": []}
{"id": 168991, "text": "Thus , both schemes approximately weigh all samples equally .", "label": [], "Comments": []}
{"id": 168992, "text": "Table 3 : P @ 1 of representative methods on various entity types in the AIDA-CoNLL dataset .", "label": [], "Comments": []}
{"id": 168993, "text": "While the achieved recall ( 52.4 % and 35.4 % respectively ) are still a distance from the maximum possible recall , the achieved recall is much higher than using the Generator alone .", "label": [], "Comments": []}
{"id": 168994, "text": "These studies reported the effectiveness of selftraining but self-training is hard to generate diverse pseudo data [ Gu et al. , , 2018 ] .", "label": [], "Comments": []}
{"id": 168995, "text": "The best performing model ( DPR Retriever + FiD ) is 6.9 points EM and 14.2 points F1 below human performance , suggesting that advances in modeling are needed .", "label": [], "Comments": []}
{"id": 168996, "text": "Generalize to complex language .", "label": [], "Comments": []}
{"id": 168997, "text": "As shown in previous work [ Thakur et al. , , 2021b ] , the performance of dense retrievers severely degrades under a domain shift .", "label": [], "Comments": []}
{"id": 168998, "text": "Our framework relies on a discretized embedding space created via vector quantization that is shared across different modalities .", "label": [], "Comments": []}
{"id": 168999, "text": "Inner loop/Test learning rates are used with SGD , outer loop LRs are used with the Adam optimizer .", "label": [], "Comments": []}
{"id": 169000, "text": "Note that the method does require the existence of an unlabeled corpus , in the order of several thousand examples .", "label": [], "Comments": []}
